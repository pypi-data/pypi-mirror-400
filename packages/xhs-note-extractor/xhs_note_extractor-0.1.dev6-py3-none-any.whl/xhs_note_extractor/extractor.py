"""
å°çº¢ä¹¦ç¬”è®°æå–å™¨æ¨¡å—

è¯¥æ¨¡å—æä¾›äº†ä»å°çº¢ä¹¦URLä¸­æå–ç¬”è®°ä¿¡æ¯çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- URLè§£æå’Œè½¬æ¢
- è®¾å¤‡è¿æ¥å’Œé¡µé¢è·³è½¬
- ç¬”è®°å†…å®¹æå–ï¼ˆæ­£æ–‡ã€å›¾ç‰‡ã€ç‚¹èµæ•°ç­‰ï¼‰
- ç»“æ„åŒ–æ•°æ®è¿”å›

ä½œè€…: JoyCode Agent
ç‰ˆæœ¬: 1.0.0
"""

import uiautomator2 as u2
import time
import re
import requests
import logging
from typing import Dict, List, Optional, Union
from urllib.parse import urlparse, parse_qs
import xml.etree.ElementTree as ET

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class XHSNoteExtractor:
    """
    å°çº¢ä¹¦ç¬”è®°æå–å™¨ç±»
    
    æä¾›äº†ä»å°çº¢ä¹¦URLä¸­æå–ç¬”è®°ä¿¡æ¯çš„å®Œæ•´åŠŸèƒ½ï¼Œ
    åŒ…æ‹¬URLè§£æã€è®¾å¤‡è¿æ¥ã€é¡µé¢è·³è½¬å’Œç¬”è®°å†…å®¹æå–ã€‚
    """
    
    def __init__(self, device_serial: Optional[str] = None, enable_time_logging: bool = True):
        """
        åˆå§‹åŒ–å°çº¢ä¹¦ç¬”è®°æå–å™¨
        
        Args:
            device_serial (str, optional): è®¾å¤‡åºåˆ—å·ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è¿æ¥å¯ç”¨è®¾å¤‡
            enable_time_logging (bool, optional): æ˜¯å¦å¯ç”¨è€—æ—¶æ‰“å°ï¼Œé»˜è®¤ä¸ºTrue
            
        Raises:
            RuntimeError: å½“æ²¡æœ‰å¯ç”¨è®¾å¤‡æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        self.device = None
        self.device_serial = device_serial
        self.enable_time_logging = enable_time_logging
        if not self.connect_device():
            raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„Androidè®¾å¤‡ï¼Œè¯·è¿æ¥è®¾å¤‡åå†è¯•")
    
    def _time_method(self, method_name, start_time):
        """
        è®°å½•æ–¹æ³•æ‰§è¡Œæ—¶é—´
        
        Args:
            method_name (str): æ–¹æ³•åç§°
            start_time (float): å¼€å§‹æ—¶é—´
        """
        if self.enable_time_logging:
            elapsed_time = time.time() - start_time
            logger.info(f"[{method_name}] è€—æ—¶: {elapsed_time:.3f}ç§’")
    
    def connect_device(self) -> bool:
        """
        è¿æ¥è®¾å¤‡
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸè¿æ¥è®¾å¤‡
        """
        start_time = time.time()
        try:
            if self.device_serial:
                self.device = u2.connect(self.device_serial)
            else:
                self.device = u2.connect()
            logger.info(f"âœ“ å·²è¿æ¥è®¾å¤‡: {self.device.serial}")
            self._time_method("connect_device", start_time)
            return True
        except Exception as e:
            logger.error(f"âœ— è®¾å¤‡è¿æ¥å¤±è´¥: {e}")
            self._time_method("connect_device", start_time)
            return False
    def is_device_connected(self) -> bool:
        """
        æ£€æŸ¥è®¾å¤‡æ˜¯å¦ä»ç„¶è¿æ¥
        
        Returns:
            bool: è®¾å¤‡æ˜¯å¦è¿æ¥
        """
        if not self.device:
            return False
        try:
            # é€šè¿‡è·å–è®¾å¤‡ä¿¡æ¯æ¥éªŒè¯è¿æ¥
            self.device.info
            return True
        except:
            return False

    @staticmethod
    def parse_xhs_url(url: str) -> Dict[str, str]:
        """
        è§£æå°çº¢ä¹¦URLï¼Œæå–note_idå’Œxsec_token
        
        Args:
            url (str): å°çº¢ä¹¦URLï¼Œæ”¯æŒæ ‡å‡†æ ¼å¼æˆ–xhsdiscoveråè®®æ ¼å¼
            
        Returns:
            Dict[str, str]: åŒ…å«note_idå’Œxsec_tokençš„å­—å…¸
            
        Raises:
            ValueError: å½“URLæ ¼å¼ä¸æ­£ç¡®æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        start_time = time.time()
        # å¤„ç†xhsdiscoveråè®®æ ¼å¼
        if url.startswith("xhsdiscover://"):
            # æå–note_id
            note_id_match = re.search(r'item/([^?]+)', url)
            if not note_id_match:
                raise ValueError("æ— æ³•ä»xhsdiscover URLä¸­æå–note_id")
            
            note_id = note_id_match.group(1)
            
            # å°è¯•ä»open_urlå‚æ•°ä¸­æå–åŸå§‹URL
            open_url_match = re.search(r'open_url=([^&]+)', url)
            xsec_token = ""
            if open_url_match:
                open_url = open_url_match.group(1)
                # è§£ç URL
                import urllib.parse
                decoded_url = urllib.parse.unquote(open_url)
                # ä»åŸå§‹URLä¸­æå–xsec_token
                token_match = re.search(r'xsec_token=([^&]+)', decoded_url)
                if token_match:
                    xsec_token = token_match.group(1)
            
            return {
                "note_id": note_id,
                "xsec_token": xsec_token,
                "original_url": url
            }
        
        # å¤„ç†æ ‡å‡†URLæ ¼å¼
        elif "xiaohongshu.com" in url:
            parsed_url = urlparse(url)
            path_parts = parsed_url.path.strip('/').split('/')
            
            # æŸ¥æ‰¾exploreéƒ¨åˆ†å’Œnote_id
            if 'explore' in path_parts:
                explore_index = path_parts.index('explore')
                if explore_index + 1 < len(path_parts):
                    note_id = path_parts[explore_index + 1]
                else:
                    raise ValueError("URLä¸­ç¼ºå°‘note_id")
            # å…¼å®¹ /discovery/item/ æ ¼å¼
            elif 'discovery' in path_parts and 'item' in path_parts:
                item_index = path_parts.index('item')
                if item_index + 1 < len(path_parts):
                    note_id = path_parts[item_index + 1]
                else:
                    raise ValueError("URLä¸­ç¼ºå°‘note_id")
            else:
                raise ValueError("URLæ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘/explore/æˆ–/discovery/item/è·¯å¾„")
            
            # æå–æŸ¥è¯¢å‚æ•°ä¸­çš„xsec_token
            query_params = parse_qs(parsed_url.query)
            xsec_token = query_params.get('xsec_token', [''])[0]
            
            elapsed_time = time.time() - start_time
            logger.info(f"[parse_xhs_url] è€—æ—¶: {elapsed_time:.3f}ç§’")
            return {
                "note_id": note_id,
                "xsec_token": xsec_token,
                "original_url": url
            }
        
        else:
            elapsed_time = time.time() - start_time
            logger.info(f"[parse_xhs_url] è€—æ—¶: {elapsed_time:.3f}ç§’")
            raise ValueError("ä¸æ”¯æŒçš„URLæ ¼å¼")
    
    @staticmethod
    def validate_url(url: str) -> bool:
        """
        éªŒè¯URLæ˜¯å¦æ˜¯æœ‰æ•ˆçš„å°çº¢ä¹¦URL
        
        Args:
            url (str): è¦éªŒè¯çš„URL
            
        Returns:
            bool: URLæ˜¯å¦æœ‰æ•ˆ
        """
        try:
            XHSNoteExtractor.parse_xhs_url(url)
            return True
        except ValueError:
            return False
    
    @staticmethod
    def convert_to_xhsdiscover_format(note_id: str, xsec_token: str = "") -> str:
        """
        å°†note_idå’Œxsec_tokenè½¬æ¢ä¸ºxhsdiscoveråè®®æ ¼å¼
        
        Args:
            note_id (str): ç¬”è®°ID
            xsec_token (str): xsec_tokenå‚æ•°
            
        Returns:
            str: xhsdiscoveråè®®æ ¼å¼çš„URL
        """
        start_time = time.time()
        result = ""
        if xsec_token:
            original_url = f"http://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}&xsec_source=pc_feed"
            encoded_url = requests.utils.quote(original_url)
            result = f"xhsdiscover://item/{note_id}?open_url={encoded_url}"
        else:
            result = f"xhsdiscover://item/{note_id}"
        
        elapsed_time = time.time() - start_time
        logger.info(f"[convert_to_xhsdiscover_format] è€—æ—¶: {elapsed_time:.3f}ç§’")
        return result
    
    def extract_note_data(self, url: Optional[str] = None, note_id: Optional[str] = None, 
                         xsec_token: Optional[str] = None) -> Dict[str, Union[str, List[str]]]:
        """
        ä»å°çº¢ä¹¦ç¬”è®°ä¸­æå–æ•°æ®
        
        Args:
            url (str, optional): å°çº¢ä¹¦URLï¼Œå¦‚æœæä¾›åˆ™ä¼šè§£æå…¶ä¸­çš„note_idå’Œxsec_token
            note_id (str, optional): ç¬”è®°IDï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨
            xsec_token (str, optional): xsec_tokenå‚æ•°
            
        Returns:
            Dict[str, Union[str, List[str]]]: åŒ…å«ç¬”è®°æ•°æ®çš„å­—å…¸ï¼Œæ ¼å¼ä¸xhs_utils.get_detail_data()ä¸€è‡´
            
        Raises:
            RuntimeError: å½“è®¾å¤‡æœªè¿æ¥æ—¶æŠ›å‡ºå¼‚å¸¸
            Exception: å½“æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        start_time = time.time()
        # å¦‚æœæä¾›äº†URLï¼Œåˆ™å…ˆè§£æå®ƒï¼ˆéªŒè¯URLæœ‰æ•ˆæ€§ï¼‰
        if url:
            parsed_data = self.parse_xhs_url(url)
            note_id = parsed_data["note_id"]
            xsec_token = parsed_data["xsec_token"]
            
        # æ£€æŸ¥è®¾å¤‡æ˜¯å¦è¿æ¥
        if self.device is None:
            self._time_method("extract_note_data", start_time)
            raise RuntimeError("è®¾å¤‡æœªè¿æ¥ï¼Œè¯·å…ˆè¿æ¥è®¾å¤‡")
        
        # æ„å»ºè·³è½¬URL
        jump_url = self.convert_to_xhsdiscover_format(note_id, xsec_token)
        
        logger.info(f"æ­£åœ¨å°è¯•è·³è½¬è‡³ç¬”è®°: {note_id}")
        
        try:
            # å‘èµ·è·³è½¬
            self.device.open_url(jump_url)
            logger.info("âœ“ å·²å‘é€è·³è½¬æŒ‡ä»¤ï¼Œç­‰å¾…é¡µé¢åŠ è½½...")
            
            # ä½¿ç”¨ç°æœ‰çš„xhs_utilsåŠŸèƒ½æå–æ•°æ®
            data = self._get_detail_data()
            
            logger.info(f"âœ“ æˆåŠŸæå–ç¬”è®°æ•°æ®ï¼Œç‚¹èµæ•°: {data['likes']}, å›¾ç‰‡æ•°: {len(data['image_urls'])}")
            
            self._time_method("extract_note_data", start_time)
            return data
            
        except Exception as e:
            logger.error(f"âœ— æå–ç¬”è®°æ•°æ®å¤±è´¥: {e}")
            self._time_method("extract_note_data", start_time)
            raise
    
    def _get_detail_data(self) -> Dict[str, Union[str, List[str]]]:
        """
        ä»å½“å‰å·²ç»æ‰“å¼€çš„å°çº¢ä¹¦è¯¦æƒ…é¡µæå–å®Œæ•´æ­£æ–‡ã€å›¾ç‰‡å’Œç‚¹èµæ•°ã€‚
        ä¼˜åŒ–ç‰ˆæœ¬: ä½¿ç”¨ dump_hierarchy æ›¿ä»£éå†ï¼Œå¤§å¹…æå‡é€Ÿåº¦ã€‚
        
        Returns:
            Dict[str, Union[str, List[str]]]: åŒ…å«ç¬”è®°æ•°æ®çš„å­—å…¸
        """
        start_time = time.time()
        logger.info("ğŸ” è¿›å…¥æ·±åº¦æå–æ¨¡å¼ (XMLä¼˜åŒ–ç‰ˆ)...")
        
        # 1. éªŒè¯æ˜¯å¦è¿›å…¥è¯¦æƒ…é¡µ & å±•å¼€å…¨æ–‡
        detail_loaded = False
        detail_keywords = ["è¯´ç‚¹ä»€ä¹ˆ", "å†™è¯„è®º", "å†™ç‚¹ä»€ä¹ˆ", "æ”¶è—", "ç‚¹èµ", "è¯„è®º", "åˆ†äº«", "å‘å¼¹å¹•"]
        
        # å°è¯•ç‚¹å‡»å±•å¼€ (é¢„å…ˆåŠ¨ä½œ)
        try:
            # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦æœ‰å±•å¼€æŒ‰é’®
            for btn_text in ["å±•å¼€", "æŸ¥çœ‹å…¨éƒ¨", "å…¨æ–‡"]:
                if self.device(text=btn_text).exists:
                    self.device(text=btn_text).click()
                    break
        except: pass

        # ç­‰å¾…åŠ è½½å®Œæ•´
        for i in range(5):
            if any(self.device(textContains=kw).exists or self.device(descriptionContains=kw).exists for kw in detail_keywords):
                detail_loaded = True
                break
            if i == 2:
                # å¯èƒ½æ˜¯è§†é¢‘ï¼Œç‚¹å‡»å±å¹•ä¸­å¿ƒå°è¯•æ¿€æ´» UI
                self.device.click(540, 900)
            time.sleep(0.5)
        
        if not detail_loaded:
            logger.warning("âš  è­¦å‘Šï¼šè¯¦æƒ…é¡µç‰¹å¾æœªå‘ç°ï¼Œæå–å¯èƒ½ä¸å®Œæ•´")

        # 2. è·å– UIå±‚çº§ (æ ¸å¿ƒä¼˜åŒ–)
        xml_dump_start = time.time()
        xml_content = self.device.dump_hierarchy()
        self._time_method("dump_hierarchy", xml_dump_start)
        
        # 3. è§£æ XML
        root = ET.fromstring(xml_content)
        
        content = ""
        likes = "0"
        collects = "0"
        comments = "0"
        author_name = "Unknown"
        image_urls = []
        
        # æ”¶é›†æ‰€æœ‰ TextView èŠ‚ç‚¹ä¿¡æ¯
        text_nodes = []
        
        def parse_nodes(node):
            if node.attrib.get('class') == 'android.widget.TextView':
                text = node.attrib.get('text', '')
                bounds_str = node.attrib.get('bounds', '[0,0][0,0]')
                # è§£æ bounds: [x1,y1][x2,y2]
                try:
                    coords = bounds_str.replace('][', ',').replace('[', '').replace(']', '').split(',')
                    x1, y1, x2, y2 = map(int, coords)
                    if text:
                        text_nodes.append({
                            'text': text,
                            'l': x1, 't': y1, 'r': x2, 'b': y2,
                            'cx': (x1 + x2) / 2, 'cy': (y1 + y2) / 2
                        })
                except: pass
            for child in node:
                parse_nodes(child)
                
        parse_nodes(root)
        
        # 4. åˆ†æèŠ‚ç‚¹æ•°æ®
        
        # A. ä½œè€…æå– (å¯»æ‰¾ "å…³æ³¨" é™„è¿‘çš„æ–‡æœ¬)
        # ç­–ç•¥: æ‰¾åˆ°åŒ…å« "å…³æ³¨" çš„èŠ‚ç‚¹ï¼Œå–å…¶å·¦ä¾§æœ€è¿‘çš„èŠ‚ç‚¹
        follow_node = None
        for n in text_nodes:
            if n['text'] in ["å…³æ³¨", "å·²å…³æ³¨"]:
                follow_node = n
                break
        
        if follow_node:
            best_dist = 9999
            for n in text_nodes:
                if n == follow_node: continue
                if n['text'] in ["å…³æ³¨", "å·²å…³æ³¨"] or len(n['text']) > 30: continue
                
                # å‚ç›´æ¥è¿‘
                if abs(n['cy'] - follow_node['cy']) < 100:
                    # åœ¨å·¦ä¾§
                    if n['r'] <= follow_node['l'] + 50:
                        dist = follow_node['l'] - n['r']
                        if dist < best_dist:
                            best_dist = dist
                            author_name = n['text']
            logger.info(f"âœ“ è¯†åˆ«åˆ°ä½œè€…: {author_name}")

        # B. äº’åŠ¨æ•°æ®æå– (åº•éƒ¨åŒºåŸŸ)
        bottom_nodes = [n for n in text_nodes if n['t'] > 2000] # å‡è®¾å±å¹•é«˜åº¦è¶³å¤Ÿ
        bottom_nodes.sort(key=lambda x: x['l']) # ä»å·¦åˆ°å³
        
        for n in bottom_nodes:
            txt = n['text']
            num_txt = ''.join(c for c in txt if c.isdigit() or c in ['.', 'w', 'W'])
            if not num_txt: continue
            
            cx = n['cx']
            if 500 < cx < 750:
                likes = num_txt
            elif 750 < cx < 900:
                collects = num_txt
            elif cx >= 900:
                comments = num_txt

        # C. æ­£æ–‡æå–
        # è¿‡æ»¤æ‰éæ­£æ–‡å†…å®¹
        content_lines = []
        exclude_keywords = ['æ”¶è—', 'ç‚¹èµ', 'è¯„è®º', 'åˆ†äº«', 'å‘å¸ƒäº', 'è¯´ç‚¹ä»€ä¹ˆ', 'æ¡è¯„è®º', 'å…³æ³¨', author_name]
        
        # æŒ‰ç…§å‚ç›´ä½ç½®æ’åº
        content_nodes = [n for n in text_nodes if 200 < n['t'] < 2000]
        content_nodes.sort(key=lambda x: x['t'])
        
        for n in content_nodes:
            t = n['text']
            if len(t) < 2: continue
            if any(k in t for k in exclude_keywords): continue
            
            # ç®€å•çš„å»é‡ç­–ç•¥
            if content_lines and t in content_lines[-1]: continue
            content_lines.append(t)
            
        content = "\n".join(content_lines)

        # 5. å›¾ç‰‡æå– (ä¿æŒåŸæœ‰é€»è¾‘ä½†ä¼˜åŒ–ç­‰å¾…)
        try:
             # è¿™é‡Œè¿˜æ˜¯éœ€è¦äº¤äº’ï¼Œæ— æ³•çº¯é XML
            share_btn = self.device(description="åˆ†äº«")
            if share_btn.exists:
                share_btn.click()
                # æ˜¾å¼ç­‰å¾… "å¤åˆ¶é“¾æ¥"
                copy_link = self.device(text="å¤åˆ¶é“¾æ¥")
                if copy_link.wait(timeout=2.0):
                    copy_link.click()
                    # ç­‰å¾…å‰ªè´´æ¿æ›´æ–°? ç¨å¾®ç¼“ä¸€ä¸‹
                    time.sleep(0.5)
                    share_link = self.device.clipboard
                    if "http" in str(share_link):
                        image_urls = self._fetch_web_images(share_link)
                else:
                    logger.warning("æœªæ‰¾åˆ°å¤åˆ¶é“¾æ¥æŒ‰é’®")
                    self.device.press("back")
        except Exception as e:
            logger.warning(f"âš  å›¾ç‰‡æå–å¼‚å¸¸: {e}")

        self._time_method("_get_detail_data", start_time)
        return {
            "content": content,
            "image_urls": image_urls,
            "likes": likes,
            "collects": collects,
            "comments": comments,
            "author_name": author_name
        }
    
    def _fetch_web_images(self, url: str) -> List[str]:
        """
        ä»åˆ†äº«é“¾æ¥ä¸­è§£æå›¾ç‰‡åœ°å€
        
        Args:
            url (str): åˆ†äº«é“¾æ¥URL
            
        Returns:
            List[str]: å›¾ç‰‡URLåˆ—è¡¨
        """
        start_time = time.time()
        try:
            headers = {"User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_8 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1"}
            res = requests.get(url, headers=headers, timeout=10)
            html = res.text
            img_patterns = [
                r'property="og:image" content="(https://[^"]+)"',
                r'"url":"(https://sns-img-[^"]+)"',
                r'"url":"(https://sns-img-qc\.xhscdn\.com/[^"]+)"'
            ]
            found = []
            for pattern in img_patterns:
                matches = re.findall(pattern, html)
                for m in matches:
                    clean_url = m.replace('\\u002F', '/')
                    if clean_url not in found: found.append(clean_url)
            self._time_method("_fetch_web_images", start_time)
            return found
        except:
            self._time_method("_fetch_web_images", start_time)
            return []
    
    def save_note_data(self, data: Dict[str, Union[str, List[str]]], 
                      filename: str = "last_extracted_note.txt", 
                      note_url: str = "") -> None:
        """
        ä¿å­˜ç¬”è®°æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            data (Dict[str, Union[str, List[str]]]): ç¬”è®°æ•°æ®
            filename (str): ä¿å­˜æ–‡ä»¶å
            note_url (str): ç¬”è®°URL
        """
        start_time = time.time()
        try:
            with open(filename, "w", encoding="utf-8") as f:
                f.write("=" * 50 + "\n")
                f.write("ã€å°çº¢ä¹¦ç¬”è®°æå–ç»“æœã€‘\n")
                f.write("=" * 50 + "\n")
                if note_url:
                    f.write(f"ç¬”è®°URL: {note_url}\n")
                    f.write("=" * 50 + "\n")
                f.write(f"ä½œè€…: {data.get('author_name', 'Unknown')}\n")
                f.write(f"ç‚¹èµæ•°: {data.get('likes', '0')}\n")
                f.write(f"æ”¶è—æ•°: {data.get('collects', '0')}\n")
                f.write(f"è¯„è®ºæ•°: {data.get('comments', '0')}\n")
                f.write(f"å›¾ç‰‡æ•°: {len(data.get('image_urls', []))}\n")
                f.write("=" * 50 + "\n")
                f.write("ã€æ­£æ–‡å†…å®¹ã€‘\n")
                f.write(data['content'])
                f.write("\n" + "=" * 50 + "\n")
                if data['image_urls']:
                    f.write("ã€å›¾ç‰‡URLã€‘\n")
                    for i, url in enumerate(data['image_urls'], 1):
                        f.write(f"{i}. {url}\n")
                    f.write("=" * 50 + "\n")
            
            logger.info(f"âœ“ ç¬”è®°æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            self._time_method("save_note_data", start_time)
        except Exception as e:
            logger.error(f"âœ— ä¿å­˜ç¬”è®°æ•°æ®å¤±è´¥: {e}")
            self._time_method("save_note_data", start_time)
            raise


def extract_note_from_url(url: str, device_serial: Optional[str] = None, enable_time_logging: bool = True) -> Dict[str, Union[str, List[str]]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šç›´æ¥ä»URLæå–ç¬”è®°æ•°æ®
    
    Args:
        url (str): å°çº¢ä¹¦ç¬”è®°URL
        device_serial (str, optional): è®¾å¤‡åºåˆ—å·
        enable_time_logging (bool, optional): æ˜¯å¦å¯ç”¨è€—æ—¶æ‰“å°ï¼Œé»˜è®¤ä¸ºTrue
        
    Returns:
        Dict[str, Union[str, List[str]]]: ç¬”è®°æ•°æ®
    """
    start_time = time.time()
    logger.info(f"[extract_note_from_url] å¼€å§‹å¤„ç†URL: {url}")
    extractor = XHSNoteExtractor(device_serial=device_serial, enable_time_logging=enable_time_logging)
    result = extractor.extract_note_data(url=url)
    elapsed_time = time.time() - start_time
    logger.info(f"[extract_note_from_url] æ€»è€—æ—¶: {elapsed_time:.3f}ç§’")
    return result


def convert_url_format(url: str) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šè½¬æ¢URLæ ¼å¼
    
    Args:
        url (str): è¾“å…¥URL
        
    Returns:
        str: è½¬æ¢åçš„xhsdiscoveråè®®æ ¼å¼URL
    """
    start_time = time.time()
    logger.info(f"[convert_url_format] å¼€å§‹è½¬æ¢URL: {url}")
    parsed_data = XHSNoteExtractor.parse_xhs_url(url)
    result = XHSNoteExtractor.convert_to_xhsdiscover_format(
        parsed_data["note_id"], 
        parsed_data["xsec_token"]
    )
    elapsed_time = time.time() - start_time
    logger.info(f"[convert_url_format] è€—æ—¶: {elapsed_time:.3f}ç§’ï¼Œç»“æœ: {result}")
    return result