{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac9c52d",
   "metadata": {},
   "source": [
    "# SVD Time Series Imputation - Augmented Demo\n",
    "\n",
    "This notebook demonstrates the key functionality of the SVD time series imputer package, including:\n",
    "\n",
    "1. Generating synthetic time series with correlated and uncorrelated components\n",
    "2. Basic imputation with automatic rank estimation\n",
    "3. Data augmentation techniques for improved imputation\n",
    "\n",
    "The package uses Singular Value Decomposition (SVD) to identify low-dimensional patterns in multivariate time series and impute missing values based on these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "repo_root = os.path.abspath(os.path.join('..'))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "from svd_imputer import Imputer\n",
    "from svd_imputer.preprocessing import (\n",
    "    create_derivative_augmented_matrix,\n",
    "    create_symmetric_augmented_matrix\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb2ef1",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Time Series Data\n",
    "\n",
    "We create a synthetic dataset with:\n",
    "- 3 correlated time series (Series A, B, C) with seasonal patterns and trends\n",
    "- 1 uncorrelated time series (Series D) with random walk behavior\n",
    "- Missing values randomly distributed across all series except one reference series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_timeseries(n_periods=200, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data with correlated and uncorrelated components.\n",
    "    \n",
    "    Returns:\n",
    "        df_with_missing: DataFrame with missing values\n",
    "        df_complete: Complete DataFrame (ground truth)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dates = pd.date_range('2020-01-01', periods=n_periods, freq='D')\n",
    "    t = np.arange(n_periods)\n",
    "    \n",
    "    # Create correlated time series (A, B, C)\n",
    "    # Shared seasonal and trend components\n",
    "    seasonal_component = 2 * np.sin(2 * np.pi * t / 30)  # 30-day cycle\n",
    "    trend_component = -0.01 * t  # Slight declining trend\n",
    "    \n",
    "    # Series A: Base series with seasonal pattern\n",
    "    series_a = 10 + seasonal_component + trend_component + rng.normal(0, 0.3, n_periods)\n",
    "    \n",
    "    # Series B: Correlated with A, different offset and noise\n",
    "    series_b = 15 + seasonal_component + trend_component + rng.normal(0, 0.4, n_periods)\n",
    "    \n",
    "    # Series C: Also correlated, with phase shift\n",
    "    seasonal_shifted = 1.5 * np.sin(2 * np.pi * t / 30 + np.pi/4)  # Phase shifted\n",
    "    series_c = 8 + seasonal_shifted + 0.8 * trend_component + rng.normal(0, 0.35, n_periods)\n",
    "    \n",
    "    # Series D: Uncorrelated random walk\n",
    "    series_d = np.cumsum(rng.normal(0, 0.5, n_periods)) + 5\n",
    "    \n",
    "    # Create complete DataFrame\n",
    "    df_complete = pd.DataFrame({\n",
    "        'Series_A': series_a,\n",
    "        'Series_B': series_b, \n",
    "        'Series_C': series_c,\n",
    "        'Series_D': series_d\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Create version with missing values\n",
    "    df_with_missing = df_complete.copy()\n",
    "    \n",
    "    # Introduce missing values in Series B, C and D (keep A complete as reference)\n",
    "    for col in ['Series_B', 'Series_C', 'Series_D']:\n",
    "        n_missing = int(0.25 * n_periods)  # 25% missing\n",
    "        missing_indices = rng.choice(n_periods, size=n_missing, replace=False)\n",
    "        df_with_missing.iloc[missing_indices, df_with_missing.columns.get_loc(col)] = np.nan\n",
    "    \n",
    "    # blank portion of one column\n",
    "    df_with_missing.iloc[50:80, df_with_missing.columns.get_loc('Series_B')] = np.nan\n",
    "\n",
    "    return df_with_missing, df_complete\n",
    "\n",
    "# Generate the data\n",
    "df_missing, df_truth = generate_synthetic_timeseries(n_periods=200)\n",
    "\n",
    "print(\"Dataset shape:\", df_missing.shape)\n",
    "print(\"\\nMissing values per series:\")\n",
    "print(df_missing.isna().sum())\n",
    "print(f\"\\nTotal missing values: {df_missing.isna().sum().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_missing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries_comparison(df_observed, df_truth, df_imputed=None, title=\"Time Series Data\",oe=None):\n",
    "    \"\"\"\n",
    "    Plot observed vs truth vs imputed time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    colors = {'observed': 'black', 'truth': 'red', 'imputed': 'blue'}\n",
    "    \n",
    "    for i, col in enumerate(df_observed.columns):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot truth (complete data)\n",
    "        ax.plot(df_truth.index, df_truth[col], color=colors['truth'], \n",
    "               alpha=0.7, linewidth=1, label='True values')\n",
    "        \n",
    "        # Plot observed (non-missing) values\n",
    "        observed_mask = ~df_observed[col].isna()\n",
    "        ax.scatter(df_observed.index[observed_mask], df_observed[col][observed_mask], \n",
    "                  color=colors['observed'], s=8, alpha=0.8, label='Observed', zorder=5)\n",
    "        \n",
    "        # Plot imputed values if available\n",
    "        if df_imputed is not None:\n",
    "            missing_mask = df_observed[col].isna()\n",
    "            if missing_mask.any():\n",
    "                ax.scatter(df_observed.index[missing_mask], df_imputed[col][missing_mask], \n",
    "                          color=colors['imputed'], s=12, alpha=0.9, label='Imputed', \n",
    "                          marker='x', zorder=6)\n",
    "            if oe is not None:\n",
    "                # (50,200,4)\n",
    "                arr = oe[:,:,i]\n",
    "                [ax.plot(df_observed.index, arr[j,:], color='gray', alpha=0.1) for j in range(arr.shape[0])]\n",
    "            \n",
    "        ax.set_ylabel(col)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right')\n",
    "    \n",
    "    axes[-1].set_xlabel('Date')\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the generated data\n",
    "plot_timeseries_comparison(df_missing, df_truth, title=\"Synthetic Time Series with Missing Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd19d2",
   "metadata": {},
   "source": [
    "## 2. Analyze Correlations\n",
    "\n",
    "Let's examine the correlation structure to understand the relationships between series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39553ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix using only observed values\n",
    "correlation_matrix = df_truth.corr()\n",
    "\n",
    "print(\"Correlation Matrix (True Values):\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix)):\n",
    "        text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', \n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "ax.set_xticks(range(len(correlation_matrix)))\n",
    "ax.set_yticks(range(len(correlation_matrix)))\n",
    "ax.set_xticklabels(correlation_matrix.columns)\n",
    "ax.set_yticklabels(correlation_matrix.columns)\n",
    "plt.colorbar(im, ax=ax, label='Correlation')\n",
    "plt.title('Correlation Matrix of Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Series A, B, C are moderately correlated (share seasonal patterns)\")\n",
    "print(\"- Series D is uncorrelated with others (random walk behavior)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82f0ee",
   "metadata": {},
   "source": [
    "## 3. Basic SVD Imputation\n",
    "\n",
    "First, we apply the SVD imputer with default settings using automatic rank estimation based on percentage of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize imputer with automatic rank estimation\n",
    "imputer_basic = Imputer(\n",
    "    data=df_missing,\n",
    "    variance_threshold=0.95,  # Capture 95% of variance\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "df_imputed_basic = imputer_basic.fit_transform()\n",
    "\n",
    "print(f\"\\nEstimated rank: {imputer_basic.rank_}\")\n",
    "print(f\"Data shape: {imputer_basic.shape_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residuals_df, stats_dict = imputer_basic.calculate_reconstruction_residuals(\n",
    "#    return_stats=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962daac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plot_timeseries_comparison(df_missing, df_truth, df_imputed_basic, \n",
    "                          title=f\"Basic SVD Imputation (Rank = {imputer_basic.rank_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e35991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imputation_metrics(df_truth, df_observed, df_imputed):\n",
    "    \"\"\"\n",
    "    Calculate imputation performance metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for col in df_observed.columns:\n",
    "        # Get missing positions\n",
    "        missing_mask = df_observed[col].isna()\n",
    "        \n",
    "        if missing_mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate metrics for missing positions only\n",
    "        true_vals = df_truth[col][missing_mask]\n",
    "        imputed_vals = df_imputed[col][missing_mask]\n",
    "        \n",
    "        rmse = np.sqrt(np.mean((true_vals - imputed_vals) ** 2))\n",
    "        mae = np.mean(np.abs(true_vals - imputed_vals))\n",
    "        \n",
    "        # Correlation between true and imputed values\n",
    "        correlation = np.corrcoef(true_vals, imputed_vals)[0, 1]\n",
    "        \n",
    "        metrics[col] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'Correlation': correlation,\n",
    "            'N_missing': missing_mask.sum()\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate basic imputation\n",
    "metrics_basic = calculate_imputation_metrics(df_truth, df_missing, df_imputed_basic)\n",
    "\n",
    "print(\"Basic Imputation Performance:\")\n",
    "print(\"-\" * 50)\n",
    "for col, metric in metrics_basic.items():\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  RMSE: {metric['RMSE']:.3f}\")\n",
    "    print(f\"  MAE:  {metric['MAE']:.3f}\")\n",
    "    print(f\"  Corr: {metric['Correlation']:.3f}\")\n",
    "    print(f\"  N_missing: {metric['N_missing']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56ba93",
   "metadata": {},
   "source": [
    "## 4. Automatic Rank Estimation\n",
    "\n",
    "That is OK i guess...but not amazing. Note that a single rank (rank=1) is being calcualted as explaining 95% of the variance. This might be too simplistic. Lets try estiamting the optimal rank to explain randomly missing data (from the data that we have). What we will do is randomly drop pieces of data from the data set and optimize the rank to minimize mae/rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For optimal performance, the rank can be selected through cross-validation.\n",
    "\n",
    "# Optimize rank using cross-validation\n",
    "imputer_optimized = Imputer(\n",
    "    data=df_missing,\n",
    "    rank=\"auto\",  # This triggers rank optimization\n",
    "    tol=1e-3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df_imputed_optimized = imputer_optimized.fit_transform()\n",
    "\n",
    "print(f\"\\nOptimized rank: {imputer_optimized.rank_}\")\n",
    "\n",
    "# Get optimization details\n",
    "opt_results = imputer_optimized.get_optimization_results()\n",
    "if opt_results:\n",
    "    print(f\"Optimal CV score: {opt_results['optimal_score']:.4f}\")\n",
    "    print(\"\\nRank performance summary:\")\n",
    "    print(opt_results['results_df'][['rank', 'mean_rmse', 'std_rmse']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cb993",
   "metadata": {},
   "source": [
    "Hmmm...seems the optimal rank is still 1. This tells us that we might simply not have enough information content in the dataset to do any better. Using higher rank adds to much noise back in, and we cant use less than 1 rank to explain the rest of the data...so what can we do? We can sythenticaly augment our dataset to implicilty extract information component sform our time series!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1fbfa",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation for Enhanced Imputation\n",
    "\n",
    "The package provides data augmentation methods that can improve imputation by incorporating temporal structure:\n",
    "\n",
    "1. **Derivative augmentation**: Adds first and second differences\n",
    "2. **Symmetric lag augmentation**: Includes past and future values around each time point\n",
    "\n",
    "These methods expand the feature space and can help SVD capture more complex temporal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f3bbf",
   "metadata": {},
   "source": [
    "### 4.1 Derivative Augmentation\n",
    "\n",
    "This method augments the data with first and second differences, helping capture trend and acceleration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8faad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply derivative augmentation\n",
    "df_augmented_deriv = create_derivative_augmented_matrix(df_missing)\n",
    "\n",
    "print(\"Original data shape:\", df_missing.shape)\n",
    "print(\"Augmented data shape:\", df_augmented_deriv.shape)\n",
    "print(\"\\nAugmented columns:\")\n",
    "print(df_augmented_deriv.columns.tolist())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows of augmented data:\")\n",
    "df_augmented_deriv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD imputation to derivative-augmented data\n",
    "imputer_deriv = Imputer(\n",
    "    data=df_augmented_deriv,\n",
    "    rank=\"auto\",  # This triggers rank optimization\n",
    "    tol=1e-3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df_imputed_deriv_aug = imputer_deriv.fit_transform()\n",
    "\n",
    "print(f\"\\nEstimated rank with derivative augmentation: {imputer_deriv.rank_}\")\n",
    "\n",
    "# Extract original columns from augmented result\n",
    "original_cols = ['Series_A', 'Series_B', 'Series_C', 'Series_D']\n",
    "df_imputed_deriv = df_imputed_deriv_aug[original_cols].copy()\n",
    "\n",
    "# Align indices (derivative augmentation loses first 2 rows)\n",
    "df_missing_aligned = df_missing.loc[df_imputed_deriv.index]\n",
    "df_truth_aligned = df_truth.loc[df_imputed_deriv.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322008b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot derivative augmentation results\n",
    "plot_timeseries_comparison(df_missing_aligned, df_truth_aligned, df_imputed_deriv,\n",
    "                          title=f\"Derivative Augmentation (Rank = {imputer_deriv.rank_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553eea2",
   "metadata": {},
   "source": [
    "Doing alot better for SiteB (which only has an offset + noise). Not much diff for SiteC (also has a lag). And SiteD is terrible (as expected; no correlation). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126fe139",
   "metadata": {},
   "source": [
    "### 4.2 Symmetric Lag Augmentation\n",
    "\n",
    "This method includes past and future values around each time point, which can be particularly effective for interpolating gaps in the middle of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply symmetric lag augmentation with window=2 (includes t-2, t-1, t, t+1, t+2)\n",
    "window_size = 2\n",
    "df_augmented_lag = create_symmetric_augmented_matrix(df_missing, window=window_size)\n",
    "\n",
    "print(\"Original data shape:\", df_missing.shape)\n",
    "print(\"Lag-augmented data shape:\", df_augmented_lag.shape)\n",
    "print(f\"\\nWindow size: {window_size} (includes {2*window_size + 1} time points per variable)\")\n",
    "\n",
    "# Show column structure\n",
    "print(\"\\nAugmented columns (first 10):\")\n",
    "print(df_augmented_lag.columns.tolist()[:10])\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows of lag-augmented data (showing subset of columns):\")\n",
    "df_augmented_lag.iloc[:5, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD imputation to lag-augmented data\n",
    "imputer_lag = Imputer(\n",
    "    data=df_augmented_lag,\n",
    "    rank=\"auto\",  # This triggers rank optimization\n",
    "    tol=1e-3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df_imputed_lag_aug = imputer_lag.fit_transform()\n",
    "\n",
    "print(f\"\\nEstimated rank with lag augmentation: {imputer_lag.rank_}\")\n",
    "\n",
    "# Extract original columns (lag+0 columns) from augmented result\n",
    "lag0_cols = [col for col in df_imputed_lag_aug.columns if '_lag+0' in col]\n",
    "df_imputed_lag = df_imputed_lag_aug[lag0_cols].copy()\n",
    "\n",
    "# Rename columns to original names\n",
    "df_imputed_lag.columns = [col.replace('_lag+0', '') for col in df_imputed_lag.columns]\n",
    "\n",
    "# Align indices (lag augmentation loses first and last 'window' rows)\n",
    "df_missing_lag_aligned = df_missing.loc[df_imputed_lag.index]\n",
    "df_truth_lag_aligned = df_truth.loc[df_imputed_lag.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lag augmentation results\n",
    "plot_timeseries_comparison(df_missing_lag_aligned, df_truth_lag_aligned, df_imputed_lag,\n",
    "                          title=f\"Symmetric Lag Augmentation (Rank = {imputer_lag.rank_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8dcf6",
   "metadata": {},
   "source": [
    "Wow...even SiteD is doing well....how? Because we have information about what happens before/after missing data to informin the imputation. Sweet az.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c54f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Imputation with Uncertainty (Rubin's Rules)\n",
    "df_imputed, df_uncertainty = imputer_lag.fit_transform(\n",
    "    return_uncertainty=True, \n",
    "    n_imputations=10\n",
    ")\n",
    "cols = [i for i in df_imputed.columns if 'lag+0' in i]\n",
    "df_imputed = df_imputed[cols]\n",
    "df_imputed.columns = [col.replace('_lag+0','') for col in df_imputed.columns]\n",
    "df_uncertainty = df_uncertainty[cols]\n",
    "df_uncertainty.columns = [col.replace('_lag+0','') for col in df_uncertainty.columns]\n",
    "# plot\n",
    "fig,axs = plt.subplots(4,1, figsize=(7,6),sharex=True)\n",
    "\n",
    "for e,ax in enumerate(axs):\n",
    "    col = df_missing.columns[e]\n",
    "    # Original data points\n",
    "    ax.plot(df_missing.index,df_missing[col],'.',label='Input',alpha=0.5,zorder=100)\n",
    "    ax.plot(df_truth.index,df_truth[col],c='k',label='Truth',alpha=1,zorder=1)\n",
    "    # Imputed data line\n",
    "    ax.plot(df_imputed.index,df_imputed[col],c='r',linestyle='--',label='Imputed')\n",
    "    # Uncertainty intervals\n",
    "    lb = df_imputed[col] - 1.96 * np.sqrt(df_uncertainty[col])\n",
    "    ub = df_imputed[col] + 1.96 * np.sqrt(df_uncertainty[col])\n",
    "    ax.fill_between(df_imputed.index, lb, ub, color='0.5', alpha=0.3, label='95% CI', zorder=0) \n",
    "    ax.set_title(col)\n",
    "ax.legend(loc='lower left',fontsize=8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829a6dd",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Let's compare the performance of different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all approaches\n",
    "metrics_deriv = calculate_imputation_metrics(df_truth_aligned, df_missing_aligned, df_imputed_deriv)\n",
    "metrics_lag = calculate_imputation_metrics(df_truth_lag_aligned, df_missing_lag_aligned, df_imputed_lag)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "methods = {\n",
    "    'Basic SVD': metrics_basic,\n",
    "    'Derivative Aug': metrics_deriv, \n",
    "    'Lag Aug': metrics_lag\n",
    "}\n",
    "\n",
    "ranks = {\n",
    "    'Basic SVD': imputer_basic.rank_,\n",
    "    'Derivative Aug': imputer_deriv.rank_,\n",
    "    'Lag Aug': imputer_lag.rank_\n",
    "}\n",
    "\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print header\n",
    "print(f\"{'Method':<15} {'Rank':<6} {'Series':<10} {'RMSE':<8} {'MAE':<8} {'Correlation':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method_name, method_metrics in methods.items():\n",
    "    rank = ranks[method_name]\n",
    "    for i, (series, metrics) in enumerate(method_metrics.items()):\n",
    "        if i == 0:\n",
    "            print(f\"{method_name:<15} {rank:<6} {series:<10} {metrics['RMSE']:<8.3f} {metrics['MAE']:<8.3f} {metrics['Correlation']:<12.3f}\")\n",
    "        else:\n",
    "            print(f\"{'':>22} {series:<10} {metrics['RMSE']:<8.3f} {metrics['MAE']:<8.3f} {metrics['Correlation']:<12.3f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Calculate average performance\n",
    "print(\"\\nAverage Performance Across Series:\")\n",
    "print(\"-\" * 50)\n",
    "for method_name, method_metrics in methods.items():\n",
    "    avg_rmse = np.mean([m['RMSE'] for m in method_metrics.values()])\n",
    "    avg_mae = np.mean([m['MAE'] for m in method_metrics.values()])\n",
    "    avg_corr = np.mean([m['Correlation'] for m in method_metrics.values()])\n",
    "    print(f\"{method_name:<15} RMSE: {avg_rmse:.3f}  MAE: {avg_mae:.3f}  Corr: {avg_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83194535",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key features of the SVD time series imputer:\n",
    "\n",
    "1. **Basic Usage**: Simple imputation with automatic rank estimation\n",
    "2. **Data Augmentation**: Enhanced imputation using derivative and lag augmentation\n",
    "3. **Performance Evaluation**: Quantitative comparison of different approaches\n",
    "4. **Rank Optimization**: Automated rank selection through cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svdimpute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
