from __future__ import annotations

import contextlib
import csv
import os
import resource
import time
from dataclasses import dataclass, field, replace
from statistics import mean, median
from types import SimpleNamespace
from typing import Dict, List, Optional

import numpy as np
import typer
from numpy.random import default_rng

from .runtime_config import runtime_from_args
from covertreex import config as cx_config
from covertreex.algo import batch_insert
from covertreex.algo.mis import NUMBA_AVAILABLE
from covertreex.core.tree import PCCTree, get_runtime_backend
from covertreex.queries.knn import knn
from covertreex.baseline import (
    BaselineCoverTree,
    ExternalCoverTreeBaseline,
    GPBoostCoverTreeBaseline,
    has_external_cover_tree,
    has_gpboost_cover_tree,
)
from covertreex.telemetry import (
    RUNTIME_BREAKDOWN_CHUNK_FIELDS,
    RUNTIME_BREAKDOWN_SCHEMA_ID,
    generate_run_id,
    resolve_artifact_path,
    runtime_breakdown_fieldnames,
    timestamped_artifact,
)
from tests.utils.datasets import gaussian_dataset
from . import option_defs as opts


breakdown_app = typer.Typer(
    add_completion=False,
    pretty_exceptions_enable=False,
    invoke_without_command=True,
    help="Generate runtime breakdown plots for cover tree implementations.",
)

try:  # pragma: no cover - plotting exercised in manual workflows
    import matplotlib.pyplot as plt
except ImportError:  # pragma: no cover - guarded runtime path
    plt = None


@dataclass(frozen=True)
class ImplementationResult:
    label: str
    build_seconds: float
    query_seconds: float
    segments: Dict[str, float]
    build_warmup_seconds: float
    build_steady_seconds: float
    query_warmup_seconds: float
    query_steady_seconds: float
    build_cpu_seconds: float
    build_cpu_utilisation: float
    build_rss_delta_bytes: Optional[int]
    build_max_rss_bytes: Optional[int]
    query_cpu_seconds: float
    query_cpu_utilisation: float
    query_rss_delta_bytes: Optional[int]
    query_max_rss_bytes: Optional[int]
    notes: Optional[str] = None
    chunk_metrics: Dict[str, float] = field(default_factory=dict)


def _generate_dataset(
    *,
    dimension: int,
    tree_points: int,
    queries: int,
    seed: int,
) -> tuple[np.ndarray, np.ndarray]:
    rng = default_rng(seed)
    points, queries_arr = gaussian_dataset(
        rng,
        tree_points=tree_points,
        queries=queries,
        dimension=dimension,
        dtype=np.float64,
    )
    return (
        np.asarray(points, dtype=np.float64, copy=False),
        np.asarray(queries_arr, dtype=np.float64, copy=False),
    )


def _block_until_ready(value: object) -> None:
    blocker = getattr(value, "block_until_ready", None)
    if callable(blocker):
        blocker()


@dataclass(frozen=True)
class _ResourceSnapshot:
    wall: float
    cpu_user: float
    cpu_system: float
    rss_bytes: Optional[int]
    max_rss_bytes: Optional[int]


def _read_rss_bytes() -> Optional[int]:
    try:
        with open("/proc/self/statm", "r", encoding="utf-8") as handle:
            fields = handle.readline().strip().split()
        if len(fields) < 2:
            return None
        rss_pages = int(fields[1])
        page_size = os.sysconf("SC_PAGE_SIZE")
        return int(rss_pages * page_size)
    except (OSError, ValueError, IndexError):
        return None


def _resource_snapshot() -> _ResourceSnapshot:
    usage = resource.getrusage(resource.RUSAGE_SELF)
    max_rss = int(usage.ru_maxrss * 1024) if usage.ru_maxrss else None
    rss_bytes = _read_rss_bytes()
    return _ResourceSnapshot(
        wall=time.perf_counter(),
        cpu_user=float(usage.ru_utime),
        cpu_system=float(usage.ru_stime),
        rss_bytes=rss_bytes,
        max_rss_bytes=max_rss,
    )


def _resource_delta(
    start: _ResourceSnapshot, end: _ResourceSnapshot
) -> tuple[float, float, Optional[int], Optional[int]]:
    wall_seconds = max(0.0, end.wall - start.wall)
    cpu_seconds = max(
        0.0,
        (end.cpu_user - start.cpu_user) + (end.cpu_system - start.cpu_system),
    )
    rss_delta = None
    if start.rss_bytes is not None and end.rss_bytes is not None:
        rss_delta = end.rss_bytes - start.rss_bytes
    max_rss_candidates = [
        value for value in (start.max_rss_bytes, end.max_rss_bytes) if value is not None
    ]
    max_rss = max(max_rss_candidates) if max_rss_candidates else None
    return wall_seconds, cpu_seconds, rss_delta, max_rss


def _compute_cpu_utilisation(cpu_seconds: float, wall_seconds: float) -> float:
    if wall_seconds <= 0.0:
        return 0.0
    return cpu_seconds / wall_seconds


def _format_bytes(value: Optional[int]) -> str:
    if value is None:
        return "NA"
    if value == 0:
        return "0B"
    sign = "-" if value < 0 else ""
    abs_value = abs(value)
    units = ["B", "KB", "MB", "GB", "TB"]
    idx = min(int(np.log(abs_value) / np.log(1024)) if abs_value > 0 else 0, len(units) - 1)
    scaled = abs_value / (1024**idx)
    return f"{sign}{scaled:.2f}{units[idx]}"


def _format_resource_summary(
    cpu_seconds: float,
    cpu_utilisation: float,
    rss_delta: Optional[int],
    max_rss: Optional[int],
) -> str:
    return (
        f"cpu={cpu_seconds:.3f}s "
        f"util={cpu_utilisation:.2f}x "
        f"rssÎ”={_format_bytes(rss_delta)} "
        f"max={_format_bytes(max_rss)}"
    )


def _print_multi_run_summary(all_results: List[List[ImplementationResult]]) -> None:
    labels = sorted({res.label for run in all_results for res in run})
    print("=== Aggregate Summary ===")
    for label in labels:
        label_results: List[ImplementationResult] = []
        for run in all_results:
            match = next((res for res in run if res.label == label), None)
            if match is not None:
                label_results.append(match)
        if not label_results:
            continue

        build_totals = [res.build_seconds for res in label_results]
        build_warmups = [res.build_warmup_seconds for res in label_results]
        query_totals = [res.query_seconds for res in label_results]

        print(f"{label}:")
        print(
            f"  build total: first={build_totals[0]:.3f}s "
            f"mean={mean(build_totals):.3f}s median={median(build_totals):.3f}s "
            f"min={min(build_totals):.3f}s max={max(build_totals):.3f}s"
        )
        print(
            f"  build warmup: first={build_warmups[0]:.3f}s "
            f"mean={mean(build_warmups):.3f}s median={median(build_warmups):.3f}s "
            f"min={min(build_warmups):.3f}s max={max(build_warmups):.3f}s"
        )
        print(
            f"  query steady: first={query_totals[0]:.3f}s "
            f"mean={mean(query_totals):.3f}s median={median(query_totals):.3f}s "
            f"min={min(query_totals):.3f}s max={max(query_totals):.3f}s"
        )


@contextlib.contextmanager
def _temporary_runtime_override(**overrides: object) -> None:
    base_context = cx_config.runtime_context()
    updated_config = replace(base_context.config, **overrides)
    cx_config.configure_runtime(updated_config)
    try:
        yield
    finally:
        cx_config.set_runtime_context(base_context)


def _chunk_points(points: np.ndarray, batch_size: int) -> List[np.ndarray]:
    return [points[idx : idx + batch_size] for idx in range(0, points.shape[0], batch_size)]


def _summarise_batches(batch_metrics: List[Dict[str, float]], build_seconds: float) -> Dict[str, float]:
    totals = {
        "pairwise": 0.0,
        "mask": 0.0,
        "chain": 0.0,
        "nonzero": 0.0,
        "sort": 0.0,
        "assemble": 0.0,
        "semisort": 0.0,
        "conflict_graph": 0.0,
        "mis": 0.0,
    }
    for metrics in batch_metrics:
        for key in totals:
            totals[key] += metrics.get(key, 0.0)

    semisort_residual = max(
        0.0,
        totals["semisort"]
        - (
            totals["chain"]
            + totals["nonzero"]
            + totals["sort"]
            + totals["assemble"]
        ),
    )

    segments = {
        "pairwise": totals["pairwise"],
        "mask": totals["mask"],
        "chain": totals["chain"],
        "nonzero": totals["nonzero"],
        "sort": totals["sort"],
        "assemble": totals["assemble"],
        "semisort_residual": semisort_residual,
        "conflict_graph": totals["conflict_graph"],
        "mis": totals["mis"],
    }

    accounted = sum(segments.values())
    segments["other"] = max(0.0, build_seconds - accounted)
    return segments


def _summarise_chunk_metrics(batch_metrics: List[Dict[str, float]]) -> Dict[str, float]:
    def _phases(key: str) -> tuple[float, float]:
        if not batch_metrics:
            return 0.0, 0.0
        values = [metrics.get(key, 0.0) for metrics in batch_metrics]
        warmup = values[0]
        if len(values) > 1:
            steady = float(sum(values[1:])) / float(len(values) - 1)
        else:
            steady = warmup
        return warmup, steady

    summary: Dict[str, float] = {}
    metric_fields = (
        ("traversal", "segments"),
        ("traversal", "emitted"),
        ("traversal", "max_members"),
        ("conflict", "segments"),
        ("conflict", "emitted"),
        ("conflict", "max_members"),
    )
    for prefix, field in metric_fields:
        key = f"{prefix}_chunk_{field}"
        warmup, steady = _phases(key)
        summary[f"{key}_warmup"] = warmup
        summary[f"{key}_steady"] = steady
    return summary


def _run_pcct_variant(
    *,
    label: str,
    points: np.ndarray,
    queries: np.ndarray,
    batch_size: int,
    k: int,
    seed: int,
    enable_numba: bool,
) -> ImplementationResult:
    notes: Optional[str] = None
    if enable_numba and not NUMBA_AVAILABLE:
        notes = "Numba backend unavailable; falling back to JAX MIS."

    with _temporary_runtime_override(enable_numba=enable_numba):
        backend = get_runtime_backend()
        tree = PCCTree.empty(dimension=int(points.shape[1]), backend=backend)
        batch_metrics: List[Dict[str, float]] = []
        batch_totals: List[float] = []

        batches = _chunk_points(points, batch_size)
        build_resource_start = _resource_snapshot()
        for idx, batch_np in enumerate(batches):
            if batch_np.size == 0:
                continue
            batch = backend.asarray(batch_np, dtype=backend.default_float)
            batch_start = time.perf_counter()
            tree, plan = batch_insert(tree, batch, mis_seed=seed + idx)
            batch_totals.append(time.perf_counter() - batch_start)
            timing = plan.traversal.timings
            batch_metrics.append(
                {
                    "pairwise": float(timing.pairwise_seconds),
                    "mask": float(timing.mask_seconds),
                    "chain": float(timing.chain_seconds),
                    "nonzero": float(timing.nonzero_seconds),
                    "sort": float(timing.sort_seconds),
                    "assemble": float(timing.assemble_seconds),
                    "semisort": float(timing.semisort_seconds),
                    "conflict_graph": float(plan.timings.conflict_graph_seconds),
                    "mis": float(plan.timings.mis_seconds),
                    "traversal_chunk_segments": float(timing.scope_chunk_segments),
                    "traversal_chunk_emitted": float(timing.scope_chunk_emitted),
                    "traversal_chunk_max_members": float(timing.scope_chunk_max_members),
                    "conflict_chunk_segments": float(
                        plan.conflict_graph.timings.scope_chunk_segments
                    ),
                    "conflict_chunk_emitted": float(
                        plan.conflict_graph.timings.scope_chunk_emitted
                    ),
                    "conflict_chunk_max_members": float(
                        plan.conflict_graph.timings.scope_chunk_max_members
                    ),
                }
            )

        build_resource_end = _resource_snapshot()
        build_wall_seconds, build_cpu_seconds, build_rss_delta, build_max_rss = _resource_delta(
            build_resource_start, build_resource_end
        )
        build_seconds = float(sum(batch_totals))
        if batch_totals:
            build_warmup = batch_totals[0]
            build_steady = (
                float(sum(batch_totals[1:])) / (len(batch_totals) - 1)
                if len(batch_totals) > 1
                else build_warmup
            )
        else:
            build_warmup = 0.0
            build_steady = 0.0

        query_array = backend.asarray(queries, dtype=backend.default_float)
        warm_start = time.perf_counter()
        indices, distances = knn(tree, query_array, k=k, return_distances=True)
        _block_until_ready(indices)
        _block_until_ready(distances)
        query_warmup = time.perf_counter() - warm_start
        query_resource_start = _resource_snapshot()
        steady_start = time.perf_counter()
        indices, distances = knn(tree, query_array, k=k, return_distances=True)
        _block_until_ready(indices)
        _block_until_ready(distances)
        query_steady = time.perf_counter() - steady_start
        query_resource_end = _resource_snapshot()
        (
            query_wall_seconds,
            query_cpu_seconds,
            query_rss_delta,
            query_max_rss,
        ) = _resource_delta(query_resource_start, query_resource_end)

    segments = _summarise_batches(batch_metrics, build_seconds)
    chunk_summary = _summarise_chunk_metrics(batch_metrics)
    return ImplementationResult(
        label=label,
        build_seconds=build_seconds,
        query_seconds=query_steady,
        segments=segments,
        build_warmup_seconds=build_warmup,
        build_steady_seconds=build_steady,
        query_warmup_seconds=query_warmup,
        query_steady_seconds=query_steady,
        build_cpu_seconds=build_cpu_seconds,
        build_cpu_utilisation=_compute_cpu_utilisation(build_cpu_seconds, build_wall_seconds),
        build_rss_delta_bytes=build_rss_delta,
        build_max_rss_bytes=build_max_rss,
        query_cpu_seconds=query_cpu_seconds,
        query_cpu_utilisation=_compute_cpu_utilisation(query_cpu_seconds, query_wall_seconds),
        query_rss_delta_bytes=query_rss_delta,
        query_max_rss_bytes=query_max_rss,
        notes=notes,
        chunk_metrics=chunk_summary,
    )


def _run_sequential_baseline(
    *,
    label: str,
    points: np.ndarray,
    queries: np.ndarray,
    k: int,
    constructor,
) -> ImplementationResult:
    build_resource_start = _resource_snapshot()
    start_build = time.perf_counter()
    tree = constructor.from_points(points)
    build_seconds = time.perf_counter() - start_build
    build_resource_end = _resource_snapshot()
    build_wall_seconds, build_cpu_seconds, build_rss_delta, build_max_rss = _resource_delta(
        build_resource_start, build_resource_end
    )

    query_resource_start = _resource_snapshot()
    start_query = time.perf_counter()
    tree.knn(queries, k=k, return_distances=False)
    query_seconds = time.perf_counter() - start_query
    query_resource_end = _resource_snapshot()
    (
        query_wall_seconds,
        query_cpu_seconds,
        query_rss_delta,
        query_max_rss,
    ) = _resource_delta(query_resource_start, query_resource_end)

    segments = {"other": build_seconds}
    return ImplementationResult(
        label=label,
        build_seconds=build_seconds,
        query_seconds=query_seconds,
        segments=segments,
        build_warmup_seconds=build_seconds,
        build_steady_seconds=build_seconds,
        query_warmup_seconds=query_seconds,
        query_steady_seconds=query_seconds,
        build_cpu_seconds=build_cpu_seconds,
        build_cpu_utilisation=_compute_cpu_utilisation(build_cpu_seconds, build_wall_seconds),
        build_rss_delta_bytes=build_rss_delta,
        build_max_rss_bytes=build_max_rss,
        query_cpu_seconds=query_cpu_seconds,
        query_cpu_utilisation=_compute_cpu_utilisation(query_cpu_seconds, query_wall_seconds),
        query_rss_delta_bytes=query_rss_delta,
        query_max_rss_bytes=query_max_rss,
    )


def _plot_results(results: List[ImplementationResult], *, output: Optional[str], show: bool) -> None:
    if plt is None:
        typer.echo("matplotlib is required for plotting. Install it with `pip install matplotlib`.", err=True)
        return

    segment_order = [
        "pairwise",
        "mask",
        "chain",
        "nonzero",
        "sort",
        "assemble",
        "semisort_residual",
        "conflict_graph",
        "mis",
        "other",
    ]
    segment_labels = {
        "pairwise": "Pairwise Distances",
        "mask": "Radius Mask",
        "chain": "Next-Chain Merge",
        "nonzero": "Scope Nonzero",
        "sort": "Scope Sort",
        "assemble": "Scope Assemble",
        "semisort_residual": "Semisort Residual",
        "conflict_graph": "Conflict Graph",
        "mis": "MIS",
        "other": "Other",
    }
    colors = {
        "pairwise": "#1f77b4",
        "mask": "#ff7f0e",
        "chain": "#2ca02c",
        "nonzero": "#d62728",
        "sort": "#9467bd",
        "assemble": "#8c564b",
        "semisort_residual": "#e377c2",
        "conflict_graph": "#7f7f7f",
        "mis": "#bcbd22",
        "other": "#17becf",
    }

    labels = [res.label for res in results]
    x = np.arange(len(labels))
    width = 0.6

    fig, (ax_runtime, ax_query) = plt.subplots(
        2,
        1,
        figsize=(11, 8),
        sharex=True,
        gridspec_kw={"height_ratios": [3, 1]},
    )

    bottom = np.zeros(len(results))
    for segment in segment_order:
        heights = np.array([res.segments.get(segment, 0.0) for res in results])
        if np.allclose(heights, 0.0):
            continue
        ax_runtime.bar(
            x,
            heights,
            width,
            bottom=bottom,
            label=segment_labels[segment],
            color=colors.get(segment),
            edgecolor="black",
            linewidth=0.3,
        )
        bottom += heights

    ax_runtime.set_ylabel("Build Time (s)")
    ax_runtime.set_title("Cover Tree Build Runtime Breakdown")
    ax_runtime.grid(axis="y", alpha=0.3)
    ax_runtime.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0))

    query_seconds = [res.query_steady_seconds for res in results]
    ax_query.bar(
        x,
        query_seconds,
        width,
        color="#6c6f7f",
        edgecolor="black",
        linewidth=0.3,
    )
    ax_query.set_ylabel("Query Time (s)")
    ax_query.set_xlabel("Implementation")
    ax_query.grid(axis="y", alpha=0.3)
    ax_query.set_xticks(x)
    ax_query.set_xticklabels(labels, rotation=20, ha="right")

    fig.tight_layout()

    if output:
        fig.savefig(output, dpi=150, bbox_inches="tight")
        print(f"Saved runtime breakdown to {output}")
    if show:
        plt.show()
    else:
        plt.close(fig)


@breakdown_app.callback()
def breakdown(
    ctx: typer.Context,
    dimension: opts.DimensionOption = 8,
    tree_points: opts.TreePointsOption = 2048,
    batch_size: opts.BatchSizeOption = 128,
    queries: opts.QueriesOption = 512,
    k: opts.KOption = 8,
    seed: opts.SeedOption = 42,
    run_id: opts.RunIdOption = None,
    output: str = typer.Option("runtime_breakdown.png", help="Path to save the generated plot."),
    show: bool = typer.Option(False, help="Display the plot interactively."),
    skip_numba: bool = typer.Option(False, help="Skip the Numba-enabled PCCT run."),
    skip_jax: bool = typer.Option(False, help="Skip the JAX-backed PCCT run."),
    skip_external: bool = typer.Option(True, help="Skip the external cover tree baseline."),
    include_external: bool = typer.Option(False, help="Include the external cover tree baseline."),
    skip_gpboost: bool = typer.Option(False, help="Skip the GPBoost cover tree baseline if numba is installed."),
    skip_sequential: bool = typer.Option(True, help="Skip the sequential cover tree baseline."),
    include_sequential: bool = typer.Option(False, help="Include the sequential cover tree baseline."),
    csv_output: str = typer.Option("", help="Optional path to write metrics as CSV."),
    no_csv_output: bool = typer.Option(False, help="Disable CSV telemetry emission."),
    runs: int = typer.Option(1, help="Number of repeated benchmark runs."),
    backend: opts.BackendOption = "jax",
    precision: opts.PrecisionOption = "float64",
    metric: opts.MetricOption = "euclidean",
) -> None:
    if metric == "residual":
        raise typer.BadParameter("runtime_breakdown currently supports only the euclidean metric.")

    # Handle mutually exclusive flags manually
    if include_external:
        skip_external = False
    if include_sequential:
        skip_sequential = False

    run_id = run_id or generate_run_id(prefix="runtime")

    # Construct args for runtime_from_args (which expects an object with attributes)
    # We only pass what runtime_from_args needs
    runtime_args = SimpleNamespace(
        metric=metric,
        backend=backend,
        precision=precision,
        # Add other runtime options as needed if defaults aren't sufficient
    )

    runtime_from_args(runtime_args).activate()
    print(f"[breakdown] run_id={run_id}")

    plot_output = str(resolve_artifact_path(output, category="benchmarks")) if output else None
    if no_csv_output:
        final_csv_output = None
    elif csv_output:
        final_csv_output = str(resolve_artifact_path(csv_output, category="benchmarks"))
    else:
        final_csv_output = str(
            timestamped_artifact(
                category="benchmarks",
                prefix=f"runtime_breakdown_{run_id}",
                suffix=".csv",
            )
        )

    points_np, queries_np = _generate_dataset(
        dimension=dimension,
        tree_points=tree_points,
        queries=queries,
        seed=seed,
    )

    runs_count = max(1, runs)
    all_results: List[List[ImplementationResult]] = []

    for run_idx in range(runs_count):
        if runs_count > 1:
            print(f"=== Run {run_idx + 1}/{runs_count} ===")
        run_seed = seed + run_idx
        run_results: List[ImplementationResult] = []

        current_backend = get_runtime_backend()
        if not skip_jax and current_backend.name == "jax":
            run_results.append(
                _run_pcct_variant(
                    label="PCCT (JAX)",
                    points=points_np,
                    queries=queries_np,
                    batch_size=batch_size,
                    k=k,
                    seed=run_seed,
                    enable_numba=False,
                )
            )

        if not skip_numba:
            run_results.append(
                _run_pcct_variant(
                    label="PCCT (Numba)",
                    points=points_np,
                    queries=queries_np,
                    batch_size=batch_size,
                    k=k,
                    seed=run_seed,
                    enable_numba=True,
                )
            )

        if not skip_sequential:
            run_results.append(
                _run_sequential_baseline(
                    label="Sequential Baseline",
                    points=points_np,
                    queries=queries_np,
                    k=k,
                    constructor=BaselineCoverTree,
                )
            )

        if not skip_gpboost:
            if has_gpboost_cover_tree():
                run_results.append(
                    _run_sequential_baseline(
                        label="GPBoost CoverTree",
                        points=points_np,
                        queries=queries_np,
                        k=k,
                        constructor=GPBoostCoverTreeBaseline,
                    )
                )
            elif run_idx == 0:
                print("GPBoost cover tree baseline unavailable; skipping.")

        if not skip_external and has_external_cover_tree():
            run_results.append(
                _run_sequential_baseline(
                    label="External CoverTree",
                    points=points_np,
                    queries=queries_np,
                    k=k,
                    constructor=ExternalCoverTreeBaseline,
                )
            )
        elif (not skip_external) and run_idx == 0:
            print("External cover tree baseline unavailable; skipping.")

        for res in run_results:
            notes = f" ({res.notes})" if res.notes else ""
            print(
                f"{res.label}: "
                f"build={res.build_seconds:.3f}s "
                f"[{_format_resource_summary(res.build_cpu_seconds, res.build_cpu_utilisation, res.build_rss_delta_bytes, res.build_max_rss_bytes)}] "
                f"query={res.query_seconds:.3f}s "
                f"[{_format_resource_summary(res.query_cpu_seconds, res.query_cpu_utilisation, res.query_rss_delta_bytes, res.query_max_rss_bytes)}]"
                f"{notes}"
            )

        all_results.append(run_results)

    # Use the last run for plotting to keep behaviour predictable
    results = all_results[-1] if all_results else []

    _plot_results(results, output=plot_output, show=show)

    if final_csv_output:
        print(f"[breakdown] writing CSV metrics to {final_csv_output}")
        chunk_fieldnames = list(RUNTIME_BREAKDOWN_CHUNK_FIELDS)
        fieldnames = list(runtime_breakdown_fieldnames())
        with open(final_csv_output, "w", newline="") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
            writer.writeheader()
            for run_idx, run in enumerate(all_results, start=1):
                for res in run:
                    row = {
                        "schema_id": RUNTIME_BREAKDOWN_SCHEMA_ID,
                        "run_id": run_id,
                        "run": run_idx,
                        "label": res.label,
                        "build_warmup_seconds": res.build_warmup_seconds,
                        "build_steady_seconds": res.build_steady_seconds,
                        "build_total_seconds": res.build_seconds,
                        "query_warmup_seconds": res.query_warmup_seconds,
                        "query_steady_seconds": res.query_steady_seconds,
                        "build_cpu_seconds": res.build_cpu_seconds,
                        "build_cpu_utilisation": res.build_cpu_utilisation,
                        "build_rss_delta_bytes": res.build_rss_delta_bytes
                        if res.build_rss_delta_bytes is not None
                        else "",
                        "build_max_rss_bytes": res.build_max_rss_bytes
                        if res.build_max_rss_bytes is not None
                        else "",
                        "query_cpu_seconds": res.query_cpu_seconds,
                        "query_cpu_utilisation": res.query_cpu_utilisation,
                        "query_rss_delta_bytes": res.query_rss_delta_bytes
                        if res.query_rss_delta_bytes is not None
                        else "",
                        "query_max_rss_bytes": res.query_max_rss_bytes
                        if res.query_max_rss_bytes is not None
                        else "",
                    }
                    for chunk_field in chunk_fieldnames:
                        row[chunk_field] = res.chunk_metrics.get(chunk_field, "")
                    writer.writerow(row)

    if runs_count > 1:
        _print_multi_run_summary(all_results)


__all__ = ["breakdown_app"]
