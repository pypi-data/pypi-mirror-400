monitorInterval = 5
packages = ch.alloy.log4j.glue

appender.Fluentd.type = Fluentd
appender.Fluentd.name = Fluentd
appender.Fluentd.fluentdHost = localhost
appender.Fluentd.fluentdPort = 24224
appender.Fluentd.additionalFields = type=fluentd;env=test;version=0.0.0;repository=my-test-repo;software_component=my-software-component;dag_id=undefined;dag_run_id=undefined;task_id=undefined;glue_job_name=undefined;alloy_tag=undefined

rootLogger.level = info
rootLogger.appenderRef.Fluentd.ref = Fluentd

logger.logger01.name = com.amazon.ws.emr.hadoop.fs.util.PlatformInfo
logger.logger01.level = warn

logger.logger02.name = com.amazonaws.services.glue.AnalyzerLogHelper$
logger.logger02.level = warn

logger.logger03.name = com.hadoop
logger.logger03.level = warn

# to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
logger.logger04.name = org.apache.hadoop.hive.metastore.RetryingHMSHandler
logger.logger04.level = fatal

logger.logger05.name = org.apache.hadoop.hive.ql.exec.FunctionRegistry
logger.logger05.level = error

logger.logger06.name = org.apache.parquet
logger.logger06.level = error

## set the default spark-shell/spark-sql log level to WARN. When running the
## spark-shell/spark-sql, the log level for these classes is used to overwrite
## the root logger's log level, so that the user can have different defaults
## for the shell and regular Spark apps.

logger.logger07.name = org.apache.spark
logger.logger07.level = warn

logger.logger08.name = org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
logger.logger08.level = info

logger.logger09.name = org.apache.spark.storage.memory
logger.logger09.level = info

#
# mute third party logs that are too verbose
logger.logger10.name = org.sparkproject.jetty
logger.logger10.level = warn
logger.logger11.name = org.sparkproject.jetty.util.component.AbstractLifeCycle
logger.logger11.level = error
logger.logger12.name = parquet
logger.logger12.level = error
