# Runner Packing Rules
#
# - backend:   (Required) The accelerated backend to pack for,
#              used to select the packing rules and Dockerfile below `pack` directory.
# - services:  (Required) The inference service to pack for,
#              used to select the Docker build phase described in `pack/${backend}/Dockerfile`.
# - platforms: (Optional) The platforms to build for,
#              used to select the Docker Linux build platforms.
#              Default to `linux/amd64` and `linux/arm64`.
# - args:      (Optional) The build arguments to pass to the Docker build,
#              used to override the default build arguments in `pack/${backend}/Dockerfile`.

rules:

  #
  # Ascend CANN
  #

  ## Ascend CANN 8.3.rc2, using CANN Kernel for A3.
  ##
  - backend: "cann"
    services:
      - "mindie"
      - "vllm"
    args:
      - "CANN_VERSION=8.3.rc2"
      - "CANN_ARCHS=a3"
  - backend: "cann"
    services:
      - "sglang"
    platforms:
      - "linux/arm64"
    args:
      - "CANN_VERSION=8.3.rc2"
      - "CANN_ARCHS=a3"
  ## Ascend CANN 8.3.rc2, using CANN Kernel for 910B.
  ##
  - backend: "cann"
    services:
      - "mindie"
      - "vllm"
    args:
      - "CANN_VERSION=8.3.rc2"
      - "CANN_ARCHS=910b"
  - backend: "cann"
    services:
      - "sglang"
    platforms:
      - "linux/arm64"
    args:
      - "CANN_VERSION=8.3.rc2"
      - "CANN_ARCHS=910b"
  ## Ascend CANN 8.3.rc2, using CANN Kernel for 310P.
  ##
  - backend: "cann"
    services:
      - "mindie"
    args:
      - "CANN_VERSION=8.3.rc2"
      - "CANN_ARCHS=310p"

  #
  # Iluvatar CoreX
  #

  ## Iluvatar CoreX 4.2.0
  ##
  - backend: "corex"
    services:
      - "vllm"
    platforms:
      - "linux/amd64"
    args:
      - "COREX_VERSION=4.2.0"

  #
  # NVIDIA CUDA
  #

  ## NVIDIA CUDA 12.6.3, using PyTorch +cu128.
  ##
  - backend: "cuda"
    services:
      - "voxbox"
      - "vllm"
    args:
      - "CUDA_VERSION=12.6.3"
      - "VOXBOX_TORCH_CUDA_VERSION=12.8.1"
      - "VLLM_TORCH_CUDA_VERSION=12.8.1"
  ## NVIDIA CUDA 12.8.1, using PyTorch +cu128.
  ##
  - backend: "cuda"
    services:
      - "voxbox"
      - "vllm"
      - "sglang"
    args:
      - "CUDA_VERSION=12.8.1"
      - "SGLANG_BASE_IMAGE=gpustack/runner:cuda12.8-vllm0.13.0"
      - "SGLANG_BUILD_BASE_IMAGE=gpustack/runner:cuda12.8-vllm0.13.0"
  ## NVIDIA CUDA 12.9.1, using PyTorch +cu129.
  ##
  - backend: "cuda"
    services:
      - "vllm"
      - "sglang"
    args:
      - "CUDA_VERSION=12.9.1"
      - "SGLANG_BASE_IMAGE=gpustack/runner:cuda12.9-vllm0.13.0"
      - "SGLANG_BUILD_BASE_IMAGE=gpustack/runner:cuda12.9-vllm0.13.0"

  #
  # Hygon DTK
  #

  ## Hygon DTK 25.04.2
  ##
  - backend: "dtk"
    services:
      - "vllm"
    platforms:
      - "linux/amd64"
    args:
      - "DTK_VERSION=25.04.2"

  #
  # MateX MACA
  #

  ## MateX MACA 3.2.1
  ##
  - backend: "maca"
    services:
      - "vllm"
    platforms:
      - "linux/amd64"
    args:
      - "MACA_VERSION=3.2.1"

  #
  # MThreads MUSA
  #

  ## MThreads MUSA 4.1.0
  ##
  - backend: "musa"
    services:
      - "vllm"
    platforms:
      - "linux/amd64"
    args:
      - "MUSA_VERSION=4.1.0"
  ## MThreads MUSA 4.3.2
  ##
  - backend: "musa"
    services:
      - "sglang"
    platforms:
      - "linux/amd64"
    args:
      - "MUSA_VERSION=4.3.2"
      - "MUSA_VERSION_EXTRA=-kuae2.1-1201"

  #
  # AMD ROCm
  #

  ## AMD ROCm 7.0.2, using PyTorch +rocm7.0.
  ##
  - backend: "rocm"
    services:
      - "vllm"
      - "sglang"
    platforms:
      - "linux/amd64"
    args:
      - "ROCM_VERSION=7.0.2"
      - "VLLM_TORCH_SOURCE=radeon"
      - "SGLANG_BASE_IMAGE=gpustack/runner:rocm7.0-vllm0.13.0"
      - "SGLANG_BUILD_BASE_IMAGE=gpustack/runner:rocm7.0-vllm0.13.0"
  ##
  - backend: "rocm"
    services:
      - "vllm"
      - "sglang"
    platforms:
      - "linux/amd64"
    args:
      - "ROCM_VERSION=6.4.4"
      - "VLLM_AITER_VERSION=0.1.7.post2"
      - "SGLANG_BASE_IMAGE=gpustack/runner:rocm6.4-vllm0.13.0"
      - "SGLANG_BUILD_BASE_IMAGE=gpustack/runner:rocm6.4-vllm0.13.0"
