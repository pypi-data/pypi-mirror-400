from functools import reduce
from typing import Callable, Tuple, Union, List, Optional
from hestia_earth.schema import TermTermType, EmissionMethodTier
from hestia_earth.utils.lookup import is_missing_value, lookup_columns
from hestia_earth.utils.model import find_term_match, filter_list_term_type
from hestia_earth.utils.tools import flatten, non_empty_list, safe_parse_float, omit
from hestia_earth.utils.emission import cycle_emissions_in_system_boundary

from hestia_earth.models.log import logShouldRun, logRequirements, debugValues
from . import is_from_model
from .term import get_lookup_value


def _animal_inputs(animal: dict):
    inputs = animal.get("inputs", [])
    return [
        (input | {"animalId": animal["animalId"], "animal": animal.get("term", {})})
        for input in inputs
    ]


def _should_run_input(
    products: list, filter_term_types: Union[TermTermType, List[TermTermType]]
):
    term_types = (
        [t for t in filter_term_types]
        if isinstance(filter_term_types, list)
        else [filter_term_types]
    )
    term_types = [(t if isinstance(t, str) else t.value) for t in term_types]

    def should_run(input: dict):
        return all(
            [
                not term_types or input.get("term", {}).get("termType") in term_types,
                # make sure Input is not a Product as well or we might double-count emissions
                find_term_match(products, input.get("term", {}).get("@id"), None)
                is None,
                # ignore inputs which are flagged as Product of the Cycle
                not input.get("fromCycle", False),
                not input.get("producedInCycle", False),
            ]
        )

    return should_run


def get_background_inputs(
    cycle: dict,
    extra_inputs: list = [],
    filter_term_types: Optional[Union[TermTermType, List[TermTermType]]] = [],
) -> List[dict]:
    # add all the properties of some Term that include others with the mapping
    inputs = flatten(
        cycle.get("inputs", [])
        + list(map(_animal_inputs, cycle.get("animals", [])))
        + extra_inputs
    )
    return list(
        filter(_should_run_input(cycle.get("products", []), filter_term_types), inputs)
    )


def no_gap_filled_background_emissions(
    node: dict,
    list_key: str = "emissions",
    term_type: TermTermType = TermTermType.EMISSION,
):
    blank_nodes = filter_list_term_type(node.get(list_key, []), term_type)

    def check_input(input: dict):
        input_term_id = input.get("term", {}).get("@id")
        operation_term_id = input.get("operation", {}).get("@id")
        animal_term_id = input.get("animal", {}).get("@id")

        return not any(
            [
                is_from_model(blank_node)
                for blank_node in blank_nodes
                if all(
                    [
                        any(
                            [
                                i.get("@id") == input_term_id
                                for i in blank_node.get("inputs", [])
                            ]
                        ),
                        blank_node.get("operation", {}).get("@id") == operation_term_id,
                        blank_node.get("animal", {}).get("@id") == animal_term_id,
                    ]
                )
            ]
        )

    return check_input


def _all_background_emission_term_ids(node: dict, termType: TermTermType):
    term_ids = cycle_emissions_in_system_boundary(node, termType=termType)
    background_ids = list(
        set(
            [
                get_lookup_value(
                    {"termType": termType.value, "@id": term_id},
                    "inputProductionGroupId",
                )
                for term_id in term_ids
            ]
        )
    )
    # make sure input production emission is itself in the system boundary
    return [term_id for term_id in background_ids if term_id in term_ids]


def log_missing_emissions(
    node: dict, termType: TermTermType = TermTermType.EMISSION, **log_args
):
    all_emission_term_ids = _all_background_emission_term_ids(node, termType)

    def log_input(
        input_term_id: str, included_emission_term_ids: list, **extra_log_args
    ):
        missing_emission_term_ids = non_empty_list(
            [
                term_id
                for term_id in all_emission_term_ids
                if term_id not in included_emission_term_ids
            ]
        )

        for emission_id in missing_emission_term_ids:
            # debug value on the emission itself so it appears for the input
            debugValues(
                node,
                term=emission_id,
                value=None,
                coefficient=None,
                input=input_term_id,
                **log_args,
                **extra_log_args
            )
            logRequirements(
                node,
                term=input_term_id,
                emission_id=emission_id,
                has_emission_factor=False,
                **log_args,
                **extra_log_args
            )
            logShouldRun(
                node,
                term=input_term_id,
                should_run=False,
                emission_id=emission_id,
                **log_args,
                **extra_log_args
            )

    return log_input


_KEY_TO_FIELD = {"inputs": "key"}


def _key_to_field(key: str):
    return _KEY_TO_FIELD.get(key) or key


def _values_from_column(index_column: str, column: str, value: str):
    values = column.split("+")
    term_id = values[0]
    value = safe_parse_float(value, default=None)
    return (
        {
            term_id: {"value": value}
            | {_key_to_field(v.split("[")[0]): v.split("[")[1][:-1] for v in values[1:]}
        }
        if all(
            [
                column != index_column,
                not column.startswith("ecoinvent"),
                not column.startswith("ecoalim"),
                not is_missing_value(value),
            ]
        )
        else {}
    )


def convert_background_lookup(lookup, index_column: str):
    columns = lookup_columns(lookup)
    indexed_df = lookup.set_index(index_column, drop=False).copy()
    return {
        index_key: reduce(
            lambda prev, curr: prev
            | _values_from_column(index_column, curr, row_data[curr]),
            columns,
            {},
        )
        for index_key, row_data in indexed_df.to_dict("index").items()
    }


def parse_term_id(term_id: str):
    return term_id.split("-")[0]


def join_term_id(term_id: str, data: dict):
    return "-".join(non_empty_list([term_id] + list(omit(data, ["value"]).values())))


def _process_mapping(
    node: dict,
    input: dict,
    term_type: TermTermType,
    extract_mapping: Callable[[Tuple, TermTermType], Tuple[dict, float]],
    **log_args
) -> dict:
    input_term_id = input.get("term", {}).get("@id")
    operation_term_id = input.get("operation", {}).get("@id")
    animal_term_id = input.get("animal", {}).get("@id")

    def add(prev: dict, mapping: Tuple):
        values, coefficient = extract_mapping(mapping, term_type)
        for term_id, data in values:
            # log run on each node so we know it did run
            logShouldRun(
                node,
                term=input_term_id,
                should_run=True,
                methodTier=EmissionMethodTier.BACKGROUND.value,
                emission_id=term_id,
                **log_args
            )
            debugValues(
                node,
                term=term_id,
                value=data.get("value"),
                coefficient=coefficient,
                input=input_term_id,
                operation=operation_term_id,
                animal=animal_term_id,
                **log_args
            )
            group_id = join_term_id(term_id, data)
            prev[group_id] = prev.get(group_id, []) + [
                data | {"coefficient": coefficient}
            ]
        return prev

    return add


def process_input_mappings(
    node: dict,
    input: dict,
    mappings: list,
    term_type: TermTermType,
    extract_mapping: Callable[[tuple, TermTermType], Tuple[dict, float]],
    **log_args
):
    return reduce(
        _process_mapping(node, input, term_type, extract_mapping, **log_args),
        mappings,
        {},
    )
