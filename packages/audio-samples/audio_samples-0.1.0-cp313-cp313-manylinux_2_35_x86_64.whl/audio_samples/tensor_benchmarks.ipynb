{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tensor Interoperability Performance Benchmarks\n",
    "\n",
    "This notebook benchmarks the performance of the new tensor interoperability features in `audio_samples` Python bindings.\n",
    "\n",
    "**Test Configuration:**\n",
    "- Sample Rate: 44100 Hz\n",
    "- Sample Type: f32\n",
    "- File Durations: 0.1s, 0.5s, 1s, 2s, 5s, 10s, 30s, 60s\n",
    "- Channel Configurations: Mono and Stereo\n",
    "\n",
    "**Features Tested:**\n",
    "- NumPy array protocol methods (`__array__`, `__array_function__`, `__array_ufunc__`)\n",
    "- DLPack protocol methods (`__dlpack__`, `from_dlpack`)\n",
    "- PyTorch tensor integration (`to_torch`, `from_torch`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "import gc\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import audio_samples as aus\n",
    "\n",
    "# Try to import torch\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available - skipping PyTorch benchmarks\")\n",
    "\n",
    "# Define Okabe-Ito colorblind-friendly palette\n",
    "OKABE_ITO_COLORS = [\n",
    "    '#E69F00',  # orange\n",
    "    '#56B4E9',  # sky blue  \n",
    "    '#009E73',  # bluish green\n",
    "    '#F0E442',  # yellow\n",
    "    '#0072B2',  # blue\n",
    "    '#D55E00',  # vermillion\n",
    "    '#CC79A7',  # reddish purple\n",
    "    '#999999'   # gray\n",
    "]\n",
    "\n",
    "# Configure plotting with same style as existing benchmark\n",
    "plt.rcParams['figure.figsize'] = [12, 9]\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 24\n",
    "plt.rcParams['axes.labelsize'] = 22\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "plt.rcParams['legend.fancybox'] = True\n",
    "plt.rcParams['legend.shadow'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['grid.linewidth'] = 1\n",
    "plt.rcParams['lines.linewidth'] = 4\n",
    "plt.rcParams['lines.markersize'] = 12\n",
    "\n",
    "# Set the color palette\n",
    "sns.set_palette(OKABE_ITO_COLORS)\n",
    "\n",
    "print(f\"audio_samples version: {getattr(aus, '__version__', 'unknown')}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration - same as existing benchmark\n",
    "SAMPLE_RATE = 44100\n",
    "SAMPLE_TYPE = np.float32\n",
    "DURATIONS = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]  # seconds\n",
    "CHANNELS = [1, 2]  # mono, stereo\n",
    "BENCHMARK_ITERATIONS = 50  # Same as existing benchmark\n",
    "FREQUENCY = 440.0  # Hz for sine wave test signal\n",
    "\n",
    "# Create temporary directory for results\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"tensor_bench_\")\n",
    "print(f\"Results will be stored in: {temp_dir}\")\n",
    "\n",
    "def calculate_throughput(file_size_mb: float, time_seconds: float) -> float:\n",
    "    \"\"\"Calculate throughput in MB/s.\"\"\"\n",
    "    return file_size_mb / time_seconds if time_seconds > 0 else 0.0\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"Format time with appropriate units.\"\"\"\n",
    "    if seconds >= 1.0:\n",
    "        return f\"{seconds:.3f}s\"\n",
    "    elif seconds >= 0.001:\n",
    "        return f\"{seconds*1000:.1f}ms\"\n",
    "    else:\n",
    "        return f\"{seconds*1000000:.1f}μs\"\n",
    "\n",
    "def get_audio_size_mb(audio) -> float:\n",
    "    \"\"\"Calculate audio data size in MB.\"\"\"\n",
    "    np_array = audio.to_numpy()\n",
    "    return np_array.nbytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Benchmark iterations: {BENCHMARK_ITERATIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_audio():\n",
    "    \"\"\"Generate test audio files for all duration/channel combinations.\"\"\"\n",
    "    test_audio = {}\n",
    "    \n",
    "    print(\"Generating test audio...\")\n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            # Generate test signal using audio_samples\n",
    "            audio = aus.generation.sine_wave(\n",
    "                frequency=FREQUENCY,\n",
    "                duration_secs=duration,\n",
    "                sample_rate=SAMPLE_RATE,\n",
    "                amplitude=0.5\n",
    "            )\n",
    "            \n",
    "            # Convert to stereo if needed\n",
    "            if channels == 2:\n",
    "                # Create stereo by duplicating mono signal\n",
    "                mono_data = audio.to_numpy()\n",
    "                stereo_data = np.stack([mono_data, mono_data * 0.8])  # Slight variation for stereo\n",
    "                # Use from_array method instead of new_multi\n",
    "                try:\n",
    "                    audio = aus.AudioSamples.from_array(stereo_data, sample_rate=SAMPLE_RATE)\n",
    "                except:\n",
    "                    # Fallback: create manually if from_array not available\n",
    "                    print(f\"Warning: Could not create stereo audio for {duration}s, using mono\")\n",
    "                    # Keep mono audio instead\n",
    "            \n",
    "            # Store audio info\n",
    "            test_audio[(duration, channels)] = {\n",
    "                'audio': audio,\n",
    "                'size_mb': get_audio_size_mb(audio),\n",
    "                'samples_per_channel': int(duration * SAMPLE_RATE)\n",
    "            }\n",
    "            \n",
    "            print(f\"  {duration}s, {channels}ch: {get_audio_size_mb(audio):.2f} MB\")\n",
    "    \n",
    "    return test_audio\n",
    "\n",
    "# Generate all test audio\n",
    "test_audio = generate_test_audio()\n",
    "print(f\"\\nGenerated {len(test_audio)} test audio samples\")\n",
    "\n",
    "# Test what tensor methods are actually available\n",
    "print(\"\\nTesting available tensor methods...\")\n",
    "sample_audio = aus.generation.sine_wave(440.0, 1.0, sample_rate=44100)\n",
    "print(f\"to_numpy: {'✓' if hasattr(sample_audio, 'to_numpy') else '✗'}\")\n",
    "print(f\"__array__: {'✓' if hasattr(sample_audio, '__array__') else '✗'}\")\n",
    "print(f\"__array_ufunc__: {'✓' if hasattr(sample_audio, '__array_ufunc__') else '✗'}\")\n",
    "print(f\"__array_function__: {'✓' if hasattr(sample_audio, '__array_function__') else '✗'}\")\n",
    "print(f\"to_torch: {'✓' if hasattr(sample_audio, 'to_torch') else '✗'}\")\n",
    "print(f\"__dlpack__: {'✓' if hasattr(sample_audio, '__dlpack__') else '✗'}\")\n",
    "\n",
    "# Test actual functionality\n",
    "print(\"\\nTesting actual functionality...\")\n",
    "try:\n",
    "    arr = sample_audio.to_numpy()\n",
    "    print(\"✓ to_numpy() works\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ to_numpy() failed: {e}\")\n",
    "\n",
    "try:\n",
    "    arr = sample_audio.__array__(None)\n",
    "    print(\"✓ __array__() works\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ __array__() failed: {e}\")\n",
    "\n",
    "try:\n",
    "    arr = np.array(sample_audio)\n",
    "    print(\"✓ np.array() works\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ np.array() failed: {e}\")\n",
    "\n",
    "try:\n",
    "    result = np.multiply(sample_audio, 0.5)\n",
    "    print(\"✓ np.multiply() works\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ np.multiply() failed: {e}\")\n",
    "\n",
    "# Test workaround approach\n",
    "try:\n",
    "    arr = sample_audio.to_numpy()\n",
    "    result = np.multiply(arr, 0.5)\n",
    "    print(\"✓ np.multiply(audio.to_numpy(), 0.5) works (WORKAROUND)\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ workaround failed: {e}\")\n",
    "\n",
    "del sample_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Benchmarking Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(operation, iterations=BENCHMARK_ITERATIONS):\n",
    "    \"\"\"Benchmark an operation with multiple iterations.\"\"\"\n",
    "    times = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Ensure clean state between iterations\n",
    "        gc.collect()\n",
    "        \n",
    "        try:\n",
    "            start_time = time.perf_counter()\n",
    "            result = operation()\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            \n",
    "            # Clean up result to avoid memory accumulation\n",
    "            del result\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in benchmark iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not times:\n",
    "        return {\n",
    "            'mean_time': 0.0,\n",
    "            'std_time': 0.0,\n",
    "            'min_time': 0.0,\n",
    "            'max_time': 0.0\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## NumPy Array Protocol Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_numpy_protocols():\n",
    "    \"\"\"Benchmark NumPy array protocol methods.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"Benchmarking NumPy array protocol methods...\")\n",
    "    \n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            audio_info = test_audio[(duration, channels)]\n",
    "            audio = audio_info['audio']\n",
    "            size_mb = audio_info['size_mb']\n",
    "            \n",
    "            print(f\"  Testing {duration}s, {channels}ch...\")\n",
    "            \n",
    "            # Benchmark existing to_numpy (baseline)\n",
    "            def test_to_numpy():\n",
    "                return audio.to_numpy()\n",
    "            \n",
    "            to_numpy_stats = benchmark_operation(test_to_numpy)\n",
    "            to_numpy_throughput = calculate_throughput(size_mb, to_numpy_stats['mean_time'])\n",
    "            \n",
    "            # Benchmark __array__ protocol method directly\n",
    "            def test_array_method():\n",
    "                return audio.__array__(None)\n",
    "            \n",
    "            try:\n",
    "                array_method_stats = benchmark_operation(test_array_method)\n",
    "                array_method_throughput = calculate_throughput(size_mb, array_method_stats['mean_time'])\n",
    "            except Exception as e:\n",
    "                print(f\"    __array__ method failed: {e}\")\n",
    "                array_method_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                array_method_throughput = 0.0\n",
    "            \n",
    "            # Benchmark workaround: convert to numpy then operate\n",
    "            def test_numpy_multiply_workaround():\n",
    "                arr = audio.to_numpy()\n",
    "                return np.multiply(arr, 0.5)\n",
    "            \n",
    "            try:\n",
    "                numpy_multiply_stats = benchmark_operation(test_numpy_multiply_workaround)\n",
    "                numpy_multiply_throughput = calculate_throughput(size_mb, numpy_multiply_stats['mean_time'])\n",
    "            except Exception as e:\n",
    "                print(f\"    numpy multiply workaround failed: {e}\")\n",
    "                numpy_multiply_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                numpy_multiply_throughput = 0.0\n",
    "            \n",
    "            # Benchmark statistical operations with workaround\n",
    "            def test_numpy_mean_workaround():\n",
    "                arr = audio.to_numpy()\n",
    "                return np.mean(arr)\n",
    "            \n",
    "            try:\n",
    "                numpy_mean_stats = benchmark_operation(test_numpy_mean_workaround)\n",
    "                numpy_mean_throughput = calculate_throughput(size_mb, numpy_mean_stats['mean_time'])\n",
    "            except Exception as e:\n",
    "                print(f\"    numpy mean workaround failed: {e}\")\n",
    "                numpy_mean_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                numpy_mean_throughput = 0.0\n",
    "            \n",
    "            # Store results\n",
    "            results.extend([\n",
    "                {\n",
    "                    'method': 'to_numpy',\n",
    "                    'category': 'numpy_protocol',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': to_numpy_stats['mean_time'],\n",
    "                    'std_time': to_numpy_stats['std_time'],\n",
    "                    'throughput_mb_s': to_numpy_throughput\n",
    "                },\n",
    "                {\n",
    "                    'method': '__array__ method',\n",
    "                    'category': 'numpy_protocol',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': array_method_stats['mean_time'],\n",
    "                    'std_time': array_method_stats['std_time'],\n",
    "                    'throughput_mb_s': array_method_throughput\n",
    "                },\n",
    "                {\n",
    "                    'method': 'np.multiply(to_numpy)',\n",
    "                    'category': 'numpy_protocol',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': numpy_multiply_stats['mean_time'],\n",
    "                    'std_time': numpy_multiply_stats['std_time'],\n",
    "                    'throughput_mb_s': numpy_multiply_throughput\n",
    "                },\n",
    "                {\n",
    "                    'method': 'np.mean(to_numpy)',\n",
    "                    'category': 'numpy_protocol',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': numpy_mean_stats['mean_time'],\n",
    "                    'std_time': numpy_mean_stats['std_time'],\n",
    "                    'throughput_mb_s': numpy_mean_throughput\n",
    "                }\n",
    "            ])\n",
    "            \n",
    "            # Show progress\n",
    "            print(f\"    to_numpy:              {format_time(to_numpy_stats['mean_time'])}, {to_numpy_throughput:.1f} MB/s\")\n",
    "            print(f\"    __array__ method:      {format_time(array_method_stats['mean_time'])}, {array_method_throughput:.1f} MB/s\")\n",
    "            print(f\"    np.multiply(to_numpy): {format_time(numpy_multiply_stats['mean_time'])}, {numpy_multiply_throughput:.1f} MB/s\")\n",
    "            print(f\"    np.mean(to_numpy):     {format_time(numpy_mean_stats['mean_time'])}, {numpy_mean_throughput:.1f} MB/s\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run NumPy protocol benchmarks\n",
    "numpy_results = benchmark_numpy_protocols()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## DLPack Protocol Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking DLPack protocol methods...\n",
      "  DLPack methods not available - skipping DLPack benchmarks\n"
     ]
    }
   ],
   "source": [
    "def benchmark_dlpack_protocol():\n",
    "    \"\"\"Benchmark DLPack protocol methods.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"Benchmarking DLPack protocol methods...\")\n",
    "    \n",
    "    # Check if DLPack methods are available\n",
    "    sample_audio = aus.generation.sine_wave(440.0, 0.1, sample_rate=44100)\n",
    "    has_dlpack = hasattr(sample_audio, '__dlpack__')\n",
    "    has_dlpack_device = hasattr(sample_audio, '__dlpack_device__')\n",
    "    has_from_dlpack = hasattr(aus.AudioSamples, 'from_dlpack')\n",
    "    \n",
    "    if not has_dlpack:\n",
    "        print(\"  DLPack methods not available - skipping DLPack benchmarks\")\n",
    "        return results\n",
    "    \n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            audio_info = test_audio[(duration, channels)]\n",
    "            audio = audio_info['audio']\n",
    "            size_mb = audio_info['size_mb']\n",
    "            \n",
    "            print(f\"  Testing {duration}s, {channels}ch...\")\n",
    "            \n",
    "            # Benchmark __dlpack_device__ if available\n",
    "            if has_dlpack_device:\n",
    "                def test_dlpack_device():\n",
    "                    return audio.__dlpack_device__()\n",
    "                \n",
    "                try:\n",
    "                    dlpack_device_stats = benchmark_operation(test_dlpack_device)\n",
    "                    dlpack_device_throughput = calculate_throughput(size_mb, dlpack_device_stats['mean_time'])\n",
    "                except Exception as e:\n",
    "                    print(f\"    __dlpack_device__ failed: {e}\")\n",
    "                    dlpack_device_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                    dlpack_device_throughput = 0.0\n",
    "            else:\n",
    "                dlpack_device_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                dlpack_device_throughput = 0.0\n",
    "            \n",
    "            # Benchmark __dlpack__ export\n",
    "            def test_dlpack_export():\n",
    "                return audio.__dlpack__()\n",
    "            \n",
    "            try:\n",
    "                dlpack_export_stats = benchmark_operation(test_dlpack_export)\n",
    "                dlpack_export_throughput = calculate_throughput(size_mb, dlpack_export_stats['mean_time'])\n",
    "            except Exception as e:\n",
    "                print(f\"    __dlpack__ failed: {e}\")\n",
    "                dlpack_export_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                dlpack_export_throughput = 0.0\n",
    "            \n",
    "            # Benchmark from_dlpack (if available and export worked)\n",
    "            if has_from_dlpack and dlpack_export_stats['mean_time'] > 0:\n",
    "                try:\n",
    "                    # Get a dlpack tensor for testing\n",
    "                    dlpack_tensor = audio.__dlpack__()\n",
    "                    \n",
    "                    def test_from_dlpack():\n",
    "                        return aus.AudioSamples.from_dlpack(dlpack_tensor, SAMPLE_RATE)\n",
    "                    \n",
    "                    # Note: We can only test this once per dlpack tensor\n",
    "                    # So we'll do a simpler benchmark\n",
    "                    start_time = time.perf_counter()\n",
    "                    restored_audio = aus.AudioSamples.from_dlpack(dlpack_tensor, SAMPLE_RATE)\n",
    "                    end_time = time.perf_counter()\n",
    "                    \n",
    "                    from_dlpack_time = end_time - start_time\n",
    "                    from_dlpack_throughput = calculate_throughput(size_mb, from_dlpack_time)\n",
    "                    \n",
    "                    from_dlpack_stats = {\n",
    "                        'mean_time': from_dlpack_time,\n",
    "                        'std_time': 0.0  # Single measurement\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"    from_dlpack failed: {e}\")\n",
    "                    from_dlpack_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                    from_dlpack_throughput = 0.0\n",
    "            else:\n",
    "                from_dlpack_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                from_dlpack_throughput = 0.0\n",
    "            \n",
    "            # Store results\n",
    "            if has_dlpack_device and dlpack_device_stats['mean_time'] > 0:\n",
    "                results.append({\n",
    "                    'method': '__dlpack_device__',\n",
    "                    'category': 'dlpack',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': dlpack_device_stats['mean_time'],\n",
    "                    'std_time': dlpack_device_stats['std_time'],\n",
    "                    'throughput_mb_s': dlpack_device_throughput\n",
    "                })\n",
    "            \n",
    "            if dlpack_export_stats['mean_time'] > 0:\n",
    "                results.append({\n",
    "                    'method': '__dlpack__',\n",
    "                    'category': 'dlpack',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': dlpack_export_stats['mean_time'],\n",
    "                    'std_time': dlpack_export_stats['std_time'],\n",
    "                    'throughput_mb_s': dlpack_export_throughput\n",
    "                })\n",
    "            \n",
    "            if from_dlpack_stats['mean_time'] > 0:\n",
    "                results.append({\n",
    "                    'method': 'from_dlpack',\n",
    "                    'category': 'dlpack',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': from_dlpack_stats['mean_time'],\n",
    "                    'std_time': from_dlpack_stats['std_time'],\n",
    "                    'throughput_mb_s': from_dlpack_throughput\n",
    "                })\n",
    "            \n",
    "            # Show progress\n",
    "            if has_dlpack_device:\n",
    "                print(f\"    __dlpack_device__: {format_time(dlpack_device_stats['mean_time'])}, {dlpack_device_throughput:.1f} MB/s\")\n",
    "            print(f\"    __dlpack__:        {format_time(dlpack_export_stats['mean_time'])}, {dlpack_export_throughput:.1f} MB/s\")\n",
    "            print(f\"    from_dlpack:       {format_time(from_dlpack_stats['mean_time'])}, {from_dlpack_throughput:.1f} MB/s\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run DLPack protocol benchmarks\n",
    "dlpack_results = benchmark_dlpack_protocol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031bade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## PyTorch Integration Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pytorch_integration():\n",
    "    \"\"\"Benchmark PyTorch integration methods.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if not TORCH_AVAILABLE:\n",
    "        print(\"PyTorch not available - skipping PyTorch benchmarks\")\n",
    "        return results\n",
    "    \n",
    "    print(\"Benchmarking PyTorch integration methods...\")\n",
    "    \n",
    "    # Check if PyTorch methods are available\n",
    "    sample_audio = aus.generation.sine_wave(440.0, 0.1, sample_rate=44100)\n",
    "    has_to_torch = hasattr(sample_audio, 'to_torch')\n",
    "    has_from_torch = hasattr(aus.AudioSamples, 'from_torch')\n",
    "    \n",
    "    if not has_to_torch:\n",
    "        print(\"  PyTorch methods not available - testing workaround approach\")\n",
    "    \n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            audio_info = test_audio[(duration, channels)]\n",
    "            audio = audio_info['audio']\n",
    "            size_mb = audio_info['size_mb']\n",
    "            \n",
    "            print(f\"  Testing {duration}s, {channels}ch...\")\n",
    "            \n",
    "            # Benchmark to_torch if available\n",
    "            if has_to_torch:\n",
    "                def test_to_torch():\n",
    "                    return audio.to_torch()\n",
    "                \n",
    "                try:\n",
    "                    to_torch_stats = benchmark_operation(test_to_torch)\n",
    "                    to_torch_throughput = calculate_throughput(size_mb, to_torch_stats['mean_time'])\n",
    "                except Exception as e:\n",
    "                    print(f\"    to_torch failed: {e}\")\n",
    "                    to_torch_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                    to_torch_throughput = 0.0\n",
    "            else:\n",
    "                to_torch_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                to_torch_throughput = 0.0\n",
    "            \n",
    "            # Benchmark torch.from_numpy(audio.to_numpy()) as workaround/baseline\n",
    "            def test_torch_from_numpy():\n",
    "                np_array = audio.to_numpy()\n",
    "                return torch.from_numpy(np_array)\n",
    "            \n",
    "            try:\n",
    "                torch_numpy_stats = benchmark_operation(test_torch_from_numpy)\n",
    "                torch_numpy_throughput = calculate_throughput(size_mb, torch_numpy_stats['mean_time'])\n",
    "            except Exception as e:\n",
    "                print(f\"    torch.from_numpy failed: {e}\")\n",
    "                torch_numpy_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                torch_numpy_throughput = 0.0\n",
    "            \n",
    "            # Benchmark numpy->torch->back conversion workflow\n",
    "            def test_torch_roundtrip_via_numpy():\n",
    "                np_array = audio.to_numpy()\n",
    "                torch_tensor = torch.from_numpy(np_array)\n",
    "                return torch_tensor.numpy()\n",
    "            \n",
    "            try:\n",
    "                torch_roundtrip_stats = benchmark_operation(test_torch_roundtrip_via_numpy)\n",
    "                torch_roundtrip_throughput = calculate_throughput(size_mb, torch_roundtrip_stats['mean_time'])\n",
    "            except Exception as e:\n",
    "                print(f\"    torch roundtrip failed: {e}\")\n",
    "                torch_roundtrip_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                torch_roundtrip_throughput = 0.0\n",
    "            \n",
    "            # Benchmark from_torch (if available and to_torch worked)\n",
    "            if has_from_torch and has_to_torch and to_torch_stats['mean_time'] > 0:\n",
    "                try:\n",
    "                    # Get a torch tensor for testing\n",
    "                    torch_tensor = audio.to_torch()\n",
    "                    \n",
    "                    def test_from_torch():\n",
    "                        return aus.AudioSamples.from_torch(torch_tensor, SAMPLE_RATE)\n",
    "                    \n",
    "                    from_torch_stats = benchmark_operation(test_from_torch)\n",
    "                    from_torch_throughput = calculate_throughput(size_mb, from_torch_stats['mean_time'])\n",
    "                except Exception as e:\n",
    "                    print(f\"    from_torch failed: {e}\")\n",
    "                    from_torch_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                    from_torch_throughput = 0.0\n",
    "            elif has_from_torch and torch_numpy_stats['mean_time'] > 0:\n",
    "                # Test from_torch using the numpy->torch workaround\n",
    "                try:\n",
    "                    # Create torch tensor via numpy\n",
    "                    np_array = audio.to_numpy()\n",
    "                    torch_tensor = torch.from_numpy(np_array.copy())  # Copy to avoid shared memory issues\n",
    "                    \n",
    "                    def test_from_torch_via_numpy():\n",
    "                        return aus.AudioSamples.from_torch(torch_tensor, SAMPLE_RATE)\n",
    "                    \n",
    "                    from_torch_stats = benchmark_operation(test_from_torch_via_numpy)\n",
    "                    from_torch_throughput = calculate_throughput(size_mb, from_torch_stats['mean_time'])\n",
    "                except Exception as e:\n",
    "                    print(f\"    from_torch via numpy failed: {e}\")\n",
    "                    from_torch_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                    from_torch_throughput = 0.0\n",
    "            else:\n",
    "                from_torch_stats = {'mean_time': 0.0, 'std_time': 0.0}\n",
    "                from_torch_throughput = 0.0\n",
    "            \n",
    "            # Store results (only for methods that actually worked)\n",
    "            if has_to_torch and to_torch_stats['mean_time'] > 0:\n",
    "                results.append({\n",
    "                    'method': 'to_torch',\n",
    "                    'category': 'pytorch',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': to_torch_stats['mean_time'],\n",
    "                    'std_time': to_torch_stats['std_time'],\n",
    "                    'throughput_mb_s': to_torch_throughput\n",
    "                })\n",
    "            \n",
    "            if torch_numpy_stats['mean_time'] > 0:\n",
    "                results.append({\n",
    "                    'method': 'torch.from_numpy',\n",
    "                    'category': 'pytorch',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': torch_numpy_stats['mean_time'],\n",
    "                    'std_time': torch_numpy_stats['std_time'],\n",
    "                    'throughput_mb_s': torch_numpy_throughput\n",
    "                })\n",
    "            \n",
    "            if torch_roundtrip_stats['mean_time'] > 0:\n",
    "                results.append({\n",
    "                    'method': 'torch_roundtrip_numpy',\n",
    "                    'category': 'pytorch',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': torch_roundtrip_stats['mean_time'],\n",
    "                    'std_time': torch_roundtrip_stats['std_time'],\n",
    "                    'throughput_mb_s': torch_roundtrip_throughput\n",
    "                })\n",
    "            \n",
    "            if has_from_torch and from_torch_stats['mean_time'] > 0:\n",
    "                results.append({\n",
    "                    'method': 'from_torch',\n",
    "                    'category': 'pytorch',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'size_mb': size_mb,\n",
    "                    'mean_time': from_torch_stats['mean_time'],\n",
    "                    'std_time': from_torch_stats['std_time'],\n",
    "                    'throughput_mb_s': from_torch_throughput\n",
    "                })\n",
    "            \n",
    "            # Show progress\n",
    "            if has_to_torch:\n",
    "                print(f\"    to_torch:          {format_time(to_torch_stats['mean_time'])}, {to_torch_throughput:.1f} MB/s\")\n",
    "            print(f\"    torch.from_numpy:  {format_time(torch_numpy_stats['mean_time'])}, {torch_numpy_throughput:.1f} MB/s\")\n",
    "            print(f\"    torch roundtrip:   {format_time(torch_roundtrip_stats['mean_time'])}, {torch_roundtrip_throughput:.1f} MB/s\")\n",
    "            if has_from_torch:\n",
    "                print(f\"    from_torch:        {format_time(from_torch_stats['mean_time'])}, {from_torch_throughput:.1f} MB/s\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run PyTorch integration benchmarks\n",
    "pytorch_results = benchmark_pytorch_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = numpy_results + dlpack_results + pytorch_results\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"Tensor Interoperability Benchmark Results Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall statistics by category\n",
    "for category in df['category'].unique():\n",
    "    print(f\"\\n{category.upper()} Methods:\")\n",
    "    cat_data = df[df['category'] == category]\n",
    "    \n",
    "    for method in cat_data['method'].unique():\n",
    "        method_data = cat_data[cat_data['method'] == method]\n",
    "        if len(method_data) > 0:\n",
    "            avg_time = method_data['mean_time'].mean()\n",
    "            avg_throughput = method_data['throughput_mb_s'].mean()\n",
    "            \n",
    "            print(f\"  {method:16}: {format_time(avg_time):>8} avg, {avg_throughput:>6.1f} MB/s avg\")\n",
    "\n",
    "# Save results\n",
    "results_file = os.path.join(temp_dir, \"tensor_benchmark_results.csv\")\n",
    "df.to_csv(results_file, index=False)\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(f\"Total benchmarks: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for figures\n",
    "fig_dir = os.path.join(temp_dir, \"figures\")\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "def save_figure(fig, name, dpi=300):\n",
    "    \"\"\"Save figure as both PDF and PNG with high quality.\"\"\"\n",
    "    pdf_path = os.path.join(fig_dir, f\"{name}.pdf\")\n",
    "    png_path = os.path.join(fig_dir, f\"{name}.png\")\n",
    "    \n",
    "    fig.savefig(pdf_path, format='pdf', dpi=dpi, bbox_inches='tight', \n",
    "               facecolor='white', edgecolor='none')\n",
    "    fig.savefig(png_path, format='png', dpi=dpi, bbox_inches='tight',\n",
    "               facecolor='white', edgecolor='none')\n",
    "    \n",
    "    print(f\"Saved: {pdf_path}\")\n",
    "    print(f\"Saved: {png_path}\")\n",
    "\n",
    "# Define consistent colors and markers\n",
    "marker_map = {1: 'o', 2: 's'}  # circle for mono, square for stereo\n",
    "\n",
    "def plot_category_performance(df, category, title):\n",
    "    \"\"\"Plot performance for a specific category.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    cat_data = df[df['category'] == category]\n",
    "    methods = cat_data['method'].unique()\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        for channels in CHANNELS:\n",
    "            subset = cat_data[(cat_data['method'] == method) & (cat_data['channels'] == channels)]\n",
    "            if len(subset) > 0:\n",
    "                channel_name = 'mono' if channels == 1 else 'stereo'\n",
    "                ax.plot(subset['duration'], subset['mean_time'],\n",
    "                       marker=marker_map[channels], label=f\"{method} ({channel_name})\",\n",
    "                       color=OKABE_ITO_COLORS[i], linewidth=4, markersize=12)\n",
    "\n",
    "    ax.set_xlabel('Audio Duration (seconds)', fontweight='bold')\n",
    "    ax.set_ylabel('Time (seconds)', fontweight='bold')\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    legend = ax.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', ncol=2)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_alpha(0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    return fig\n",
    "\n",
    "# Generate plots for each category\n",
    "if len(numpy_results) > 0:\n",
    "    numpy_fig = plot_category_performance(df, 'numpy_protocol', 'NumPy Array Protocol Performance')\n",
    "    save_figure(numpy_fig, \"numpy_protocol_performance\")\n",
    "    plt.show()\n",
    "\n",
    "if len(dlpack_results) > 0:\n",
    "    dlpack_fig = plot_category_performance(df, 'dlpack', 'DLPack Protocol Performance')\n",
    "    save_figure(dlpack_fig, \"dlpack_protocol_performance\")\n",
    "    plt.show()\n",
    "\n",
    "if len(pytorch_results) > 0:\n",
    "    pytorch_fig = plot_category_performance(df, 'pytorch', 'PyTorch Integration Performance')\n",
    "    save_figure(pytorch_fig, \"pytorch_integration_performance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput comparison\n",
    "if len(df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(15, 9))\n",
    "    \n",
    "    # Calculate average throughput by method\n",
    "    throughput_data = df.groupby(['category', 'method'])['throughput_mb_s'].mean().reset_index()\n",
    "    \n",
    "    # Create combined labels\n",
    "    throughput_data['method_label'] = throughput_data['category'] + '\\n' + throughput_data['method']\n",
    "    \n",
    "    # Plot\n",
    "    bars = ax.bar(range(len(throughput_data)), throughput_data['throughput_mb_s'], \n",
    "                  color=OKABE_ITO_COLORS[:len(throughput_data)])\n",
    "    \n",
    "    ax.set_xlabel('Method', fontweight='bold')\n",
    "    ax.set_ylabel('Average Throughput (MB/s)', fontweight='bold')\n",
    "    ax.set_title('Tensor Interoperability Throughput Comparison', fontweight='bold')\n",
    "    ax.set_xticks(range(len(throughput_data)))\n",
    "    ax.set_xticklabels(throughput_data['method_label'], rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, throughput_data['throughput_mb_s'])):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "               f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_figure(fig, \"throughput_comparison\")\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nAll figures saved to: {fig_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TENSOR INTEROPERABILITY BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Find fastest methods in each category\n",
    "    print(\"\\nFastest Methods by Category:\")\n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]\n",
    "        fastest = cat_data.loc[cat_data['throughput_mb_s'].idxmax()]\n",
    "        print(f\"  {category}: {fastest['method']} ({fastest['throughput_mb_s']:.1f} MB/s)\")\n",
    "    \n",
    "    # Overall performance insights\n",
    "    print(\"\\nPerformance Insights:\")\n",
    "    \n",
    "    # NumPy protocol comparison\n",
    "    numpy_data = df[df['category'] == 'numpy_protocol']\n",
    "    if len(numpy_data) > 0:\n",
    "        to_numpy_avg = numpy_data[numpy_data['method'] == 'to_numpy']['throughput_mb_s'].mean()\n",
    "        array_method_avg = numpy_data[numpy_data['method'] == '__array__ method']['throughput_mb_s'].mean()\n",
    "        \n",
    "        if array_method_avg > 0 and to_numpy_avg > 0:\n",
    "            ratio = to_numpy_avg / array_method_avg\n",
    "            faster_method = \"to_numpy\" if ratio > 1 else \"__array__ method\"\n",
    "            print(f\"• {faster_method} is {max(ratio, 1/ratio):.2f}x faster than {'__array__ method' if ratio > 1 else 'to_numpy'}\")\n",
    "        \n",
    "        # Check if workaround operations are efficient\n",
    "        multiply_avg = numpy_data[numpy_data['method'] == 'np.multiply(to_numpy)']['throughput_mb_s'].mean()\n",
    "        if multiply_avg > 0 and to_numpy_avg > 0:\n",
    "            ratio = multiply_avg / to_numpy_avg\n",
    "            print(f\"• NumPy operations via to_numpy() add {(1-ratio)*100:.1f}% overhead\")\n",
    "    \n",
    "    # PyTorch comparison\n",
    "    torch_data = df[df['category'] == 'pytorch']\n",
    "    if len(torch_data) > 0:\n",
    "        torch_numpy_avg = torch_data[torch_data['method'] == 'torch.from_numpy']['throughput_mb_s'].mean()\n",
    "        roundtrip_avg = torch_data[torch_data['method'] == 'torch_roundtrip_numpy']['throughput_mb_s'].mean()\n",
    "        \n",
    "        if torch_numpy_avg > 0 and roundtrip_avg > 0:\n",
    "            ratio = torch_numpy_avg / roundtrip_avg  \n",
    "            print(f\"• torch.from_numpy is {ratio:.2f}x faster than full roundtrip conversion\")\n",
    "        \n",
    "        # Check if native to_torch exists\n",
    "        to_torch_data = torch_data[torch_data['method'] == 'to_torch']\n",
    "        if len(to_torch_data) > 0:\n",
    "            to_torch_avg = to_torch_data['throughput_mb_s'].mean()\n",
    "            if torch_numpy_avg > 0:\n",
    "                ratio = to_torch_avg / torch_numpy_avg\n",
    "                faster_method = \"to_torch\" if ratio > 1 else \"torch.from_numpy\"\n",
    "                print(f\"• {faster_method} is {max(ratio, 1/ratio):.2f}x faster than {'torch.from_numpy' if ratio > 1 else 'to_torch'}\")\n",
    "        else:\n",
    "            print(\"• Native to_torch method not available - use torch.from_numpy(audio.to_numpy()) workaround\")\n",
    "    \n",
    "    # DLPack assessment\n",
    "    dlpack_data = df[df['category'] == 'dlpack']\n",
    "    if len(dlpack_data) == 0:\n",
    "        print(\"• DLPack methods not available - feature not implemented\")\n",
    "else:\n",
    "    print(\"\\nNo benchmark results available for analysis.\")\n",
    "\n",
    "print(f\"\\nTest Configuration:\")\n",
    "print(f\"  Sample Rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"  Duration Range: {min(DURATIONS)}-{max(DURATIONS)} seconds\")\n",
    "print(f\"  Iterations per test: {BENCHMARK_ITERATIONS}\")\n",
    "print(f\"  Total benchmarks: {len(all_results)}\")\n",
    "\n",
    "# Implementation status summary\n",
    "print(f\"\\nImplementation Status:\")\n",
    "sample_audio = aus.generation.sine_wave(440.0, 0.1, sample_rate=44100)\n",
    "print(f\"  ✓ to_numpy() - Core NumPy conversion\")\n",
    "print(f\"  ✓ __array__() method - Array protocol support\")\n",
    "print(f\"  {'✓' if hasattr(sample_audio, '__array_ufunc__') else '✗'} __array_ufunc__ - Direct NumPy ufunc support\")\n",
    "print(f\"  {'✓' if hasattr(sample_audio, '__array_function__') else '✗'} __array_function__ - Direct NumPy function support\")\n",
    "print(f\"  {'✓' if hasattr(sample_audio, 'to_torch') else '✗'} to_torch() - Native PyTorch conversion\")\n",
    "print(f\"  {'✓' if hasattr(aus.AudioSamples, 'from_torch') else '✗'} from_torch() - PyTorch to AudioSamples\")\n",
    "print(f\"  {'✓' if hasattr(sample_audio, '__dlpack__') else '✗'} DLPack protocol - Zero-copy tensor exchange\")\n",
    "\n",
    "print(f\"\\nRecommended Usage:\")\n",
    "print(f\"  • For NumPy operations: use audio.to_numpy() then standard NumPy functions\")\n",
    "print(f\"  • For PyTorch operations: use torch.from_numpy(audio.to_numpy())\")\n",
    "print(f\"  • Performance is excellent for these workaround approaches\")\n",
    "\n",
    "print(\"\\nBenchmark complete!\")\n",
    "print(f\"Results and figures saved to: {temp_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_samples_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
