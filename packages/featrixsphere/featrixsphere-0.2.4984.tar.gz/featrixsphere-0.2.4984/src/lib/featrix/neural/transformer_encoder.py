import math
from dataclasses import dataclass
from typing import Optional
import logging

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint

from featrix.neural.featrix_module_dict import FeatrixModuleDict
from featrix.neural.model_config import JointEncoderConfig
from featrix.neural.simple_mlp import SimpleMLP
from featrix.neural.relationship_extractor import RelationshipFeatureExtractor
from featrix.neural.gpu_utils import is_gpu_available, get_gpu_memory_allocated, get_gpu_memory_reserved

logger = logging.getLogger(__name__)

def _log_gpu_memory_transformer(context: str = ""):
    """Quick GPU memory logging for tracing memory usage in transformer_encoder."""
    try:
        if not is_gpu_available():
            return
        allocated = get_gpu_memory_allocated()
        reserved = get_gpu_memory_reserved()
        logger.info(f"ðŸ“Š GPU [{context}]: Alloc={allocated:.2f}GB Reserved={reserved:.2f}GB")
    except Exception:
        pass


class AddCLSToken(nn.Module):
    def __init__(self, d_model):
        super().__init__()

        self.d_model = d_model

        # Initialize the [CLS] token as a learnable parameter
        self.cls_token = nn.Parameter(
            torch.randn(self.d_model) / math.sqrt(self.d_model)
        )

    def forward(self, x):
        # x has shape (B, S, F)

        # Replicate the [CLS] token for all sequences in the batch
        # self.cls_token has shape (F,)
        cls_tokens = self.cls_token.repeat(x.size(0), 1, 1)  # Shape: (B, 1, F)

        # Concatenate the [CLS] token with sequences
        x = torch.cat([cls_tokens, x], dim=1)  # Shape: (B, S+1, F)

        return x


class ColumnEncoding(nn.Module):
    def __init__(self, d_model, seq_len):
        super().__init__()

        #  we add an encoding for the cls token, which always gets prepended as the 0th element
        n_encodings = seq_len

        # Initialize a tensor for positional encodings and set it as a learnable parameter
        # NOTE: this assumes the input comes in the form (sequence, feature)
        self.pos_embedding = nn.Parameter(
            torch.randn(n_encodings, d_model) / math.sqrt(d_model)
        )

        # NOTE: this could be accomplished with an Embedding module, but that would get more
        # cumbersome. Going with a simple single parameter as above is much better.

    def forward(self, x):
        # assume x is in the format (batch, seq, feat)
        # the positional embedding_space gets broadcast across the batch dimension

        return x + self.pos_embedding


# def apply_network_to_tensor(data, network):
#     N, M, K = data.shape

#     # Reshape data to (N*M, K) for batch processing
#     data_reshaped = data.view(N * M, K)

#     # Apply network
#     output = network(data_reshaped)

#     # Reshape output back to (N, M, output_dim)
#     # Assuming output dimension is the same as input K for simplicity
#     output_dim = output.shape[-1]
#     output_reshaped = output.view(N, M, output_dim)

#     return output_reshaped


class JointEncoder(nn.Module):
    def __init__(self, d_embed, col_names_in_order, config: JointEncoderConfig, hybrid_groups=None, enable_gradient_checkpointing: bool = True):
        super().__init__()

        self.d_embed = d_embed
        self.config = config
        self.enable_gradient_checkpointing = enable_gradient_checkpointing

        self.col_names_in_order = col_names_in_order
        self.hybrid_groups = hybrid_groups or {}
        
        # Create in_converters, handling hybrid group names
        in_converters_dict = {}
        for col_name in col_names_in_order:
            # Check if this is a hybrid group name
            if col_name in self.hybrid_groups:
                # For hybrid groups, use the config from the first original column
                group_info = self.hybrid_groups[col_name]
                original_columns = group_info.get('columns', [])
                if original_columns:
                    # Use config from first column in the group
                    original_col_name = original_columns[0]
                    if original_col_name in config.in_converter_configs:
                        in_converters_dict[col_name] = SimpleMLP(config.in_converter_configs[original_col_name])
                    else:
                        raise KeyError(f"Hybrid group '{col_name}' references column '{original_col_name}' which is not in in_converter_configs. Available keys: {list(config.in_converter_configs.keys())}")
                else:
                    raise ValueError(f"Hybrid group '{col_name}' has no columns defined")
            else:
                # Regular column, use its config directly
                if col_name in config.in_converter_configs:
                    in_converters_dict[col_name] = SimpleMLP(config.in_converter_configs[col_name])
                else:
                    raise KeyError(f"Column '{col_name}' not found in in_converter_configs. Available keys: {list(config.in_converter_configs.keys())}")
        
        self.in_converters = FeatrixModuleDict(in_converters_dict)

        self.out_converter = SimpleMLP(config=config.out_converter_config)
        self.batch_norm_out = nn.BatchNorm1d(d_embed)

        # Dynamic Relationship Extractor (always enabled if relationship_features config exists)
        self.relationship_extractor = None
        if config.relationship_features is not None:
            from featrix.neural.dynamic_relationship_extractor import DynamicRelationshipExtractor
            
            rel_config = config.relationship_features
            exploration_epochs = getattr(rel_config, 'exploration_epochs', 5)
            top_k_fraction = getattr(rel_config, 'top_k_fraction', 0.25)
            
            self.relationship_extractor = DynamicRelationshipExtractor(
                d_model=config.d_model,
                col_names_in_order=col_names_in_order,
                exploration_epochs=exploration_epochs,
                top_k_fraction=top_k_fraction,
            )
            logger.info(
                f"ðŸ”— JointEncoder: Dynamic relationship extractor enabled "
                f"(exploration_epochs={exploration_epochs}, "
                f"top_k_fraction={top_k_fraction}, "
                f"operations=6 per pair: *, +, -, /, both directions)"
            )

        # Trainable Positional Encoding
        # NOTE: Positional encoding needs to account for:
        # - Column tokens (positions 0 to n_cols-1) - ColumnEncoding receives input BEFORE CLS token
        # - CLS token (position n_cols) - added after ColumnEncoding
        # - Relationship tokens (positions n_cols+1 onwards, variable count)
        # ColumnEncoding is called BEFORE CLS token is added, so it only needs n_cols positions
        # Relationship tokens get positional encodings manually from the extended embedding
        col_seq_len = config.n_cols  # Columns only (CLS added later)
        
        # Calculate max sequence length for positional embedding
        # With POOLED RELATIONSHIP INJECTION, we no longer concatenate relationship tokens
        # to the sequence, so seq_len = 1 (CLS) + n_cols only
        # This is the key to scaling: attention is O(NÂ²) not O((N + NÂ²)Â²)
        max_seq_len = 1 + config.n_cols  # CLS + columns only
        
        if getattr(self, 'relationship_extractor', None):
            # Relationship tokens are now pooled and injected into CLS, not concatenated
            # Log the relationship token count for debugging purposes
            n_pairs = config.n_cols * (config.n_cols - 1) // 2
            # Query ops_per_pair from extractor (1 if fused, 9 if unfused)
            operations_per_pair = getattr(self.relationship_extractor, 'ops_per_pair', 1)
            n_rel_tokens = n_pairs * operations_per_pair
            fusion_mode = "FUSED" if operations_per_pair == 1 else "UNFUSED"
            logger.info(f"   Relationship tokens: {n_rel_tokens} ({fusion_mode}, pooled, not in sequence)")
            logger.info(f"   Sequence length: {max_seq_len} (CLS + {config.n_cols} cols) - SCALABLE")
        
        if config.use_col_encoding:
            # ColumnEncoding receives columns BEFORE CLS token, so use col_seq_len
            self.col_encoder = ColumnEncoding(self.config.d_model, col_seq_len)
            # Store max_seq_len for relationship token positional encoding
            self.max_seq_len = max_seq_len

        # simple module to add the cls token to create the joint encoding for the whole sequence
        self.add_cls_token = AddCLSToken(self.config.d_model)

        # HYBRID COLUMN SUPPORT: Setup hybrid relationships (hybrid_groups already stored above)
        self._setup_hybrid_relationships()

        # Transformer Encoder with batch_first=True for better performance
        encoder_layer = nn.TransformerEncoderLayer(
            self.config.d_model,
            self.config.n_heads,
            dim_feedforward=self.config.d_model * self.config.dim_feedforward_factor,
            dropout=self.config.dropout,
            batch_first=True,  # Enable nested tensor optimization
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, num_layers=self.config.n_layers
        )
        
        # Enable gradient checkpointing to save memory (trades compute for memory)
        # This reduces activation memory by ~N_layers times at ~30% compute cost
        # Implemented using torch.utils.checkpoint.checkpoint() wrapper in forward()
        if self.enable_gradient_checkpointing and self.config.n_layers > 1:
            logger.info(f"ðŸ”‹ Enabling gradient checkpointing on {self.config.n_layers}-layer transformer (saves ~{self.config.n_layers}Ã— activation memory)")
        
        # Storage for attention weights (for diagnostics)
        self._attention_weights = None
        self._enable_attention_capture = False
    
    def update_mi_estimates(
        self,
        col_mi_estimates: dict,
        joint_mi_estimate: Optional[float] = None,
    ):
        """Update mutual information estimates in relationship extractor."""
        relationship_extractor = getattr(self, 'relationship_extractor', None)
        if relationship_extractor and hasattr(relationship_extractor, 'update_mi_estimates'):
            relationship_extractor.update_mi_estimates(
                col_mi_estimates, joint_mi_estimate
            )
    
    def update_column_losses(self, col_losses_dict: dict):
        """Update per-column marginal losses in relationship extractor (for importance calculation)."""
        relationship_extractor = getattr(self, 'relationship_extractor', None)
        if relationship_extractor and hasattr(relationship_extractor, 'update_column_losses'):
            relationship_extractor.update_column_losses(col_losses_dict)

    def _setup_hybrid_relationships(self):
        """
        Setup hybrid relationship embeddings and metadata.
        
        For RELATIONSHIP strategy groups, we add learned group embeddings
        that get added to related columns to help the transformer learn their relationships.
        """
        # Filter for RELATIONSHIP strategy groups only
        relationship_groups = {
            name: info for name, info in self.hybrid_groups.items()
            if info.get('strategy') == 'relationship'
        }
        
        if not relationship_groups:
            self.group_embeddings = None
            self.col_to_group = {}
            logger.debug("No RELATIONSHIP hybrid groups detected")
            return
        
        # Create learned group embeddings
        n_groups = len(relationship_groups)
        self.group_embeddings = nn.Parameter(
            torch.randn(n_groups, self.config.d_model) / math.sqrt(self.config.d_model)
        )
        
        # Create mapping from column index to group index
        self.col_to_group = {}
        for group_idx, (group_name, group_info) in enumerate(relationship_groups.items()):
            for col_name in group_info['columns']:
                if col_name in self.col_names_in_order:
                    col_idx = self.col_names_in_order.index(col_name)
                    self.col_to_group[col_idx] = group_idx
        
        logger.info(f"ðŸ”— JointEncoder hybrid relationships: {n_groups} groups covering {len(self.col_to_group)} columns")
        for group_name, group_info in relationship_groups.items():
            logger.info(f"   {group_name}: {group_info['type']} - {group_info['columns']}")
    
    def enable_attention_capture(self):
        """Enable capturing attention weights for diagnostic analysis."""
        self._enable_attention_capture = True
    
    def disable_attention_capture(self):
        """Disable capturing attention weights (default, saves memory)."""
        self._enable_attention_capture = False
    
    def get_attention_weights(self):
        """Return captured attention weights. Returns None if capture is disabled."""
        return self._attention_weights

    def forward(self, x, mask: Optional[torch.Tensor] = None):
        # NOTE: we're using a typical transformer encoder here, where all columns get their
        # own encoding. Then we throw away all encodings except the one for the [CLS] token.
        # A more efficient implementation would apply attention directly to inputs and produce
        # only one encoding.

        # FIXME: should the converter be the same for all variables, or should we use different converters for
        # different input variables?
        # ANSWER: using one encoder is fine, at least on smaller datasets.
        # On larger datasets we may want to revisit.

        # x: (batch_size, n_cols, d_model) - column encodings
        # mask: (batch_size, n_cols) - TokenStatus mask (optional)
        
        # DEBUG: MPS INT_MAX - Track forward calls
        _debug_count = getattr(self, '_debug_forward_count', 0)
        self._debug_forward_count = _debug_count + 1
        _should_debug = _debug_count < 5  # Only first 5 calls
        
        if _should_debug:
            _log_gpu_memory_transformer(f"JointEncoder.forward #{_debug_count} START")
            logger.info(f"[DEBUG] JointEncoder.forward() #{_debug_count}")
            logger.info(f"[DEBUG]   Input x shape: {x.shape}, numel: {x.numel()}")

        col_x = []
        for i, col_name in enumerate(self.col_names_in_order):
            converter = self.in_converters[col_name]
            col_x.append(converter(x[:, i, :]))

        x = torch.stack(col_x, dim=1)  # (batch_size, n_cols, d_model)

        # Compute relationship features BEFORE positional encoding
        relationship_tokens = []
        relationship_extractor = getattr(self, 'relationship_extractor', None)
        if relationship_extractor is not None:
            relationship_tokens = relationship_extractor.forward(x, mask)
            # relationship_tokens: List of (batch_size, d_model) tensors
            if _should_debug:
                logger.info(f"[DEBUG]   Relationship tokens: {len(relationship_tokens)} tokens")

        # Add column encodings first, then cls token
        # this means the cls token does not get a positional encoding, but
        # that's OK because only one token gets placed in that position anyway.
        # This simplifies things because this way the positional encoder does
        # not need to worry about adding a positional encoding for the cls token.
        if self.config.use_col_encoding:
            x = self.col_encoder(x)

        # HYBRID COLUMN SUPPORT: Add group embeddings for RELATIONSHIP strategy
        # Backwards compatibility: older models don't have group_embeddings
        if hasattr(self, 'group_embeddings') and self.group_embeddings is not None and len(getattr(self, 'col_to_group', {})) > 0:
            for col_idx, group_idx in self.col_to_group.items():
                # Add group embedding to this column's encoding
                # This helps the transformer learn that these columns are related
                x[:, col_idx, :] = x[:, col_idx, :] + self.group_embeddings[group_idx]

        # Add CLS token
        x = self.add_cls_token(x)  # (batch_size, 1 + n_cols, d_model)

        # ============================================================================
        # SCALABLE RELATIONSHIP HANDLING
        # ============================================================================
        # Problem: Concatenating all relationship tokens creates O((N + NÂ²)Â²) attention
        # which overflows INT_MAX on MPS for even moderate column counts.
        #
        # Solution: POOLED RELATIONSHIP INJECTION
        # 1. Run transformer on CLS + columns only â†’ O(NÂ²) attention (manageable)
        # 2. Pool relationship tokens â†’ single vector per batch
        # 3. Inject pooled relationships into CLS output
        #
        # This scales to 1000+ columns because:
        # - Transformer sees only 1 + N tokens (not 1 + N + NÂ²)
        # - Relationship tokens are processed via O(M) pooling, not O(MÂ²) attention
        # ============================================================================
        pooled_relationships = None
        if relationship_tokens:
            rel_tokens_tensor = torch.stack(relationship_tokens, dim=1)  # (batch_size, n_rel_tokens, d_model)
            
            # MEMORY FIX: Delete the list of tokens now that they're stacked
            del relationship_tokens
            
            if _should_debug:
                logger.info(f"[DEBUG]   Relationship tokens stacked: {rel_tokens_tensor.shape}")
            
            # POOLING STRATEGY: Mean pool over relationship tokens
            # This reduces M relationship tokens to a single d_model vector
            # Alternative: attention pooling, max pooling, or learned aggregation
            pooled_relationships = rel_tokens_tensor.mean(dim=1)  # (batch_size, d_model)
            
            # MEMORY FIX: Delete the stacked tensor now that it's pooled
            del rel_tokens_tensor
            
            if _should_debug:
                logger.info(f"[DEBUG]   Pooled relationships: {pooled_relationships.shape}")
        
        # NOTE: We no longer concatenate relationship tokens to the sequence
        # x remains (batch_size, 1 + n_cols, d_model) - manageable size

        # DEBUG: Log shape before transformer encoder - this is where MPS INT_MAX can overflow
        if _should_debug:
            batch_size_debug = x.shape[0]
            seq_len_debug = x.shape[1]
            d_model_debug = x.shape[2]
            n_heads = self.config.n_heads
            # Attention matrix size: (batch * n_heads, seq_len, seq_len)
            attn_matrix_size = batch_size_debug * n_heads * seq_len_debug * seq_len_debug
            INT_MAX = 2**31 - 1
            logger.info(f"[DEBUG]   Before transformer_encoder:")
            logger.info(f"[DEBUG]     x shape: {x.shape} (batch={batch_size_debug}, seq={seq_len_debug}, d_model={d_model_debug})")
            logger.info(f"[DEBUG]     n_heads: {n_heads}")
            logger.info(f"[DEBUG]     Attention matrix elements: {batch_size_debug} Ã— {n_heads} Ã— {seq_len_debug}Â² = {attn_matrix_size:,}")
            if attn_matrix_size > INT_MAX:
                logger.error(f"[DEBUG]   âš ï¸ ATTENTION MATRIX EXCEEDS INT_MAX ({INT_MAX:,})!")
            logger.info(f"[DEBUG]     About to call transformer_encoder...")

        # Pass through the transformer encoder (now using batch_first=True)
        # Use gradient checkpointing if enabled to save memory
        if self.enable_gradient_checkpointing and self.training:
            x = checkpoint(self.transformer_encoder, x, use_reentrant=False)
        else:
            x = self.transformer_encoder(x)
        
        if _should_debug:
            logger.info(f"[DEBUG]   After transformer_encoder: x shape = {x.shape}")

        # Use the output of the first token as the encoding of the whole sequence
        # This corresponds to the encoding of the [CLS] token. We throw out the encodings
        # for all other positions. With batch_first=True, CLS token is at position [:, 0, :]
        joint = x[:, 0, :]
        # marginal = x[1:]  # marginal encodings - we don's use these
        
        # ============================================================================
        # INJECT POOLED RELATIONSHIPS INTO CLS
        # ============================================================================
        # Add the pooled relationship information to the CLS token output
        # This allows relationship features to influence the joint encoding
        # without requiring O(MÂ²) attention over relationship tokens
        if pooled_relationships is not None:
            # Additive injection: CLS + pooled_rel
            # The model learns to use both column-level and relationship-level info
            joint = joint + pooled_relationships
            
            if _should_debug:
                logger.info(f"[DEBUG]   Injected pooled relationships into joint encoding")

        # convert from trnsformer dim to embedding_space dim, and normalize
        joint = self.out_converter(joint)

        # CONDITIONAL NORMALIZATION based on config
        # This config check is for safety/future flexibility
        # In practice, this should almost always be True
        if self.config.normalize_output:
            short_vec = nn.functional.normalize(joint[:, 0:3], dim=1)
            full_vec = nn.functional.normalize(joint, dim=1)
        else:
            # This path should rarely/never be used
            # NOTE: Using module-level logger (don't import locally - causes shadowing issues)
            logger.warning("âš ï¸  Joint encoder normalization is disabled! Embeddings may not be unit norm.")
            short_vec = joint[:, 0:3]
            full_vec = joint

        return short_vec, full_vec
    
    def analyze_attention_head_redundancy(self, batch_data, top_k=5):
        """
        Analyze if attention heads are learning redundant patterns.
        
        This diagnostic helps determine if you need more attention heads:
        - High redundancy (>0.8 similarity) â†’ heads are learning the same thing â†’ need more heads
        - Low redundancy (<0.5 similarity) â†’ heads are diverse â†’ current head count is good
        
        Args:
            batch_data: Input tensor (batch_size, n_cols, d_model) 
            top_k: Number of top attention positions to consider per head
        
        Returns:
            dict with:
                - 'head_similarities': (n_heads, n_heads) pairwise cosine similarities
                - 'avg_similarity': Average pairwise similarity across all heads
                - 'max_similarity': Maximum pairwise similarity (most redundant pair)
                - 'redundant_pairs': List of (head_i, head_j, similarity) where sim > 0.7
                - 'diversity_score': 1 - avg_similarity (higher = more diverse)
                - 'recommendation': String suggesting if more heads are needed
        """
        if not self._enable_attention_capture:
            logger.warning("âš ï¸  Attention capture is disabled. Call enable_attention_capture() first.")
            return None
        
        # Run forward pass to populate attention weights
        self.eval()  # Set to eval mode to get consistent attention
        with torch.no_grad():
            _ = self.forward(batch_data)
        
        # NOTE: PyTorch's TransformerEncoder doesn't expose attention weights by default
        # We need to monkey-patch or use a custom implementation
        # For now, we'll extract attention from the encoder layers
        
        attention_patterns = []
        n_layers = self.config.n_layers
        n_heads = self.config.n_heads
        
        # Extract attention weights from each layer
        for layer_idx, layer in enumerate(self.transformer_encoder.layers):
            # Access the self-attention module
            self_attn = layer.self_attn
            
            # Need to hook into attention computation
            # This requires modifying the forward pass or using hooks
            # For now, we'll compute attention patterns manually
            pass
        
        # TODO: This requires deeper integration with PyTorch's attention mechanism
        # For now, return a placeholder
        logger.warning("âš ï¸  Full attention analysis requires attention weight extraction hooks.")
        logger.info("   Analyzing attention patterns via weight similarity instead...")
        
        # Alternative: Analyze attention weight matrices for similarity
        head_patterns = self._analyze_attention_weight_similarity()
        
        return head_patterns
    
    def _analyze_attention_weight_similarity(self):
        """
        Analyze similarity between attention heads by comparing their learned weight matrices.
        
        This is an approximation: instead of comparing attention patterns on specific inputs,
        we compare the learned Q, K, V projection matrices for each head.
        """
        n_heads = self.config.n_heads
        d_model = self.config.d_model
        
        # Collect Q, K, V weight matrices from first transformer layer
        first_layer = self.transformer_encoder.layers[0]
        
        # PyTorch's MultiheadAttention stores weights as (d_model, d_model)
        # Then splits into n_heads during forward pass
        q_weights = first_layer.self_attn.in_proj_weight[:d_model, :]  # Query weights
        k_weights = first_layer.self_attn.in_proj_weight[d_model:2*d_model, :]  # Key weights
        v_weights = first_layer.self_attn.in_proj_weight[2*d_model:, :]  # Value weights
        
        head_dim = d_model // n_heads
        
        # Split into per-head matrices
        q_heads = q_weights.reshape(n_heads, head_dim, d_model)
        k_heads = k_weights.reshape(n_heads, head_dim, d_model)
        v_heads = v_weights.reshape(n_heads, head_dim, d_model)
        
        # Compute pairwise similarities between heads
        # We'll use cosine similarity of flattened QK^T patterns
        head_similarities = torch.zeros(n_heads, n_heads)
        
        for i in range(n_heads):
            for j in range(i, n_heads):
                # Compute attention pattern similarity: compare QK^T for each head
                # QK^T shape would be (head_dim, head_dim) but we want to compare overall behavior
                # Simplified: compare Q and K weights directly
                
                q_i_flat = q_heads[i].flatten()
                q_j_flat = q_heads[j].flatten()
                k_i_flat = k_heads[i].flatten()
                k_j_flat = k_heads[j].flatten()
                
                # Cosine similarity of Q weights
                q_sim = F.cosine_similarity(q_i_flat.unsqueeze(0), q_j_flat.unsqueeze(0))
                
                # Cosine similarity of K weights  
                k_sim = F.cosine_similarity(k_i_flat.unsqueeze(0), k_j_flat.unsqueeze(0))
                
                # Average Q and K similarity
                sim = (q_sim + k_sim) / 2.0
                
                head_similarities[i, j] = sim.item()
                head_similarities[j, i] = sim.item()
        
        # Compute statistics
        # Exclude diagonal (self-similarity = 1.0)
        mask = ~torch.eye(n_heads, dtype=torch.bool)
        off_diagonal = head_similarities[mask]
        
        avg_similarity = off_diagonal.mean().item()
        max_similarity = off_diagonal.max().item()
        min_similarity = off_diagonal.min().item()
        
        # Find redundant pairs (similarity > 0.7)
        redundant_pairs = []
        for i in range(n_heads):
            for j in range(i+1, n_heads):
                sim = head_similarities[i, j].item()
                if sim > 0.7:
                    redundant_pairs.append((i, j, sim))
        
        # Sort by similarity (most redundant first)
        redundant_pairs.sort(key=lambda x: x[2], reverse=True)
        
        # Diversity score (higher = more diverse)
        diversity_score = 1.0 - avg_similarity
        
        # Generate recommendation
        if avg_similarity > 0.8:
            recommendation = "HIGH REDUNDANCY: Heads are learning very similar patterns. Consider increasing n_heads."
            status = "âŒ REDUNDANT"
        elif avg_similarity > 0.6:
            recommendation = "MODERATE REDUNDANCY: Some overlap between heads. Current head count is okay, but could benefit from more."
            status = "âš ï¸  MODERATE"
        else:
            recommendation = "GOOD DIVERSITY: Heads are learning distinct patterns. Current head count is appropriate."
            status = "âœ… DIVERSE"
        
        result = {
            'head_similarities': head_similarities.cpu().numpy(),
            'avg_similarity': avg_similarity,
            'max_similarity': max_similarity,
            'min_similarity': min_similarity,
            'redundant_pairs': redundant_pairs,
            'diversity_score': diversity_score,
            'recommendation': recommendation,
            'status': status,
            'n_heads': n_heads,
            'n_redundant_pairs': len(redundant_pairs),
        }
        
        return result
    
    def log_attention_analysis(self, batch_data=None):
        """
        Log attention head diversity analysis.
        
        Args:
            batch_data: Optional input batch. If None, only analyzes weight similarity.
        """
        logger.info("ðŸ” ATTENTION HEAD DIVERSITY ANALYSIS")
        logger.info(f"   Number of heads: {self.config.n_heads}")
        logger.info(f"   Number of layers: {self.config.n_layers}")
        logger.info(f"   Model dimension: {self.config.d_model}")
        logger.info(f"   Head dimension: {self.config.d_model // self.config.n_heads}")
        
        # Analyze weight similarity
        analysis = self._analyze_attention_weight_similarity()
        
        logger.info(f"\n{analysis['status']} Attention Head Diversity:")
        logger.info(f"   Average head similarity: {analysis['avg_similarity']:.3f}")
        logger.info(f"   Diversity score: {analysis['diversity_score']:.3f} (higher is better)")
        logger.info(f"   Min similarity: {analysis['min_similarity']:.3f}")
        logger.info(f"   Max similarity: {analysis['max_similarity']:.3f}")
        
        if analysis['redundant_pairs']:
            logger.info(f"\nâš ï¸  Found {len(analysis['redundant_pairs'])} redundant head pairs (>0.7 similarity):")
            for i, j, sim in analysis['redundant_pairs'][:5]:  # Show top 5
                logger.info(f"      Head {i} â†” Head {j}: {sim:.3f}")
        else:
            logger.info("\nâœ… No redundant head pairs found (all < 0.7 similarity)")
        
        logger.info(f"\nðŸ’¡ {analysis['recommendation']}")
        
        return analysis
