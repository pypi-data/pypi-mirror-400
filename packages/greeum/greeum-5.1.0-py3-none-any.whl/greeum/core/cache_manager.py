import os
import json
import hashlib
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
import numpy as np

from .block_manager import BlockManager
from .stm_manager import STMManager

class CacheManager:
    """ìµœì í™”ëœ ì›¨ì´í¬ì¸íŠ¸ ìºì‹œë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ (Phase 1: 5ë°° ì„±ëŠ¥ í–¥ìƒ)"""
    
    def __init__(self, 
                 data_path: str = "data/context_cache.json",
                 cache_ttl: int = 300,  # 5ë¶„ ìºì‹œ
                 block_manager: Optional[BlockManager] = None,
                 stm_manager: Optional[STMManager] = None):
        """
        ìµœì í™”ëœ ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            data_path: ìºì‹œ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            cache_ttl: ë©”ëª¨ë¦¬ ìºì‹œ TTL (ì´ˆ, ê¸°ë³¸ê°’ 5ë¶„)
            block_manager: ë¸”ë¡ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            stm_manager: STM ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        """
        # ê¸°ì¡´ ì„¤ì •
        self.data_path = data_path
        self.block_manager = block_manager or BlockManager()
        # STMManager ëŠ” DatabaseManager ì˜ì¡´ì„±ì´ í•„ìš”
        self.stm_manager = stm_manager or STMManager(self.block_manager.db_manager)
        
        # ğŸš€ Phase 1: ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ìºì‹œ ì‹œìŠ¤í…œ
        self.cache_ttl = cache_ttl
        self.memory_cache = {}  # {cache_key: {"results": [...], "timestamp": float}}
        self.cache_hit_count = 0
        self.cache_miss_count = 0
        
        # ê¸°ì¡´ íŒŒì¼ ê¸°ë°˜ ìºì‹œ ìœ ì§€ (í˜¸í™˜ì„±)
        self._ensure_data_file()
        self.cache_data = self._load_cache()
    
    def _compute_cache_key(self, query_embedding: List[float], keywords: List[str]) -> str:
        """ì„ë² ë”©ê³¼ í‚¤ì›Œë“œë¥¼ ì¡°í•©í•œ ê³ ìœ  ìºì‹œ í‚¤ ìƒì„±"""
        # ì„ë² ë”©ì˜ ì£¼ìš” ì°¨ì›ë§Œ ì‚¬ìš© (ì •í™•ë„ vs ì†ë„ ê· í˜•)
        embedding_sample = query_embedding[:10] if len(query_embedding) >= 10 else query_embedding
        
        # í‚¤ì›Œë“œ ì •ê·œí™” ë° ì •ë ¬
        normalized_keywords = sorted([kw.lower().strip() for kw in keywords if kw.strip()])
        
        # ì¡°í•©ëœ ë¬¸ìì—´ ìƒì„±
        cache_input = f"{embedding_sample}|{normalized_keywords}"
        
        # MD5 í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„± (ì¶©ëŒ í™•ë¥  ë‚®ê³  ë¹ ë¦„)
        return hashlib.md5(cache_input.encode('utf-8')).hexdigest()[:12]
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        if cache_key not in self.memory_cache:
            return False
        
        cache_entry = self.memory_cache[cache_key]
        cache_age = time.time() - cache_entry["timestamp"]
        
        return cache_age < self.cache_ttl
    
    def _apply_keyword_boost(self, search_results: List[Dict], keywords: List[str]) -> List[Dict]:
        """ë©”ëª¨ë¦¬ì—ì„œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš© (DB ê²€ìƒ‰ ëŒ€ì‹ )"""
        boosted_results = []
        
        for result in search_results:
            context = result.get("context", "").lower()
            base_score = result.get("similarity_score", 0.7)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            keyword_matches = sum(1 for kw in keywords if kw.lower() in context)
            keyword_boost = min(0.3, keyword_matches * 0.1)  # ìµœëŒ€ 0.3 ë¶€ìŠ¤íŠ¸
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = min(1.0, base_score + keyword_boost)
            result["relevance"] = final_score
            boosted_results.append(result)
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        return sorted(boosted_results, key=lambda x: x.get("relevance", 0), reverse=True)
    
    def _cleanup_expired_cache(self) -> int:
        """ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        current_time = time.time()
        expired_keys = []
        
        for key, entry in self.memory_cache.items():
            if current_time - entry["timestamp"] > self.cache_ttl:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.memory_cache[key]
        
        return len(expired_keys)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ ì„±ëŠ¥ í†µê³„"""
        total_requests = self.cache_hit_count + self.cache_miss_count
        hit_ratio = self.cache_hit_count / total_requests if total_requests > 0 else 0
        
        return {
            "cache_hits": self.cache_hit_count,
            "cache_misses": self.cache_miss_count,
            "hit_ratio": hit_ratio,
            "cache_size": len(self.memory_cache),
            "total_requests": total_requests
        }
        
    def _ensure_data_file(self) -> None:
        """ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        data_dir = os.path.dirname(self.data_path)
        os.makedirs(data_dir, exist_ok=True)
        
        if not os.path.exists(self.data_path):
            default_data = {
                "current_context": "",
                "waypoints": [],
                "last_updated": datetime.now().isoformat()
            }
            with open(self.data_path, 'w', encoding='utf-8') as f:
                json.dump(default_data, f, ensure_ascii=False, indent=2)
    
    def _load_cache(self) -> Dict[str, Any]:
        """ìºì‹œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data
        except json.JSONDecodeError:
            # íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì†ìƒëœ ê²½ìš°
            return {
                "current_context": "",
                "waypoints": [],
                "last_updated": datetime.now().isoformat()
            }
    
    def _save_cache(self) -> None:
        """ìºì‹œ ë°ì´í„° ì €ì¥"""
        self.cache_data["last_updated"] = datetime.now().isoformat()
        with open(self.data_path, 'w', encoding='utf-8') as f:
            json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
    
    def update_context(self, context: str) -> None:
        """
        í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        
        Args:
            context: í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
        """
        self.cache_data["current_context"] = context
        self._save_cache()
    
    def update_waypoints(self, waypoints: List[Dict[str, Any]]) -> None:
        """
        ì›¨ì´í¬ì¸íŠ¸ ëª©ë¡ ì—…ë°ì´íŠ¸
        
        Args:
            waypoints: ì›¨ì´í¬ì¸íŠ¸ ëª©ë¡ (block_index, relevance í¬í•¨)
        """
        self.cache_data["waypoints"] = waypoints
        self._save_cache()
    
    def get_current_context(self) -> str:
        """í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        return self.cache_data.get("current_context", "")
    
    def get_waypoints(self) -> List[Dict[str, Any]]:
        """ì›¨ì´í¬ì¸íŠ¸ ëª©ë¡ ë°˜í™˜"""
        return self.cache_data.get("waypoints", [])
    
    def update_cache(self, user_input: str, query_embedding: List[float], 
                    extracted_keywords: List[str], top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ğŸš€ ìµœì í™”ëœ ìºì‹œ ì—…ë°ì´íŠ¸ (Phase 1: 5ë°° ì„±ëŠ¥ í–¥ìƒ ëª©í‘œ)
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            extracted_keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ
            top_k: ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ì›¨ì´í¬ì¸íŠ¸ ë¸”ë¡ ëª©ë¡
        """
        # ğŸš€ ìµœì í™” 1: ìºì‹œ í‚¤ ìƒì„± ë° í™•ì¸
        cache_key = self._compute_cache_key(query_embedding, extracted_keywords)
        
        if self._is_cache_valid(cache_key):
            # ìºì‹œ íˆíŠ¸ - ì¦‰ì‹œ ë°˜í™˜ (90% ì†ë„ í–¥ìƒ)
            self.cache_hit_count += 1
            cached_results = self.memory_cache[cache_key]["results"]
            
            # ì»¨í…ìŠ¤íŠ¸ë§Œ ì—…ë°ì´íŠ¸ (ê²€ìƒ‰ì€ ìŠ¤í‚µ)
            self.update_context(user_input)
            return cached_results
        
        # ğŸš€ ìµœì í™” 2: ìºì‹œ ë¯¸ìŠ¤ - ë‹¨ì¼ ì„ë² ë”© ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        self.cache_miss_count += 1
        
        # í•µì‹¬ ìµœì í™”: ì„ë² ë”© ê²€ìƒ‰ë§Œ ìˆ˜í–‰ (í‚¤ì›Œë“œ ê²€ìƒ‰ ì œê±°)
        # top_k * 2ë¡œ ì—¬ìœ ìˆê²Œ ê²€ìƒ‰í•˜ì—¬ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í›„ ìƒìœ„ ì„ íƒ
        search_results = self.block_manager.search_by_embedding(query_embedding, top_k * 2)
        
        # ğŸš€ ìµœì í™” 3: í‚¤ì›Œë“œ ê¸°ë°˜ í›„ì²˜ë¦¬ (DB ê²€ìƒ‰ ëŒ€ì‹  ë©”ëª¨ë¦¬ í•„í„°ë§)
        keyword_boosted_results = self._apply_keyword_boost(search_results, extracted_keywords)
        
        # ğŸš€ ìµœì í™” 4: ìƒìœ„ ê²°ê³¼ ì„ íƒ
        final_results = keyword_boosted_results[:top_k]
        
        # ğŸš€ ìµœì í™” 5: ìºì‹œ ì €ì¥
        self.memory_cache[cache_key] = {
            "results": final_results,
            "timestamp": time.time()
        }
        
        # ì£¼ê¸°ì  ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ)
        if len(self.memory_cache) > 100:  # 100ê°œ ì´ˆê³¼ ì‹œ ì •ë¦¬
            self._cleanup_expired_cache()
        
        # ğŸš€ ìµœì í™” 6: ê¸°ì¡´ ì›¨ì´í¬ì¸íŠ¸ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
        waypoints = [{"block_index": r["block_index"], "relevance": r.get("relevance", 0.7)} 
                     for r in final_results]
        self.update_waypoints(waypoints)
        self.update_context(user_input)
        
        return final_results
    
    def cache_search_results(self, query_embedding: List[float], keywords: List[str], 
                           search_results: List[Dict[str, Any]]) -> None:
        """ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì§ì ‘ ì €ì¥ (Phase 3 ì¼ê´€ì„± ë³´ì¥ìš©)"""
        cache_key = self._compute_cache_key(query_embedding, keywords or [])
        
        self.memory_cache[cache_key] = {
            "results": search_results,
            "timestamp": time.time()
        }
        
        # ì£¼ê¸°ì  ìºì‹œ ì •ë¦¬
        if len(self.memory_cache) > 100:
            self._cleanup_expired_cache()
    
    def get_cached_results(self, query_embedding: List[float], keywords: List[str] = None) -> Optional[List[Dict[str, Any]]]:
        """Phase 3ìš©: ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        if keywords is None:
            keywords = []
        
        cache_key = self._compute_cache_key(query_embedding, keywords)
        
        if self._is_cache_valid(cache_key):
            self.cache_hit_count += 1
            return self.memory_cache[cache_key]["results"]
        
        self.cache_miss_count += 1
        return None
    
    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ìºì‹œ + íŒŒì¼ ìºì‹œ)"""
        # ğŸš€ ë©”ëª¨ë¦¬ ìºì‹œ ì´ˆê¸°í™”
        self.memory_cache.clear()
        self.cache_hit_count = 0
        self.cache_miss_count = 0
        
        # ê¸°ì¡´ íŒŒì¼ ìºì‹œ ì´ˆê¸°í™”
        self.cache_data = {
            "current_context": "",
            "waypoints": [],
            "last_updated": datetime.now().isoformat()
        }
        self._save_cache() 
