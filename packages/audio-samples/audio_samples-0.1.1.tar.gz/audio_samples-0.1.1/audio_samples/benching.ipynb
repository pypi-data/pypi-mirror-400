{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio I/O Performance Benchmarks: audio_samples vs soundfile\n",
    "\n",
    "This notebook benchmarks the performance of `audio_samples` Python bindings against `soundfile` for reading and writing WAV files.\n",
    "\n",
    "**Test Configuration:**\n",
    "- Sample Rate: 44100 Hz\n",
    "- Sample Type: f32\n",
    "- File Durations: 0.1s, 0.5s, 1s, 2s, 5s, 10s, 30s, 60s\n",
    "- Channel Configurations: Mono and Stereo\n",
    "\n",
    "**Metrics:**\n",
    "- Read performance (time to load)\n",
    "- Write performance (time to save)\n",
    "- Throughput analysis\n",
    "- Memory efficiency comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import soundfile as sf\n",
    "import audio_samples as aus\n",
    "from scipy.io import wavfile as scipy_wav\n",
    "import torchaudio\n",
    "import scipy\n",
    "\n",
    "# Define Okabe-Ito colorblind-friendly palette for 4 libraries\n",
    "OKABE_ITO_COLORS = [\n",
    "    '#E69F00',  # orange - audio_samples\n",
    "    '#56B4E9',  # sky blue - soundfile  \n",
    "    '#009E73',  # bluish green - scipy\n",
    "    '#F0E442',  # yellow - torchaudio\n",
    "    '#0072B2',  # blue\n",
    "    '#D55E00',  # vermillion\n",
    "    '#CC79A7',  # reddish purple\n",
    "    '#999999'   # gray\n",
    "]\n",
    "\n",
    "# Configure plotting with cleaner styling\n",
    "plt.rcParams['figure.figsize'] = [12, 9]\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 24\n",
    "plt.rcParams['axes.labelsize'] = 22\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['xtick.major.width'] = 2  # Normal thickness\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rcParams['axes.linewidth'] = 2  # Normal thickness, not bold\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "plt.rcParams['legend.fancybox'] = True\n",
    "plt.rcParams['legend.shadow'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['grid.linewidth'] = 1\n",
    "plt.rcParams['lines.linewidth'] = 4\n",
    "plt.rcParams['lines.markersize'] = 12\n",
    "\n",
    "# Set the color palette\n",
    "sns.set_palette(OKABE_ITO_COLORS)\n",
    "\n",
    "# Define libraries for benchmarking\n",
    "LIBRARIES = ['audio_samples', 'soundfile', 'scipy', 'torchaudio']\n",
    "\n",
    "print(f\"audio_samples version: {getattr(aus, '__version__', 'unknown')}\")\n",
    "print(f\"soundfile version: {sf.__version__}\")\n",
    "print(f\"scipy version: {scipy.__version__}\")\n",
    "print(f\"torchaudio version: {torchaudio.__version__}\")\n",
    "\n",
    "print(f\"Benchmarking libraries: {LIBRARIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration\n",
    "SAMPLE_RATE = 44100\n",
    "SAMPLE_TYPE = np.float32\n",
    "DURATIONS = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]  # seconds\n",
    "CHANNELS = [1, 2]  # mono, stereo\n",
    "BENCHMARK_ITERATIONS = 50  # Increased for statistical significance (was 10)\n",
    "FREQUENCY = 440.0  # Hz for sine wave test signal\n",
    "\n",
    "# Create temporary directory for test files\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"audio_bench_\")\n",
    "print(f\"Test files will be stored in: {temp_dir}\")\n",
    "\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"Clean up temporary test files.\"\"\"\n",
    "    import shutil\n",
    "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    print(f\"Cleaned up temporary directory: {temp_dir}\")\n",
    "\n",
    "def get_file_size_mb(filepath: str) -> float:\n",
    "    \"\"\"Get file size in MB.\"\"\"\n",
    "    return os.path.getsize(filepath) / (1024 * 1024)\n",
    "\n",
    "def calculate_throughput(file_size_mb: float, time_seconds: float) -> float:\n",
    "    \"\"\"Calculate throughput in MB/s.\"\"\"\n",
    "    return file_size_mb / time_seconds if time_seconds > 0 else 0.0\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"Format time with appropriate units.\"\"\"\n",
    "    if seconds >= 1.0:\n",
    "        return f\"{seconds:.3f}s\"\n",
    "    elif seconds >= 0.001:\n",
    "        return f\"{seconds*1000:.1f}ms\"\n",
    "    else:\n",
    "        return f\"{seconds*1000000:.1f}Î¼s\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_files():\n",
    "    \"\"\"Generate test WAV files for all duration/channel combinations.\"\"\"\n",
    "    test_files = {}\n",
    "    \n",
    "    print(\"Generating test files...\")\n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            # Generate test signal using audio_samples\n",
    "            audio = aus.generation.sine_wave(\n",
    "                frequency=FREQUENCY,\n",
    "                duration_secs=duration,\n",
    "                sample_rate=SAMPLE_RATE,\n",
    "                amplitude=0.5\n",
    "            )\n",
    "            print(f\"Generated sine wave with dtype {audio.dtype}\")\n",
    "            # Convert to stereo if needed\n",
    "            if channels == 2:\n",
    "                # Create stereo by duplicating mono signal\n",
    "                mono_data = audio\n",
    "                stereo_data = aus.AudioSamples.stack([mono_data, mono_data * 0.8])  # Slight variation for stereo\n",
    "                audio = stereo_data\n",
    "            filename = f\"test_{duration}s_{channels}ch.wav\"\n",
    "            filepath = os.path.join(temp_dir, filename)\n",
    "            aus.io.save(filepath, audio)\n",
    "            \n",
    "            # Store file info\n",
    "            test_files[(duration, channels)] = {\n",
    "                'filepath': filepath,\n",
    "                'audio_samples': audio,\n",
    "                'size_mb': get_file_size_mb(filepath),\n",
    "                'samples_per_channel': int(duration * SAMPLE_RATE)\n",
    "            }\n",
    "            \n",
    "            print(f\"  {filename}: {get_file_size_mb(filepath):.2f} MB\")\n",
    "    \n",
    "    return test_files\n",
    "\n",
    "# Generate all test files\n",
    "test_files = generate_test_files()\n",
    "print(f\"\\nGenerated {len(test_files)} test files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkTimer:\n",
    "    \"\"\"Context manager for timing operations with memory tracking.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        gc.collect()  # Clean up before measurement\n",
    "        self.start_time = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end_time = time.perf_counter()\n",
    "\n",
    "    @property\n",
    "    def elapsed(self):\n",
    "        return self.end_time - self.start_time\n",
    "\n",
    "def benchmark_operation(operation, iterations=BENCHMARK_ITERATIONS):\n",
    "    \"\"\"Benchmark an operation with multiple iterations.\"\"\"\n",
    "    times = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Ensure clean state between iterations\n",
    "        gc.collect()\n",
    "        \n",
    "        try:\n",
    "            with BenchmarkTimer() as timer:\n",
    "                result = operation()\n",
    "            \n",
    "            times.append(timer.elapsed)\n",
    "            \n",
    "            # Clean up result to avoid memory accumulation\n",
    "            del result\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in benchmark iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not times:\n",
    "        return {\n",
    "            'mean_time': 0.0,\n",
    "            'std_time': 0.0,\n",
    "            'min_time': 0.0,\n",
    "            'max_time': 0.0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_read_performance():\n",
    "    \"\"\"Benchmark read performance for all libraries.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"Benchmarking read performance...\")\n",
    "    \n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            file_info = test_files[(duration, channels)]\n",
    "            filepath = file_info['filepath']\n",
    "            file_size_mb = file_info['size_mb']\n",
    "            \n",
    "            print(f\"  Testing {duration}s, {channels}ch...\")\n",
    "            \n",
    "            # Benchmark audio_samples\n",
    "            def read_audio_samples():\n",
    "                return aus.io.read(filepath)\n",
    "            \n",
    "            aus_stats = benchmark_operation(read_audio_samples)\n",
    "            aus_throughput = calculate_throughput(file_size_mb, aus_stats['mean_time'])\n",
    "            \n",
    "            # Benchmark soundfile\n",
    "            def read_soundfile():\n",
    "                data, sr = sf.read(filepath, dtype=np.float32)\n",
    "                return data\n",
    "            \n",
    "            sf_stats = benchmark_operation(read_soundfile)\n",
    "            sf_throughput = calculate_throughput(file_size_mb, sf_stats['mean_time'])\n",
    "            \n",
    "            # Benchmark scipy\n",
    "            def read_scipy():\n",
    "                sr, data = scipy_wav.read(filepath)\n",
    "                # Convert to float32 if needed\n",
    "                if data.dtype != np.float32:\n",
    "                    if data.dtype == np.int16:\n",
    "                        data = data.astype(np.float32) / 32767.0\n",
    "                    elif data.dtype == np.int32:\n",
    "                        data = data.astype(np.float32) / 2147483647.0\n",
    "                    else:\n",
    "                        data = data.astype(np.float32)\n",
    "                return data\n",
    "            \n",
    "            scipy_stats = benchmark_operation(read_scipy)\n",
    "            scipy_throughput = calculate_throughput(file_size_mb, scipy_stats['mean_time'])\n",
    "            \n",
    "            # Benchmark torchaudio\n",
    "            def read_torchaudio():\n",
    "                waveform, sr = torchaudio.load(filepath)\n",
    "                return waveform\n",
    "            \n",
    "            torch_stats = benchmark_operation(read_torchaudio)\n",
    "            torch_throughput = calculate_throughput(file_size_mb, torch_stats['mean_time'])\n",
    "            \n",
    "            # Store results for all libraries\n",
    "            results.extend([\n",
    "                {\n",
    "                    'operation': 'read',\n",
    "                    'library': 'audio_samples',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': aus_stats['mean_time'],\n",
    "                    'std_time': aus_stats['std_time'],\n",
    "                    'throughput_mb_s': aus_throughput,\n",
    "                },\n",
    "                {\n",
    "                    'operation': 'read',\n",
    "                    'library': 'soundfile',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': sf_stats['mean_time'],\n",
    "                    'std_time': sf_stats['std_time'],\n",
    "                    'throughput_mb_s': sf_throughput,\n",
    "                },\n",
    "                {\n",
    "                    'operation': 'read',\n",
    "                    'library': 'scipy',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': scipy_stats['mean_time'],\n",
    "                    'std_time': scipy_stats['std_time'],\n",
    "                    'throughput_mb_s': scipy_throughput,\n",
    "                },\n",
    "                {\n",
    "                    'operation': 'read',\n",
    "                    'library': 'torchaudio',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': torch_stats['mean_time'],\n",
    "                    'std_time': torch_stats['std_time'],\n",
    "                    'throughput_mb_s': torch_throughput,\n",
    "                }\n",
    "            ])\n",
    "            \n",
    "            # Show progress\n",
    "            print(f\"    audio_samples: {format_time(aus_stats['mean_time'])}, {aus_throughput:.1f} MB/s\")\n",
    "            print(f\"    soundfile:     {format_time(sf_stats['mean_time'])}, {sf_throughput:.1f} MB/s\")\n",
    "            print(f\"    scipy:         {format_time(scipy_stats['mean_time'])}, {scipy_throughput:.1f} MB/s\")\n",
    "            print(f\"    torchaudio:    {format_time(torch_stats['mean_time'])}, {torch_throughput:.1f} MB/s\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run read benchmarks\n",
    "read_results = benchmark_read_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_write_performance():\n",
    "    \"\"\"Benchmark write performance for all libraries.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"Benchmarking write performance...\")\n",
    "    \n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            file_info = test_files[(duration, channels)]\n",
    "            audio_data = file_info['audio_samples']\n",
    "            file_size_mb = file_info['size_mb']\n",
    "            \n",
    "            print(f\"  Testing {duration}s, {channels}ch...\")\n",
    "            \n",
    "            # Prepare data for different libraries\n",
    "            numpy_data = audio_data.to_numpy()\n",
    "            if channels == 2:\n",
    "                # soundfile and scipy expect (samples, channels)\n",
    "                sf_data = numpy_data.T\n",
    "                scipy_data = numpy_data.T\n",
    "                # torchaudio expects (channels, samples)\n",
    "                torch_data = numpy_data\n",
    "            else:\n",
    "                sf_data = numpy_data\n",
    "                scipy_data = numpy_data\n",
    "                torch_data = numpy_data.reshape(1, -1)  # Add channel dimension\n",
    "            \n",
    "            # Benchmark audio_samples\n",
    "            def write_audio_samples():\n",
    "                out_path = os.path.join(temp_dir, f\"write_test_aus_{duration}_{channels}.wav\")\n",
    "                aus.io.save(out_path, audio_data)\n",
    "                return out_path\n",
    "            \n",
    "            aus_stats = benchmark_operation(write_audio_samples)\n",
    "            aus_throughput = calculate_throughput(file_size_mb, aus_stats['mean_time'])\n",
    "            \n",
    "            # Benchmark soundfile\n",
    "            def write_soundfile():\n",
    "                out_path = os.path.join(temp_dir, f\"write_test_sf_{duration}_{channels}.wav\")\n",
    "                sf.write(out_path, sf_data, SAMPLE_RATE, subtype='FLOAT')\n",
    "                return out_path\n",
    "            \n",
    "            sf_stats = benchmark_operation(write_soundfile)\n",
    "            sf_throughput = calculate_throughput(file_size_mb, sf_stats['mean_time'])\n",
    "            \n",
    "            # Benchmark scipy\n",
    "            def write_scipy():\n",
    "                out_path = os.path.join(temp_dir, f\"write_test_scipy_{duration}_{channels}.wav\")\n",
    "                # Convert float32 to int16 for scipy (it doesn't handle float32 well)\n",
    "                scipy_int_data = (scipy_data * 32767).astype(np.int16)\n",
    "                scipy_wav.write(out_path, SAMPLE_RATE, scipy_int_data)\n",
    "                return out_path\n",
    "            \n",
    "            scipy_stats = benchmark_operation(write_scipy)\n",
    "            scipy_throughput = calculate_throughput(file_size_mb, scipy_stats['mean_time'])\n",
    "            \n",
    "            # Benchmark torchaudio\n",
    "            def write_torchaudio():\n",
    "                out_path = os.path.join(temp_dir, f\"write_test_torch_{duration}_{channels}.wav\")\n",
    "                import torch\n",
    "                torch_tensor = torch.from_numpy(torch_data.astype(np.float32))\n",
    "                torchaudio.save(out_path, torch_tensor, SAMPLE_RATE)\n",
    "                return out_path\n",
    "            \n",
    "            torch_stats = benchmark_operation(write_torchaudio)\n",
    "            torch_throughput = calculate_throughput(file_size_mb, torch_stats['mean_time'])\n",
    "            \n",
    "            # Store results for all libraries\n",
    "            results.extend([\n",
    "                {\n",
    "                    'operation': 'write',\n",
    "                    'library': 'audio_samples',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': aus_stats['mean_time'],\n",
    "                    'std_time': aus_stats['std_time'],\n",
    "                    'throughput_mb_s': aus_throughput,\n",
    "                },\n",
    "                {\n",
    "                    'operation': 'write',\n",
    "                    'library': 'soundfile',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': sf_stats['mean_time'],\n",
    "                    'std_time': sf_stats['std_time'],\n",
    "                    'throughput_mb_s': sf_throughput,\n",
    "                },\n",
    "                {\n",
    "                    'operation': 'write',\n",
    "                    'library': 'scipy',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': scipy_stats['mean_time'],\n",
    "                    'std_time': scipy_stats['std_time'],\n",
    "                    'throughput_mb_s': scipy_throughput,\n",
    "                },\n",
    "                {\n",
    "                    'operation': 'write',\n",
    "                    'library': 'torchaudio',\n",
    "                    'duration': duration,\n",
    "                    'channels': channels,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'mean_time': torch_stats['mean_time'],\n",
    "                    'std_time': torch_stats['std_time'],\n",
    "                    'throughput_mb_s': torch_throughput,\n",
    "                }\n",
    "            ])\n",
    "            \n",
    "            # Show progress\n",
    "            print(f\"    audio_samples: {format_time(aus_stats['mean_time'])}, {aus_throughput:.1f} MB/s\")\n",
    "            print(f\"    soundfile:     {format_time(sf_stats['mean_time'])}, {sf_throughput:.1f} MB/s\")\n",
    "            print(f\"    scipy:         {format_time(scipy_stats['mean_time'])}, {scipy_throughput:.1f} MB/s\")\n",
    "            print(f\"    torchaudio:    {format_time(torch_stats['mean_time'])}, {torch_throughput:.1f} MB/s\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run write benchmarks\n",
    "write_results = benchmark_write_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = read_results + write_results\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"Benchmark Results Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall statistics\n",
    "for operation in ['read', 'write']:\n",
    "    print(f\"\\n{operation.upper()} Performance:\")\n",
    "    op_data = df[df['operation'] == operation]\n",
    "    \n",
    "    for lib in LIBRARIES:\n",
    "        lib_data = op_data[op_data['library'] == lib]\n",
    "        avg_time = lib_data['mean_time'].mean()\n",
    "        avg_throughput = lib_data['throughput_mb_s'].mean()\n",
    "        \n",
    "        print(f\"  {lib:12}: {format_time(avg_time):>8} avg, {avg_throughput:>6.1f} MB/s avg\")\n",
    "\n",
    "df.to_csv(\"benchmark_results.csv\", index=False)\n",
    "\n",
    "# Calculate speedup ratios relative to audio_samples as baseline\n",
    "speedup_data = []\n",
    "for operation in ['read', 'write']:\n",
    "    for duration in DURATIONS:\n",
    "        for channels in CHANNELS:\n",
    "            subset = df[\n",
    "                (df['operation'] == operation) & \n",
    "                (df['duration'] == duration) & \n",
    "                (df['channels'] == channels)\n",
    "            ]\n",
    "            \n",
    "            if len(subset) >= 2:\n",
    "                # Get audio_samples baseline\n",
    "                aus_time = subset[subset['library'] == 'audio_samples']['mean_time']\n",
    "                if len(aus_time) > 0:\n",
    "                    aus_baseline = aus_time.iloc[0]\n",
    "                    \n",
    "                    for lib in LIBRARIES:\n",
    "                        lib_time = subset[subset['library'] == lib]['mean_time']\n",
    "                        if len(lib_time) > 0:\n",
    "                            speedup = aus_baseline / lib_time.iloc[0]  # Higher is better\n",
    "                            \n",
    "                            speedup_data.append({\n",
    "                                'operation': operation,\n",
    "                                'duration': duration,\n",
    "                                'channels': channels,\n",
    "                                'library': lib,\n",
    "                                'speedup': speedup,\n",
    "                                'winner': lib if speedup > 1.0 else 'audio_samples'\n",
    "                            })\n",
    "\n",
    "\n",
    "speedup_df = pd.DataFrame(speedup_data)\n",
    "\n",
    "print(\"\\nSpeedup Analysis (relative to audio_samples baseline):\")\n",
    "print(\"Values > 1.0 mean the library is faster than audio_samples\")\n",
    "if len(speedup_df) > 0:\n",
    "    summary = speedup_df.groupby(['operation', 'library'])['speedup'].agg(['mean', 'min', 'max']).round(2)\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"No speedup data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define consistent color and marker mapping for all plots\n",
    "color_map = {\n",
    "    'audio_samples': OKABE_ITO_COLORS[0],  # orange\n",
    "    'soundfile': OKABE_ITO_COLORS[1],      # sky blue\n",
    "    'scipy': OKABE_ITO_COLORS[2],          # bluish green\n",
    "    'torchaudio': OKABE_ITO_COLORS[3],     # yellow\n",
    "}\n",
    "\n",
    "marker_map = {\n",
    "    1: 'o',   # circle for mono\n",
    "    2: 's'    # square for stereo\n",
    "}\n",
    "\n",
    "# Create output directory for figures\n",
    "fig_dir = os.path.join(temp_dir, \"figures\")\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "def save_figure(fig, name, dpi=300):\n",
    "    \"\"\"Save figure as both PDF and PNG with high quality.\"\"\"\n",
    "    pdf_path = os.path.join(fig_dir, f\"{name}.pdf\")\n",
    "    png_path = os.path.join(fig_dir, f\"{name}.png\")\n",
    "    \n",
    "    fig.savefig(pdf_path, format='pdf', dpi=dpi, bbox_inches='tight', \n",
    "               facecolor='white', edgecolor='none')\n",
    "    fig.savefig(png_path, format='png', dpi=dpi, bbox_inches='tight',\n",
    "               facecolor='white', edgecolor='none')\n",
    "    \n",
    "    print(f\"Saved: {pdf_path}\")\n",
    "    print(f\"Saved: {png_path}\")\n",
    "\n",
    "def plot_read_performance(df):\n",
    "    \"\"\"Plot read performance vs file duration.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    read_data = df[df['operation'] == 'read']\n",
    "    for channels in CHANNELS:\n",
    "        for lib in LIBRARIES:\n",
    "            subset = read_data[(read_data['channels'] == channels) & (read_data['library'] == lib)]\n",
    "            if len(subset) > 0:\n",
    "                channel_name = 'mono' if channels == 1 else 'stereo'\n",
    "                ax.plot(subset['duration'], subset['mean_time'],\n",
    "                       marker=marker_map[channels], label=f\"{lib} ({channel_name})\",\n",
    "                       color=color_map[lib], linewidth=4, markersize=12)\n",
    "\n",
    "    ax.set_xlabel('File Duration (seconds)', fontweight='bold')\n",
    "    ax.set_ylabel('Read Time (seconds)', fontweight='bold')\n",
    "    ax.set_title('Read Performance vs File Duration', fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    legend = ax.legend(bbox_to_anchor=(0.5, -0.12), loc='upper center', ncol=2)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_alpha(0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    save_figure(fig, \"read_performance\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_write_performance(df):\n",
    "    \"\"\"Plot write performance vs file duration.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    write_data = df[df['operation'] == 'write']\n",
    "    for channels in CHANNELS:\n",
    "        for lib in LIBRARIES:\n",
    "            subset = write_data[(write_data['channels'] == channels) & (write_data['library'] == lib)]\n",
    "            if len(subset) > 0:\n",
    "                channel_name = 'mono' if channels == 1 else 'stereo'\n",
    "                ax.plot(subset['duration'], subset['mean_time'],\n",
    "                       marker=marker_map[channels], label=f\"{lib} ({channel_name})\",\n",
    "                       color=color_map[lib], linewidth=4, markersize=12)\n",
    "\n",
    "    ax.set_xlabel('File Duration (seconds)', fontweight='bold')\n",
    "    ax.set_ylabel('Write Time (seconds)', fontweight='bold')\n",
    "    ax.set_title('Write Performance vs File Duration', fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    legend = ax.legend(bbox_to_anchor=(0.5, -0.12), loc='upper center', ncol=2)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_alpha(0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    save_figure(fig, \"write_performance\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_throughput_comparison(df):\n",
    "    \"\"\"Plot average throughput comparison.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    throughput_data = df.groupby(['operation', 'library'])['throughput_mb_s'].mean().reset_index()\n",
    "    palette = [color_map[lib] for lib in LIBRARIES]\n",
    "    sns.barplot(data=throughput_data, x='operation', y='throughput_mb_s', hue='library',\n",
    "               ax=ax, palette=palette)\n",
    "    \n",
    "    ax.set_title('Average Throughput Comparison', fontweight='bold')\n",
    "    ax.set_ylabel('Throughput (MB/s)', fontweight='bold')\n",
    "    ax.set_xlabel('Operation', fontweight='bold')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18, width=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    legend = ax.legend(bbox_to_anchor=(0.5, -0.12), loc='upper center', ncol=2)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_alpha(0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    save_figure(fig, \"throughput_comparison\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Generate all individual plots\n",
    "print(\"Generating individual performance plots...\")\n",
    "read_fig = plot_read_performance(df)\n",
    "write_fig = plot_write_performance(df)\n",
    "throughput_fig = plot_throughput_comparison(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to clean up temporary files\n",
    "# cleanup_temp_files()\n",
    "\n",
    "print(\"Benchmark complete!\")\n",
    "print(f\"Temporary files are preserved in: {temp_dir}\")\n",
    "print(\"Uncomment the cleanup_temp_files() call above to remove them.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_samples_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
