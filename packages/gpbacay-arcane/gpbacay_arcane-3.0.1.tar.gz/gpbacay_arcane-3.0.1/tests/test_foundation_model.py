#!/usr/bin/env python3
"""
Test script for the Arcane Foundational Small Language Model.
This script tests the model without requiring a tokenizer.
"""

import os
import sys

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from gpbacay_arcane.ollama_integration import OllamaARCANEHybrid
    print("‚úÖ A.R.C.A.N.E. modules loaded successfully")
except ImportError as e:
    print(f"‚ùå Error importing A.R.C.A.N.E. modules: {e}")
    sys.exit(1)

def test_foundation_model():
    """Test the Arcane Foundational Small Language Model."""
    print("üîÑ Creating Arcane Foundational Small Language Model...")
    
    try:
        # Create the hybrid model
        foundational_model = OllamaARCANEHybrid(
            ollama_model="llama3.2:1b",
            vocab_size=5000,
            embed_dim=256,
            seq_len=32,
            model_name="arcane_foundational_slm"
        )
        
        # Load the saved model (without tokenizer)
        model_path = "Models/arcane_foundational_slm_saved"
        if os.path.exists(model_path):
            foundational_model.load_model(model_path)
            print(f"‚úÖ Loaded Arcane Foundational Model from: {model_path}")
        else:
            print("‚ö†Ô∏è  Pre-trained model not found, using initialization...")
            # Build the model architecture
            foundational_model.build_neuromimetic_architecture()
        
        # Test the model with a simple prompt
        print("\nüîç Testing model with prompt: 'Hello, how are you?'")
        response = foundational_model.generate_text_with_improved_sampling(
            seed_text="Hello, how are you?",
            max_length=100,
            temperature=0.8
        )
        
        print(f"ü§ñ A.R.C.A.N.E. Response: {response}")
        
        # Test with the prompt that was causing issues
        print("\nüîç Testing model with prompt: 'hi'")
        response = foundational_model.generate_text_with_improved_sampling(
            seed_text="hi",
            max_length=100,
            temperature=0.8
        )
        
        print(f"ü§ñ A.R.C.A.N.E. Response: {response}")
        
        # Test with another prompt
        print("\nüîç Testing model with prompt: 'What is artificial intelligence?'")
        response = foundational_model.generate_text_with_improved_sampling(
            seed_text="What is artificial intelligence?",
            max_length=100,
            temperature=0.8
        )
        
        print(f"ü§ñ A.R.C.A.N.E. Response: {response}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_foundation_model()