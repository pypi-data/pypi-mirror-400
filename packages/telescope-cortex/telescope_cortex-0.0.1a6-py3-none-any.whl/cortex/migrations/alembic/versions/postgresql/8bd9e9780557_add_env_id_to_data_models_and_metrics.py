"""add env_id to data models and metrics

Revision ID: 8bd9e9780557
Revises: 450bd09cd019
Create Date: 2025-11-13 13:26:02.360463

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '8bd9e9780557'
down_revision: Union[str, None] = '450bd09cd019'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    conn = op.get_bind()
    inspector = sa.inspect(conn)
    
    # Handle data_models table
    columns_data_models = [c['name'] for c in inspector.get_columns('data_models')]
    if 'environment_id' not in columns_data_models:
        op.add_column('data_models', sa.Column('environment_id', sa.UUID(), nullable=True))
        
        # Get the first environment ID from the environments table
        first_env = conn.execute(sa.text("SELECT id FROM environments LIMIT 1")).fetchone()
        if first_env:
            first_env_id = first_env[0]
            # Assign the first environment ID to all existing rows with NULL environment_id
            op.execute(sa.text(f"UPDATE data_models SET environment_id = '{first_env_id}' WHERE environment_id IS NULL"))
        
        # Now alter columns to be NOT NULL
        # For SQLite, batch_alter_table is used because of render_as_batch=True in env.py
        # but we can also use it explicitly here for better compatibility
        with op.batch_alter_table('data_models') as batch_op:
            batch_op.alter_column('environment_id', nullable=False)
            
    # Check index and FK for data_models
    indices_data_models = [i['name'] for i in inspector.get_indexes('data_models')]
    if 'ix_data_models_environment_id' not in indices_data_models:
        op.create_index(op.f('ix_data_models_environment_id'), 'data_models', ['environment_id'], unique=False)
        
    fks_data_models = [fk['referred_table'] for fk in inspector.get_foreign_keys('data_models')]
    if 'environments' not in fks_data_models:
        op.create_foreign_key(None, 'data_models', 'environments', ['environment_id'], ['id'])

    # Handle metrics table
    columns_metrics = [c['name'] for c in inspector.get_columns('metrics')]
    if 'environment_id' not in columns_metrics:
        op.add_column('metrics', sa.Column('environment_id', sa.UUID(), nullable=True))
        
        # Get the first environment ID from the environments table
        first_env = conn.execute(sa.text("SELECT id FROM environments LIMIT 1")).fetchone()
        if first_env:
            first_env_id = first_env[0]
            # Assign the first environment ID to all existing rows with NULL environment_id
            op.execute(sa.text(f"UPDATE metrics SET environment_id = '{first_env_id}' WHERE environment_id IS NULL"))
            
        with op.batch_alter_table('metrics') as batch_op:
            batch_op.alter_column('environment_id', nullable=False)

    # Check index and FK for metrics
    indices_metrics = [i['name'] for i in inspector.get_indexes('metrics')]
    if 'ix_metrics_environment_id' not in indices_metrics:
        op.create_index(op.f('ix_metrics_environment_id'), 'metrics', ['environment_id'], unique=False)

    fks_metrics = [fk['referred_table'] for fk in inspector.get_foreign_keys('metrics')]
    if 'environments' not in fks_metrics:
        op.create_foreign_key(None, 'metrics', 'environments', ['environment_id'], ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    conn = op.get_bind()
    inspector = sa.inspect(conn)

    # Handle metrics table
    columns_metrics = [c['name'] for c in inspector.get_columns('metrics')]
    if 'environment_id' in columns_metrics:
        # Check and drop index
        indices_metrics = [i['name'] for i in inspector.get_indexes('metrics')]
        if 'ix_metrics_environment_id' in indices_metrics:
            op.drop_index(op.f('ix_metrics_environment_id'), table_name='metrics')
            
        # Drop FK and column
        with op.batch_alter_table('metrics') as batch_op:
            # Drop foreign key if it exists
            # Note: finding the specific FK name can be tricky across dialects, 
            # but batch_alter_table helps with SQLite
            fks = inspector.get_foreign_keys('metrics')
            for fk in fks:
                if fk['referred_table'] == 'environments' and 'environment_id' in fk['constrained_columns']:
                    batch_op.drop_constraint(fk['name'], type_='foreignkey')
            
            batch_op.drop_column('environment_id')

    # Handle data_models table
    columns_data_models = [c['name'] for c in inspector.get_columns('data_models')]
    if 'environment_id' in columns_data_models:
        # Check and drop index
        indices_data_models = [i['name'] for i in inspector.get_indexes('data_models')]
        if 'ix_data_models_environment_id' in indices_data_models:
            op.drop_index(op.f('ix_data_models_environment_id'), table_name='data_models')
            
        # Drop FK and column
        with op.batch_alter_table('data_models') as batch_op:
            fks = inspector.get_foreign_keys('data_models')
            for fk in fks:
                if fk['referred_table'] == 'environments' and 'environment_id' in fk['constrained_columns']:
                    batch_op.drop_constraint(fk['name'], type_='foreignkey')
                    
            batch_op.drop_column('environment_id')
    # ### end Alembic commands ###
