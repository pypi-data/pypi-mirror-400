# Example configuration file for Refine Vibe Code
# Copy this file to your project root or use --config option

[scan]
# File patterns to include in scanning
include_patterns = [
    "*.py",
    "*requirements*.txt",
]

# File patterns to exclude from scanning
exclude_patterns = [
    "__pycache__/",
    "*.pyc",
    "node_modules/",
    ".git/",
    ".venv/",
    ".env/",
    "build/",
    "dist/",
]

[checkers]
# List of enabled checkers
enabled = [
    "hardcoded_secrets",
    "sql_injection",
    "dangerous_ai_logic",
    "vibe_naming",
    "comment_quality",
    "edge_cases",
    "package_check",
    "dependency_validation",
    "boilerplate",
]

# Only run classical (AST-based) checkers
classical_only = false

# Only run LLM-based checkers
llm_only = false

[chunking]
# Maximum lines per chunk (larger = fewer API calls, faster scans)
max_chunk_lines = 500

# Process chunks in parallel (significant speedup for large files)
parallel_chunks = true

# Maximum parallel API requests (4 is a good balance for rate limits)
max_parallel_requests = 10

# Use AST-based boundaries (function/class) instead of line counts
use_ast_boundaries = true

# Combine small files into single chunks to reduce API requests
stack_small_files = true

# Stack files if total is under this fraction of max_chunk_lines (0.0-1.0)
stack_threshold = 0.5

[llm]
# LLM provider (openai, google, claude)
provider = "google"

# Model name to use (see examples below for different providers)
model = "gemini-2.0-flash-exp"

# API key (can also be set via environment variables: OPENAI_API_KEY, GOOGLE_API_KEY, ANTHROPIC_API_KEY)
api_key = ""

# Temperature for responses (0.0 = deterministic, 2.0 = creative)
temperature = 0.1

# Maximum tokens in response
max_tokens = 100000

# Request timeout in seconds
timeout = 60

# LLM Integration Examples:
#
# Google Gemini Models (Recommended - high free tier limits):
# provider = "google"
# model = "gemini-2.0-flash-exp"      # Latest experimental, fast (default)
# model = "gemini-2.0-pro-exp"        # Most capable Gemini 2
# model = "gemini-2.0-flash"          # Stable fast model
# model = "gemini-2.5-pro-exp-03-25"  # Latest and most capable
# model = "gemini-1.5-pro"            # Stable production model, good performance
# model = "gemini-1.5-flash"          # Fast and cost-effective model
#
# OpenAI Models:
# provider = "openai"
# model = "gpt-4o-mini"               # Fast and cost-effective (default)
# model = "gpt-4o"                    # Most capable GPT-4
# model = "gpt-5"                     # Latest and most capable
# model = "o1-mini"                   # Reasoning model, slower but thorough
# model = "gpt-4-turbo"               # Previous generation, good balance
# model = "gpt-3.5-turbo"             # Fastest and cheapest, good for simple checks
#
# Anthropic Claude Models:
# provider = "claude"
# model = "claude-sonnet-4-20250514"  # Latest Sonnet, best balance (default)
# model = "claude-3-5-sonnet-20241022" # Previous Sonnet, proven
# model = "claude-3-5-haiku-20241022"  # Fast and cost-effective
# model = "claude-opus-4-20250514"    # Most capable, premium

[output]
# Output format (rich, json, plain)
format = "rich"

# Enable verbose output
verbose = false

# Show suggested fixes
show_fixes = true

# Enable colored output
color = true

