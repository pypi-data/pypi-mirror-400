# æµ‹è¯•ç¼–å†™æŒ‡å—

æœ¬æ–‡æ¡£ä¸ºtree-sitter-analyzeré¡¹ç›®æä¾›å…¨é¢çš„æµ‹è¯•ç¼–å†™æŒ‡å—ï¼Œå¸®åŠ©å¼€å‘è€…ç¼–å†™é«˜è´¨é‡ã€å¯ç»´æŠ¤çš„æµ‹è¯•ç”¨ä¾‹ã€‚

## ğŸ“‹ ç›®å½•

- [æµ‹è¯•ç»“æ„](#æµ‹è¯•ç»“æ„)
- [æµ‹è¯•æœ€ä½³å®è·µ](#æµ‹è¯•æœ€ä½³å®è·µ)
- [æµ‹è¯•ç¤ºä¾‹](#æµ‹è¯•ç¤ºä¾‹)
- [å¸¸è§é—®é¢˜è§£ç­”](#å¸¸è§é—®é¢˜è§£ç­”)
- [å·¥å…·å’Œèµ„æº](#å·¥å…·å’Œèµ„æº)

---

## æµ‹è¯•ç»“æ„

### åŸºæœ¬æµ‹è¯•ç»“æ„

æ‰€æœ‰æµ‹è¯•åº”éµå¾ª **Arrange-Act-Assert (AAA)** æ¨¡å¼ï¼š

```python
def test_example():
    """æµ‹è¯•ç¤ºä¾‹åŠŸèƒ½çš„æè¿°ã€‚"""
    # Arrange (å‡†å¤‡): è®¾ç½®æµ‹è¯•æ•°æ®å’Œä¾èµ–
    input_data = create_test_data()
    expected_result = calculate_expected(input_data)

    # Act (æ‰§è¡Œ): è°ƒç”¨è¢«æµ‹è¯•çš„å‡½æ•°
    actual_result = function_under_test(input_data)

    # Assert (æ–­è¨€): éªŒè¯ç»“æœ
    assert actual_result == expected_result
```

### æµ‹è¯•æ–‡ä»¶ç»„ç»‡

```
tests/
â”œâ”€â”€ unit/              # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ core/         # æ ¸å¿ƒæ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ mcp/          # MCPå·¥å…·æµ‹è¯•
â”‚   â”œâ”€â”€ cli/          # CLIå‘½ä»¤æµ‹è¯•
â”‚   â””â”€â”€ languages/     # è¯­è¨€æ’ä»¶æµ‹è¯•
â”œâ”€â”€ integration/        # é›†æˆæµ‹è¯•
â”œâ”€â”€ regression/         # å›å½’æµ‹è¯•
â”œâ”€â”€ compatibility/      # å…¼å®¹æ€§æµ‹è¯•
â”œâ”€â”€ property/          # å±æ€§æµ‹è¯•
â”œâ”€â”€ benchmarks/        # æ€§èƒ½åŸºå‡†æµ‹è¯•
â””â”€â”€ fixtures/          # æµ‹è¯•æ•°æ®å’Œè¾…åŠ©å·¥å…·
```

### æµ‹è¯•å‘½åçº¦å®š

- **æ–‡ä»¶å**: `test_<module_name>.py`
- **æµ‹è¯•ç±»**: `Test<ClassName>`
- **æµ‹è¯•å‡½æ•°**: `test_<function_name>_<scenario>`

ç¤ºä¾‹ï¼š
```python
# æ–‡ä»¶: tests/unit/core/test_performance.py

class TestPerformanceMonitor:
    def test_start_monitoring(self):
        """æµ‹è¯•å¼€å§‹ç›‘æ§åŠŸèƒ½ã€‚"""
        pass

    def test_stop_monitoring_with_active_session(self):
        """æµ‹è¯•åœ¨æ´»åŠ¨ä¼šè¯æ—¶åœæ­¢ç›‘æ§ã€‚"""
        pass
```

---

## æµ‹è¯•æœ€ä½³å®è·µ

### 1. ä½¿ç”¨æ¸…æ™°çš„docstring

æ‰€æœ‰æµ‹è¯•å‡½æ•°å¿…é¡»æœ‰Googleæ ¼å¼çš„docstringï¼š

```python
def test_analyze_python_file_success():
    """æµ‹è¯•æˆåŠŸåˆ†æPythonæ–‡ä»¶ã€‚

    åº”è¯¥æ­£ç¡®è¯†åˆ«æ‰€æœ‰ç±»ã€æ–¹æ³•å’Œå‡½æ•°ã€‚

    Args:
        None

    Returns:
        None

    Raises:
        AssertionError: å¦‚æœåˆ†æç»“æœä¸ç¬¦åˆé¢„æœŸ
    """
    # æµ‹è¯•ä»£ç 
```

### 2. é€‚å½“çš„æµ‹è¯•éš”ç¦»

æ¯ä¸ªæµ‹è¯•åº”è¯¥æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¾èµ–äºå…¶ä»–æµ‹è¯•çš„æ‰§è¡Œé¡ºåºï¼š

```python
@pytest.fixture
def fresh_test_file(tmp_path):
    """åˆ›å»ºä¸€ä¸ªæ–°çš„æµ‹è¯•æ–‡ä»¶ã€‚"""
    test_file = tmp_path / "test.py"
    test_file.write_text("def foo(): pass")
    return test_file

def test_isolated_test_1(fresh_test_file):
    """ç¬¬ä¸€ä¸ªç‹¬ç«‹æµ‹è¯•ã€‚"""
    # ä½¿ç”¨fresh_test_fileï¼Œä¸ä¾èµ–å…¶ä»–æµ‹è¯•
    pass

def test_isolated_test_2(fresh_test_file):
    """ç¬¬äºŒä¸ªç‹¬ç«‹æµ‹è¯•ã€‚"""
    # ä½¿ç”¨æ–°çš„fresh_test_fileå®ä¾‹
    pass
```

### 3. ä½¿ç”¨fixtures

ä½¿ç”¨pytest fixturesæ¥ç®¡ç†æµ‹è¯•æ•°æ®å’Œè®¾ç½®ï¼š

```python
# conftest.py
@pytest.fixture
def sample_python_code():
    """æä¾›ç¤ºä¾‹Pythonä»£ç ã€‚"""
    return """
def hello():
    print("Hello, World!")

class MyClass:
    def method(self):
        return 42
"""

@pytest.fixture
def temp_test_file(tmp_path, sample_python_code):
    """åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶ã€‚"""
    test_file = tmp_path / "sample.py"
    test_file.write_text(sample_python_code)
    return test_file

# æµ‹è¯•æ–‡ä»¶
def test_with_fixtures(temp_test_file):
    """ä½¿ç”¨fixturesçš„æµ‹è¯•ã€‚"""
    result = analyze_file(temp_test_file)
    assert result is not None
```

### 4. ä½¿ç”¨å‚æ•°åŒ–æµ‹è¯•

ä½¿ç”¨`@pytest.mark.parametrize`æ¥æµ‹è¯•å¤šä¸ªåœºæ™¯ï¼š

```python
@pytest.mark.parametrize("language,extension", [
    ("python", ".py"),
    ("java", ".java"),
    ("javascript", ".js"),
    ("typescript", ".ts"),
])
def test_language_detection(language, extension):
    """æµ‹è¯•ä¸åŒè¯­è¨€çš„æ–‡ä»¶æ‰©å±•åæ£€æµ‹ã€‚"""
    file_path = Path(f"test{extension}")
    detected = detect_language(file_path)
    assert detected == language
```

### 5. å¼‚å¸¸æµ‹è¯•

ä½¿ç”¨`pytest.raises`æ¥æµ‹è¯•å¼‚å¸¸æƒ…å†µï¼š

```python
def test_file_not_found_error():
    """æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨æ—¶çš„é”™è¯¯å¤„ç†ã€‚"""
    non_existent_file = Path("/non/existent/path.py")

    with pytest.raises(FileNotFoundError):
        analyze_file(non_existent_file)

def test_unsupported_language_error():
    """æµ‹è¯•ä¸æ”¯æŒçš„è¯­è¨€é”™è¯¯ã€‚"""
    with pytest.raises(LanguageNotSupportedError) as exc_info:
        analyze_file(Path("test.xyz"))

    assert "xyz" in str(exc_info.value)
```

### 6. å¼‚æ­¥æµ‹è¯•

å¯¹äºå¼‚æ­¥å‡½æ•°ï¼Œä½¿ç”¨`@pytest.mark.asyncio`ï¼š

```python
@pytest.mark.asyncio
async def test_async_analysis():
    """æµ‹è¯•å¼‚æ­¥åˆ†æåŠŸèƒ½ã€‚"""
    result = await analyze_file_async(Path("test.py"))
    assert result is not None
```

### 7. Mockå’ŒPatch

ä½¿ç”¨`unittest.mock`æ¥æ¨¡æ‹Ÿå¤–éƒ¨ä¾èµ–ï¼š

```python
from unittest.mock import patch, AsyncMock

def test_with_mock():
    """ä½¿ç”¨mockçš„æµ‹è¯•ã€‚"""
    with patch('tree_sitter_analyzer.core.load_parser') as mock_load:
        mock_load.return_value = mock_parser

        result = analyze_file(Path("test.py"))

        mock_load.assert_called_once()
        assert result is not None

@pytest.mark.asyncio
async def test_with_async_mock():
    """ä½¿ç”¨async mockçš„æµ‹è¯•ã€‚"""
    with patch('tree_sitter_analyzer.core.async_load_parser') as mock_load:
        mock_load.return_value = AsyncMock()

        result = await analyze_file_async(Path("test.py"))

        mock_load.assert_awaited_once()
```

### 8. ä½¿ç”¨æµ‹è¯•å·¥å‚

ä½¿ç”¨`tests/fixtures/factories.py`ä¸­çš„å·¥å‚å‡½æ•°åˆ›å»ºæµ‹è¯•æ•°æ®ï¼š

```python
from tests.fixtures.factories import (
    CodeElementFactory,
    AnalysisResultFactory,
    QueryResultFactory,
)

def test_with_factory():
    """ä½¿ç”¨æµ‹è¯•å·¥å‚çš„æµ‹è¯•ã€‚"""
    # åˆ›å»ºæµ‹è¯•å…ƒç´ 
    test_class = CodeElementFactory.create_class(name="TestClass")
    test_method = CodeElementFactory.create_method(name="testMethod")

    # åˆ›å»ºæµ‹è¯•ç»“æœ
    result = AnalysisResultFactory.create(
        elements=[test_class, test_method],
        metadata={"language": "python"}
    )

    assert len(result["elements"]) == 2
    assert result["metadata"]["language"] == "python"
```

---

## æµ‹è¯•ç¤ºä¾‹

### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```python
"""æµ‹è¯•æ€§èƒ½ç›‘æ§æ¨¡å—ã€‚"""

import pytest
from tree_sitter_analyzer.core.performance import PerformanceMonitor

class TestPerformanceMonitor:
    """PerformanceMonitorç±»çš„æµ‹è¯•å¥—ä»¶ã€‚"""

    def test_initialization(self):
        """æµ‹è¯•PerformanceMonitoråˆå§‹åŒ–ã€‚"""
        monitor = PerformanceMonitor()
        assert monitor is not None
        assert len(monitor.metrics) == 0

    def test_start_and_stop_monitoring(self):
        """æµ‹è¯•å¼€å§‹å’Œåœæ­¢ç›‘æ§ã€‚"""
        monitor = PerformanceMonitor()

        # å¼€å§‹ç›‘æ§
        monitor.start_monitoring("test_operation")
        assert "test_operation" in monitor.metrics

        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring("test_operation")
        assert monitor.metrics["test_operation"]["duration"] > 0

    @pytest.mark.parametrize("iterations", [1, 10, 100])
    def test_multiple_operations(self, iterations):
        """æµ‹è¯•å¤šæ¬¡æ“ä½œçš„æ€§èƒ½ç›‘æ§ã€‚"""
        monitor = PerformanceMonitor()

        for i in range(iterations):
            monitor.start_monitoring(f"operation_{i}")
            monitor.stop_monitoring(f"operation_{i}")

        assert len(monitor.metrics) == iterations
```

### é›†æˆæµ‹è¯•ç¤ºä¾‹

```python
"""æµ‹è¯•MCPå·¥å…·é›†æˆã€‚"""

import pytest
from tree_sitter_analyzer.mcp.tools.analyze_code_structure_tool import (
    AnalyzeCodeStructureTool,
)

class TestAnalyzeCodeStructureIntegration:
    """AnalyzeCodeStructureToolé›†æˆæµ‹è¯•ã€‚"""

    @pytest.mark.asyncio
    async def test_full_analysis_workflow(self, temp_test_file):
        """æµ‹è¯•å®Œæ•´çš„åˆ†æå·¥ä½œæµã€‚"""
        tool = AnalyzeCodeStructureTool()

        # æ‰§è¡Œåˆ†æ
        result = await tool.execute({
            "file_path": str(temp_test_file),
            "format_type": "full",
        })

        # éªŒè¯ç»“æœ
        assert result["success"] is True
        assert "elements" in result
        assert len(result["elements"]) > 0
```

### å›å½’æµ‹è¯•ç¤ºä¾‹

```python
"""æµ‹è¯•æ ¼å¼è¾“å‡ºå›å½’ã€‚"""

import pytest
from pathlib import Path

class TestFormatRegression:
    """æ ¼å¼è¾“å‡ºå›å½’æµ‹è¯•ã€‚"""

    @pytest.mark.regression
    def test_python_format_stability(self, sample_python_file):
        """æµ‹è¯•Pythonæ ¼å¼è¾“å‡ºçš„ç¨³å®šæ€§ã€‚"""
        # æ‰§è¡Œåˆ†æ
        result = analyze_code_structure(
            sample_python_file,
            format_type="full"
        )

        # åŠ è½½Golden Master
        golden_master_path = Path(
            "tests/golden_masters/python_sample_full.txt"
        )
        with open(golden_master_path, 'r') as f:
            expected_output = f.read()

        # éªŒè¯ä¸€è‡´æ€§
        assert result == expected_output, (
            f"Format output changed. "
            f"Run: uv run pytest tests/regression/test_format_regression.py "
            f"--update-golden-masters"
        )
```

### å±æ€§æµ‹è¯•ç¤ºä¾‹

```python
"""æµ‹è¯•è¯­è¨€æ£€æµ‹å±æ€§ã€‚"""

import pytest
from hypothesis import given, strategies as st
from tree_sitter_analyzer.core.language_detection import detect_language

class TestLanguageDetectionProperties:
    """è¯­è¨€æ£€æµ‹å±æ€§æµ‹è¯•ã€‚"""

    @given(st.text(min_size=1, max_size=100))
    def test_language_detection_never_fails(self, code):
        """è¯­è¨€æ£€æµ‹æ°¸è¿œä¸ä¼šå¤±è´¥ã€‚"""
        file_path = Path("test.py")
        result = detect_language(file_path, code)
        assert result is not None

    @given(st.lists(st.sampled_from(["python", "java", "javascript"]), min_size=1))
    def test_language_detection_consistency(self, languages):
        """è¯­è¨€æ£€æµ‹åœ¨å¤šæ¬¡è°ƒç”¨ä¸­ä¿æŒä¸€è‡´ã€‚"""
        file_path = Path("test.py")
        code = "def foo(): pass"

        results = [detect_language(file_path, code) for _ in range(10)]
        assert all(r == results[0] for r in results)
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•ç¤ºä¾‹

```python
"""æµ‹è¯•åˆ†ææ€§èƒ½åŸºå‡†ã€‚"""

import pytest

class TestPythonAnalysisBenchmarks:
    """Pythonåˆ†ææ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚"""

    def test_small_file_analysis(self, benchmark, small_python_file):
        """æµ‹è¯•å°æ–‡ä»¶åˆ†ææ€§èƒ½ã€‚"""
        result = benchmark(analyze_file, small_python_file)
        assert result is not None

    def test_large_file_analysis(self, benchmark, large_python_file):
        """æµ‹è¯•å¤§æ–‡ä»¶åˆ†ææ€§èƒ½ã€‚"""
        result = benchmark(analyze_file, large_python_file)
        assert result is not None

    @pytest.mark.parametrize("file_size", [100, 1000, 10000])
    def test_scaling_performance(self, benchmark, file_size):
        """æµ‹è¯•ä¸åŒæ–‡ä»¶å¤§å°çš„æ€§èƒ½æ‰©å±•ã€‚"""
        code = "def foo(): pass\n" * file_size
        test_file = Path("test.py")
        test_file.write_text(code)

        result = benchmark(analyze_file, test_file)
        assert result is not None
```

---

## å¸¸è§é—®é¢˜è§£ç­”

### Q1: å¦‚ä½•æµ‹è¯•ç§æœ‰æ–¹æ³•ï¼Ÿ

**A:** é€šå¸¸ä¸å»ºè®®ç›´æ¥æµ‹è¯•ç§æœ‰æ–¹æ³•ï¼ˆä»¥`_`å¼€å¤´ï¼‰ã€‚åº”è¯¥é€šè¿‡å…¬å…±APIæµ‹è¯•å®ƒä»¬çš„è¡Œä¸ºã€‚å¦‚æœå¿…é¡»æµ‹è¯•ï¼Œå¯ä»¥ä½¿ç”¨`patch`ï¼š

```python
def test_private_method_via_public_api():
    """é€šè¿‡å…¬å…±APIæµ‹è¯•ç§æœ‰æ–¹æ³•ã€‚"""
    with patch('module._private_method') as mock_private:
        mock_private.return_value = expected_value

        result = module.public_method()

        mock_private.assert_called_once()
        assert result == expected_value
```

### Q2: å¦‚ä½•å¤„ç†å¤–éƒ¨ä¾èµ–ï¼Ÿ

**A:** ä½¿ç”¨mockæ¥éš”ç¦»å¤–éƒ¨ä¾èµ–ï¼š

```python
from unittest.mock import patch

def test_with_external_dependency():
    """æµ‹è¯•å¤–éƒ¨ä¾èµ–éš”ç¦»ã€‚"""
    with patch('module.external_service.call') as mock_call:
        mock_call.return_value = {"status": "success"}

        result = module.function_using_external_service()

        mock_call.assert_called_once()
        assert result["status"] == "success"
```

### Q3: å¦‚ä½•æµ‹è¯•æ–‡ä»¶ç³»ç»Ÿæ“ä½œï¼Ÿ

**A:** ä½¿ç”¨`tmp_path` fixtureè¿›è¡Œä¸´æ—¶æ–‡ä»¶æ“ä½œï¼š

```python
def test_file_operations(tmp_path):
    """æµ‹è¯•æ–‡ä»¶ç³»ç»Ÿæ“ä½œã€‚"""
    test_file = tmp_path / "test.txt"
    test_file.write_text("content")

    result = process_file(test_file)

    assert test_file.exists()
    assert result == "processed"
```

### Q4: å¦‚ä½•æµ‹è¯•å¼‚æ­¥ä»£ç ï¼Ÿ

**A:** ä½¿ç”¨`@pytest.mark.asyncio`æ ‡è®°å’Œ`async def`ï¼š

```python
@pytest.mark.asyncio
async def test_async_function():
    """æµ‹è¯•å¼‚æ­¥å‡½æ•°ã€‚"""
    result = await async_function()
    assert result is not None
```

### Q5: å¦‚ä½•å¤„ç†æµ‹è¯•æ•°æ®ï¼Ÿ

**A:** ä½¿ç”¨fixtureså’Œæµ‹è¯•å·¥å‚ï¼š

```python
# ä½¿ç”¨fixture
@pytest.fixture
def test_data():
    return {"key": "value"}

# ä½¿ç”¨å·¥å‚
from tests.fixtures.factories import CodeElementFactory

test_element = CodeElementFactory.create_class(name="TestClass")
```

---

## å·¥å…·å’Œèµ„æº

### pytestæ’ä»¶

- **pytest-asyncio**: å¼‚æ­¥æµ‹è¯•æ”¯æŒ
- **pytest-benchmark**: æ€§èƒ½åŸºå‡†æµ‹è¯•
- **pytest-cov**: ä»£ç è¦†ç›–ç‡
- **pytest-timeout**: æµ‹è¯•è¶…æ—¶æ§åˆ¶
- **pytest-mock**: Mockå’Œpatchæ”¯æŒ

### æµ‹è¯•åº“

- **hypothesis**: åŸºäºå±æ€§çš„æµ‹è¯•
- **unittest.mock**: Mockå’Œpatch
- **pytest fixtures**: æµ‹è¯•æ•°æ®ç®¡ç†

### å‘½ä»¤

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
uv run pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
uv run pytest tests/unit/core/test_performance.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
uv run pytest tests/unit/core/test_performance.py::TestPerformanceMonitor::test_initialization

# è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•
uv run pytest tests/ --cov=tree_sitter_analyzer --cov-report=term-missing

# è¿è¡Œå›å½’æµ‹è¯•
uv run pytest tests/ -m regression

# è¿è¡ŒåŸºå‡†æµ‹è¯•
uv run pytest tests/benchmarks/ --benchmark-only

# è¿è¡Œå±æ€§æµ‹è¯•
uv run pytest tests/property/

# æŸ¥çœ‹æµ‹è¯•è¦†ç›–ç‡
uv run pytest --cov=tree_sitter_analyzer --cov-report=html
open htmlcov/index.html
```

### è´¨é‡æ£€æŸ¥

```bash
# è¿è¡Œä»£ç æ ¼å¼åŒ–
uv run black tests/

# è¿è¡Œlinter
uv run ruff check tests/

# è¿è¡Œç±»å‹æ£€æŸ¥
uv run mypy tests/

# è¿è¡Œå®‰å…¨æ£€æŸ¥
uv run bandit -r tests/
```

---

## æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡

- **å•å…ƒæµ‹è¯•**: > 80%
- **é›†æˆæµ‹è¯•**: > 70%
- **å›å½’æµ‹è¯•**: 100% (æ‰€æœ‰å…³é”®è·¯å¾„)
- **æ€»ä½“è¦†ç›–ç‡**: > 75%

---

## è´¡çŒ®æŒ‡å—

1. **ç¼–å†™æµ‹è¯•**: éµå¾ªæœ¬æŒ‡å—ç¼–å†™æ–°æµ‹è¯•
2. **è¿è¡Œæµ‹è¯•**: ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡
3. **æ£€æŸ¥è¦†ç›–ç‡**: ç¡®ä¿è¦†ç›–ç‡ä¸ä¸‹é™
4. **ä»£ç å®¡æŸ¥**: æäº¤PRè¿›è¡Œä»£ç å®¡æŸ¥
5. **æŒç»­æ”¹è¿›**: æ ¹æ®åé¦ˆæ”¹è¿›æµ‹è¯•

---

## å‚è€ƒèµ„æ–™

- [pytestæ–‡æ¡£](https://docs.pytest.org/)
- [hypothesisæ–‡æ¡£](https://hypothesis.readthedocs.io/)
- [unittest.mockæ–‡æ¡£](https://docs.python.org/3/library/unittest.mock.html)
- [é¡¹ç›®æµ‹è¯•è§„èŒƒ](../TESTING.md)
- [å›å½’æµ‹è¯•æŒ‡å—](./regression-testing-guide.md)
