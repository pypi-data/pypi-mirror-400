Metadata-Version: 2.4
Name: sparkwise
Version: 1.4.2
Summary: Automated Data Engineering specialist for Fabric Spark workloads - intelligent configuration analysis and optimization recommendations
Author-email: Santhosh Ravindran <santhoshravindran7@users.noreply.github.com>
License: MIT
Project-URL: Homepage, https://github.com/santhoshravindran7/sparkwise
Project-URL: Repository, https://github.com/santhoshravindran7/sparkwise
Project-URL: Issues, https://github.com/santhoshravindran7/sparkwise/issues
Keywords: spark,fabric,microsoft-fabric,optimization,pyspark,delta,configuration
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pyspark>=3.3.0
Requires-Dist: rich>=10.0.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: typing-extensions>=4.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Dynamic: license-file

# ğŸ”¥ Sparkwise

> **Achieve optimal Fabric Spark price-performance with automated insights - simplifies tuning, makes optimization fun**

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

sparkwise is an automated Data Engineering specialist for Apache Spark on Microsoft Fabric. It provides intelligent diagnostics, configuration recommendations, and comprehensive session profiling to help you achieve the best price-performance for your workloads - all while making Spark tuning simple and enjoyable.

## ğŸ¯ Why sparkwise?

Spark tuning on Microsoft Fabric doesn't have to be complex or expensive. sparkwise helps you:
- ğŸ’° **Optimize costs** - Detect configurations that waste capacity and increase runtime
- âš¡ **Maximize performance** - Enable Fabric-specific optimizations (Native Engine, V-Order, resource profiles)
- ğŸ“ **Simplify learning** - Interactive Q&A for 133 Spark/Delta/Fabric configurations
- ğŸ” **Understand workloads** - Comprehensive profiling of sessions, executors, jobs, and resources
- â±ï¸ **Save time** - Avoid 3-5min cold-starts by detecting Starter Pool blockers
- ğŸ“Š **Make data-driven decisions** - Priority-ranked recommendations with impact analysis

## âœ¨ Key Features

### ğŸ”¬ **Automated Diagnostics**
- **Native Execution Engine** - Verifies Velox usage, detects fallbacks to row-based processing
- **Spark Compute** - Analyzes Starter vs Custom Pool usage, warns about immutable configs
- **Data Skew Detection** - Identifies imbalanced task distributions
- **Delta Optimizations** - Checks V-Order, Deletion Vectors, Optimize Write, Auto Compaction
- **Runtime Tuning** - Validates AQE, partition sizing, scheduler mode

### ğŸ“Š **Comprehensive Profiling**
- **Session Profiling** - Application metadata, resource allocation, memory breakdown
- **Executor Profiling** - Executor status, memory utilization, task distribution
- **Job Profiling** - Job/stage/task metrics, bottleneck detection
- **Resource Profiling** - Efficiency scoring, utilization analysis, optimization recommendations

### ğŸš€ **Advanced Performance Analysis** (NEW!)
- **Real Metrics Collection** - Uses actual Spark stage/task data instead of estimates
- **Scalability Prediction** - Compare Starter vs Custom Pool with real VCore-hour calculations
- **Stage Timeline** - Visualize execution patterns with parallel/sequential analysis
- **Efficiency Analysis** - Quantify wasted compute in VCore-hours with actionable recommendations

### ğŸ” **Advanced Skew Detection** (NEW!)
- **Task Duration Analysis** - Detect stragglers and long-running tasks with variance detection
- **Partition-Level Analysis** - Identify data distribution imbalances with statistical metrics
- **Skewed Join Detection** - Analyze join patterns and recommend broadcast vs salting strategies
- **Automatic Mitigation** - Get code examples for salting, AQE, and broadcast optimizations

### ğŸ¯ **SQL Query Plan Analysis** (NEW!)
- **Anti-Pattern Detection** - Identify cartesian products, full scans, and excessive shuffles
- **Native Engine Compatibility** - Check if queries use Fabric Native Engine (3-8x faster)
- **Z-Order Recommendations** - Suggest best columns for Delta optimization based on cardinality
- **Caching Opportunities** - Detect repeated table scans that benefit from caching
- **Fabric Best Practices** - V-Order, broadcast joins, AQE, and partition recommendations

### ï¿½ **Storage Optimization** (NEW in v1.4.0!)
- **Small File Detection** - Identify Delta tables with excessive small files (<10MB configurable threshold)
- **VACUUM ROI Calculator** - Estimate storage savings vs compute cost using OneLake pricing ($0.023/GB/month)
- **Partition Effectiveness** - Analyze partition count, skew ratios, and detect over/under-partitioning
- **Comprehensive Analysis** - Run all storage checks in one command with actionable recommendations
- **Storage Cost Tracking** - Calculate monthly OneLake storage costs and optimization opportunities

### ï¿½ğŸ’¡ **Interactive Configuration Assistant**
- **133 documented configurations** - Spark, Delta Lake, Fabric-specific, and Runtime 1.2 configs
- **Context-aware guidance** - Workload-specific recommendations with impact analysis
- **Resource profile support** - Understand writeHeavy, readHeavyForSpark, readHeavyForPBI profiles
- **Search capabilities** - Find configs by keyword or partial name

### ğŸ“ˆ **Priority-Based Recommendations**
- **Color-coded priorities** - Critical (red) â†’ High (yellow) â†’ Medium (blue) â†’ Low (dim)
- **Formatted tables** - Clean, readable output with impact explanations
- **Actionable guidance** - Specific commands and configuration values

## ğŸš€ Quick Start

### Installation

```bash
pip install sparkwise
```

Or install the wheel file directly in Fabric:

```python
%pip install sparkwise-0.1.0-py3-none-any.whl
```

### Basic Usage

```python
from sparkwise import diagnose, ask

# Run comprehensive analysis on current session
diagnose.analyze()

# Ask about any configuration
ask.config('spark.native.enabled')

# Search for configurations
ask.search('optimize')
```

### Session Profiling

```python
from sparkwise import (profile, profile_executors, profile_jobs, profile_resources,
                       predict_scalability, show_timeline, analyze_efficiency)

# Profile complete session
profile()

# Profile executor metrics
profile_executors()

# Analyze job performance
profile_jobs()

# Check resource efficiency
profile_resources()

# Advanced profiling features
predict_scalability()  # Compare pool configurations
show_timeline()        # Visualize stage execution
analyze_efficiency()   # Quantify compute waste
```

### Advanced Analysis

```python
from sparkwise import detect_skew, analyze_query

# Detect data skew
skew_results = detect_skew()  # Analyze task-level skew

# Analyze specific DataFrame for partition skew
from sparkwise.core.advanced_skew_detector import AdvancedSkewDetector
detector = AdvancedSkewDetector()
detector.analyze_partition_skew(your_df, ["key_column"])

# Detect skewed joins
detector.detect_skewed_joins(large_df, small_df, "join_key")

# Analyze SQL query plans
query_results = analyze_query(your_df)

# Get Z-Order recommendations
from sparkwise.core.query_plan_analyzer import QueryPlanAnalyzer
analyzer = QueryPlanAnalyzer()
zorder_cols = analyzer.suggest_zorder_columns(delta_df, ["filtered_col"])

# Detect caching opportunities
analyzer.detect_repeated_subqueries(your_df)
```

### Storage Optimization

```python
import sparkwise

# Comprehensive storage analysis
sparkwise.analyze_storage("Tables/mytable")

# Individual analyses
sparkwise.check_small_files("Tables/mytable", threshold_mb=10)
sparkwise.vacuum_roi("Tables/mytable", retention_hours=168)
sparkwise.check_partitions("Tables/mytable")
```

**CLI Usage:**
```bash
# Comprehensive storage analysis
sparkwise storage analyze Tables/mytable

# Check for small files
sparkwise storage small-files Tables/mytable --threshold 10

# Calculate VACUUM ROI
sparkwise storage vacuum-roi Tables/mytable --retention-hours 168

# Analyze partition effectiveness
sparkwise storage partitions Tables/mytable
```

## ğŸ“Š Sample Output

### Diagnostic Analysis

```
ğŸ”¥ sparkwise Analysis ğŸ”¥

ğŸ” Native Execution Engine
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš ï¸ Warning: Native keywords not found in physical plan
   ğŸ’¡ Check for unsupported operators or complex UDFs

âš¡ Spark Compute
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Your job uses 1 executors - fits in Starter Pool
   ğŸ’¡ Ensure 'Starter Pool' is selected in workspace settings

ğŸ’¾ Storage & Delta Optimizations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â„¹ï¸ V-Order is DISABLED (optimal for write-heavy workloads)
   Benefit: 2x faster writes vs V-Order enabled
   ğŸ’¡ Enable only for read-heavy workloads (Power BI/analytics)
      Trade-off: 3-10x faster reads, but 15-20% slower writes

â„¹ï¸ Optimize Write is DISABLED (optimal for writeHeavy profile - default)
   Benefit: Maximum write throughput for ETL and data ingestion
   ğŸ’¡ Enable only for read-heavy or streaming workloads
      - readHeavyForSpark: spark.fabric.resourceProfile=readHeavyForSpark
      - readHeavyForPBI: spark.fabric.resourceProfile=readHeavyForPBI

âš™ï¸ Runtime Tuning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â›” CRITICAL: Adaptive Query Execution (AQE) is DISABLED
   ğŸ’¡ Enable immediately: spark.sql.adaptive.enabled=true
      Benefits: Dynamic coalescing, skew joins, better parallelism

ğŸ“‹ Summary of Findings
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Category            â”‚ Status â”‚ Critical Issues â”‚ Recommendations â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Native Execution    â”‚ âš ï¸     â”‚ 1               â”‚ 1               â”‚
â”‚ Spark Compute       â”‚ âœ…     â”‚ 0               â”‚ 1               â”‚
â”‚ Data Skew           â”‚ âœ…     â”‚ 0               â”‚ 0               â”‚
â”‚ Delta               â”‚ âœ…     â”‚ 0               â”‚ 3               â”‚
â”‚ Runtime             â”‚ âš ï¸     â”‚ 1               â”‚ 2               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”§ Configuration Recommendations
Total recommendations: 7

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Priority â”‚ Configuration                   â”‚ Action         â”‚ Impact       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CRITICAL â”‚ spark.sql.adaptive.enabled      â”‚ Set to 'true'  â”‚ Enable       â”‚
â”‚          â”‚                                 â”‚                â”‚ dynamic      â”‚
â”‚          â”‚                                 â”‚                â”‚ partition    â”‚
â”‚          â”‚                                 â”‚                â”‚ coalescing   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MEDIUM   â”‚ spark.sql.parquet.vorder.enabledâ”‚ Enable for     â”‚ 3-10x faster â”‚
â”‚          â”‚                                 â”‚ read-heavy     â”‚ reads for    â”‚
â”‚          â”‚                                 â”‚ workloads only â”‚ Power BI     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ¨ Analysis complete!
```

### Interactive Q&A

```python
ask.config('spark.fabric.resourceProfile')
```

**Output:**
```
ğŸ“š spark.fabric.resourceProfile

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Default: writeHeavy
Scope: session

What it does:
FABRIC CRITICAL: Selects predefined Spark resource profiles optimized 
for specific workload patterns. Simplifies configuration tuning.

Recommendations for your workload:
  â€¢ etl_ingestion: writeHeavy - optimized for ETL and data ingestion
  â€¢ analytics_spark: readHeavyForSpark - optimized for analytical queries
  â€¢ power_bi: readHeavyForPBI - optimized for Power BI Direct Lake
  â€¢ custom_needs: custom - user-defined configuration

Fabric-specific notes:
Microsoft Fabric resource profiles provide workload-optimized settings:

**writeHeavy (DEFAULT):**
- V-Order: DISABLED for faster writes
- Optimize Write: NULL/DISABLED for maximum throughput
- Use Case: ETL pipelines, data ingestion, batch transformations

**readHeavyForSpark:**
- Optimize Write: ENABLED with 128MB bins
- Use Case: Interactive Spark queries, analytical workloads

**readHeavyForPBI:**
- V-Order: ENABLED for Power BI optimization
- Optimize Write: ENABLED with 1GB bins
- Use Case: Power BI dashboards, Direct Lake scenarios

Related configurations:
  â€¢ spark.sql.parquet.vorder.enabled
  â€¢ spark.databricks.delta.optimizeWrite.enabled
  â€¢ spark.microsoft.delta.optimizeWrite.enabled

Examples:
  spark.conf.set('spark.fabric.resourceProfile', 'readHeavyForSpark')
  spark.conf.set('spark.fabric.resourceProfile', 'writeHeavy')

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Scalability Prediction

```python
from sparkwise import predict_scalability

# Run after executing your workload
predict_scalability(runs_per_month=100)
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š SCALABILITY ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ Workload Profile
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Current Runtime: 45.2 seconds
  Monthly Runs: 100
  Total Monthly Runtime: 75.3 minutes

ğŸ¯ Starter Pool (Current Configuration)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Configuration: 2 vCores, 8GB memory
  VCore-Hours/Month: 2.51 hours
  Estimated Cost: $2.76/month
  Startup Overhead: ~5-10 seconds
  Status: âœ… OPTIMAL - Workload fits in Starter Pool

âš¡ Custom Pool Comparison
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Configuration: 8 vCores, 32GB memory
  VCore-Hours/Month: 10.04 hours
  Estimated Cost: $11.04/month
  Startup Overhead: 3-5 minutes
  Performance Gain: ~2-3x faster execution

ğŸ’¡ Recommendation: STAY ON STARTER POOL
  â€¢ Your workload is well-suited for Starter Pool
  â€¢ Custom Pool would cost 4x more with cold-start delays
  â€¢ Consider Custom Pool only if runs exceed 500/month
```

### Efficiency Analysis

```python
from sparkwise import analyze_efficiency

# Run after your Spark job completes
analyze_efficiency(runs_per_month=100)
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš¡ JOB EFFICIENCY ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Execution Metrics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total Runtime: 45.2 seconds
  Active Compute: 38.6 seconds (85.4%)
  Wasted Compute: 6.6 seconds (14.6%)
  
  VCore-Hours Used: 0.025 hours
  VCore-Hours Wasted: 0.004 hours

ğŸ’° Cost Impact (100 runs/month)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Monthly Compute: 2.51 VCore-hours
  Monthly Waste: 0.37 VCore-hours (14.6%)
  Wasted Cost: $0.41/month

ğŸ¯ Efficiency Score: 85.4% (GOOD)

âœ¨ Top Optimization Opportunities
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. Enable AQE for dynamic partition coalescing
     Impact: Reduce shuffle overhead by 20-30%
  
  2. Optimize shuffle partitions
     Current: 200 partitions
     Recommended: 50 partitions (based on data size)
     Impact: Reduce task overhead, improve parallelism
```

### Storage Optimization - Small Files

```python
import sparkwise
sparkwise.check_small_files("Tables/green_tripdata_2017", threshold_mb=10)
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ SMALL FILE ANALYSIS: green_tripdata_2017
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š File Statistics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                 â”‚ Value            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Files            â”‚ 1,247            â”‚
â”‚ Total Size             â”‚ 15.3 GB          â”‚
â”‚ Average File Size      â”‚ 12.6 MB          â”‚
â”‚ Smallest File          â”‚ 1.2 MB           â”‚
â”‚ Largest File           â”‚ 128.4 MB         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”´ CRITICAL: Small File Problem Detected
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Estimated Small Files (<10MB): 498 files (39.9%)
  
  Performance Impact:
    â€¢ 40% of files are too small
    â€¢ Excessive metadata operations
    â€¢ Poor query performance
    â€¢ Increased storage costs

ğŸ’¡ Recommendations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. Run OPTIMIZE to compact small files:
     spark.sql("OPTIMIZE delta.`Tables/green_tripdata_2017`")
  
  2. Enable Auto-Optimize for future writes:
     spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
     spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
  
  3. Consider repartitioning on write:
     df.repartition(50).write.format("delta").save("Tables/green_tripdata_2017")
  
  Expected Improvements:
    â€¢ Reduce file count by 60-80%
    â€¢ 3-5x faster query performance
    â€¢ 20-30% reduction in metadata overhead
```

### Storage Optimization - VACUUM ROI

```python
import sparkwise
sparkwise.vacuum_roi("Tables/green_tripdata_2017", retention_hours=168)
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’° VACUUM ROI ANALYSIS: green_tripdata_2017
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Current Storage State
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                 â”‚ Value            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Current Size           â”‚ 15.3 GB          â”‚
â”‚ Retention Period       â”‚ 168 hours (7d)   â”‚
â”‚ Removable Operations   â”‚ 23 operations    â”‚
â”‚ Last VACUUM            â”‚ 45 days ago      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¾ Storage Savings Estimate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Reclaimable Space: 4.59 GB (30.0%)
  
  OneLake Storage Cost:
    Current: $0.35/month ($0.023/GB)
    After VACUUM: $0.25/month
    Monthly Savings: $0.11/month

âš¡ VACUUM Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Estimated Compute: $1.50
  Break-even Period: 13.6 months

âœ… RECOMMENDATION: RUN VACUUM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Although break-even is 14 months, VACUUM provides benefits:
    â€¢ Improved query performance (fewer files to scan)
    â€¢ Reduced metadata overhead
    â€¢ Better data governance
    â€¢ Simplified time travel queries

  Command:
    spark.sql("VACUUM delta.`Tables/green_tripdata_2017` RETAIN 168 HOURS")
  
  Best Practice:
    â€¢ Run VACUUM quarterly for large tables
    â€¢ Run VACUUM monthly for frequently updated tables
    â€¢ Adjust retention based on time travel needs
```

### Storage Optimization - Partition Analysis

```python
import sparkwise
sparkwise.check_partitions("Tables/green_tripdata_2017")
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ—‚ï¸ PARTITION EFFECTIVENESS: green_tripdata_2017
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Partition Statistics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                 â”‚ Value            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Partition Columns      â”‚ year, month      â”‚
â”‚ Total Partitions       â”‚ 12               â”‚
â”‚ Partitions Scanned     â”‚ 12 (100%)        â”‚
â”‚ Average Rows/Partition â”‚ 850,423          â”‚
â”‚ Max Rows (Jan)         â”‚ 1,104,518        â”‚
â”‚ Min Rows (Nov)         â”‚ 612,847          â”‚
â”‚ Skew Ratio             â”‚ 1.8x             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸŸ¢ GOOD: Well-Balanced Partitions
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ Partition count is optimal (10-100 range)
  â€¢ Skew ratio is acceptable (<3x)
  â€¢ Each partition has sufficient data

ğŸ’¡ Optimization Opportunities
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. Enable Z-Order for frequently filtered columns:
     spark.sql("OPTIMIZE delta.`Tables/green_tripdata_2017` 
                ZORDER BY (vendor_id, payment_type)")
     
     Benefits:
       â€¢ 2-5x faster queries on vendor_id, payment_type
       â€¢ No partition overhead
       â€¢ Maintains good compression
  
  2. Consider liquid clustering for high-cardinality columns:
     ALTER TABLE green_tripdata_2017 
     CLUSTER BY (vendor_id, payment_type, pickup_location)
     
     Benefits:
       â€¢ Automatic optimization on writes
       â€¢ Better for evolving query patterns
       â€¢ Handles high-cardinality columns

ğŸ¯ Partition Health: âœ… OPTIMAL
  Your partitioning strategy is working well!
```

### Comprehensive Storage Analysis

```python
import sparkwise
sparkwise.analyze_storage("Tables/green_tripdata_2017")
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” COMPREHENSIVE STORAGE ANALYSIS: green_tripdata_2017
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[Shows combined output of all three analyses above:]
  1. Small File Detection (with recommendations)
  2. VACUUM ROI Calculation (with cost analysis)
  3. Partition Effectiveness (with optimization suggestions)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ PRIORITY ACTION ITEMS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸ”´ CRITICAL (Do Now):
    â€¢ Run OPTIMIZE to compact 498 small files
    â€¢ Enable Auto-Optimize for future writes
  
  ğŸŸ¡ HIGH (This Week):
    â€¢ Add Z-Order on vendor_id, payment_type
    â€¢ Run VACUUM to reclaim 4.59 GB
  
  ğŸŸ¢ MEDIUM (This Month):
    â€¢ Review partition strategy quarterly
    â€¢ Monitor file growth patterns
    â€¢ Set up automated OPTIMIZE jobs

ğŸ’° Total Potential Savings:
  â€¢ Storage: $0.11/month (after VACUUM)
  â€¢ Compute: 20-30% reduction (after OPTIMIZE)
  â€¢ Query Performance: 3-5x faster
```

## ğŸ“¦ What's Included

### Core Modules
- `diagnose` - Main diagnostic engine with 5 check categories
- `ask` - Interactive configuration Q&A system
- `profile` - Session profiling
- `profile_executors` - Executor-level metrics
- `profile_jobs` - Job/stage/task analysis
- `profile_resources` - Resource efficiency scoring
- `predict_scalability` - Compare Starter vs Custom Pool configurations
- `analyze_efficiency` - Quantify wasted compute with VCore-hour metrics
- `show_timeline` - Visualize stage execution patterns
- `detect_skew` - Advanced skew detection with mitigation strategies
- `analyze_query` - SQL query plan analysis with anti-pattern detection
- `analyze_storage` - Comprehensive storage optimization (v1.4.0)
- `check_small_files` - Small file detection with thresholds (v1.4.0)
- `vacuum_roi` - VACUUM ROI calculator with OneLake pricing (v1.4.0)
- `check_partitions` - Partition effectiveness analysis (v1.4.0)

### Knowledge Base (133 Configurations)
- **33 Spark configs** - Core settings for shuffle, memory, AQE, serialization
- **45 Delta configs** - Delta Lake optimizations, V-Order, Deletion Vectors
- **10 Fabric configs** - Native Engine, resource profiles, OneLake storage
- **45 Runtime 1.2 configs** - Latest Fabric Runtime 1.2 features

### Latest Features
- âœ… Storage optimization suite - Small files, VACUUM ROI, partition analysis (v1.4.0)
- âœ… OneLake cost tracking - Real pricing ($0.023/GB/month) for storage decisions
- âœ… Advanced skew detection - Task duration, partition-level, and join analysis
- âœ… SQL query plan analyzer - Anti-patterns, Native Engine checks, Z-Order suggestions
- âœ… Real metrics profiling - VCore-hour calculations, efficiency scoring
- âœ… Scalability prediction - Starter vs Custom Pool cost comparison
- âœ… Fabric resource profiles (writeHeavy, readHeavyForSpark, readHeavyForPBI)
- âœ… Advanced Delta optimizations (Fast Optimize, Adaptive File Size, File Level Target)
- âœ… Driver Mode Snapshot for faster metadata operations
- âœ… Priority-based recommendation tables
- âœ… Color-coded terminal output with Rich library

## ğŸ¯ Use Cases

### Data Engineers
- **Optimize ETL pipelines** - Detect bottlenecks, tune parallelism, reduce costs
- **Validate configurations** - Ensure proper resource profiles and pool usage
- **Debug job failures** - Understand errors with plain English explanations
- **Manage storage costs** - Track OneLake usage, optimize file layouts, VACUUM ROI
- **Monitor table health** - Detect small files, partition skew, storage bloat

### Data Scientists
- **Improve notebook performance** - Enable Native Engine, optimize memory usage
- **Understand Spark behavior** - Learn configurations through interactive Q&A
- **Profile experiments** - Track resource usage and efficiency
- **Optimize data access** - Identify caching opportunities, partition pruning

### Platform Admins
- **Standardize best practices** - Share optimal configurations across teams
- **Monitor capacity usage** - Identify jobs forcing Custom Pool usage
- **Cost optimization** - Detect over-provisioned or misconfigured workloads
- **Storage governance** - Track OneLake costs, enforce OPTIMIZE/VACUUM policies
- **Performance tracking** - Monitor VCore-hour usage, identify waste

## ğŸ“ Examples

Check out the [examples](examples/) directory:
- [basic_analysis.py](examples/basic_analysis.py) - Basic diagnostic workflow
- [config_qa_demo.py](examples/config_qa_demo.py) - Configuration Q&A usage
- [profiling_demo.py](examples/profiling_demo.py) - Comprehensive profiling examples
- [scalability_demo.py](examples/scalability_demo.py) - Scalability prediction and efficiency analysis
- [skew_detection_demo.py](examples/skew_detection_demo.py) - Advanced skew detection
- [query_analysis_demo.py](examples/query_analysis_demo.py) - SQL query plan analysis
- [storage_optimization_demo.py](examples/storage_optimization_demo.py) - Storage optimization (v1.4.0)
- [knowledge_base_demo.py](examples/knowledge_base_demo.py) - Knowledge base exploration
- [immutable_configs_demo.py](examples/immutable_configs_demo.py) - Starter Pool optimization

## ğŸ§ª Running Tests

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run all tests
pytest

# Run with coverage
pytest --cov=sparkwise --cov-report=html

# Run specific test file
pytest tests/test_advisor.py
```

## ğŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone the repository
git clone https://github.com/santhoshravindran7/sparkwise.git
cd sparkwise

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install in development mode
pip install -e ".[dev]"

# Run tests
pytest
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

Built with â¤ï¸ for the Microsoft Fabric Data Engineering and Data Science community.

## ğŸ“¬ Contact & Support

- **Author**: Santhosh Ravindran
- **GitHub**: [@santhoshravindran7](https://github.com/santhoshravindran7)
- **Feedback**: [Share your feedback, report bugs, or request features](https://forms.office.com/r/cWBb4Y0n4z)

## ğŸ‰ What's New in v1.4.0

### ğŸ’¾ Storage Optimization Suite
- âœ… **Small file detection** - Identify tables with excessive files <10MB (configurable threshold)
- âœ… **VACUUM ROI calculator** - Estimate storage savings vs compute cost with OneLake pricing ($0.023/GB/month)
- âœ… **Partition effectiveness** - Analyze partition count, skew ratios, detect over/under-partitioning
- âœ… **Comprehensive analysis** - Run all storage checks with one command
- âœ… **CLI integration** - `sparkwise storage analyze|small-files|vacuum-roi|partitions`
- âœ… **Actionable recommendations** - Get SQL commands for OPTIMIZE, VACUUM, Z-Order, partitioning

### Use Cases
- **Cost optimization** - Track OneLake storage costs, identify VACUUM opportunities
- **Performance tuning** - Detect small file problems impacting query speed
- **Data governance** - Monitor table health, enforce optimization policies
- **Capacity planning** - Understand storage growth patterns, predict costs

### Example
```python
import sparkwise

# Run comprehensive storage analysis
sparkwise.analyze_storage("Tables/mytable")

# Get small file recommendations
sparkwise.check_small_files("Tables/mytable", threshold_mb=10)

# Calculate VACUUM ROI
sparkwise.vacuum_roi("Tables/mytable", retention_hours=168)

# Analyze partition effectiveness
sparkwise.check_partitions("Tables/mytable")
```

---

**Previous Releases:**

<details>
<summary>v0.1.0 - Initial Release</summary>

- âœ¨ Complete profiling suite (session, executor, job, resource profilers)
- ğŸ¨ Rich terminal output with color-coded priorities
- ğŸ“Š Priority-based recommendation tables
- ğŸ”§ Fabric resource profile support (writeHeavy, readHeavy profiles)
- âš¡ 4 new advanced Delta optimizations
- ğŸ“š 133 documented configurations (up from 100)
- ğŸ¯ Context-aware Optimize Write recommendations
- ğŸš€ CLI support for all profiling operations

</details>

---

Make Spark tuning fun again! ğŸš€âœ¨
