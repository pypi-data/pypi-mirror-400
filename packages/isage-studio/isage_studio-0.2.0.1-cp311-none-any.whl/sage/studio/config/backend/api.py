"""
SAGE Studio Backend API

A simple FastAPI backend service that provides real SAGE data to the Studio frontend.
"""

import importlib
import inspect
import ipaddress
import json
import os
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Annotated, Any
from urllib.parse import urlparse, urlunparse

import requests
import uvicorn
from fastapi import Depends, FastAPI, File, HTTPException, Request, UploadFile, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from pydantic import BaseModel

from sage.common.config.ports import SagePorts
from sage.common.config.user_paths import get_user_data_dir as get_common_user_data_dir
from sage.studio.services.agent_orchestrator import get_orchestrator
from sage.studio.services.auth_service import (
    ACCESS_TOKEN_EXPIRE_MINUTES,
    AuthService,
    Token,
    User,
    UserCreate,
    get_auth_service,
)
from sage.studio.services.file_upload_service import get_file_upload_service
from sage.studio.services.memory_integration import get_memory_service
from sage.studio.services.stream_handler import get_stream_handler

# Gateway URL for API calls
# Use 127.0.0.1 instead of localhost to avoid IPv6 issues and ensure consistent behavior
GATEWAY_HOST = os.getenv("SAGE_GATEWAY_HOST", "127.0.0.1")
GATEWAY_BASE_URL = f"http://{GATEWAY_HOST}:{SagePorts.GATEWAY_DEFAULT}"

# Load environment variables from .env file
try:
    from dotenv import load_dotenv

    from sage.common.config import find_sage_project_root

    # Use centralized function to find project root
    repo_root = find_sage_project_root()
    if repo_root:
        env_file = repo_root / ".env"
        if env_file.exists():
            load_dotenv(env_file, override=True)  # override=True to ensure env vars are updated
            # Use logging instead of print for production
            import logging

            logging.info(f"Loaded environment variables from {env_file}")
        else:
            import logging

            logging.warning(f".env file not found at {env_file}")
    else:
        import logging

        logging.warning("Could not find SAGE project root, skipping .env loading")
except ImportError as e:
    import logging

    logging.warning(f"Failed to load environment: {e}")


def _convert_pipeline_to_job(
    pipeline_data: dict, pipeline_id: str, file_path: Path | None = None
) -> dict:
    """å°†æ‹“æ‰‘å›¾æ•°æ®è½¬æ¢ä¸º Job æ ¼å¼"""
    from datetime import datetime

    # ä»æ‹“æ‰‘å›¾æ•°æ®ä¸­æå–ä¿¡æ¯
    name = pipeline_data.get("name", f"æ‹“æ‰‘å›¾ {pipeline_id}")
    description = pipeline_data.get("description", "")
    nodes = pipeline_data.get("nodes", [])
    edges = pipeline_data.get("edges", [])

    # åˆ›å»ºæ“ä½œç¬¦åˆ—è¡¨
    operators = []
    for i, node in enumerate(nodes):
        # æ„å»ºä¸‹æ¸¸è¿æ¥
        downstream = []
        for edge in edges:
            if edge.get("source") == node.get("id"):
                # æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹çš„ç´¢å¼•
                target_node = next((n for n in nodes if n.get("id") == edge.get("target")), None)
                if target_node:
                    target_index = next(
                        (j for j, n in enumerate(nodes) if n.get("id") == edge.get("target")),
                        None,
                    )
                    if target_index is not None:
                        downstream.append(target_index)

        operator = {
            "id": i,
            "name": node.get("name", f"Operator_{i}"),
            "numOfInstances": 1,
            "downstream": downstream,
        }
        operators.append(operator)

    # ä»æ–‡ä»¶åæˆ–æ–‡ä»¶å…ƒæ•°æ®ä¸­æå–åˆ›å»ºæ—¶é—´
    create_time = None

    # æ–¹æ³•1: ä»æ–‡ä»¶åè§£ææ—¶é—´æˆ³ (pipeline_1759908680.json)
    if pipeline_id.startswith("pipeline_"):
        try:
            timestamp_str = pipeline_id.replace("pipeline_", "")
            timestamp = int(timestamp_str)
            create_time = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, OSError) as e:
            print(f"Failed to parse timestamp from pipeline_id {pipeline_id}: {e}")

    # æ–¹æ³•2: å¦‚æœè§£æå¤±è´¥,ä½¿ç”¨æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
    if create_time is None and file_path and file_path.exists():
        try:
            mtime = file_path.stat().st_mtime
            create_time = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")
        except Exception as e:
            print(f"Failed to get file mtime for {file_path}: {e}")

    # æ–¹æ³•3: å…œåº•ä½¿ç”¨å½“å‰æ—¶é—´
    if create_time is None:
        create_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    job = {
        "jobId": pipeline_id,
        "name": name,
        "description": description,  # æ·»åŠ æè¿°å­—æ®µ
        "isRunning": False,  # æ‹“æ‰‘å›¾é»˜è®¤ä¸åœ¨è¿è¡Œ
        "nthreads": "1",
        "cpu": "0%",
        "ram": "0GB",
        "startTime": create_time,
        "duration": "00:00:00",
        "nevents": 0,
        "minProcessTime": 0,
        "maxProcessTime": 0,
        "meanProcessTime": 0,
        "latency": 0,
        "throughput": 0,
        "ncore": 1,
        "periodicalThroughput": [0],
        "periodicalLatency": [0],
        "totalTimeBreakdown": {
            "totalTime": 0,
            "serializeTime": 0,
            "persistTime": 0,
            "streamProcessTime": 0,
            "overheadTime": 0,
        },
        "schedulerTimeBreakdown": {
            "overheadTime": 0,
            "streamTime": 0,
            "totalTime": 0,
            "txnTime": 0,
        },
        "operators": operators,
        # æ·»åŠ  config å­—æ®µï¼Œä¿ç•™åŸå§‹çš„ React Flow æ ¼å¼æ•°æ®
        "config": {
            "name": name,
            "description": description,
            "nodes": nodes,
            "edges": edges,
        },
    }

    return job


def _get_sage_dir() -> Path:
    """è·å– SAGE ç›®å½•è·¯å¾„"""
    # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
    env_dir = os.environ.get("SAGE_OUTPUT_DIR")
    if env_dir:
        sage_dir = Path(env_dir)
    else:
        # æ£€æŸ¥æ˜¯å¦åœ¨å¼€å‘ç¯å¢ƒä¸­
        current_dir = Path.cwd()
        if (current_dir / "packages" / "sage-common").exists():
            sage_dir = current_dir / ".sage"
        else:
            sage_dir = Path.home() / ".sage"

    sage_dir.mkdir(parents=True, exist_ok=True)
    return sage_dir


def get_user_data_dir(user_id: str) -> Path:
    """Get user-specific data directory."""
    # Use the common user data dir as base
    base_dir = get_common_user_data_dir()
    user_dir = base_dir / "users" / str(user_id)
    user_dir.mkdir(parents=True, exist_ok=True)
    return user_dir


def get_user_pipelines_dir(user_id: str) -> Path:
    """Get user-specific pipelines directory."""
    pipelines_dir = get_user_data_dir(user_id) / "pipelines"
    pipelines_dir.mkdir(parents=True, exist_ok=True)
    return pipelines_dir


def get_user_sessions_dir(user_id: str) -> Path:
    """Get user-specific sessions directory."""
    sessions_dir = get_user_data_dir(user_id) / "sessions"
    sessions_dir.mkdir(parents=True, exist_ok=True)
    return sessions_dir


# Pydantic æ¨¡å‹å®šä¹‰
class Job(BaseModel):
    jobId: str
    name: str
    description: str | None = ""  # æ·»åŠ æè¿°å­—æ®µ
    isRunning: bool
    nthreads: str
    cpu: str
    ram: str
    startTime: str
    duration: str
    nevents: int
    minProcessTime: int
    maxProcessTime: int
    meanProcessTime: int
    latency: int
    throughput: int
    ncore: int
    periodicalThroughput: list[int]
    periodicalLatency: list[int]
    totalTimeBreakdown: dict
    schedulerTimeBreakdown: dict
    operators: list[dict]
    config: dict | None = None  # æ·»åŠ  config å­—æ®µï¼Œç”¨äºå­˜å‚¨ React Flow æ ¼å¼çš„èŠ‚ç‚¹å’Œè¾¹æ•°æ®


class ParameterConfig(BaseModel):
    """èŠ‚ç‚¹å‚æ•°é…ç½®"""

    name: str
    label: str
    type: str  # text, textarea, number, select, password, json
    required: bool | None = False
    defaultValue: str | int | float | dict | list | None = None  # æ”¯æŒ JSON å¯¹è±¡å’Œæ•°ç»„
    placeholder: str | None = None
    description: str | None = None
    options: list[str] | None = None
    min: int | float | None = None
    max: int | float | None = None
    step: int | float | None = None


class OperatorInfo(BaseModel):
    id: int
    name: str
    description: str
    code: str
    isCustom: bool
    parameters: list[ParameterConfig] | None = []  # æ·»åŠ å‚æ•°é…ç½®å­—æ®µ


# Auth Dependency
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="api/auth/login")


async def get_current_user(
    token: Annotated[str, Depends(oauth2_scheme)],
    auth_service: Annotated[AuthService, Depends(get_auth_service)],
) -> User:
    username = auth_service.verify_token(token)
    if username is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    user = auth_service.get_user(username)
    if user is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="User not found",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return User(id=user.id, username=user.username, created_at=user.created_at)


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="SAGE Studio Backend",
    description="Backend API service for SAGE Studio frontend",
    version="1.0.0",
)

# åŠ¨æ€æ„å»ºå…è®¸çš„æ¥æºåˆ—è¡¨
allowed_origins = [
    "http://localhost:5173",  # Vite å¼€å‘æœåŠ¡å™¨é»˜è®¤ç«¯å£
    "http://localhost:4173",  # Vite preview æœåŠ¡å™¨é»˜è®¤ç«¯å£
    f"http://localhost:{SagePorts.STUDIO_FRONTEND}",
    f"http://127.0.0.1:{SagePorts.STUDIO_FRONTEND}",
    f"http://0.0.0.0:{SagePorts.STUDIO_FRONTEND}",
]

# æ·»åŠ å¸¸ç”¨å¼€å‘ç«¯å£
for port in [5173, 4173, 35180]:
    if port != SagePorts.STUDIO_FRONTEND:
        allowed_origins.extend(
            [
                f"http://localhost:{port}",
                f"http://127.0.0.1:{port}",
                f"http://0.0.0.0:{port}",
            ]
        )

# ä»ç¯å¢ƒå˜é‡æ·»åŠ é¢å¤–æ¥æº
extra_origins = os.getenv("SAGE_STUDIO_ALLOWED_ORIGINS", "")
if extra_origins:
    allowed_origins.extend(
        [origin.strip() for origin in extra_origins.split(",") if origin.strip()]
    )

# å»é‡
allowed_origins = list(set(allowed_origins))

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Auth Endpoints
@app.post("/api/auth/register", response_model=User)
async def register(
    user: UserCreate,
    auth_service: Annotated[AuthService, Depends(get_auth_service)],
):
    db_user = auth_service.get_user(user.username)
    if db_user:
        raise HTTPException(status_code=400, detail="Username already registered")
    return auth_service.create_user(user.username, user.password)


@app.post("/api/auth/login", response_model=Token)
async def login(
    form_data: Annotated[OAuth2PasswordRequestForm, Depends()],
    auth_service: Annotated[AuthService, Depends(get_auth_service)],
):
    # Strip whitespace from username to match registration behavior
    username = form_data.username.strip()
    user = auth_service.get_user(username)

    if not user or not auth_service.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = auth_service.create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer"}


@app.post("/api/auth/guest", response_model=Token)
async def login_guest(
    auth_service: Annotated[AuthService, Depends(get_auth_service)],
):
    user = auth_service.create_guest_user()
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = auth_service.create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer"}


@app.get("/api/auth/me", response_model=User)
async def read_users_me(
    current_user: Annotated[User, Depends(get_current_user)],
):
    return current_user


@app.post("/api/auth/logout")
async def logout(
    current_user: Annotated[User, Depends(get_current_user)],
    auth_service: Annotated[AuthService, Depends(get_auth_service)],
):
    if getattr(current_user, "is_guest", False):
        # Clean up guest data
        import shutil

        # Delete user from DB
        auth_service.delete_user(current_user.id)

        # Delete user data directory
        # Use the local get_user_data_dir function defined in this file
        user_dir = get_user_data_dir(str(current_user.id))
        if user_dir.exists():
            try:
                shutil.rmtree(user_dir)
            except Exception as e:
                print(f"Error deleting guest data: {e}")

    return {"message": "Successfully logged out"}


def _read_sage_data_from_files(user_id: str | None = None):
    """ä» .sage ç›®å½•çš„æ–‡ä»¶ä¸­è¯»å–å®é™…çš„ SAGE æ•°æ®"""
    # Global dir for system data
    global_sage_dir = _get_sage_dir()

    # User dir for user data
    if user_id:
        user_sage_dir = get_user_data_dir(user_id)
    else:
        user_sage_dir = global_sage_dir

    data = {"jobs": [], "operators": [], "pipelines": []}

    try:
        # è¯»å–ä½œä¸šä¿¡æ¯
        states_dir = user_sage_dir / "states"
        if states_dir.exists():
            for job_file in states_dir.glob("*.json"):
                try:
                    with open(job_file, encoding="utf-8") as f:
                        job_data = json.load(f)
                        data["jobs"].append(job_data)
                except Exception as e:
                    print(f"Error reading job file {job_file}: {e}")

        # è¯»å–ä¿å­˜çš„æ‹“æ‰‘å›¾å¹¶è½¬æ¢ä¸º Job æ ¼å¼
        pipelines_dir = user_sage_dir / "pipelines"
        if pipelines_dir.exists():
            for pipeline_file in pipelines_dir.glob("pipeline_*.json"):
                try:
                    with open(pipeline_file, encoding="utf-8") as f:
                        pipeline_data = json.load(f)
                        # å°†æ‹“æ‰‘å›¾è½¬æ¢ä¸º Job æ ¼å¼ï¼Œä¼ é€’æ–‡ä»¶è·¯å¾„ä»¥æå–çœŸå®åˆ›å»ºæ—¶é—´
                        job_from_pipeline = _convert_pipeline_to_job(
                            pipeline_data, pipeline_file.stem, pipeline_file
                        )
                        data["jobs"].append(job_from_pipeline)
                except Exception as e:
                    print(f"Error reading pipeline file {pipeline_file}: {e}")

        # è¯»å–æ“ä½œç¬¦ä¿¡æ¯
        operators_file = global_sage_dir / "output" / "operators.json"
        if operators_file.exists():
            try:
                with open(operators_file, encoding="utf-8") as f:
                    operators_data = json.load(f)
                    data["operators"] = operators_data
            except Exception as e:
                print(f"Error reading operators file: {e}")

        # è¯»å–ç®¡é“ä¿¡æ¯
        pipelines_file = global_sage_dir / "output" / "pipelines.json"
        if pipelines_file.exists():
            try:
                with open(pipelines_file) as f:
                    pipelines_data = json.load(f)
                    data["pipelines"] = pipelines_data
            except Exception as e:
                print(f"Error reading pipelines file: {e}")

    except Exception as e:
        print(f"Error reading SAGE data: {e}")

    return data


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "SAGE Studio Backend API", "status": "running"}


@app.get("/api/jobs/all", response_model=list[Job])
async def get_all_jobs(
    current_user: Annotated[User, Depends(get_current_user)],
):
    """è·å–æ‰€æœ‰ä½œä¸šä¿¡æ¯"""
    try:
        sage_data = _read_sage_data_from_files(user_id=str(current_user.id))
        jobs = sage_data.get("jobs", [])

        print(f"DEBUG: Read {len(jobs)} jobs from files for user {current_user.username}")
        print(f"DEBUG: sage_data = {sage_data}")

        # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºå¼€å‘ï¼‰
        if not jobs:
            print("DEBUG: No real jobs found, using fallback data")
            jobs = [
                {
                    "jobId": "job_001",
                    "name": "RAGé—®ç­”ç®¡é“ç¤ºä¾‹",
                    "isRunning": False,
                    "nthreads": "4",
                    "cpu": "0%",
                    "ram": "0GB",
                    "startTime": "2025-08-18 10:30:00",
                    "duration": "00:45:12",
                    "nevents": 1000,
                    "minProcessTime": 10,
                    "maxProcessTime": 500,
                    "meanProcessTime": 150,
                    "latency": 200,
                    "throughput": 800,
                    "ncore": 4,
                    "periodicalThroughput": [750, 800, 820, 785, 810],
                    "periodicalLatency": [180, 200, 190, 210, 195],
                    "totalTimeBreakdown": {
                        "totalTime": 2712000,
                        "serializeTime": 50000,
                        "persistTime": 100000,
                        "streamProcessTime": 2500000,
                        "overheadTime": 62000,
                    },
                    "schedulerTimeBreakdown": {
                        "overheadTime": 50000,
                        "streamTime": 2600000,
                        "totalTime": 2712000,
                        "txnTime": 62000,
                    },
                    "operators": [
                        {
                            "id": 1,
                            "name": "FileSource",
                            "numOfInstances": 1,
                            "throughput": 800,
                            "latency": 50,
                            "explorationStrategy": "greedy",
                            "schedulingGranularity": "batch",
                            "abortHandling": "rollback",
                            "numOfTD": 10,
                            "numOfLD": 5,
                            "numOfPD": 2,
                            "lastBatch": 999,
                            "downstream": [2],
                        }
                    ],
                }
            ]

        return jobs
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šä¿¡æ¯å¤±è´¥: {str(e)}")


def _get_studio_operators_dir() -> Path:
    """è·å– Studio operators æ•°æ®ç›®å½•è·¯å¾„"""
    current_file = Path(__file__)
    # ä» api.py æ–‡ä»¶è·¯å¾„æ‰¾åˆ° studio æ ¹ç›®å½•:
    # ../../../ ä» backend/ åˆ° studio/
    studio_root = current_file.parent.parent.parent
    operators_dir = studio_root / "data" / "operators"
    return operators_dir


def _load_operator_class_source(module_path: str, class_name: str) -> str:
    """åŠ¨æ€åŠ è½½operatorç±»å¹¶è·å–å…¶æºä»£ç """
    try:
        # æ·»åŠ SAGEé¡¹ç›®è·¯å¾„åˆ°sys.path
        sage_root = find_sage_project_root()
        if sage_root and str(sage_root) not in sys.path:
            sys.path.insert(0, str(sage_root))

        # åŠ¨æ€å¯¼å…¥æ¨¡å—
        module = importlib.import_module(module_path)

        # è·å–ç±»
        operator_class = getattr(module, class_name)

        # è·å–æºä»£ç 
        source_code = inspect.getsource(operator_class)

        return source_code

    except Exception as e:
        print(f"Error loading operator class {module_path}.{class_name}: {e}")
        return f"# Error loading source code for {class_name}\n# {str(e)}"


def _read_real_operators():
    """ä» studio data ç›®å½•è¯»å–çœŸå®çš„æ“ä½œç¬¦æ•°æ®å¹¶åŠ¨æ€åŠ è½½æºä»£ç """
    operators = []
    operators_dir = _get_studio_operators_dir()

    if not operators_dir.exists():
        print(f"Operators directory not found: {operators_dir}")
        return []

    try:
        # è¯»å–æ‰€æœ‰ JSON æ–‡ä»¶
        for json_file in operators_dir.glob("*.json"):
            try:
                with open(json_file, encoding="utf-8") as f:
                    operator_data = json.load(f)

                    # æ£€æŸ¥æ˜¯å¦æœ‰module_pathå’Œclass_nameå­—æ®µ
                    if "module_path" in operator_data and "class_name" in operator_data:
                        # åŠ¨æ€åŠ è½½æºä»£ç 
                        source_code = _load_operator_class_source(
                            operator_data["module_path"], operator_data["class_name"]
                        )
                        operator_data["code"] = source_code
                    else:
                        # å¦‚æœæ²¡æœ‰æ¨¡å—è·¯å¾„ä¿¡æ¯ï¼Œä½¿ç”¨ç©ºä»£ç 
                        operator_data["code"] = ""

                    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                    required_fields = ["id", "name", "description", "isCustom"]
                    if all(key in operator_data for key in required_fields):
                        # æ¸…ç†ä¸éœ€è¦çš„å­—æ®µ
                        clean_data = {
                            "id": operator_data["id"],
                            "name": operator_data["name"],
                            "description": operator_data["description"],
                            "code": operator_data.get("code", ""),
                            "isCustom": operator_data["isCustom"],
                            "parameters": operator_data.get("parameters", []),  # æ·»åŠ å‚æ•°é…ç½®
                        }
                        operators.append(clean_data)
                        print(
                            f"Loaded operator: {operator_data['name']} with {len(clean_data['parameters'])} parameters"
                        )
                    else:
                        print(f"Invalid operator data in {json_file}")

            except Exception as e:
                print(f"Error reading operator file {json_file}: {e}")

    except Exception as e:
        print(f"Error reading operators directory: {e}")

    return operators


@app.get("/api/operators", response_model=list[OperatorInfo])
async def get_operators():
    """è·å–æ‰€æœ‰æ“ä½œç¬¦ä¿¡æ¯"""
    try:
        # é¦–å…ˆå°è¯•è¯»å–çœŸå®çš„æ“ä½œç¬¦æ•°æ®
        operators = _read_real_operators()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨åå¤‡æ•°æ®
        if not operators:
            print("No real operator data found, using fallback data")
            operators = [
                {
                    "id": 1,
                    "name": "FileSource",
                    "description": "ä»æ–‡ä»¶è¯»å–æ•°æ®çš„æºæ“ä½œç¬¦",
                    "code": "class FileSource:\n    def __init__(self, file_path):\n        self.file_path = file_path\n    \n    def read_data(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()",
                    "isCustom": True,
                },
                {
                    "id": 2,
                    "name": "SimpleRetriever",
                    "description": "ç®€å•çš„æ£€ç´¢æ“ä½œç¬¦",
                    "code": "class SimpleRetriever:\n    def __init__(self, top_k=5):\n        self.top_k = top_k\n    \n    def retrieve(self, query):\n        return query[:self.top_k]",
                    "isCustom": True,
                },
            ]

        print(f"Returning {len(operators)} operators")
        return operators
    except Exception as e:
        print(f"Error in get_operators: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ“ä½œç¬¦ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/api/operators/list")
async def get_operators_list(page: int = 1, size: int = 10, search: str = ""):
    """è·å–æ“ä½œç¬¦åˆ—è¡¨ - æ”¯æŒåˆ†é¡µå’Œæœç´¢"""
    try:
        # è·å–æ‰€æœ‰æ“ä½œç¬¦
        all_operators = _read_real_operators()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œä½¿ç”¨åå¤‡æ•°æ®
        if not all_operators:
            print("No real operator data found, using fallback data")
            all_operators = [
                {
                    "id": 1,
                    "name": "FileSource",
                    "description": "ä»æ–‡ä»¶è¯»å–æ•°æ®çš„æºæ“ä½œç¬¦",
                    "code": "class FileSource:\n    def __init__(self, file_path):\n        self.file_path = file_path\n    \n    def read_data(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()",
                    "isCustom": True,
                },
                {
                    "id": 2,
                    "name": "SimpleRetriever",
                    "description": "ç®€å•çš„æ£€ç´¢æ“ä½œç¬¦",
                    "code": "class SimpleRetriever:\n    def __init__(self, top_k=5):\n        self.top_k = top_k\n    \n    def retrieve(self, query):\n        return query[:self.top_k]",
                    "isCustom": True,
                },
            ]

        # æœç´¢è¿‡æ»¤
        if search:
            filtered_operators = [
                op
                for op in all_operators
                if search.lower() in op["name"].lower()
                or search.lower() in op["description"].lower()
            ]
        else:
            filtered_operators = all_operators

        # åˆ†é¡µè®¡ç®—
        total = len(filtered_operators)
        start = (page - 1) * size
        end = start + size
        items = filtered_operators[start:end]

        result = {"items": items, "total": total}

        print(f"Returning page {page} with {len(items)} operators (total: {total})")
        return result

    except Exception as e:
        print(f"Error in get_operators_list: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ“ä½œç¬¦åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/api/pipelines")
async def get_pipelines():
    """è·å–æ‰€æœ‰ç®¡é“ä¿¡æ¯"""
    try:
        sage_data = _read_sage_data_from_files()
        pipelines = sage_data.get("pipelines", [])

        # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®
        if not pipelines:
            pipelines = [
                {
                    "id": "pipeline_001",
                    "name": "ç¤ºä¾‹RAGç®¡é“",
                    "description": "æ¼”ç¤ºRAGé—®ç­”ç³»ç»Ÿçš„æ•°æ®å¤„ç†ç®¡é“",
                    "status": "running",
                    "operators": [
                        {
                            "id": "source1",
                            "type": "FileSource",
                            "config": {"file_path": "/data/documents.txt"},
                        },
                        {
                            "id": "retriever1",
                            "type": "SimpleRetriever",
                            "config": {"top_k": 5},
                        },
                        {
                            "id": "sink1",
                            "type": "TerminalSink",
                            "config": {"format": "json"},
                        },
                    ],
                    "connections": [
                        {"from": "source1", "to": "retriever1"},
                        {"from": "retriever1", "to": "sink1"},
                    ],
                }
            ]

        return {"pipelines": pipelines}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç®¡é“ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.post("/api/pipeline/submit")
async def submit_pipeline(
    topology_data: dict,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """æäº¤æ‹“æ‰‘å›¾/ç®¡é“é…ç½®"""
    try:
        print(f"Received pipeline submission from {current_user.username}: {topology_data}")

        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“çš„é€»è¾‘
        pipelines_dir = get_user_pipelines_dir(str(current_user.id))

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        import time

        timestamp = int(time.time())
        pipeline_file = pipelines_dir / f"pipeline_{timestamp}.json"

        # ä¿å­˜æ‹“æ‰‘æ•°æ®åˆ°æ–‡ä»¶
        with open(pipeline_file, "w", encoding="utf-8") as f:
            json.dump(topology_data, f, indent=2, ensure_ascii=False)

        return {
            "status": "success",
            "message": "æ‹“æ‰‘å›¾æäº¤æˆåŠŸ",
            "pipeline_id": f"pipeline_{timestamp}",
            "file_path": str(pipeline_file),
        }
    except Exception as e:
        print(f"Error submitting pipeline: {e}")
        raise HTTPException(status_code=500, detail=f"æäº¤æ‹“æ‰‘å›¾å¤±è´¥: {str(e)}")


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "service": "SAGE Studio Backend"}


# ==================== ç®¡é“è¯¦æƒ…ç›¸å…³ç«¯ç‚¹ ====================
# ç”¨äºæ”¯æŒå‰ç«¯ View Details åŠŸèƒ½çš„å ä½ç¬¦ç«¯ç‚¹

# å…¨å±€çŠ¶æ€å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
job_runtime_status = {}  # {job_id: {status, use_ray, isRunning, ...}}
job_logs = {}  # {job_id: [log_lines]}
job_configs_cache = {}  # {pipeline_id: yaml_config}
user_queries = {}  # {job_id: [(query, answer), ...]}


@app.get("/jobInfo/get/{job_id}")
async def get_job_detail(job_id: str):
    """è·å–ä½œä¸šè¯¦ç»†ä¿¡æ¯ - åŒ…å«æ“ä½œç¬¦æ‹“æ‰‘ç»“æ„"""
    try:
        # é¦–å…ˆå°è¯•ä»å·²ä¿å­˜çš„æ•°æ®ä¸­æŸ¥æ‰¾
        sage_data = _read_sage_data_from_files()
        jobs = sage_data.get("jobs", [])

        # æŸ¥æ‰¾åŒ¹é…çš„ä½œä¸š
        job = next((j for j in jobs if j.get("jobId") == job_id), None)

        if job:
            return job

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®é™…æ•°æ®ï¼Œè¿”å›å ä½ç¬¦æ•°æ®ï¼ˆç”¨äºå¼€å‘ï¼‰
        print(f"Job {job_id} not found in saved data, returning placeholder")
        return {
            "jobId": job_id,
            "name": f"ç®¡é“ {job_id}",
            "isRunning": False,
            "nthreads": "4",
            "cpu": "0%",
            "ram": "0GB",
            "startTime": "2025-10-10 15:00:00",
            "duration": "00:00:00",
            "nevents": 0,
            "minProcessTime": 0,
            "maxProcessTime": 0,
            "meanProcessTime": 0,
            "latency": 0,
            "throughput": 0,
            "ncore": 4,
            "periodicalThroughput": [0],
            "periodicalLatency": [0],
            "totalTimeBreakdown": {
                "totalTime": 0,
                "serializeTime": 0,
                "persistTime": 0,
                "streamProcessTime": 0,
                "overheadTime": 0,
            },
            "schedulerTimeBreakdown": {
                "overheadTime": 0,
                "streamTime": 0,
                "totalTime": 0,
                "txnTime": 0,
            },
            "operators": [
                {
                    "id": 1,
                    "name": "FileSource",
                    "numOfInstances": 1,
                    "throughput": 0,
                    "latency": 0,
                    "explorationStrategy": "greedy",
                    "schedulingGranularity": "batch",
                    "abortHandling": "rollback",
                    "numOfTD": 0,
                    "numOfLD": 0,
                    "numOfPD": 0,
                    "lastBatch": 0,
                    "downstream": [2],
                },
                {
                    "id": 2,
                    "name": "TerminalSink",
                    "numOfInstances": 1,
                    "throughput": 0,
                    "latency": 0,
                    "explorationStrategy": "greedy",
                    "schedulingGranularity": "batch",
                    "abortHandling": "rollback",
                    "numOfTD": 0,
                    "numOfLD": 0,
                    "numOfPD": 0,
                    "lastBatch": 0,
                    "downstream": [],
                },
            ],
        }
    except Exception as e:
        print(f"Error getting job detail: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šè¯¦æƒ…å¤±è´¥: {str(e)}")


@app.get("/api/signal/status/{job_id}")
async def get_job_status(job_id: str):
    """è·å–ä½œä¸šè¿è¡ŒçŠ¶æ€"""
    try:
        # ä»å†…å­˜ä¸­è·å–çŠ¶æ€
        status = job_runtime_status.get(
            job_id,
            {
                "job_id": job_id,
                "status": "idle",
                "use_ray": False,
                "isRunning": False,
            },
        )
        return status
    except Exception as e:
        print(f"Error getting job status: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šçŠ¶æ€å¤±è´¥: {str(e)}")


@app.post("/api/signal/start/{job_id}")
async def start_job(job_id: str):
    """å¯åŠ¨ä½œä¸š"""
    try:
        # æ›´æ–°è¿è¡ŒçŠ¶æ€
        job_runtime_status[job_id] = {
            "job_id": job_id,
            "status": "running",
            "use_ray": False,
            "isRunning": True,
        }

        # åˆå§‹åŒ–æ—¥å¿—
        if job_id not in job_logs:
            job_logs[job_id] = []

        job_logs[job_id].append(f"[SYSTEM] Job {job_id} started at 2025-10-10 15:30:00")

        return {"status": "success", "message": f"ä½œä¸š {job_id} å·²å¯åŠ¨"}
    except Exception as e:
        print(f"Error starting job: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ä½œä¸šå¤±è´¥: {str(e)}")


@app.post("/api/signal/stop/{job_id}/{duration}")
async def stop_job(job_id: str, duration: str):
    """åœæ­¢ä½œä¸š"""
    try:
        # æ›´æ–°è¿è¡ŒçŠ¶æ€
        job_runtime_status[job_id] = {
            "job_id": job_id,
            "status": "stopped",
            "use_ray": False,
            "isRunning": False,
        }

        # æ·»åŠ åœæ­¢æ—¥å¿—
        if job_id in job_logs:
            job_logs[job_id].append(f"[SYSTEM] Job {job_id} stopped after {duration}")

        return {"status": "success", "message": f"ä½œä¸š {job_id} å·²åœæ­¢"}
    except Exception as e:
        print(f"Error stopping job: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢ä½œä¸šå¤±è´¥: {str(e)}")


@app.get("/api/signal/sink/{job_id}")
async def get_job_logs(job_id: str, offset: int = 0):
    """è·å–ä½œä¸šæ—¥å¿—ï¼ˆå¢é‡ï¼‰"""
    try:
        # è·å–è¯¥ä½œä¸šçš„æ—¥å¿—
        logs = job_logs.get(job_id, [])

        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆoffset=0ï¼‰ä¸”æ²¡æœ‰æ—¥å¿—ï¼Œè¿”å›ç§å­æ¶ˆæ¯
        if offset == 0 and len(logs) == 0:
            seed_line = (
                f"[SYSTEM] Console ready for {job_id}. Click Start or submit a FileSource query."
            )
            job_logs[job_id] = [seed_line]
            return {"offset": 1, "lines": [seed_line]}

        # è¿”å›ä» offset å¼€å§‹çš„æ–°æ—¥å¿—
        new_logs = logs[offset:]
        new_offset = len(logs)

        return {"offset": new_offset, "lines": new_logs}
    except Exception as e:
        print(f"Error getting job logs: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½œä¸šæ—¥å¿—å¤±è´¥: {str(e)}")


@app.get("/batchInfo/get/all/{job_id}/{operator_id}")
async def get_all_batches(job_id: str, operator_id: str):
    """è·å–æ“ä½œç¬¦çš„æ‰€æœ‰æ‰¹æ¬¡ä¿¡æ¯"""
    try:
        # operator_id å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "s1", "r1"ï¼‰æˆ–æ•°å­—
        # è¿”å›ç©ºæ•°ç»„ä½œä¸ºå ä½ç¬¦
        # å®é™…å®ç°éœ€è¦ä» SAGE è¿è¡Œæ—¶è·å–æ‰¹æ¬¡ç»Ÿè®¡æ•°æ®
        print(f"Getting batches for job={job_id}, operator={operator_id}")
        return []
    except Exception as e:
        print(f"Error getting batches: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ‰¹æ¬¡ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/batchInfo/get/{job_id}/{batch_id}/{operator_id}")
async def get_batch_detail(job_id: str, batch_id: int, operator_id: str):
    """è·å–å•ä¸ªæ‰¹æ¬¡çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        # operator_id å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "s1", "r1"ï¼‰æˆ–æ•°å­—
        # è¿”å›å ä½ç¬¦æ‰¹æ¬¡æ•°æ®
        return {
            "batchId": batch_id,
            "operatorId": operator_id,
            "processTime": 0,
            "tupleCount": 0,
            "timestamp": "2025-10-10 15:30:00",
        }
    except Exception as e:
        print(f"Error getting batch detail: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ‰¹æ¬¡è¯¦æƒ…å¤±è´¥: {str(e)}")


@app.get("/jobInfo/config/{pipeline_id}")
async def get_pipeline_config(pipeline_id: str):
    """è·å–ç®¡é“é…ç½®ï¼ˆYAMLæ ¼å¼ï¼‰"""
    try:
        # å°è¯•ä»ç¼“å­˜è·å–
        if pipeline_id in job_configs_cache:
            return {"config": job_configs_cache[pipeline_id]}

        # è¿”å›é»˜è®¤é…ç½®æ¨¡æ¿
        default_config = """# SAGE Pipeline Configuration
name: Example RAG Pipeline
version: 1.0.0

operators:
  - name: FileSource
    type: source
    config:
      file_path: /data/documents.txt

  - name: SimpleRetriever
    type: retriever
    config:
      top_k: 5

  - name: TerminalSink
    type: sink
    config:
      output_path: /tmp/output.txt
"""
        return {"config": default_config}
    except Exception as e:
        print(f"Error getting pipeline config: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç®¡é“é…ç½®å¤±è´¥: {str(e)}")


@app.put("/jobInfo/config/update/{pipeline_id}")
async def update_pipeline_config(pipeline_id: str, config: dict):
    """æ›´æ–°ç®¡é“é…ç½®"""
    try:
        # ä¿å­˜é…ç½®åˆ°ç¼“å­˜
        config_yaml = config.get("config", "")
        job_configs_cache[pipeline_id] = config_yaml

        # å¯é€‰ï¼šä¿å­˜åˆ°æ–‡ä»¶
        sage_dir = _get_sage_dir()
        config_dir = sage_dir / "configs"
        config_dir.mkdir(exist_ok=True)

        config_file = config_dir / f"{pipeline_id}.yaml"
        with open(config_file, "w", encoding="utf-8") as f:
            f.write(config_yaml)

        return {
            "status": "success",
            "message": "é…ç½®æ›´æ–°æˆåŠŸ",
            "file_path": str(config_file),
        }
    except Exception as e:
        print(f"Error updating pipeline config: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ç®¡é“é…ç½®å¤±è´¥: {str(e)}")


# ==================== Playground API ====================


def _load_flow_data(flow_id: str, user_id: str | None = None) -> dict | None:
    """åŠ è½½ Flow æ•°æ®"""
    if user_id:
        pipelines_dir = get_user_pipelines_dir(user_id)
    else:
        sage_dir = _get_sage_dir()
        pipelines_dir = sage_dir / "pipelines"

    print(f"ğŸ” Looking for flow: {flow_id}")
    print(f"ğŸ“ Pipelines dir: {pipelines_dir}")
    print(f"ğŸ“ Pipelines dir exists: {pipelines_dir.exists()}")

    # å°è¯•åŠ è½½ pipeline æ–‡ä»¶
    flow_file = pipelines_dir / f"{flow_id}.json"
    print(f"ğŸ“„ Flow file path: {flow_file}")
    print(f"ğŸ“„ Flow file exists: {flow_file.exists()}")

    if flow_file.exists():
        with open(flow_file, encoding="utf-8") as f:
            data = json.load(f)
            print(f"âœ… Loaded flow: {data.get('name', 'Unnamed')}")
            return data

    print("âŒ Flow file not found")
    return None


def _convert_to_flow_definition(flow_data: dict, flow_id: str):
    """å°†å‰ç«¯ Flow æ•°æ®è½¬æ¢ä¸º FlowDefinition"""
    import sys

    # æ·»åŠ  sage-studio åˆ° Python è·¯å¾„
    studio_root = find_sage_project_root()
    if studio_root:
        studio_path = studio_root / "packages" / "sage-studio"
        if str(studio_path) not in sys.path:
            sys.path.insert(0, str(studio_path))

    from sage.studio.models import (  # type: ignore[import-not-found]
        VisualConnection,
        VisualNode,
        VisualPipeline,
    )
    from sage.studio.services.node_registry import (  # type: ignore[import-not-found]
        convert_node_type_to_snake_case,
    )

    name = flow_data.get("name", "Unnamed Flow")
    description = flow_data.get("description", "")
    nodes_data = flow_data.get("nodes", [])
    edges_data = flow_data.get("edges", [])

    # è½¬æ¢èŠ‚ç‚¹
    nodes = []
    for node_data in nodes_data:
        # è·å–èŠ‚ç‚¹ç±»å‹å¹¶è½¬æ¢ä¸º snake_case
        node_id = node_data.get("data", {}).get("nodeId", "unknown")
        node_type = convert_node_type_to_snake_case(node_id)

        print(f"ğŸ”„ Converting node: {node_id} â†’ {node_type}")

        node = VisualNode(
            id=node_data.get("id", ""),
            type=node_type,  # ä½¿ç”¨è½¬æ¢åçš„ç±»å‹
            label=node_data.get("data", {}).get("label", "Unnamed Node"),
            position=node_data.get("position", {"x": 0, "y": 0}),
            config=node_data.get("data", {}).get("properties", {}),
        )
        nodes.append(node)

    # è½¬æ¢è¿æ¥
    connections = []
    for edge_data in edges_data:
        connection = VisualConnection(
            id=edge_data.get("id", f"{edge_data.get('source')}-{edge_data.get('target')}"),
            source_node_id=edge_data.get("source", ""),
            source_port="output",  # é»˜è®¤è¾“å‡ºç«¯å£
            target_node_id=edge_data.get("target", ""),
            target_port="input",  # é»˜è®¤è¾“å…¥ç«¯å£
        )
        connections.append(connection)

    return VisualPipeline(
        id=flow_id,
        name=name,
        description=description,
        nodes=nodes,
        connections=connections,
    )


def _parse_execution_results(results, pipeline, execution_time):
    """
    è§£ææ‰§è¡Œç»“æœ,ç”Ÿæˆè¾“å‡ºå’Œæ­¥éª¤

    Args:
        results: Sink æ”¶é›†çš„ç»“æœåˆ—è¡¨
        pipeline: VisualPipeline å®šä¹‰
        execution_time: æ‰§è¡Œæ—¶é—´

    Returns:
        tuple: (output_text, agent_steps)
    """
    from datetime import datetime

    agent_steps = []
    output_parts = []

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆæ­¥éª¤
    step_time = int(execution_time * 1000 / len(pipeline.nodes)) if pipeline.nodes else 0

    for idx, node in enumerate(pipeline.nodes, start=1):
        # æŸ¥æ‰¾è¯¥èŠ‚ç‚¹çš„è¾“å‡º
        node_output = None
        if results and idx <= len(results):
            node_output = results[idx - 1]

        # ç”Ÿæˆæ­¥éª¤
        agent_steps.append(
            AgentStep(
                step=idx,
                type="tool_call",
                content=f"âœ“ {node.label}",
                timestamp=datetime.now().isoformat(),
                duration=step_time,
                toolName=node.label,
                toolInput={"config": node.config},
                toolOutput={"result": str(node_output) if node_output else "å®Œæˆ"},
            )
        )

        # æ”¶é›†è¾“å‡º
        if node_output:
            output_parts.append(f"## {node.label}\n{node_output}\n")

    # ç”Ÿæˆæœ€ç»ˆè¾“å‡º
    if output_parts:
        output_text = "\n".join(output_parts)
    else:
        output_text = f"Pipeline æ‰§è¡ŒæˆåŠŸï¼\n\næ€»è€—æ—¶: {execution_time:.2f}ç§’"

    return output_text, agent_steps


class PlaygroundExecuteRequest(BaseModel):
    """Playground æ‰§è¡Œè¯·æ±‚"""

    flowId: str
    input: str
    sessionId: str = "default"
    stream: bool = False


class AgentStep(BaseModel):
    """Agent æ‰§è¡Œæ­¥éª¤"""

    step: int
    type: str  # reasoning, tool_call, response
    content: str
    timestamp: str
    duration: int | None = None
    toolName: str | None = None
    toolInput: dict | None = None
    toolOutput: dict | None = None


class PlaygroundExecuteResponse(BaseModel):
    """Playground æ‰§è¡Œå“åº”"""

    output: str
    status: str
    agentSteps: list[AgentStep] | None = None


@app.post("/api/playground/execute", response_model=PlaygroundExecuteResponse)
async def execute_playground(
    request: PlaygroundExecuteRequest,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """æ‰§è¡Œ Playground Flow - ä½¿ç”¨å¢å¼ºçš„ PipelineBuilder"""
    try:
        import sys
        import time

        # æ·»åŠ  sage-studio åˆ° Python è·¯å¾„
        studio_root = find_sage_project_root()
        if studio_root:
            studio_path = studio_root / "packages" / "sage-studio"
            if str(studio_path) not in sys.path:
                sys.path.insert(0, str(studio_path))

        from sage.studio.models import PipelineStatus
        from sage.studio.services import get_pipeline_builder

        print(f"\n{'=' * 60}")
        print("ğŸ¯ Playground æ‰§è¡Œå¼€å§‹")
        print(f"   User: {current_user.username}")
        print(f"   Flow ID: {request.flowId}")
        print(f"   Session: {request.sessionId}")
        print(f"   Input: {request.input[:100]}...")
        print(f"{'=' * 60}\n")

        # 1. åŠ è½½ Flow å®šä¹‰
        flow_data = _load_flow_data(request.flowId, user_id=str(current_user.id))
        if not flow_data:
            raise HTTPException(status_code=404, detail=f"Flow not found: {request.flowId}")

        # 2. è½¬æ¢ä¸º VisualPipeline
        visual_pipeline = _convert_to_flow_definition(flow_data, request.flowId)
        print(f"ğŸ“Š Pipeline èŠ‚ç‚¹æ•°: {len(visual_pipeline.nodes)}")

        # 3. ğŸ†• ä½¿ç”¨å¢å¼ºçš„ PipelineBuilder (ä¼ å…¥ç”¨æˆ·è¾“å…¥)
        builder = get_pipeline_builder()
        sage_env = builder.build(visual_pipeline, user_input=request.input)

        # 4. æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
        start_time = time.time()
        print("âš™ï¸ å¼€å§‹æ‰§è¡Œ...")

        # æäº¤ä½œä¸šå¹¶ç­‰å¾…å®Œæˆ
        sage_env.submit(autostop=True)

        execution_time = time.time() - start_time
        print(f"âœ… æ‰§è¡Œå®Œæˆ,è€—æ—¶: {execution_time:.2f}ç§’\n")

        # 5. ğŸ†• æ”¶é›†æ‰§è¡Œç»“æœ
        from sage.libs.io.sink import RetriveSink

        results = []
        if hasattr(RetriveSink, "get_results"):
            results = RetriveSink.get_results()

        # 6. ğŸ†• è§£æç»“æœå¹¶ç”Ÿæˆæ­¥éª¤
        output_text, agent_steps = _parse_execution_results(
            results, visual_pipeline, execution_time
        )

        print(f"ğŸ“¤ è¾“å‡ºé•¿åº¦: {len(output_text)} å­—ç¬¦")
        print(f"ğŸ“‹ æ­¥éª¤æ•°: {len(agent_steps)}")
        print(f"{'=' * 60}\n")

        return PlaygroundExecuteResponse(
            output=output_text,
            status=PipelineStatus.COMPLETED.value,
            agentSteps=agent_steps if agent_steps else None,
        )

    except HTTPException:
        raise
    except Exception as e:
        import traceback

        print("\nâŒ æ‰§è¡Œå‡ºé”™:")
        print(traceback.format_exc())
        print(f"{'=' * 60}\n")

        return PlaygroundExecuteResponse(
            output=f"æ‰§è¡Œå‡ºé”™: {str(e)}", status="failed", agentSteps=None
        )


# ==================== MVP å¢å¼ºåŠŸèƒ½ ====================


# 1. èŠ‚ç‚¹è¾“å‡ºé¢„è§ˆ
@app.get("/api/node/{flow_id}/{node_id}/output")
async def get_node_output(flow_id: str, node_id: str):
    """è·å–èŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®"""
    try:
        # ä»ç¼“å­˜æˆ–çŠ¶æ€å­˜å‚¨ä¸­è·å–èŠ‚ç‚¹è¾“å‡º
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä» SAGE è¿è¡Œæ—¶è·å–
        sage_dir = _get_sage_dir()
        states_dir = sage_dir / "states" / flow_id

        if not states_dir.exists():
            raise HTTPException(404, "Flow å°šæœªæ‰§è¡Œæˆ–è¾“å‡ºä¸å¯ç”¨")

        # æŸ¥æ‰¾èŠ‚ç‚¹è¾“å‡ºæ–‡ä»¶
        output_file = states_dir / f"{node_id}_output.json"
        if not output_file.exists():
            raise HTTPException(404, "èŠ‚ç‚¹è¾“å‡ºä¸å¯ç”¨")

        import json

        with open(output_file, encoding="utf-8") as f:
            output_data = json.load(f)

        return output_data
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting node output: {e}")
        raise HTTPException(500, f"è·å–èŠ‚ç‚¹è¾“å‡ºå¤±è´¥: {str(e)}")


# 2. Flow å¯¼å…¥/å¯¼å‡º
@app.get("/api/flows/{flow_id}/export")
async def export_flow(
    flow_id: str,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """å¯¼å‡º Flow ä¸º JSON æ–‡ä»¶"""
    try:
        flow_data = _load_flow_data(flow_id, user_id=str(current_user.id))
        if not flow_data:
            raise HTTPException(404, f"Flow not found: {flow_id}")

        import json

        from fastapi.responses import Response

        # æ·»åŠ å¯¼å‡ºå…ƒæ•°æ®
        export_data = {
            "version": "1.0.0",
            "exportTime": str(datetime.now()),
            "flowId": flow_id,
            "flow": flow_data,
        }

        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        return Response(
            content=json_str,
            media_type="application/json",
            headers={"Content-Disposition": f'attachment; filename="{flow_id}.sage-flow.json"'},
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"å¯¼å‡ºå¤±è´¥: {str(e)}")


@app.post("/api/flows/import")
async def import_flow(
    current_user: Annotated[User, Depends(get_current_user)],
    file: UploadFile = File(...),
):
    """å¯¼å…¥ Flow JSON æ–‡ä»¶"""
    try:
        import json
        from datetime import datetime

        # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
        content = await file.read()
        import_data = json.loads(content)

        # éªŒè¯æ ¼å¼
        if "flow" not in import_data:
            raise HTTPException(400, "æ— æ•ˆçš„ Flow æ–‡ä»¶æ ¼å¼")

        flow_data = import_data["flow"]

        # ç”Ÿæˆæ–°çš„ flow_id
        timestamp = int(datetime.now().timestamp() * 1000)
        new_flow_id = f"pipeline_{timestamp}"

        # ä¿å­˜åˆ°æœ¬åœ°
        pipelines_dir = get_user_pipelines_dir(str(current_user.id))

        flow_file = pipelines_dir / f"{new_flow_id}.json"
        with open(flow_file, "w", encoding="utf-8") as f:
            json.dump(flow_data, f, indent=2, ensure_ascii=False)

        return {
            "flowId": new_flow_id,
            "name": flow_data.get("name", "Imported Flow"),
            "message": "Flow å¯¼å…¥æˆåŠŸ",
        }
    except json.JSONDecodeError:
        raise HTTPException(400, "æ— æ•ˆçš„ JSON æ–‡ä»¶")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"å¯¼å…¥å¤±è´¥: {str(e)}")


# 3. ç¯å¢ƒå˜é‡ç®¡ç†
@app.get("/api/env")
async def get_env_vars():
    """è·å–ç¯å¢ƒå˜é‡"""
    try:
        sage_dir = _get_sage_dir()
        env_file = sage_dir / ".env.json"

        if not env_file.exists():
            return {}

        import json

        with open(env_file, encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading env vars: {e}")
        return {}


@app.put("/api/env")
async def update_env_vars(vars: dict):
    """æ›´æ–°ç¯å¢ƒå˜é‡"""
    try:
        import json

        sage_dir = _get_sage_dir()
        env_file = sage_dir / ".env.json"

        # åŠ å¯†æ•æ„Ÿä¿¡æ¯ï¼ˆç®€åŒ–å®ç°ï¼Œå®é™…åº”ä½¿ç”¨åŠ å¯†åº“ï¼‰
        with open(env_file, "w", encoding="utf-8") as f:
            json.dump(vars, f, indent=2, ensure_ascii=False)

        return {"message": "ç¯å¢ƒå˜é‡å·²æ›´æ–°"}
    except Exception as e:
        raise HTTPException(500, f"æ›´æ–°å¤±è´¥: {str(e)}")


@app.get("/api/logs/{flow_id}")
async def get_logs(flow_id: str, last_id: int = 0):
    """è·å–æµç¨‹æ‰§è¡Œæ—¥å¿—ï¼ˆå¢é‡è·å–ï¼‰

    Args:
        flow_id: æµç¨‹ID
        last_id: ä¸Šæ¬¡è·å–çš„æœ€åä¸€æ¡æ—¥å¿—IDï¼Œç”¨äºå¢é‡è·å–

    Returns:
        æ—¥å¿—æ¡ç›®åˆ—è¡¨
    """
    try:
        sage_dir = _get_sage_dir()
        log_file = sage_dir / "logs" / f"{flow_id}.log"

        if not log_file.exists():
            return {"logs": [], "last_id": 0}

        # è¯»å–æ—¥å¿—æ–‡ä»¶
        logs = []
        with open(log_file, encoding="utf-8") as f:
            for idx, line in enumerate(f, start=1):
                if idx > last_id:  # åªè¿”å›æ–°æ—¥å¿—
                    # ç®€å•çš„æ—¥å¿—è§£æï¼ˆæ ¼å¼: [timestamp] [level] [node_id] messageï¼‰
                    try:
                        parts = line.strip().split("] ", 3)
                        if len(parts) >= 3:
                            timestamp = parts[0].replace("[", "")
                            level = parts[1].replace("[", "")
                            node_id = parts[2].replace("[", "") if len(parts) == 4 else None
                            message = parts[-1]

                            logs.append(
                                {
                                    "id": idx,
                                    "timestamp": timestamp,
                                    "level": level,
                                    "message": message,
                                    "nodeId": node_id,
                                }
                            )
                    except Exception:
                        # è§£æå¤±è´¥ï¼Œè·³è¿‡è¿™è¡Œ
                        continue

        return {"logs": logs, "last_id": last_id + len(logs)}
    except Exception as e:
        raise HTTPException(500, f"è·å–æ—¥å¿—å¤±è´¥: {str(e)}")


# ==================== Chat Mode API (æ–°å¢) ====================


class ChatRequest(BaseModel):
    """Chat æ¨¡å¼è¯·æ±‚"""

    message: str
    session_id: str | None = None
    model: str = "sage-default"
    stream: bool = False


class AgentChatRequest(BaseModel):
    """Agent èŠå¤©è¯·æ±‚"""

    message: str
    session_id: str
    history: list[dict[str, str]] | None = None
    route: str | None = None
    should_index: bool | None = None
    metadata: dict[str, Any] | None = None
    evidence: list[dict[str, Any]] | None = None


class ChatResponse(BaseModel):
    """Chat æ¨¡å¼å“åº”"""

    content: str
    session_id: str
    timestamp: str


class ChatSessionSummary(BaseModel):
    """Chat ä¼šè¯æ‘˜è¦"""

    id: str
    title: str
    created_at: str
    last_active: str
    message_count: int


class ChatSessionDetail(ChatSessionSummary):
    messages: list[dict]
    metadata: dict | None = None


class ChatSessionCreateRequest(BaseModel):
    title: str | None = None


class ChatSessionTitleUpdate(BaseModel):
    title: str


def _get_session_path(user_id: str, session_id: str) -> Path:
    return get_user_sessions_dir(user_id) / f"{session_id}.json"


def _load_session(user_id: str, session_id: str) -> dict | None:
    path = _get_session_path(user_id, session_id)
    if path.exists():
        try:
            with open(path, encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading session {path}: {e}")
    return None


def _save_session(user_id: str, session_id: str, data: dict):
    path = _get_session_path(user_id, session_id)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


@app.post("/api/chat/v1/chat/completions")
async def proxy_chat_completions(
    request: Request, current_user: Annotated[User, Depends(get_current_user)]
):
    """Proxy for OpenAI-compatible chat completions used by Studio frontend"""
    from datetime import datetime

    import httpx

    try:
        # Get the raw body
        body = await request.json()
        session_id = body.get("session_id")
        user_id = str(current_user.id)

        # Extract user message from request
        messages = body.get("messages", [])
        user_message_content = None
        if messages:
            # Get the last user message
            for msg in reversed(messages):
                if msg.get("role") == "user":
                    user_message_content = msg.get("content")
                    break

        # Load or create session
        session_data = None
        if session_id:
            session_data = _load_session(user_id, session_id)

        if not session_data and session_id:
            # Create new session
            now = datetime.now().isoformat()
            session_data = {
                "id": session_id,
                "title": "New Chat",
                "created_at": now,
                "last_active": now,
                "messages": [],
                "metadata": {},
            }

        # Save user message to session
        if session_data and user_message_content:
            user_msg = {
                "role": "user",
                "content": user_message_content,
                "timestamp": datetime.now().isoformat(),
            }
            session_data["messages"].append(user_msg)
            session_data["last_active"] = datetime.now().isoformat()
            _save_session(user_id, session_id, session_data)

        # Collect assistant response
        collected_content = []

        # Forward to Gateway
        # We use a stream to support SSE
        async def event_generator():
            try:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    async with client.stream(
                        "POST",
                        f"{GATEWAY_BASE_URL}/v1/chat/completions",
                        json=body,
                    ) as response:
                        if response.status_code != 200:
                            error_msg = await response.aread()
                            yield f"data: {json.dumps({'error': f'Gateway error: {response.status_code} - {error_msg.decode()}'})}\n\n"
                            return

                        async for chunk in response.aiter_bytes():
                            # Parse SSE to collect assistant content
                            chunk_str = chunk.decode("utf-8", errors="ignore")
                            for line in chunk_str.split("\n"):
                                if line.startswith("data: "):
                                    data = line[6:].strip()
                                    if data and data != "[DONE]":
                                        try:
                                            parsed = json.loads(data)
                                            content = (
                                                parsed.get("choices", [{}])[0]
                                                .get("delta", {})
                                                .get("content")
                                            )
                                            if content:
                                                collected_content.append(content)
                                        except Exception:
                                            pass
                            yield chunk

                # Save assistant message after streaming completes
                if session_data and collected_content:
                    assistant_msg = {
                        "role": "assistant",
                        "content": "".join(collected_content),
                        "timestamp": datetime.now().isoformat(),
                    }
                    session_data["messages"].append(assistant_msg)
                    session_data["last_active"] = datetime.now().isoformat()
                    _save_session(user_id, session_id, session_data)

            except Exception as e:
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(event_generator(), media_type="text/event-stream")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat/message", response_model=ChatResponse)
async def send_chat_message(
    request: ChatRequest,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """
    å‘é€èŠå¤©æ¶ˆæ¯ï¼ˆè°ƒç”¨ sage-gatewayï¼‰

    æ³¨æ„ï¼šéœ€è¦ sage-gateway æœåŠ¡è¿è¡Œåœ¨ GATEWAY_BASE_URL
    """
    import uuid
    from datetime import datetime

    import httpx

    # 1. Handle Session
    session_id = request.session_id
    session_data = None
    user_id = str(current_user.id)

    if session_id:
        session_data = _load_session(user_id, session_id)

    if not session_data:
        # Create new session if not found or not provided
        session_id = session_id or str(uuid.uuid4())
        now = datetime.now().isoformat()
        session_data = {
            "id": session_id,
            "title": "New Chat",
            "created_at": now,
            "last_active": now,
            "messages": [],
            "metadata": {},
        }

    # 2. Append User Message
    user_msg = {"role": "user", "content": request.message, "timestamp": datetime.now().isoformat()}
    session_data["messages"].append(user_msg)
    session_data["last_active"] = datetime.now().isoformat()
    _save_session(user_id, session_id, session_data)

    # Resolve model if "sage-default"
    model_to_use = request.model
    if model_to_use == "sage-default":
        # 1. Try environment variable (set by select_llm_model)
        model_to_use = os.getenv("SAGE_CHAT_MODEL")

        # 2. If not set, try to detect from Gateway
        if not model_to_use:
            try:
                from sage.common.config.ports import SagePorts
                from sage.llm import UnifiedInferenceClient

                client = UnifiedInferenceClient.create(
                    control_plane_url=f"http://localhost:{SagePorts.GATEWAY_DEFAULT}/v1"
                )
                detected = client._get_default_llm_model()
                if detected and detected != "default":
                    model_to_use = detected
            except Exception:
                pass

        # 3. Fallback to original if still failed
        if not model_to_use:
            model_to_use = request.model

    try:
        # è°ƒç”¨ sage-gateway çš„ OpenAI å…¼å®¹æ¥å£
        # We pass the session_id to gateway as well, so it can maintain its own state if needed,
        # or we can pass full history if gateway is stateless.
        # For now, let's pass session_id.
        async with httpx.AsyncClient(timeout=30.0, trust_env=False) as client:
            gateway_response = await client.post(
                f"{GATEWAY_BASE_URL}/v1/chat/completions",
                json={
                    "model": model_to_use,
                    "messages": [{"role": "user", "content": request.message}],
                    "stream": False,
                    "session_id": session_id,  # Pass session_id to gateway
                },
            )

            if gateway_response.status_code != 200:
                raise HTTPException(
                    status_code=gateway_response.status_code,
                    detail=f"Gateway error: {gateway_response.text}",
                )

            data = gateway_response.json()

            # æå–å“åº”å†…å®¹
            assistant_content = data["choices"][0]["message"]["content"]

            # 3. Append Assistant Message
            assistant_msg = {
                "role": "assistant",
                "content": assistant_content,
                "timestamp": datetime.now().isoformat(),
            }
            session_data["messages"].append(assistant_msg)
            session_data["last_active"] = datetime.now().isoformat()
            _save_session(user_id, session_id, session_data)

            return ChatResponse(
                content=assistant_content,
                session_id=session_id,
                timestamp=datetime.now().isoformat(),
            )

    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail=f"æ— æ³•è¿æ¥åˆ° SAGE Gateway ({GATEWAY_BASE_URL})ã€‚è¯·ç¡®ä¿ gateway æœåŠ¡å·²å¯åŠ¨ã€‚",
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat è¯·æ±‚å¤±è´¥: {str(e)}")


@app.post("/api/chat/agent")
async def agent_chat(request: AgentChatRequest):
    """Multi-Agent èŠå¤©æ¥å£"""
    orchestrator = get_orchestrator()
    stream_handler = get_stream_handler()

    source = orchestrator.process_message(
        message=request.message,
        session_id=request.session_id,
        history=request.history,
        should_index=request.should_index or False,
        metadata=request.metadata or {},
        evidence=request.evidence or [],
    )

    return stream_handler.create_response(source)


@app.post("/api/chat/agent/sync")
async def agent_chat_sync(request: AgentChatRequest):
    """éæµå¼ Agent èŠå¤©æ¥å£ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
    orchestrator = get_orchestrator()

    steps = []
    text_parts = []

    async for item in orchestrator.process_message(
        message=request.message,
        session_id=request.session_id,
        history=request.history,
        should_index=request.should_index or False,
        metadata=request.metadata or {},
        evidence=request.evidence or [],
    ):
        if hasattr(item, "step_id"):  # AgentStep
            # Handle both dataclass and Pydantic models
            if hasattr(item, "to_dict"):
                steps.append(item.to_dict())
            elif hasattr(item, "dict"):
                steps.append(item.dict())
            else:
                from dataclasses import asdict

                steps.append(asdict(item))
        else:  # str
            text_parts.append(item)

    return {
        "steps": steps,
        "response": "".join(text_parts),
    }


@app.get("/api/chat/sessions", response_model=list[ChatSessionSummary])
async def list_chat_sessions(
    current_user: Annotated[User, Depends(get_current_user)],
):
    """è·å–æ‰€æœ‰èŠå¤©ä¼šè¯"""
    sessions_dir = get_user_sessions_dir(str(current_user.id))
    sessions = []
    if sessions_dir.exists():
        for session_file in sessions_dir.glob("*.json"):
            try:
                with open(session_file, encoding="utf-8") as f:
                    data = json.load(f)
                    # Convert to summary
                    sessions.append(
                        ChatSessionSummary(
                            id=data["id"],
                            title=data.get("title", "Untitled Session"),
                            created_at=data.get("created_at", ""),
                            last_active=data.get("last_active", ""),
                            message_count=len(data.get("messages", [])),
                        )
                    )
            except Exception as e:
                print(f"Error reading session {session_file}: {e}")

    # Sort by last_active desc
    sessions.sort(key=lambda x: x.last_active, reverse=True)
    return sessions


@app.post("/api/chat/sessions", response_model=ChatSessionDetail)
async def create_chat_session(
    payload: ChatSessionCreateRequest,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
    import uuid
    from datetime import datetime

    try:
        session_id = str(uuid.uuid4())
        now = datetime.now().isoformat()

        session_data = {
            "id": session_id,
            "title": payload.title or "New Session",
            "created_at": now,
            "last_active": now,
            "message_count": 0,
            "messages": [],
            "metadata": {},
        }

        _save_session(str(current_user.id), session_id, session_data)

        return ChatSessionDetail(**session_data)
    except Exception as e:
        print(f"Error creating session: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create session: {str(e)}")


@app.get("/api/chat/sessions/{session_id}", response_model=ChatSessionDetail)
async def get_chat_session(
    session_id: str,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """è·å–å•ä¸ªä¼šè¯è¯¦æƒ…"""
    session = _load_session(str(current_user.id), session_id)
    if not session:
        raise HTTPException(404, "Session not found")
    return ChatSessionDetail(**session)


@app.post("/api/chat/sessions/{session_id}/clear")
async def clear_chat_session(
    session_id: str,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """æ¸…ç©ºä¼šè¯å†å²"""
    from datetime import datetime

    user_id = str(current_user.id)
    session = _load_session(user_id, session_id)
    if not session:
        raise HTTPException(404, "Session not found")

    session["messages"] = []
    session["last_active"] = datetime.now().isoformat()
    _save_session(user_id, session_id, session)

    return {"status": "success", "message": "Session cleared"}


@app.patch("/api/chat/sessions/{session_id}/title", response_model=ChatSessionSummary)
async def update_chat_session_title(
    session_id: str,
    payload: ChatSessionTitleUpdate,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """æ›´æ–°ä¼šè¯æ ‡é¢˜"""
    from datetime import datetime

    user_id = str(current_user.id)
    session = _load_session(user_id, session_id)
    if not session:
        raise HTTPException(404, "Session not found")

    session["title"] = payload.title
    session["last_active"] = datetime.now().isoformat()
    _save_session(user_id, session_id, session)

    return ChatSessionSummary(
        id=session["id"],
        title=session["title"],
        created_at=session["created_at"],
        last_active=session["last_active"],
        message_count=len(session["messages"]),
    )


@app.delete("/api/chat/sessions/{session_id}")
async def delete_chat_session(
    session_id: str,
    current_user: Annotated[User, Depends(get_current_user)],
):
    """åˆ é™¤èŠå¤©ä¼šè¯"""
    user_id = str(current_user.id)
    path = _get_session_path(user_id, session_id)
    if path.exists():
        path.unlink()
        return {"status": "success", "message": "Session deleted"}
    raise HTTPException(404, "Session not found")


@app.get("/api/studio/memory/config")
async def get_memory_config():
    """è·å–è®°å¿†é…ç½®"""
    import logging
    from pathlib import Path

    import yaml

    # é»˜è®¤é…ç½®
    config = {
        "enabled": True,
        "backends": ["short_term", "long_term"],
        "short_term": {"max_items": 20},
        "long_term": {"enabled": True},
    }

    try:
        # å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
        # api.py åœ¨ sage/studio/config/backend/
        # knowledge_sources.yaml åœ¨ sage/studio/config/
        current_dir = Path(__file__).parent
        config_path = current_dir.parent / "knowledge_sources.yaml"

        if config_path.exists():
            with open(config_path, encoding="utf-8") as f:
                yaml_data = yaml.safe_load(f)
                if "memory" in yaml_data:
                    mem_config = yaml_data["memory"]
                    config["enabled"] = mem_config.get("enabled", True)

                    # æ›´æ–° backends åˆ—è¡¨
                    if "backends" in mem_config:
                        config["backends"] = list(mem_config["backends"].keys())

                        # æ›´æ–°å…·ä½“åç«¯é…ç½®
                        if "short_term" in mem_config["backends"]:
                            config["short_term"] = mem_config["backends"]["short_term"]
                        if "long_term" in mem_config["backends"]:
                            config["long_term"] = mem_config["backends"]["long_term"]
    except Exception as e:
        logging.error(f"Failed to load memory config: {e}")

    logging.info(f"Returning memory config: {config}")
    return config


@app.get("/api/chat/memory/stats")
async def get_memory_stats(session_id: str):
    """è·å–è®°å¿†ç»Ÿè®¡"""
    service = get_memory_service(session_id)
    return await service.get_summary()


@app.post("/api/uploads")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡ä»¶"""
    from dataclasses import asdict

    service = get_file_upload_service()
    try:
        # UploadFile.file is a SpooledTemporaryFile which is a file-like object
        metadata = await service.upload_file(file.file, file.filename)
        return asdict(metadata)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.get("/api/uploads")
async def list_uploaded_files():
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨"""
    from dataclasses import asdict

    service = get_file_upload_service()
    files = service.list_files()
    return [asdict(f) for f in files]


@app.get("/api/uploads/{file_id}")
async def get_uploaded_file(file_id: str):
    """è·å–å•ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®"""
    from dataclasses import asdict

    service = get_file_upload_service()
    metadata = service.get_file(file_id)
    if not metadata:
        raise HTTPException(status_code=404, detail="File not found")
    return asdict(metadata)


@app.get("/api/uploads/{file_id}/content")
async def get_uploaded_file_content(file_id: str):
    """è·å–ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹"""
    service = get_file_upload_service()
    file_path = service.get_file_path(file_id)
    if not file_path or not file_path.exists():
        raise HTTPException(status_code=404, detail="File not found")

    # è¯»å–æ–‡ä»¶å†…å®¹
    try:
        with open(file_path, encoding="utf-8") as f:
            content = f.read()
        return {"file_id": file_id, "content": content}
    except UnicodeDecodeError:
        # äºŒè¿›åˆ¶æ–‡ä»¶
        raise HTTPException(status_code=400, detail="Binary file cannot be read as text")


class IndexFileRequest(BaseModel):
    """ç´¢å¼•æ–‡ä»¶è¯·æ±‚"""

    source_name: str = "user_uploads"  # çŸ¥è¯†æºåç§°


@app.post("/api/uploads/{file_id}/index")
async def index_uploaded_file(file_id: str, request: IndexFileRequest):
    """å°†ä¸Šä¼ çš„æ–‡ä»¶ç´¢å¼•åˆ°çŸ¥è¯†åº“

    è¿™ä¼šå°†æ–‡ä»¶å†…å®¹åˆ†å—å¹¶å­˜å…¥å‘é‡æ•°æ®åº“ï¼Œä½¿å…¶å¯é€šè¿‡è¯­ä¹‰æœç´¢æ£€ç´¢ã€‚
    """
    from sage.studio.services.knowledge_manager import KnowledgeManager

    service = get_file_upload_service()
    metadata = service.get_file(file_id)
    if not metadata:
        raise HTTPException(status_code=404, detail="File not found")

    file_path = service.get_file_path(file_id)
    if not file_path or not file_path.exists():
        raise HTTPException(status_code=404, detail="File path not found")

    # ç´¢å¼•åˆ°çŸ¥è¯†åº“
    try:
        km = KnowledgeManager()
        success = await km.add_document(file_path, source_name=request.source_name)

        if success:
            # æ ‡è®°æ–‡ä»¶å·²ç´¢å¼•
            service.mark_indexed(file_id)
            return {
                "success": True,
                "file_id": file_id,
                "source_name": request.source_name,
                "message": f"File indexed to '{request.source_name}' knowledge source",
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to index file")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Indexing failed: {str(e)}")


@app.get("/api/knowledge/sources")
async def list_knowledge_sources():
    """åˆ—å‡ºå¯ç”¨çš„çŸ¥è¯†æº"""
    from sage.studio.services.knowledge_manager import KnowledgeManager

    km = KnowledgeManager()
    sources = []
    for name, source in km.sources.items():
        sources.append(
            {
                "name": name,
                "type": source.type.value,
                "description": source.description,
                "enabled": source.enabled,
                "is_dynamic": source.is_dynamic,
            }
        )
    return sources


class KnowledgeSearchRequest(BaseModel):
    """çŸ¥è¯†æ£€ç´¢è¯·æ±‚"""

    query: str
    sources: list[str] | None = None  # None è¡¨ç¤ºæ‰€æœ‰å·²åŠ è½½çš„æº
    limit: int = 5
    score_threshold: float = 0.6


@app.post("/api/knowledge/search")
async def search_knowledge(request: KnowledgeSearchRequest):
    """åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢"""
    from sage.studio.services.knowledge_manager import KnowledgeManager

    km = KnowledgeManager()
    try:
        results = await km.search(
            query=request.query,
            sources=request.sources,
            limit=request.limit,
            score_threshold=request.score_threshold,
        )
        return {
            "query": request.query,
            "results": [
                {
                    "content": r.content,
                    "score": r.score,
                    "source": r.source,
                    "metadata": r.metadata,
                }
                for r in results
            ],
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")


@app.delete("/api/uploads/{file_id}")
async def delete_uploaded_file(file_id: str):
    """åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶"""
    service = get_file_upload_service()
    success = service.delete_file(file_id)
    if not success:
        raise HTTPException(status_code=404, detail="File not found")
    return {"success": True, "file_id": file_id}


class WorkflowGenerateRequest(BaseModel):
    """å·¥ä½œæµç”Ÿæˆè¯·æ±‚ (LLMé©±åŠ¨çš„é«˜çº§ç‰ˆæœ¬)"""

    user_input: str
    session_id: str | None = None
    enable_optimization: bool = False
    optimization_strategy: str = "greedy"  # greedy, parallelization, noop
    constraints: dict | None = None  # max_cost, max_latency, min_quality


@app.post("/api/chat/generate-workflow")
async def generate_workflow_advanced(request: WorkflowGenerateRequest):
    """ç”Ÿæˆæ™ºèƒ½å·¥ä½œæµ (ä½¿ç”¨ LLM Pipeline Builder)

    è¿™ä¸ªç«¯ç‚¹ä½¿ç”¨æ›´é«˜çº§çš„ LLM é©±åŠ¨ç”Ÿæˆï¼Œè€Œä¸æ˜¯ç®€å•çš„æ„å›¾è¯†åˆ«ã€‚
    å¯é€‰åœ°åº”ç”¨ sage-libs ä¸­çš„å·¥ä½œæµä¼˜åŒ–ç®—æ³•ã€‚

    Args:
        request: åŒ…å«ç”¨æˆ·è¾“å…¥ã€ä¼šè¯ä¿¡æ¯ã€ä¼˜åŒ–é€‰é¡¹

    Returns:
        {
            "success": bool,
            "visual_pipeline": {...},  # Studio å¯è§†åŒ–æ ¼å¼
            "raw_plan": {...},         # åŸå§‹ Pipeline é…ç½®
            "optimization_applied": bool,
            "optimization_metrics": {...},
            "message": str
        }
    """
    import httpx

    from sage.studio.services.workflow_generator import generate_workflow_from_chat

    # å¦‚æœæä¾›äº† session_idï¼Œè·å–å¯¹è¯å†å²
    session_messages = None
    if request.session_id:
        try:
            async with httpx.AsyncClient(timeout=10.0, trust_env=False) as client:
                response = await client.get(f"{GATEWAY_BASE_URL}/sessions/{request.session_id}")
                if response.status_code == 200:
                    session = response.json()
                    session_messages = session.get("messages", [])
        except httpx.ConnectError:
            # å¦‚æœæ— æ³•è¿æ¥ Gatewayï¼Œç»§ç»­ä½¿ç”¨ä»…ç”¨æˆ·è¾“å…¥
            pass

    # è°ƒç”¨å·¥ä½œæµç”Ÿæˆå™¨
    try:
        print("ğŸ” Calling generate_workflow_from_chat with:")
        print(f"  - user_input: {request.user_input}")
        print(f"  - session_messages: {session_messages is not None}")
        print(f"  - enable_optimization: {request.enable_optimization}")

        result = generate_workflow_from_chat(
            user_input=request.user_input,
            session_messages=session_messages,
            enable_optimization=request.enable_optimization,
        )

        print(f"âœ… Result returned: {result}")
        print(f"  - Type: {type(result)}")
        if result:
            print(f"  - success: {result.success}")
            print(f"  - visual_pipeline: {result.visual_pipeline is not None}")

    except Exception as e:
        import traceback

        print("âŒ Exception in generate_workflow_from_chat:")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å·¥ä½œæµç”Ÿæˆå¤±è´¥: {str(e)}")

    if result is None:
        raise HTTPException(status_code=500, detail="å·¥ä½œæµç”Ÿæˆå™¨è¿”å›äº† None")

    if not result.success:
        raise HTTPException(status_code=500, detail=result.error or "å·¥ä½œæµç”Ÿæˆå¤±è´¥")

    print("ğŸ“¤ Preparing response...")
    response_data = {
        "success": result.success,
        "visual_pipeline": result.visual_pipeline,
        "raw_plan": result.raw_plan,
        "optimization_applied": result.optimization_applied,
        "optimization_metrics": result.optimization_metrics,
        "message": result.message,
    }
    print(f"âœ… Response data prepared: {list(response_data.keys())}")
    return response_data


# ===== Fine-tune API Endpoints =====


class FinetuneCreateRequest(BaseModel):
    """Create fine-tune task request"""

    model_name: str = "Qwen/Qwen2.5-7B-Instruct"
    dataset_file: str  # Path to uploaded dataset
    num_epochs: int = 3
    batch_size: int = 1
    gradient_accumulation_steps: int = 16
    learning_rate: float = 5e-5
    max_length: int = 1024
    load_in_8bit: bool = True


class UseAsBackendRequest(BaseModel):
    """Use finetuned model as backend request"""

    task_id: str


@app.post("/api/finetune/create")
async def create_finetune_task(request: FinetuneCreateRequest):
    """åˆ›å»ºå¾®è°ƒä»»åŠ¡ï¼ˆå¸¦ OOM é£é™©æ£€æµ‹ï¼‰"""
    import torch

    from sage.libs.finetune import finetune_manager

    # GPU æ˜¾å­˜æ£€æµ‹
    warnings = []
    if torch.cuda.is_available():
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)

        # ä¼°ç®—æ˜¾å­˜éœ€æ±‚
        estimated_memory = 0
        if "7B" in request.model_name or "7b" in request.model_name:
            estimated_memory = 14 if request.load_in_8bit else 28
        elif "3B" in request.model_name or "3b" in request.model_name:
            estimated_memory = 6 if request.load_in_8bit else 12
        elif "1.5B" in request.model_name or "1.5b" in request.model_name:
            estimated_memory = 3 if request.load_in_8bit else 6
        elif "0.5B" in request.model_name or "0.5b" in request.model_name:
            estimated_memory = 1 if request.load_in_8bit else 2

        # æ·»åŠ  batch size å’Œ sequence length çš„é¢å¤–å¼€é”€
        estimated_memory += request.batch_size * (request.max_length / 1024) * 0.5

        # OOM é£é™©æ£€æµ‹
        if estimated_memory > gpu_memory_gb * 0.9:
            warnings.append(
                f"âš ï¸ OOM é£é™©é«˜ï¼šé¢„è®¡éœ€è¦ {estimated_memory:.1f}GBï¼Œä½†åªæœ‰ {gpu_memory_gb:.1f}GB å¯ç”¨"
            )
            warnings.append("å»ºè®®ï¼šå‡å° batch_size æˆ– max_lengthï¼Œæˆ–å¯ç”¨ 8-bit é‡åŒ–")
        elif estimated_memory > gpu_memory_gb * 0.7:
            warnings.append(
                f"âš ï¸ OOM é£é™©ä¸­ï¼šé¢„è®¡éœ€è¦ {estimated_memory:.1f}GBï¼Œå¯ç”¨ {gpu_memory_gb:.1f}GB"
            )
    else:
        warnings.append("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œè®­ç»ƒå°†éå¸¸ç¼“æ…¢")

    config = {
        "num_epochs": request.num_epochs,
        "batch_size": request.batch_size,
        "gradient_accumulation_steps": request.gradient_accumulation_steps,
        "learning_rate": request.learning_rate,
        "max_length": request.max_length,
        "load_in_8bit": request.load_in_8bit,
    }

    task = finetune_manager.create_task(
        model_name=request.model_name, dataset_path=request.dataset_file, config=config
    )

    # æ·»åŠ è­¦å‘Šæ—¥å¿—
    for warning in warnings:
        finetune_manager.add_task_log(task.task_id, warning)

    # Start training immediately
    success = finetune_manager.start_training(task.task_id)
    if not success:
        raise HTTPException(status_code=409, detail="Another training task is running")

    result = task.to_dict()
    result["warnings"] = warnings
    return result


@app.get("/api/finetune/tasks")
async def list_finetune_tasks():
    """åˆ—å‡ºæ‰€æœ‰å¾®è°ƒä»»åŠ¡"""
    from sage.libs.finetune import finetune_manager

    tasks = finetune_manager.list_tasks()
    return [task.to_dict() for task in tasks]


@app.get("/api/finetune/tasks/{task_id}")
async def get_finetune_task(task_id: str):
    """è·å–å¾®è°ƒä»»åŠ¡è¯¦æƒ…"""
    from sage.libs.finetune import finetune_manager

    task = finetune_manager.get_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    return task.to_dict()


@app.get("/api/finetune/models")
async def list_finetune_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆåŸºç¡€æ¨¡å‹ + å¾®è°ƒåçš„æ¨¡å‹ï¼‰"""
    from sage.libs.finetune import finetune_manager

    return finetune_manager.list_available_models()


@app.post("/api/finetune/switch-model")
async def switch_model(model_path: str):
    """åˆ‡æ¢å½“å‰ä½¿ç”¨çš„æ¨¡å‹å¹¶çƒ­é‡å¯ LLM æœåŠ¡ï¼ˆæ— éœ€é‡å¯ Studioï¼‰"""
    from sage.studio.chat_manager import ChatModeManager

    # Get ChatModeManager instance and apply the model
    chat_manager = ChatModeManager()
    result = chat_manager.apply_finetuned_model(model_path)

    if result["success"]:
        return {
            "message": result["message"],
            "current_model": result["model"],
            "llm_service_restarted": True,
        }
    else:
        raise HTTPException(status_code=500, detail=result["message"])


@app.get("/api/finetune/current-model")
async def get_current_model():
    """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    from sage.libs.finetune import finetune_manager

    return {"current_model": finetune_manager.get_current_model()}


@app.post("/api/finetune/upload-dataset")
async def upload_dataset(file: UploadFile = File(...)):
    """ä¸Šä¼ å¾®è°ƒæ•°æ®é›†"""
    from pathlib import Path

    # Validate file type
    if not file.filename.endswith((".json", ".jsonl")):
        raise HTTPException(status_code=400, detail="Only JSON/JSONL files are supported")

    # Save to uploads directory
    upload_dir = Path.home() / ".sage" / "studio_finetune" / "uploads"
    upload_dir.mkdir(parents=True, exist_ok=True)

    file_path = upload_dir / f"{int(time.time())}_{file.filename}"

    try:
        content = await file.read()
        with open(file_path, "wb") as f:
            f.write(content)
        return {"file_path": str(file_path), "filename": file.filename}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {e}")


@app.get("/api/finetune/tasks/{task_id}/download")
async def download_finetuned_model(task_id: str):
    """ä¸‹è½½å¾®è°ƒåçš„æ¨¡å‹ï¼ˆæ‰“åŒ…ä¸º tar.gzï¼‰"""
    import tarfile
    from pathlib import Path

    from fastapi.responses import FileResponse

    from sage.libs.finetune import finetune_manager

    task = finetune_manager.get_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")

    if task.status != "completed":
        raise HTTPException(status_code=400, detail="Task is not completed yet")

    model_dir = Path(task.output_dir)
    if not model_dir.exists():
        raise HTTPException(status_code=404, detail="Model directory not found")

    # åˆ›å»ºä¸´æ—¶æ‰“åŒ…ç›®å½•
    temp_dir = Path.home() / ".sage" / "studio_finetune" / "downloads"
    temp_dir.mkdir(parents=True, exist_ok=True)

    # æ‰“åŒ…æ¨¡å‹æ–‡ä»¶
    archive_path = temp_dir / f"{task_id}.tar.gz"
    try:
        with tarfile.open(archive_path, "w:gz") as tar:
            tar.add(model_dir, arcname=task_id)

        return FileResponse(
            path=str(archive_path),
            media_type="application/gzip",
            filename=f"{task_id}_finetuned_model.tar.gz",
            headers={
                "Content-Disposition": f'attachment; filename="{task_id}_finetuned_model.tar.gz"'
            },
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to package model: {e}")


@app.delete("/api/finetune/tasks/{task_id}")
async def delete_finetune_task(task_id: str):
    """åˆ é™¤å¾®è°ƒä»»åŠ¡ï¼ˆä»…å…è®¸åˆ é™¤å·²å®Œæˆã€å¤±è´¥æˆ–å–æ¶ˆçš„ä»»åŠ¡ï¼‰"""
    from sage.libs.finetune import FinetuneStatus, finetune_manager

    if finetune_manager.delete_task(task_id):
        return {"status": "success", "message": f"ä»»åŠ¡ {task_id} å·²åˆ é™¤"}
    else:
        task = finetune_manager.tasks.get(task_id)
        if not task:
            # å°è¯•é‡æ–°åŠ è½½ä»»åŠ¡
            finetune_manager._load_tasks()
            task = finetune_manager.tasks.get(task_id)

        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        elif task.status in (
            FinetuneStatus.TRAINING,
            FinetuneStatus.PREPARING,
            FinetuneStatus.QUEUED,
        ):
            raise HTTPException(status_code=400, detail="æ— æ³•åˆ é™¤è¿è¡Œä¸­æˆ–æ’é˜Ÿä¸­çš„ä»»åŠ¡")
        else:
            raise HTTPException(status_code=500, detail="Failed to delete task")


@app.post("/api/finetune/tasks/{task_id}/cancel")
async def cancel_finetune_task(task_id: str):
    """å–æ¶ˆè¿è¡Œä¸­çš„å¾®è°ƒä»»åŠ¡"""
    from sage.libs.finetune import FinetuneStatus, finetune_manager

    task = finetune_manager.tasks.get(task_id)
    if not task:
        # ä»»åŠ¡ä¸åœ¨å†…å­˜ä¸­ï¼Œå°è¯•é‡æ–°åŠ è½½
        print(f"[API] Task {task_id} not found in memory, attempting to reload tasks...")
        finetune_manager._load_tasks()
        task = finetune_manager.tasks.get(task_id)

        if not task:
            raise HTTPException(
                status_code=404,
                detail=f"Task not found: {task_id}. Available tasks: {list(finetune_manager.tasks.keys())}",
            )

    if task.status not in (
        FinetuneStatus.TRAINING,
        FinetuneStatus.PREPARING,
        FinetuneStatus.QUEUED,
    ):
        raise HTTPException(status_code=400, detail="ä»»åŠ¡ä¸åœ¨è¿è¡Œä¸­ï¼Œæ— æ³•å–æ¶ˆ")

    if finetune_manager.cancel_task(task_id):
        return {"status": "success", "message": f"ä»»åŠ¡ {task_id} å·²å–æ¶ˆ"}
    else:
        raise HTTPException(status_code=500, detail="Failed to cancel task")


@app.get("/api/finetune/models/base")
async def list_base_models():
    """åˆ—å‡ºæ¨èçš„åŸºç¡€æ¨¡å‹ï¼ˆæŒ‰æ˜¾å­˜éœ€æ±‚åˆ†ç±»ï¼‰"""
    return {
        "recommended_for_rtx3060": [
            {
                "name": "Qwen/Qwen2.5-Coder-1.5B-Instruct",
                "size": "1.5B",
                "vram_required": "6-8GB",
                "description": "ä»£ç ä¸“ç²¾ï¼Œæœ€é€‚åˆ RTX 3060ï¼ˆæ¨èï¼‰",
                "training_time": "2-4å°æ—¶ (1000æ ·æœ¬)",
            },
            {
                "name": "Qwen/Qwen2.5-0.5B-Instruct",
                "size": "500M",
                "vram_required": "4-6GB",
                "description": "è¶…è½»é‡çº§ï¼Œè®­ç»ƒæœ€å¿«",
                "training_time": "1-2å°æ—¶ (1000æ ·æœ¬)",
            },
            {
                "name": "Qwen/Qwen2.5-1.5B-Instruct",
                "size": "1.5B",
                "vram_required": "6-8GB",
                "description": "é€šç”¨å¯¹è¯æ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜",
                "training_time": "2-4å°æ—¶ (1000æ ·æœ¬)",
            },
        ],
        "advanced_models": [
            {
                "name": "Qwen/Qwen2.5-3B-Instruct",
                "size": "3B",
                "vram_required": "10-12GB",
                "description": "æ›´å¼ºæ€§èƒ½ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜",
                "training_time": "4-6å°æ—¶ (1000æ ·æœ¬)",
            },
            {
                "name": "Qwen/Qwen2.5-7B-Instruct",
                "size": "7B",
                "vram_required": "16-20GB",
                "description": "é«˜æ€§èƒ½æ¨¡å‹ï¼Œéœ€è¦ RTX 4090 æˆ–æ›´å¼º",
                "training_time": "8-12å°æ—¶ (1000æ ·æœ¬)",
            },
        ],
    }


@app.post("/api/finetune/prepare-sage-docs")
async def prepare_sage_docs(force_refresh: bool = False):
    """å‡†å¤‡ SAGE å®˜æ–¹æ–‡æ¡£ä½œä¸ºè®­ç»ƒæ•°æ®"""
    from sage.studio.services.docs_processor import get_docs_processor

    try:
        processor = get_docs_processor()

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        data_file = processor.prepare_training_data(force_refresh=force_refresh)

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_stats(data_file)

        return {
            "status": "success",
            "message": "SAGE æ–‡æ¡£å·²å‡†å¤‡å®Œæˆ",
            "data_file": str(data_file),
            "stats": stats,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to prepare SAGE docs: {e}")


@app.post("/api/finetune/use-as-backend")
async def use_finetuned_as_backend(request: UseAsBackendRequest):
    """å°†å¾®è°ƒåçš„æ¨¡å‹è®¾ç½®ä¸º Studio å¯¹è¯åç«¯"""
    from sage.libs.finetune import finetune_manager

    try:
        task = finetune_manager.get_task(request.task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")

        if task.status != "completed":
            raise HTTPException(status_code=400, detail="Task is not completed yet")

        # è·å–æ¨¡å‹è·¯å¾„
        model_path = Path(task.output_dir)
        if not model_path.exists():
            raise HTTPException(status_code=404, detail="Model directory not found")

        # æ³¨å†Œåˆ° vLLM Registry
        from sage.platform.llm.vllm_registry import vllm_registry

        model_name = f"sage-finetuned-{request.task_id}"

        # è‡ªåŠ¨æ£€æµ‹ GPU æ•°é‡å’Œæ˜¾å­˜
        try:
            import torch

            num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
            # è·å–å•ä¸ª GPU çš„æ˜¾å­˜ï¼ˆä»¥ GB ä¸ºå•ä½ï¼‰
            if num_gpus > 0:
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            else:
                gpu_memory_gb = 0
        except Exception:
            num_gpus = 0
            gpu_memory_gb = 0

        # æ ¹æ® GPU é…ç½®æ¨¡å‹å‚æ•°
        config = {
            "trust_remote_code": True,
            "max_model_len": 2048,  # é»˜è®¤å€¼
        }

        # åªæœ‰å½“æœ‰ GPU æ—¶æ‰è®¾ç½® GPU ç›¸å…³å‚æ•°
        if num_gpus > 0:
            # æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ max_model_len
            if gpu_memory_gb >= 24:  # 24GB+ (A100, RTX 4090, etc.)
                config["max_model_len"] = 4096
                config["gpu_memory_utilization"] = 0.85
            elif gpu_memory_gb >= 16:  # 16GB+ (V100, RTX 4080, etc.)
                config["max_model_len"] = 3072
                config["gpu_memory_utilization"] = 0.8
            elif gpu_memory_gb >= 8:  # 8GB+ (RTX 3070, etc.)
                config["max_model_len"] = 2048
                config["gpu_memory_utilization"] = 0.75
            else:  # < 8GB
                config["max_model_len"] = 1024
                config["gpu_memory_utilization"] = 0.7

            # å¦‚æœæœ‰å¤šä¸ª GPU ä¸”æ¨¡å‹è¾ƒå¤§ï¼Œå¯ç”¨å¼ é‡å¹¶è¡Œ
            if num_gpus > 1:
                config["tensor_parallel_size"] = num_gpus

        # æ³¨å†Œæ¨¡å‹
        vllm_registry.register_model(
            model_name=model_name,
            model_path=str(model_path),
            config=config,
        )

        # åˆ‡æ¢åˆ°è¯¥æ¨¡å‹
        vllm_registry.switch_model(model_name)

        # æ›´æ–°ç¯å¢ƒå˜é‡ï¼ˆä¾› RAG pipeline ä½¿ç”¨ï¼‰
        os.environ["SAGE_STUDIO_LLM_MODEL"] = model_name
        os.environ["SAGE_STUDIO_LLM_PATH"] = str(model_path)

        return {
            "status": "success",
            "message": f"å·²åˆ‡æ¢åˆ°å¾®è°ƒæ¨¡å‹: {model_name}",
            "model_name": model_name,
            "model_path": str(model_path),
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to switch backend: {e}")


@app.get("/api/system/gpu-info")
async def get_gpu_info():
    """Get GPU information for finetune recommendations"""
    try:
        import torch

        gpu_info = {
            "available": torch.cuda.is_available(),
            "count": 0,
            "devices": [],
            "recommendation": "CPU æ¨¡å¼ï¼ˆä¸æ¨èå¾®è°ƒï¼‰",
        }

        if torch.cuda.is_available():
            gpu_info["count"] = torch.cuda.device_count()

            for i in range(gpu_info["count"]):
                device_name = torch.cuda.get_device_name(i)
                device_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB

                gpu_info["devices"].append(
                    {
                        "id": i,
                        "name": device_name,
                        "memory_gb": round(device_memory, 1),
                    }
                )

            # ç”Ÿæˆæ¨èé…ç½®
            if gpu_info["count"] == 1:
                gpu_name = gpu_info["devices"][0]["name"]
                gpu_memory = gpu_info["devices"][0]["memory_gb"]

                # æ ¹æ®æ˜¾å­˜æ¨èæ¨¡å‹
                if gpu_memory >= 24:
                    gpu_info["recommendation"] = (
                        f"{gpu_name} ({gpu_memory}GB): æ¨è Qwen 2.5 Coder 7B æˆ– 3B"
                    )
                elif gpu_memory >= 12:
                    gpu_info["recommendation"] = (
                        f"{gpu_name} ({gpu_memory}GB): æ¨è Qwen 2.5 Coder 3B æˆ– 1.5B"
                    )
                elif gpu_memory >= 8:
                    gpu_info["recommendation"] = (
                        f"{gpu_name} ({gpu_memory}GB): æ¨è Qwen 2.5 Coder 1.5Bï¼ˆæœ€ä½³å¹³è¡¡ï¼‰æˆ– 0.5Bï¼ˆæœ€å¿«è®­ç»ƒï¼‰"
                    )
                else:
                    gpu_info["recommendation"] = (
                        f"{gpu_name} ({gpu_memory}GB): æ¨è Qwen 2.5 Coder 0.5B"
                    )
            else:
                total_memory = sum(d["memory_gb"] for d in gpu_info["devices"])
                gpu_info["recommendation"] = (
                    f"æ£€æµ‹åˆ° {gpu_info['count']} å— GPUï¼ˆæ€»æ˜¾å­˜ {total_memory:.1f}GBï¼‰ï¼šæ”¯æŒå¤šå¡å¹¶è¡Œè®­ç»ƒ"
                )

        return gpu_info

    except Exception as e:
        return {
            "available": False,
            "count": 0,
            "devices": [],
            "recommendation": f"GPU æ£€æµ‹å¤±è´¥: {e}",
        }


# ==================== LLM çŠ¶æ€ API ====================


class SelectModelRequest(BaseModel):
    model_name: str
    base_url: str


def _get_models_config_path(create_dir: bool = False) -> Path | None:
    """Locate config/models.json, optionally creating the directory."""
    try:
        from sage.common.config import find_sage_project_root

        project_root = find_sage_project_root()
    except Exception:
        project_root = None

    base_dir = project_root or Path.cwd()
    config_dir = base_dir / "config"

    if create_dir:
        config_dir.mkdir(parents=True, exist_ok=True)
    elif not config_dir.exists():
        return None

    return config_dir / "models.json"


def _expand_api_key(value: Any) -> str:
    if not isinstance(value, str):
        return ""
    if value.startswith("${") and value.endswith("}"):
        env_var = value[2:-1]
        return os.getenv(env_var, "")
    return value


def _load_models_config(filter_missing: bool = False) -> tuple[list[dict[str, Any]], Path | None]:
    path = _get_models_config_path()
    if not path or not path.exists():
        return ([], path)

    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        if isinstance(data, list):
            models: list[dict[str, Any]] = []
            for entry in data:
                if isinstance(entry, dict):
                    entry_copy = dict(entry)
                    raw_key = entry_copy.get("api_key")
                    expanded_key = _expand_api_key(raw_key)

                    # Skip if API key is required (variable reference) but missing/empty
                    if (
                        filter_missing
                        and isinstance(raw_key, str)
                        and raw_key.startswith("${")
                        and not expanded_key
                    ):
                        continue

                    entry_copy["api_key"] = expanded_key
                    models.append(entry_copy)
            return models, path
    except Exception:
        pass
    return ([], path)


def _save_models_config(path: Path | None, models: list[dict[str, Any]]) -> None:
    if not path:
        return
    target_path = path
    target_path.parent.mkdir(parents=True, exist_ok=True)
    target_path.write_text(json.dumps(models, indent=4, ensure_ascii=False), encoding="utf-8")


def _persist_model_selection(model_name: str, base_url: str) -> str:
    """Update config/models.json default selection and return API key."""
    models, path = _load_models_config()
    if path is None:
        path = _get_models_config_path(create_dir=True)
    target_entry: dict[str, Any] | None = None

    for entry in models:
        names_match = entry.get("name") == model_name
        url_match = (
            _base_urls_match(entry.get("base_url"), base_url) if entry.get("base_url") else False
        )
        if names_match and (url_match or not entry.get("base_url")):
            entry["base_url"] = base_url
            target_entry = entry
            break

    if target_entry is None:
        target_entry = {
            "name": model_name,
            "base_url": base_url,
            "is_local": _is_loopback_url(base_url),
        }
        models.append(target_entry)
    else:
        target_entry["is_local"] = _is_loopback_url(base_url)

    for entry in models:
        entry["default"] = entry is target_entry

    _save_models_config(path, models)
    return _expand_api_key(target_entry.get("api_key"))


def _discover_launcher_models() -> list[dict[str, Any]]:
    try:
        from sage.llm import LLMLauncher
    except ImportError:
        return []

    models: list[dict[str, Any]] = []
    for service in LLMLauncher.discover_running_services():
        # Filter out embedding models
        if service.get("config", {}).get("engine_kind") == "embedding":
            continue

        model_name = service.get("served_model_name") or service.get("model") or "local-llm"
        if "embedding" in model_name.lower():
            continue

        models.append(
            {
                "name": model_name,
                "base_url": service.get("base_url"),
                "is_local": True,
                "description": "Auto-detected Local Model",
            }
        )
    return models


def _normalize_base_url(base_url: str | None) -> str | None:
    return base_url.rstrip("/") if base_url else base_url


def _normalize_probe_base_url(base_url: str | None) -> str | None:
    if not base_url:
        return None
    parsed = urlparse(base_url)
    host = parsed.hostname
    replacement = None
    if not host or host in {"0.0.0.0", "*"}:
        replacement = "127.0.0.1"
    elif host in {"::", "[::]"}:
        replacement = "::1"

    if replacement:
        host_token = replacement
        if ":" in host_token and not host_token.startswith("["):
            host_token = f"[{host_token}]"
        if parsed.port:
            netloc = f"{host_token}:{parsed.port}"
        else:
            netloc = host_token
        parsed = parsed._replace(netloc=netloc)
        return urlunparse(parsed).rstrip("/")

    return base_url.rstrip("/")


def _canon_host(host: str | None) -> str | None:
    if not host:
        return None
    host = host.lower()
    if host in {"0.0.0.0", "*", "localhost"}:
        return "127.0.0.1"
    if host in {"::", "[::]"}:
        return "::1"
    return host


def _base_url_signature(base_url: str | None) -> tuple[str, str, int, str] | None:
    if not base_url:
        return None

    candidate = base_url.strip()
    if not candidate:
        return None

    parsed = urlparse(candidate if "://" in candidate else f"http://{candidate}")
    scheme = parsed.scheme or "http"
    host = _canon_host(parsed.hostname) or ""
    port = parsed.port
    if port is None:
        port = 443 if scheme == "https" else 80

    path = parsed.path.rstrip("/")
    if path in ("", "/v1"):
        path = ""

    return (scheme, host, port, path)


def _base_urls_match(a: str | None, b: str | None) -> bool:
    sig_a = _base_url_signature(a)
    sig_b = _base_url_signature(b)
    return bool(sig_a and sig_b and sig_a == sig_b)


def _build_health_url(base_url: str) -> str:
    parsed = urlparse(base_url)
    path = parsed.path.rstrip("/")
    if path.endswith("/v1"):
        health_path = path[:-3] + "/health"
    else:
        health_path = path + "/health"
    return urlunparse(parsed._replace(path=health_path, query="", fragment=""))


def _is_loopback_url(base_url: str | None) -> bool:
    if not base_url:
        return False
    parsed = urlparse(base_url)
    host = parsed.hostname
    if not host:
        return False
    if host == "localhost":
        return True
    try:
        return ipaddress.ip_address(host).is_loopback
    except ValueError:
        return False


def _probe_llm_endpoint(base_url: str | None, headers: dict[str, str] | None = None) -> bool:
    normalized = _normalize_probe_base_url(base_url)
    if not normalized:
        return False
    headers = headers or {}

    try:
        health_url = _build_health_url(normalized)
        resp = requests.get(health_url, headers=headers, timeout=2.0)
        if resp.status_code == 200:
            return True
    except Exception:
        pass

    try:
        resp = requests.get(f"{normalized}/models", headers=headers, timeout=2.0)
        if resp.status_code == 200:
            return True
    except Exception:
        pass

    try:
        parsed = urlparse(normalized)
        host = parsed.hostname
        port = parsed.port or (443 if parsed.scheme == "https" else 80)
        if host:
            import socket

            with socket.create_connection((host, port), timeout=2.0):
                return True
    except Exception:
        pass

    return False


@app.post("/api/llm/select")
async def select_llm_model(request: SelectModelRequest):
    """é€‰æ‹©è¦ä½¿ç”¨çš„ LLM æ¨¡å‹"""
    try:
        # æ›´æ–°ç¯å¢ƒå˜é‡ï¼Œè¿™æ ·åç»­çš„ get_llm_status è°ƒç”¨ï¼ˆä¼šåˆ›å»ºæ–°çš„ UnifiedInferenceClientï¼‰
        # å°±ä¼šä½¿ç”¨æ–°çš„é…ç½®
        os.environ["SAGE_CHAT_MODEL"] = request.model_name
        os.environ["SAGE_CHAT_BASE_URL"] = request.base_url

        api_key = ""
        try:
            api_key = _persist_model_selection(request.model_name, request.base_url)
        except Exception as exc:
            print(f"Failed to persist selected model: {exc}")

        if api_key:
            os.environ["SAGE_CHAT_API_KEY"] = api_key
            print(f"Set API key for model {request.model_name}")
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„ API Keyï¼Œæ¸…é™¤ç¯å¢ƒå˜é‡ï¼Œä»¥å…ä½¿ç”¨æ—§çš„
            if "SAGE_CHAT_API_KEY" in os.environ:
                del os.environ["SAGE_CHAT_API_KEY"]

        # Register with Control Plane
        try:
            parsed = urlparse(request.base_url)
            host = parsed.hostname or "localhost"
            port = parsed.port or (443 if parsed.scheme == "https" else 80)
            register_url = f"{GATEWAY_BASE_URL}/v1/management/engines/register"

            payload = {
                "engine_id": f"ext-{request.model_name}",
                "model_id": request.model_name,
                "host": host,
                "port": port,
                "engine_kind": "llm",
                "metadata": {"source": "studio_select", "scheme": parsed.scheme or "http"},
            }

            requests.post(register_url, json=payload, timeout=2)
            print(f"Registered model {request.model_name} with Control Plane")
        except Exception as e:
            print(f"Failed to register model with Control Plane: {e}")

        return {"status": "success", "message": f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {request.model_name}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}")


@app.get("/api/llm/status")
async def get_llm_status():
    """è·å–å½“å‰è¿è¡Œçš„ LLM æœåŠ¡çŠ¶æ€"""
    try:
        from sage.llm import UnifiedInferenceClient
    except Exception:
        UnifiedInferenceClient = None  # type: ignore[assignment]

    try:
        env_base_url = os.getenv("SAGE_CHAT_BASE_URL", "")
        env_model_name = os.getenv("SAGE_CHAT_MODEL", "")

        # Start with explicit environment variables (highest priority)
        base_url = env_base_url
        model_name = env_model_name

        # Next, honor persisted default selection from config/models.json when env is not set
        # This keeps frontend selections sticky across restarts without being overwritten by
        # Control Plane discovery.
        config_models, _ = _load_models_config(filter_missing=False)
        persisted_default = next((m for m in config_models if m.get("default")), None)
        persisted_base_url = (
            _normalize_base_url(persisted_default.get("base_url")) if persisted_default else None
        )
        if not base_url and persisted_base_url:
            base_url = persisted_base_url
        if not model_name and persisted_default:
            model_name = persisted_default.get("name", "")

        use_control_plane = not base_url and not model_name and not persisted_default

        if UnifiedInferenceClient:
            try:
                # Only fall back to Control Plane when nothing is configured/persisted
                if use_control_plane:
                    client = UnifiedInferenceClient.create(
                        control_plane_url=f"{GATEWAY_BASE_URL}/v1"
                    )
                    base_url = client.config.llm_base_url or base_url
                    model_name = client.config.llm_model or model_name

                # Try to fetch model name if still missing but base_url is known
                if not model_name and base_url:
                    model_name = UnifiedInferenceClient._fetch_model_name(base_url) or model_name
            except Exception:
                pass

        normalized_base_url = _normalize_base_url(base_url)
        is_local = _is_loopback_url(normalized_base_url)
        display_model_name = model_name or ("æœªé…ç½® LLM æœåŠ¡" if not base_url else "æœªå‘½åæ¨¡å‹")

        status = {
            "running": False,
            "healthy": False,
            "service_type": "not_configured" if not base_url else "remote_api",
            "model_name": display_model_name,
            "base_url": normalized_base_url or base_url,
            "is_local": is_local,
            "details": {},
        }

        if base_url:
            status["running"] = True

        # Local detailed status
        if is_local and base_url:
            probe_url = _normalize_probe_base_url(base_url)
            if probe_url:
                status["base_url"] = probe_url
                try:
                    health_resp = requests.get(_build_health_url(probe_url), timeout=2)
                    status["healthy"] = health_resp.status_code == 200
                    status["service_type"] = "local_vllm"

                    models_resp = requests.get(f"{probe_url}/models", timeout=2)
                    if models_resp.status_code == 200:
                        models_data = models_resp.json()
                        if models_data.get("data"):
                            first_model = models_data["data"][0]
                            status["details"] = {
                                "model_id": first_model.get("id", ""),
                                "max_model_len": first_model.get("max_model_len", 0),
                                "owned_by": first_model.get("owned_by", ""),
                            }
                            status["model_name"] = first_model.get("id", status["model_name"])
                except Exception as exc:
                    status["error"] = str(exc)

        # Build available model list
        # å¤ç”¨å‰é¢è¯»å–çš„é…ç½®ï¼Œé¿å…é‡å¤åŠ è½½
        available_models = []
        for model in config_models:
            # Filter out embedding models
            if model.get("engine_kind") == "embedding":
                continue
            # Double check for embedding in name/description if engine_kind is missing
            if "embedding" in model.get("name", "").lower():
                continue
            if "embedding" in model.get("description", "").lower():
                continue
            available_models.append(dict(model))

        def _merge_model(entry: dict[str, Any]) -> None:
            entry_url = entry.get("base_url")
            for existing in available_models:
                existing_url = existing.get("base_url")
                names_match = entry.get("name") and entry.get("name") == existing.get("name")
                urls_match = _base_urls_match(entry_url, existing_url)
                if urls_match or (not entry_url and not existing_url and names_match):
                    existing.update({k: v for k, v in entry.items() if v is not None})
                    return
            available_models.append(entry)

        for detected in _discover_launcher_models():
            _merge_model(detected)

        if not available_models:
            default_base = f"http://127.0.0.1:{SagePorts.BENCHMARK_LLM}/v1"
            defaults = [
                {
                    "name": "Qwen/Qwen2.5-0.5B-Instruct",
                    "base_url": default_base,
                    "is_local": True,
                    "description": "Local Small Model (Fast, CPU-friendly)",
                },
                {
                    "name": "Qwen/Qwen2.5-7B-Instruct",
                    "base_url": default_base,
                    "is_local": True,
                    "description": "Local Standard Model (Requires GPU)",
                },
            ]
            for entry in defaults:
                _merge_model(entry)

        cloud_api_key = os.getenv("SAGE_CHAT_API_KEY")
        cloud_base_url = os.getenv("SAGE_CHAT_BASE_URL")
        if cloud_api_key and cloud_base_url:
            cloud_entry = {
                "name": os.getenv("SAGE_CHAT_MODEL", "qwen-turbo-2025-02-11"),
                "base_url": _normalize_base_url(cloud_base_url),
                "is_local": False,
                "description": "Cloud API (Configured in .env)",
                "api_key": cloud_api_key,
                "healthy": True,
            }
            _merge_model(cloud_entry)

        openai_api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            openai_entry = {
                "name": os.getenv("OPENAI_MODEL_NAME", "gpt-3.5-turbo"),
                "base_url": os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
                "is_local": False,
                "description": "OpenAI API (Configured in .env)",
                "api_key": openai_api_key,
                "healthy": True,
            }
            _merge_model(openai_entry)

        import concurrent.futures

        def evaluate_model(model: dict[str, Any]) -> dict[str, Any]:
            entry = dict(model)
            base = entry.get("base_url")
            headers = {}
            if entry.get("api_key"):
                headers["Authorization"] = f"Bearer {entry['api_key']}"

            if not base:
                entry["healthy"] = False
                return entry

            if not entry.get("is_local") and entry.get("healthy"):
                return entry

            entry["healthy"] = _probe_llm_endpoint(base, headers=headers)
            return entry

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            available_models = list(executor.map(evaluate_model, available_models))

        # Prefer healthy local models even if cloud env vars are present; fall back to any healthy.
        preferred_model: dict[str, Any] | None = None
        for model in available_models:
            if model.get("is_local") and model.get("healthy") and model.get("default"):
                preferred_model = model
                break
        if not preferred_model:
            for model in available_models:
                if model.get("is_local") and model.get("healthy"):
                    preferred_model = model
                    break
        if not preferred_model:
            for model in available_models:
                if model.get("healthy"):
                    preferred_model = model
                    break

        if preferred_model:
            status["base_url"] = preferred_model.get("base_url") or status.get("base_url")
            status["model_name"] = preferred_model.get("name") or status.get("model_name")
            status["is_local"] = preferred_model.get("is_local", status.get("is_local"))
            status["service_type"] = (
                "local_vllm" if preferred_model.get("is_local") else "remote_api"
            )
            if preferred_model.get("base_url"):
                status["running"] = True

        status_base_url = status.get("base_url")
        match_found = False
        for model in available_models:
            if _base_urls_match(status_base_url, model.get("base_url")):
                status["healthy"] = model.get("healthy", False)
                status["model_name"] = model.get("name", status["model_name"])
                match_found = True
                break

        if not match_found and status["model_name"] and status.get("base_url"):
            available_models.insert(
                0,
                {
                    "name": status["model_name"],
                    "base_url": status["base_url"],
                    "is_local": status["is_local"],
                    "description": "Current Model",
                    "healthy": status["healthy"],
                },
            )

        status["available_models"] = available_models

        return status

    except Exception as e:
        return {
            "running": False,
            "healthy": False,
            "service_type": "error",
            "error": str(e),
        }


# ==================== Dataset Management APIs ====================


@app.get("/api/datasets/sources")
async def list_dataset_sources():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
    try:
        from sage.data import DataManager

        manager = DataManager.get_instance()
        sources = manager.list_sources()

        result = []
        for source_name in sources:
            metadata = manager.get_source_metadata(source_name)
            result.append(
                {
                    "name": metadata.name,
                    "description": metadata.description,
                    "type": metadata.type,
                    "format": metadata.format,
                    "size": metadata.size,
                    "license": metadata.license,
                    "version": metadata.version,
                    "maintainer": metadata.maintainer,
                    "tags": metadata.tags,
                }
            )

        return {"sources": result, "count": len(result)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load datasets: {str(e)}")


@app.get("/api/datasets/usages")
async def list_dataset_usages():
    """åˆ—å‡ºæ‰€æœ‰ç”¨é€”é…ç½®"""
    try:
        from sage.data import DataManager

        manager = DataManager.get_instance()
        usages = manager.list_usages()

        result = []
        for usage_name in usages:
            try:
                profile = manager.get_by_usage(usage_name)
                result.append(
                    {
                        "name": usage_name,
                        "description": profile.description,
                        "datasets": profile.list_datasets(),
                    }
                )
            except Exception as e:
                result.append(
                    {
                        "name": usage_name,
                        "description": f"Error: {str(e)}",
                        "datasets": [],
                    }
                )

        return {"usages": result, "count": len(result)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load usages: {str(e)}")


@app.get("/api/datasets/sources/{source_name}")
async def get_dataset_source(source_name: str):
    """è·å–ç‰¹å®šæ•°æ®æºçš„è¯¦ç»†ä¿¡æ¯"""
    try:
        from sage.data import DataManager

        manager = DataManager.get_instance()

        if source_name not in manager.list_sources():
            raise HTTPException(status_code=404, detail=f"Dataset '{source_name}' not found")

        metadata = manager.get_source_metadata(source_name)

        # Find which usages include this source
        usages_with_source = []
        for usage_name in manager.list_usages():
            try:
                profile = manager.get_by_usage(usage_name)
                if source_name in [profile.datasets.get(k) for k in profile.datasets]:
                    usages_with_source.append(usage_name)
            except Exception:
                pass

        return {
            "name": metadata.name,
            "description": metadata.description,
            "type": metadata.type,
            "format": metadata.format,
            "size": metadata.size,
            "license": metadata.license,
            "version": metadata.version,
            "maintainer": metadata.maintainer,
            "tags": metadata.tags,
            "used_in": usages_with_source,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load dataset: {str(e)}")


@app.get("/api/datasets/usages/{usage_name}")
async def get_dataset_usage(usage_name: str):
    """è·å–ç‰¹å®šç”¨é€”é…ç½®çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        from sage.data import DataManager

        manager = DataManager.get_instance()

        if usage_name not in manager.list_usages():
            raise HTTPException(status_code=404, detail=f"Usage '{usage_name}' not found")

        profile = manager.get_by_usage(usage_name)

        # Get metadata for each dataset in this usage
        datasets_info = []
        for ds_name, source_name in profile.datasets.items():
            try:
                metadata = manager.get_source_metadata(source_name)
                datasets_info.append(
                    {
                        "alias": ds_name,
                        "source": source_name,
                        "description": metadata.description,
                        "type": metadata.type,
                    }
                )
            except Exception:
                datasets_info.append(
                    {
                        "alias": ds_name,
                        "source": source_name,
                        "description": "N/A",
                        "type": "unknown",
                    }
                )

        return {
            "name": usage_name,
            "description": profile.description,
            "datasets": datasets_info,
            "dataset_count": len(datasets_info),
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load usage: {str(e)}")


@app.post("/api/datasets/test/{source_name}")
async def test_dataset_source(source_name: str):
    """æµ‹è¯•åŠ è½½æ•°æ®æº"""
    try:
        from sage.data import DataManager

        manager = DataManager.get_instance()

        if source_name not in manager.list_sources():
            raise HTTPException(status_code=404, detail=f"Dataset '{source_name}' not found")

        # Try to load the dataset
        loader = manager.get_by_source(source_name)

        return {
            "success": True,
            "source": source_name,
            "loader_type": type(loader).__name__,
            "message": f"Successfully loaded {source_name}",
        }
    except HTTPException:
        raise
    except Exception as e:
        return {
            "success": False,
            "source": source_name,
            "error": str(e),
            "message": f"Failed to load {source_name}: {str(e)}",
        }


if __name__ == "__main__":
    # NOTE: åç«¯ API å·²åˆå¹¶åˆ° Gatewayï¼Œæ¨èé€šè¿‡ sage gateway start å¯åŠ¨ã€‚
    # æ­¤å¤„ä¿ç•™ç”¨äºç‹¬ç«‹è°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨ Gatewayã€‚
    uvicorn.run(app, host="0.0.0.0", port=SagePorts.GATEWAY_DEFAULT)
