{
  "architectures": {
    "llama2": {
      "description": "Meta's LLaMA 2 architecture (July 2023)",
      "message_format": "inst",
      "system_prefix": "<<SYS>>\n",
      "system_suffix": "\n<</SYS>>\n\n",
      "user_prefix": "[INST] ",
      "user_suffix": " [/INST]",
      "assistant_prefix": "",
      "assistant_suffix": "",
      "tool_format": "pythonic",
      "tool_prefix": "<|python_tag|>",
      "patterns": ["llama-2", "llama2", "codellama"]
    },
    "llama4": {
      "description": "Meta's LLaMA 4 multimodal MoE architecture (April 2025)",
      "message_format": "llama3_header",
      "system_prefix": "<|start_header_id|>system<|end_header_id|>\n\n",
      "system_suffix": "<|eot_id|>",
      "user_prefix": "<|start_header_id|>user<|end_header_id|>\n\n",
      "user_suffix": "<|eot_id|>",
      "assistant_prefix": "<|start_header_id|>assistant<|end_header_id|>\n\n",
      "assistant_suffix": "<|eot_id|>",
      "tool_format": "native",
      "patterns": ["llama-4", "llama4", "meta-llama-4"]
    },
    "llama3_3": {
      "description": "Meta's LLaMA 3.3 architecture (December 2024)",
      "message_format": "llama3_header",
      "system_prefix": "<|start_header_id|>system<|end_header_id|>\n\n",
      "system_suffix": "<|eot_id|>",
      "user_prefix": "<|start_header_id|>user<|end_header_id|>\n\n",
      "user_suffix": "<|eot_id|>",
      "assistant_prefix": "<|start_header_id|>assistant<|end_header_id|>\n\n",
      "assistant_suffix": "<|eot_id|>",
      "tool_format": "prompted",
      "patterns": ["llama-3.3", "llama3.3", "meta-llama-3.3"]
    },
    "llama3_2": {
      "description": "Meta's LLaMA 3.2 multimodal architecture (September 2024)",
      "message_format": "llama3_header",
      "system_prefix": "<|start_header_id|>system<|end_header_id|>\n\n",
      "system_suffix": "<|eot_id|>",
      "user_prefix": "<|start_header_id|>user<|end_header_id|>\n\n",
      "user_suffix": "<|eot_id|>",
      "assistant_prefix": "<|start_header_id|>assistant<|end_header_id|>\n\n",
      "assistant_suffix": "<|eot_id|>",
      "tool_format": "prompted",
      "patterns": ["llama-3.2", "llama3.2", "meta-llama-3.2"]
    },
    "llama3_1": {
      "description": "Meta's LLaMA 3.1 architecture (July 2024)",
      "message_format": "llama3_header",
      "system_prefix": "<|start_header_id|>system<|end_header_id|>\n\n",
      "system_suffix": "<|eot_id|>",
      "user_prefix": "<|start_header_id|>user<|end_header_id|>\n\n",
      "user_suffix": "<|eot_id|>",
      "assistant_prefix": "<|start_header_id|>assistant<|end_header_id|>\n\n",
      "assistant_suffix": "<|eot_id|>",
      "tool_format": "native",
      "tool_prefix": "<|start_header_id|>ipython<|end_header_id|>\n\n",
      "patterns": ["llama-3.1", "llama3.1", "meta-llama-3.1"]
    },
    "llama3": {
      "description": "Meta's LLaMA 3 architecture (April 2024)",
      "message_format": "llama3_header",
      "system_prefix": "<|start_header_id|>system<|end_header_id|>\n\n",
      "system_suffix": "<|eot_id|>",
      "user_prefix": "<|start_header_id|>user<|end_header_id|>\n\n",
      "user_suffix": "<|eot_id|>",
      "assistant_prefix": "<|start_header_id|>assistant<|end_header_id|>\n\n",
      "assistant_suffix": "<|eot_id|>",
      "tool_format": "prompted",
      "patterns": ["llama-3.0", "llama3.0", "llama-3-8b", "llama-3-70b", "llama3-8b", "llama3-70b"]
    },
    "qwen3_next": {
      "description": "Alibaba's Qwen3-Next hybrid MoE architecture (September 2025)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "special_token",
      "tool_prefix": "<|tool_call|>",
      "patterns": ["qwen3-next"]
    },
    "qwen3_vl": {
      "description": "Alibaba's Qwen3-VL multimodal architecture (May 2025)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "native",
      "patterns": ["qwen3-vl-2b", "qwen3-vl-4b", "qwen3-vl-8b"]
    },
    "qwen3_vl_moe": {
      "description": "Alibaba's Qwen3-VL MoE multimodal architecture (May 2025)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "native",
      "patterns": ["qwen3-vl-30b-a3b", "qwen3-vl-235b-a22b"]
    },
      "qwen3_moe": {
        "description": "Alibaba's Qwen3 MoE architecture (April 2025)",
        "message_format": "im_start_end",
        "system_prefix": "<|im_start|>system\n",
        "system_suffix": "<|im_end|>\n",
        "user_prefix": "<|im_start|>user\n",
        "user_suffix": "<|im_end|>\n",
        "assistant_prefix": "<|im_start|>assistant\n",
        "assistant_suffix": "<|im_end|>\n",
        "tool_format": "special_token",
        "tool_prefix": "<|tool_call|>",
        "patterns": ["qwen3-30b-a3b", "qwen3-235b-a22b", "qwen3:30b-a3b", "qwen3-coder:30b", "qwen/qwen3-coder-30b"]
      },
    "qwen2_5": {
      "description": "Alibaba's Qwen2.5 architecture (September 2024)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "special_token",
      "tool_prefix": "<|tool_call|>",
      "patterns": ["qwen2.5", "qwen-2.5"]
    },
    "qwen2_vl": {
      "description": "Alibaba's Qwen2-VL multimodal architecture (September 2024)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "prompted",
      "patterns": ["qwen2-vl", "qwen2.5-vl"]
    },
      "qwen3": {
        "description": "Alibaba's Qwen3 architecture (April 2025)",
        "message_format": "im_start_end",
        "system_prefix": "<|im_start|>system\n",
        "system_suffix": "<|im_end|>\n",
        "user_prefix": "<|im_start|>user\n",
        "user_suffix": "<|im_end|>\n",
        "assistant_prefix": "<|im_start|>assistant\n",
        "assistant_suffix": "<|im_end|>\n",
        "tool_format": "special_token",
        "tool_prefix": "<|tool_call|>",
        "patterns": ["qwen3-0.6b", "qwen3-1.7b", "qwen3-4b", "qwen3-8b", "qwen3-14b", "qwen3-32b", "qwen3:", "qwen3-4b-thinking", "qwen3-4b-instruct"]
      },
    "qwen2": {
      "description": "Alibaba's Qwen2 architecture (June 2024)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "special_token",
      "tool_prefix": "<|tool_call|>",
      "patterns": ["qwen2.", "qwen2-"]
    },
    "mistral": {
      "description": "Mistral AI 7B base architecture",
      "message_format": "inst",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "[INST] ",
      "user_suffix": " [/INST]",
      "assistant_prefix": "",
      "assistant_suffix": "",
      "tool_format": "json",
      "patterns": ["mistral-7b"]
    },
    "mixtral": {
      "description": "Mistral AI Mixtral MoE architecture",
      "message_format": "inst",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "[INST] ",
      "user_suffix": " [/INST]",
      "assistant_prefix": "",
      "assistant_suffix": "",
      "tool_format": "native",
      "patterns": ["mixtral"]
    },
      "mistral_large": {
        "description": "Mistral AI Large models (Small, Medium, Large)",
        "message_format": "inst",
        "system_prefix": "",
        "system_suffix": "\n\n",
        "user_prefix": "[INST] ",
        "user_suffix": " [/INST]",
        "assistant_prefix": "",
        "assistant_suffix": "",
        "tool_format": "native",
        "patterns": ["mistral-large", "mistral-medium", "mistral-small", "mistralai/mistral-small"]
      },
    "codestral": {
      "description": "Mistral AI Codestral code-specialized architecture",
      "message_format": "inst",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "[INST] ",
      "user_suffix": " [/INST]",
      "assistant_prefix": "",
      "assistant_suffix": "",
      "tool_format": "native",
      "patterns": ["codestral"]
    },
    "mistral3": {
      "description": "Mistral AI Mistral 3 architecture with rope-scaling (December 2024)",
      "message_format": "inst",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "[INST] ",
      "user_suffix": " [/INST]",
      "assistant_prefix": "",
      "assistant_suffix": "",
      "tool_format": "native",
      "patterns": ["mistral-small-3", "devstral-small-2"]
    },
    "ministral3": {
      "description": "Mistral AI Ministral 3 architecture (December 2024)",
      "message_format": "inst",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "[INST] ",
      "user_suffix": " [/INST]",
      "assistant_prefix": "",
      "assistant_suffix": "",
      "tool_format": "native",
      "patterns": ["devstral-2-123b"]
    },
    "phi": {
      "description": "Microsoft's Phi architecture family",
      "message_format": "basic",
      "system_prefix": "System: ",
      "system_suffix": "\n\n",
      "user_prefix": "User: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "tool_format": "xml",
      "patterns": ["phi"]
    },
      "gemma3n": {
        "description": "Google's Gemma 3n device-optimized architecture (March 2025)",
        "message_format": "basic",
        "system_prefix": "",
        "system_suffix": "\n\n",
        "user_prefix": "Human: ",
        "user_suffix": "\n",
        "assistant_prefix": "Assistant: ",
        "assistant_suffix": "\n",
        "tool_format": "native",
        "patterns": ["gemma3n", "gemma3n:e4b", "gemma3n:e2b", "gemma-3n", "google/gemma-3n"]
      },
    "paligemma": {
      "description": "Google's PaliGemma vision-language architecture",
      "message_format": "basic",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "Human: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "tool_format": "none",
      "patterns": ["paligemma"]
    },
    "codegemma": {
      "description": "Google's CodeGemma code-specialized architecture",
      "message_format": "basic",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "Human: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "tool_format": "none",
      "patterns": ["codegemma"]
    },
      "gemma3": {
        "description": "Google's Gemma 3 multimodal architecture (March 2025)",
        "message_format": "basic",
        "system_prefix": "",
        "system_suffix": "\n\n",
        "user_prefix": "Human: ",
        "user_suffix": "\n",
        "assistant_prefix": "Assistant: ",
        "assistant_suffix": "\n",
        "tool_format": "native",
        "patterns": ["gemma3", "gemma-3", "gemma3:4b", "gemma3:270m", "gemma3:1b"]
      },
    "gemma2": {
      "description": "Google's Gemma 2 architecture (June 2024)",
      "message_format": "basic",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "Human: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "tool_format": "json",
      "patterns": ["gemma2-"]
    },
    "gemma": {
      "description": "Google's Gemma (v1) architecture",
      "message_format": "basic",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "Human: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "tool_format": "none",
      "patterns": ["gemma-2b", "gemma-7b"]
    },
    "glm4v_moe": {
      "description": "Zhipu AI's GLM-4.6V multimodal MoE architecture (May 2025)",
      "message_format": "glm_special_tokens",
      "system_prefix": "<|system|>\n",
      "system_suffix": "\n",
      "user_prefix": "<|user|>\n",
      "user_suffix": "\n",
      "assistant_prefix": "<|assistant|>\n",
      "assistant_suffix": "\n",
      "tool_format": "glm_xml",
      "tool_calling_format": "<tool_call>function_name\n<arg_key>key</arg_key>\n<arg_value>value</arg_value>\n</tool_call>",
      "output_wrappers": {
        "start": "<|begin_of_box|>",
        "end": "<|end_of_box|>"
      },
      "thinking_output_field": "reasoning_content",
      "thinking_tags": ["<think>", "</think>"],
      "thinking_control": "/nothink",
      "patterns": ["glm-4.6v", "glm4.6v", "zai-org/glm-4.6v", "glm-4.6v-flash", "glm-4.6v-fp8", "glm-4.6v-flash-fp8"]
    },
    "glm4_moe": {
      "description": "Zhipu AI's GLM-4.5+ MoE architecture (July 2025)",
      "message_format": "im_start_end",
      "system_prefix": "<|system|>\n",
      "system_suffix": "\n",
      "user_prefix": "<|user|>\n",
      "user_suffix": "\n",
      "assistant_prefix": "<|assistant|>\n",
      "assistant_suffix": "\n",
      "tool_format": "special_token",
      "tool_prefix": "<|tool_call|>",
      "patterns": ["glm-4.7", "glm-4.6", "glm-4.5", "glm-4.5-air"]
    },
    "glm4v": {
      "description": "Zhipu AI's GLM-4V multimodal architecture (June 2024)",
      "message_format": "glm_special_tokens",
      "system_prefix": "<|system|>\n",
      "system_suffix": "\n",
      "user_prefix": "<|user|>\n",
      "user_suffix": "\n",
      "assistant_prefix": "<|assistant|>\n",
      "assistant_suffix": "\n",
      "tool_format": "json",
      "patterns": ["glm-4v", "glm4v", "glyph", "zai-org/glyph", "glm-4.1v"]
    },
    "glm4": {
      "description": "Zhipu AI's GLM-4 architecture (June 2024)",
      "message_format": "im_start_end",
      "system_prefix": "<|system|>\n",
      "system_suffix": "\n",
      "user_prefix": "<|user|>\n",
      "user_suffix": "\n",
      "assistant_prefix": "<|assistant|>\n",
      "assistant_suffix": "\n",
      "tool_format": "json",
      "patterns": ["glm-4", "chatglm4"]
    },
      "granite": {
        "description": "IBM's Granite architecture family",
        "message_format": "special_tokens",
        "system_prefix": "<|system|>\n",
        "system_suffix": "\n",
        "user_prefix": "<|user|>\n",
        "user_suffix": "\n",
        "assistant_prefix": "<|assistant|>\n",
        "assistant_suffix": "\n",
        "tool_format": "json",
        "tool_prefix": "<|tool_call|>",
        "patterns": ["granite", "granite3.3:2b"]
      },
      "granitemoehybrid": {
        "description": "IBM's Granite 4.0 hybrid MoE architecture with Mamba2 (October 2025)",
        "message_format": "special_tokens",
        "system_prefix": "<|system|>\n",
        "system_suffix": "\n",
        "user_prefix": "<|user|>\n",
        "user_suffix": "\n",
        "assistant_prefix": "<|assistant|>\n",
        "assistant_suffix": "\n",
        "tool_format": "native",
        "tool_prefix": "<|tool_call|>",
        "patterns": ["granite-4.0-h", "granite-h-tiny", "granite-h-micro"]
      },
    "deepseek": {
      "description": "DeepSeek architecture family",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "json",
      "patterns": ["deepseek"]
    },
    "minimax_m2_1": {
      "description": "MiniMax M2.1 enhanced MoE architecture with improved coding and agentic capabilities (December 2024)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "native",
      "thinking_format": "<think>...</think>",
      "patterns": ["minimax-m2.1", "minimaxai/minimax-m2.1", "minimax/minimax-m2.1"]
    },
    "minimax_m2": {
      "description": "MiniMax M2 MoE architecture with interleaved thinking (January 2025)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "native",
      "thinking_format": "<think>...</think>",
      "patterns": ["minimax-m2", "minimaxai/minimax-m2", "minimax/minimax-m2"]
    },
    "seed_oss": {
      "description": "ByteDance's Seed-OSS long-context architecture (August 2025)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "prompted",
      "patterns": ["seed-oss"]
    },
    "yi": {
      "description": "01.AI's Yi architecture family",
      "message_format": "basic",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "Human: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "tool_format": "json",
      "patterns": ["yi-"]
    },
    "claude": {
      "description": "Anthropic's Claude (for Bedrock/vertex compatibility)",
      "message_format": "human_assistant",
      "system_prefix": "",
      "system_suffix": "\n\n",
      "user_prefix": "Human: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "default_tool_support": "native",
      "tool_format": "xml",
      "patterns": ["claude", "claude-3", "claude-4", "claude-4.1", "claude-4.5"]
    },
    "gpt": {
      "description": "OpenAI GPT architecture (for reference)",
      "message_format": "openai_chat",
      "tool_format": "openai_functions",
      "patterns": ["gpt", "chatgpt", "gpt-5"]
    },
    "gpt_oss": {
      "description": "OpenAI GPT-OSS open-weight MoE architecture with harmony format (August 2025)",
      "message_format": "harmony",
      "system_prefix": "<|system|>\n",
      "system_suffix": "\n",
      "user_prefix": "<|user|>\n",
      "user_suffix": "\n",
      "assistant_prefix": "<|assistant|>\n",
      "assistant_suffix": "\n",
      "tool_format": "native",
      "reasoning_levels": ["low", "medium", "high"],
      "patterns": ["gpt-oss", "openai/gpt-oss"]
    },
    "nemotron_hybrid_moe": {
      "description": "NVIDIA Nemotron-3-Nano hybrid MoE architecture with Mamba-2 and Attention layers (December 2025)",
      "message_format": "im_start_end",
      "system_prefix": "<|im_start|>system\n",
      "system_suffix": "<|im_end|>\n",
      "user_prefix": "<|im_start|>user\n",
      "user_suffix": "<|im_end|>\n",
      "assistant_prefix": "<|im_start|>assistant\n",
      "assistant_suffix": "<|im_end|>\n",
      "tool_format": "json",
      "reasoning_support": true,
      "patterns": ["nemotron-3-nano", "nemotron-nano", "nvidia/nemotron"]
    },
    "generic": {
      "description": "Generic/unknown architecture fallback",
      "message_format": "basic",
      "system_prefix": "System: ",
      "system_suffix": "\n\n",
      "user_prefix": "Human: ",
      "user_suffix": "\n",
      "assistant_prefix": "Assistant: ",
      "assistant_suffix": "\n",
      "tool_format": "json",
      "patterns": []
    }
  },
  "message_formats": {
    "inst": "Instruction format with [INST] tags",
    "im_start_end": "ChatML format with <|im_start|> and <|im_end|>",
    "special_tokens": "Uses role-specific special tokens",
    "basic": "Simple role: content format",
    "human_assistant": "Human/Assistant format",
    "openai_chat": "OpenAI chat completion format",
    "llama3_header": "LLaMA 3+ format with <|start_header_id|> and <|eot_id|>",
    "glm_special_tokens": "GLM format with <|system|>, <|user|>, <|assistant|> tokens",
    "harmony": "OpenAI harmony response format for GPT-OSS models"
  },
  "tool_formats": {
    "pythonic": "Python function call syntax: [func(arg=val)]",
    "json": "JSON object: {\"name\": \"func\", \"parameters\": {...}}",
    "xml": "XML wrapped: <tool>...</tool>",
    "special_token": "Special token format: <|tool_call|>{...}",
    "native": "Native API support (OpenAI/Anthropic)",
    "openai_functions": "OpenAI function calling API format",
    "prompted": "Tool use through careful prompting",
    "none": "No tool support"
  }
}
