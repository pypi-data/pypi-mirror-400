# Embedding Source Explanation

## How to Tell Which Embeddings You're Getting

SOMA supports **3 different embedding strategies**. Here's how to identify which one is being used:

---

## 1. ğŸ”¢ Feature-Based (Default) - **SOMA Math-Based**

**Source:** Pure SOMA mathematical features  
**Uses Pretrained Models:** âŒ NO

### How It Works:
1. Extracts features from SOMA tokens:
   - UIDs (64-bit unique identifiers)
   - Frontend digits (1-9)
   - Backend numbers (64-bit hashes)
   - Content IDs
   - Global IDs
   - Neighbor context (prev/next UIDs)
   - Token index
   - Stream type

2. Projects these features to embedding dimension using a learned projection matrix

3. Normalizes to unit vector

### Characteristics:
- âœ… **Deterministic** - Same token â†’ same embedding
- âœ… **No external dependencies** - Works without any ML libraries
- âœ… **Fast** - ~100K tokens/second
- âœ… **Pure SOMA** - 100% from SOMA's math
- âŒ **No semantic meaning** - Purely mathematical

### How to Use:
```python
# Default strategy is feature_based
embedding_gen = SOMAEmbeddingGenerator(
    strategy="feature_based",  # Default
    embedding_dim=768
)
```

### In Frontend:
- Strategy selector shows: "ğŸ”¢ Feature-Based (SOMA Math)"
- Badge shows: "ğŸ”¢ SOMA Math-Based"
- Info box says: "No pretrained models used - Pure mathematical transformation"

---

## 2. ğŸ¤– Hybrid - **Pretrained Model + SOMA**

**Source:** Combines sentence-transformers (pretrained) + SOMA features  
**Uses Pretrained Models:** âœ… YES (sentence-transformers)

### How It Works:
1. Gets text embedding from sentence-transformers (pretrained model)
2. Gets feature embedding from SOMA's mathematical features
3. Combines with weights (default: 70% text, 30% features)
4. Normalizes to unit vector

### Characteristics:
- âœ… **Semantic meaning** - From pretrained text embeddings
- âœ… **Preserves SOMA features** - Still includes mathematical properties
- âš ï¸ **Requires dependencies** - Needs sentence-transformers
- âš ï¸ **Slower** - ~10K tokens/second
- âš ï¸ **Less deterministic** - Depends on pretrained model

### How to Use:
```python
embedding_gen = SOMAEmbeddingGenerator(
    strategy="hybrid",
    embedding_dim=768,
    text_model="sentence-transformers/all-MiniLM-L6-v2"
)
```

### In Frontend:
- Strategy selector shows: "ğŸ¤– Hybrid (Text + Math)"
- Badge shows: "ğŸ¤– Hybrid (Text + Math)"
- Info box says: "Uses pretrained model: sentence-transformers"

---

## 3. ğŸ” Hash-Based - **Fast Hash**

**Source:** Cryptographic hash of SOMA features  
**Uses Pretrained Models:** âŒ NO

### How It Works:
1. Creates hash string from all SOMA features
2. Uses SHA-256 to generate fixed-size hash
3. Converts hash bytes to embedding vector
4. Normalizes to unit vector

### Characteristics:
- âœ… **Extremely fast** - ~200K tokens/second
- âœ… **Deterministic** - Same token â†’ same embedding
- âœ… **No dependencies** - Pure Python
- âŒ **No semantic meaning** - Just hash
- âŒ **Poor similarity properties** - Hash collisions possible

### How to Use:
```python
embedding_gen = SOMAEmbeddingGenerator(
    strategy="hash",
    embedding_dim=768
)
```

### In Frontend:
- Strategy selector shows: "ğŸ” Hash-Based"
- Badge shows: "ğŸ” Hash-Based"
- Info box says: "Fast, deterministic, no pretrained models"

---

## Quick Reference

| Strategy | Source | Pretrained Models? | Speed | Semantic? |
|----------|--------|-------------------|-------|-----------|
| **Feature-Based** | SOMA Math | âŒ NO | Fast | âŒ No |
| **Hybrid** | Text + Math | âœ… YES | Slow | âœ… Yes |
| **Hash-Based** | Hash | âŒ NO | Very Fast | âŒ No |

---

## Default Behavior

**Default strategy is `feature_based`** - This means:
- âœ… You get **SOMA's math-based embeddings** by default
- âœ… **No pretrained models** are used
- âœ… Works **without any ML dependencies**

If you want pretrained model embeddings, you must explicitly choose "Hybrid" strategy.

---

## How to Check in Code

```python
# Check which strategy is being used
print(f"Strategy: {embedding_gen.strategy}")

if embedding_gen.strategy == "feature_based":
    print("âœ… Using SOMA math-based logic")
    print("âŒ No pretrained models")
elif embedding_gen.strategy == "hybrid":
    print("âš ï¸ Using pretrained sentence-transformers")
    print("âœ… Also includes SOMA features")
elif embedding_gen.strategy == "hash":
    print("âœ… Using hash-based (no pretrained models)")
```

---

## Summary

**By default, embeddings come from SOMA's math-based logic (feature-based).**

To use pretrained models, you must:
1. Install: `pip install sentence-transformers`
2. Select "Hybrid" strategy in UI or code
3. Then embeddings will combine pretrained text embeddings with SOMA features

