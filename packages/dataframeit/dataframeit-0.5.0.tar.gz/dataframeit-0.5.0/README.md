# DataFrameIt

Uma biblioteca Python para enriquecer DataFrames com an√°lises de texto usando Modelos de Linguagem (LLMs).

## Descri√ß√£o

DataFrameIt √© uma ferramenta que permite processar textos contidos em um DataFrame e extrair informa√ß√µes estruturadas usando LLMs. A biblioteca usa **LangChain** para suportar m√∫ltiplos provedores de modelos (Gemini, OpenAI, Anthropic, etc.). Pandas √© utilizado para manipula√ß√£o de dados, com suporte para Polars via convers√£o interna.

## Funcionalidades

- Processar cada linha de um DataFrame que contenha textos
- Utilizar prompt templates para an√°lise espec√≠fica de dom√≠nio
- Extrair informa√ß√µes estruturadas usando modelos Pydantic
- **M√∫ltiplos providers**: Gemini, OpenAI, Anthropic, Cohere, Mistral, etc. via LangChain
- **M√∫ltiplos tipos de dados**: DataFrames, Series, listas e dicion√°rios
- Suporte para Polars e Pandas
- Processamento incremental com resumo autom√°tico
- **Processamento paralelo** com auto-redu√ß√£o de workers em rate limits
- **Retry autom√°tico** com backoff exponencial para resili√™ncia
- **Rate limiting** configur√°vel para respeitar limites de APIs
- **Rastreamento de erros** com coluna autom√°tica `_error_details`
- **Tracking de tokens** e m√©tricas de throughput (RPM, TPM)

## Instala√ß√£o

```bash
# Com Google Gemini (provider padr√£o, recomendado)
pip install dataframeit[google]

# Ou com outros providers
pip install dataframeit[openai]     # GPT-4, GPT-4o
pip install dataframeit[anthropic]  # Claude

# Com Polars (opcional)
pip install dataframeit[google,polars]

# Tudo inclu√≠do
pip install dataframeit[all]
```

## Uso B√°sico

### Com LangChain (comportamento padr√£o)

```python
from pydantic import BaseModel, Field
from typing import Literal
import pandas as pd
from dataframeit import dataframeit

# Defina um modelo Pydantic para estruturar as respostas
class SuaClasse(BaseModel):
    campo1: str = Field(..., description="Descri√ß√£o do campo 1")
    campo2: Literal['opcao1', 'opcao2'] = Field(..., description="Descri√ß√£o do campo 2")

# Defina seu template de prompt
TEMPLATE = "Classifique o texto conforme as categorias definidas."

# Carregue seus dados (a coluna de texto deve se chamar 'texto' por padr√£o)
df = pd.read_excel('seu_arquivo.xlsx')

# Processe os dados (usa LangChain por padr√£o)
df_resultado = dataframeit(df, SuaClasse, TEMPLATE)

# Salve o resultado
df_resultado.to_excel('resultado.xlsx', index=False)
```

### Com OpenAI (via LangChain)

```python
from dataframeit import dataframeit

# Uso b√°sico com OpenAI
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    provider='openai',
    model='gpt-4o-mini'
)

# Com par√¢metros extras (model_kwargs)
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    provider='openai',
    model='gpt-4o-mini',
    model_kwargs={
        'temperature': 0.5,          # Controle de criatividade
        'reasoning_effort': 'medium', # Para modelos com reasoning (o1, o3)
    }
)
```

### Par√¢metros Extras (model_kwargs)

O par√¢metro `model_kwargs` permite passar configura√ß√µes espec√≠ficas do provider para o LangChain:

```python
# OpenAI com reasoning (modelos o1, o3-mini)
df_resultado = dataframeit(
    df, Model, TEMPLATE,
    provider='openai',
    model='o3-mini',
    model_kwargs={
        'reasoning_effort': 'high',  # 'low', 'medium', 'high'
    }
)

# Google Gemini com configura√ß√µes extras
df_resultado = dataframeit(
    df, Model, TEMPLATE,
    provider='google_genai',
    model='gemini-3.0-flash',
    model_kwargs={
        'temperature': 0.2,
        'top_p': 0.9,
    }
)

# Anthropic Claude com configura√ß√µes extras
df_resultado = dataframeit(
    df, Model, TEMPLATE,
    provider='anthropic',
    model='claude-3-5-sonnet-20241022',
    model_kwargs={
        'max_tokens': 4096,
    }
)
```

> **Nota**: Os par√¢metros dispon√≠veis em `model_kwargs` dependem do provider. Consulte a documenta√ß√£o do LangChain para cada provider.

## Tipos de Dados Suportados

Al√©m de DataFrames, o DataFrameIt aceita outros tipos de dados para facilitar o uso em diferentes cen√°rios.

### Com Lista de Textos

```python
from dataframeit import dataframeit

textos = [
    "√ìtimo produto! Chegou r√°pido.",
    "P√©ssimo atendimento.",
    "Produto ok, nada de especial."
]

# N√£o precisa especificar text_column para listas
resultado = dataframeit(textos, SuaClasse, TEMPLATE)

# Retorna DataFrame com √≠ndice num√©rico + colunas extra√≠das
#   | sentimento | confianca
# 0 | positivo   | alta
# 1 | negativo   | alta
# 2 | neutro     | media
```

### Com Dicion√°rio

```python
documentos = {
    'doc_001': 'Texto do primeiro documento...',
    'doc_002': 'Texto do segundo documento...',
    'doc_003': 'Texto do terceiro documento...',
}

resultado = dataframeit(documentos, SuaClasse, TEMPLATE)

# Retorna DataFrame com chaves como √≠ndice
#         | sentimento | confianca
# doc_001 | positivo   | alta
# doc_002 | negativo   | media
# doc_003 | neutro     | baixa
```

### Com pandas.Series

```python
import pandas as pd

series = pd.Series(
    ['Texto A', 'Texto B', 'Texto C'],
    index=['review_1', 'review_2', 'review_3'],
    name='avaliacoes'
)

resultado = dataframeit(series, SuaClasse, TEMPLATE)

# Retorna DataFrame preservando o √≠ndice original
#          | sentimento | confianca
# review_1 | positivo   | alta
# review_2 | negativo   | media
# review_3 | neutro     | baixa
```

### Resumo dos Tipos

| Tipo de Entrada | `text_column` | Tipo de Retorno |
|-----------------|---------------|-----------------|
| `pd.DataFrame` | Obrigat√≥rio (padr√£o: `'texto'`) | `pd.DataFrame` com colunas originais + extra√≠das |
| `pl.DataFrame` | Obrigat√≥rio (padr√£o: `'texto'`) | `pl.DataFrame` com colunas originais + extra√≠das |
| `list` | Autom√°tico | `pd.DataFrame` (√≠ndice num√©rico) |
| `dict` | Autom√°tico | `pd.DataFrame` (chaves como √≠ndice) |
| `pd.Series` | Autom√°tico | `pd.DataFrame` (√≠ndice preservado) |
| `pl.Series` | Autom√°tico | `pl.DataFrame` |

## Como Funciona o Template

O template define as instru√ß√µes para o LLM analisar cada texto. Basta escrever suas instru√ß√µes:

```python
TEMPLATE = "Classifique o sentimento do texto."
```

O texto de cada linha do DataFrame ser√° adicionado automaticamente ao final do prompt.

### Controlando a Posi√ß√£o do Texto (Opcional)

Se preferir controlar onde o texto aparece no prompt, use `{texto}`:

```python
TEMPLATE = """
Voc√™ √© um analista especializado.

Documento:
{texto}

Extraia as informa√ß√µes solicitadas do documento acima.
"""
```

## Par√¢metros

### Par√¢metros Gerais
- **`data`**: Dados contendo os textos. Aceita:
  - `pandas.DataFrame` ou `polars.DataFrame`
  - `pandas.Series` ou `polars.Series`
  - `list` (lista de strings)
  - `dict` (dicion√°rio onde valores s√£o os textos)
- **`questions`**: Modelo Pydantic definindo a estrutura dos dados a extrair
- **`prompt`**: Template do prompt (use `{texto}` para controlar onde o texto aparece)
- **`text_column`**: Nome da coluna com os textos (obrigat√≥rio para DataFrames, padr√£o: `'texto'`; autom√°tico para Series/list/dict)

### Par√¢metros de Processamento
- **`resume=True`**: Continua processamento de onde parou (√∫til para grandes datasets)
- **`status_column=None`**: Nome customizado para coluna de status (padr√£o: `_dataframeit_status`)

### Par√¢metros de Resili√™ncia
- **`max_retries=3`**: N√∫mero m√°ximo de tentativas em caso de erro
- **`base_delay=1.0`**: Delay inicial em segundos para retry (cresce exponencialmente)
- **`max_delay=30.0`**: Delay m√°ximo em segundos entre tentativas
- **`rate_limit_delay=0.0`**: Delay em segundos entre requisi√ß√µes para evitar rate limits

### Par√¢metros de Paralelismo
- **`parallel_requests=1`**: N√∫mero de requisi√ß√µes paralelas (1 = sequencial)
  - Ao detectar erro 429 (rate limit), reduz automaticamente pela metade
  - M√©tricas de throughput (RPM, TPM) s√£o exibidas automaticamente

### Par√¢metros de Monitoramento
- **`track_tokens=True`**: Rastreia uso de tokens e exibe estat√≠sticas ao final (requer LangChain 1.0+)

### Par√¢metros do Modelo
- **`model='gemini-3.0-flash'`**: Modelo a ser usado
- **`provider='google_genai'`**: Provider do LangChain ('google_genai', 'openai', 'anthropic', etc.)
- **`api_key=None`**: Chave API espec√≠fica (opcional, usa vari√°veis de ambiente se None)
- **`model_kwargs=None`**: Par√¢metros extras para o modelo (ex: `temperature`, `reasoning_effort`)

## Tratamento de Erros

O DataFrameIt possui um sistema robusto de tratamento de erros:

### Colunas de Status
- **`_dataframeit_status`**: Coluna autom√°tica com status de cada linha
  - `'processed'`: Linha processada com sucesso
  - `'error'`: Linha falhou ap√≥s todas as tentativas
  - `None/NaN`: Linha ainda n√£o processada

- **`_error_details`**: Coluna autom√°tica com detalhes de erros
  - Cont√©m mensagem de erro quando status √© `'error'`
  - `None/NaN` quando processamento foi bem-sucedido

### Exemplo: Verificando Erros

```python
df_resultado = dataframeit(df, SuaClasse, TEMPLATE)

# Verificar linhas com erro
linhas_com_erro = df_resultado[df_resultado['_dataframeit_status'] == 'error']
print(f"Total de erros: {len(linhas_com_erro)}")

# Ver detalhes dos erros
for idx, row in linhas_com_erro.iterrows():
    print(f"Linha {idx}: {row['_error_details']}")

# Salvar apenas linhas processadas com sucesso
df_sucesso = df_resultado[df_resultado['_dataframeit_status'] == 'processed']
df_sucesso.to_excel('resultado_limpo.xlsx', index=False)
```

### Sistema de Retry

O DataFrameIt tenta automaticamente processar linhas com falha usando backoff exponencial:

```python
# Configurando retry mais agressivo
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    max_retries=5,        # Tentar at√© 5 vezes
    base_delay=2.0,       # Come√ßar com 2 segundos de espera
    max_delay=60.0        # Esperar no m√°ximo 60 segundos entre tentativas
)
```

A espera entre tentativas cresce exponencialmente: 2s ‚Üí 4s ‚Üí 8s ‚Üí 16s ‚Üí 32s (limitado a 60s).

## Rate Limiting

O DataFrameIt oferece controle proativo de rate limiting para evitar atingir limites de requisi√ß√µes das APIs.

### Por que usar Rate Limiting?

- **Prevenir erros**: Evita atingir limites de requisi√ß√µes antes de acontecer
- **Efici√™ncia**: Reduz desperd√≠cio de retries em datasets grandes
- **Economia**: Algumas APIs cobram por tentativa, mesmo as que falham
- **Complementar ao Retry**: O rate limiting PREVINE erros, o retry TRATA erros

### Quando usar?

Use `rate_limit_delay` quando:
- Processar datasets grandes (> 100 linhas)
- Conhecer os limites da API (ex: 60 req/min)
- Fazer processamento em lote
- Quiser economizar retries para erros reais

### Exemplos Pr√°ticos

```python
# Google Gemini: 60 requisi√ß√µes por minuto (free tier)
# Solu√ß√£o: 1 requisi√ß√£o por segundo = 60 req/min
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    rate_limit_delay=1.0
)

# OpenAI GPT-4: limite de 500 req/min (tier 1)
# Solu√ß√£o: ~0.15 segundos entre requisi√ß√µes = ~400 req/min (margem de seguran√ßa)
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    provider='openai',
    model='gpt-4o-mini',
    rate_limit_delay=0.15
)

# Anthropic Claude: limite de 50 req/min (free tier)
# Solu√ß√£o: 1.2 segundos entre requisi√ß√µes = 50 req/min
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    provider='anthropic',
    model='claude-3-5-sonnet-20241022',
    rate_limit_delay=1.2
)

# Dataset pequeno: n√£o precisa de rate limiting
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    rate_limit_delay=0.0  # Padr√£o: sem delay
)
```

### Como calcular o delay ideal?

```
rate_limit_delay = 60 / limite_de_requisi√ß√µes_por_minuto

Exemplos:
- 60 req/min  ‚Üí delay = 60/60  = 1.0 segundo
- 500 req/min ‚Üí delay = 60/500 = 0.12 segundos
- 50 req/min  ‚Üí delay = 60/50  = 1.2 segundos
```

### Rate Limiting + Retry: Dupla Prote√ß√£o

```python
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    # Rate limiting proativo
    rate_limit_delay=1.0,      # Previne rate limits

    # Retry reativo
    max_retries=3,             # Trata erros inesperados
    base_delay=2.0,
    max_delay=30.0
)
```

## Processamento Paralelo

O DataFrameIt suporta processamento paralelo para acelerar o processamento de grandes datasets.

### Par√¢metro `parallel_requests`

```python
# Processar com 5 requisi√ß√µes paralelas
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    parallel_requests=5     # N√∫mero de workers paralelos
)
```

### M√©tricas de Throughput

O DataFrameIt exibe automaticamente m√©tricas detalhadas ao final do processamento:

```
============================================================
ESTATISTICAS DE USO
============================================================
Modelo: gemini-2.5-flash
Total de tokens: 15,432
  - Input:  12,345 tokens
  - Output: 3,087 tokens
------------------------------------------------------------
METRICAS DE THROUGHPUT
------------------------------------------------------------
Tempo total: 45.2s
Workers paralelos: 5
Requisicoes: 100
  - RPM (req/min): 132.7
  - TPM (tokens/min): 20,478
============================================================
```

Use essas m√©tricas para calibrar o n√∫mero ideal de `parallel_requests` para sua conta/tier.

### Auto-redu√ß√£o de Workers em Rate Limits

Quando um erro de rate limit (429) √© detectado, o DataFrameIt **reduz automaticamente** o n√∫mero de workers pela metade:

```python
# Come√ßa com 10 workers
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    parallel_requests=10
)

# Se detectar rate limit:
# - Workers s√£o reduzidos: 10 ‚Üí 5 ‚Üí 2 ‚Üí 1
# - Voc√™ ver√° um aviso: "Rate limit detectado! Reduzindo workers de 10 para 5."
# - Ao final, as estat√≠sticas mostram a redu√ß√£o
```

**Importante**: Os workers s√£o apenas **reduzidos**, nunca aumentados automaticamente. Isso evita custos inesperados para o usu√°rio.

### Combinando com Rate Limit Delay

Para m√°xima estabilidade, combine `parallel_requests` com `rate_limit_delay`:

```python
# 5 workers paralelos, com 0.5s entre cada requisi√ß√£o por worker
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    parallel_requests=5,
    rate_limit_delay=0.5     # Delay adicional por requisi√ß√£o
)
```

### Dicas de Uso

| Cen√°rio | Configura√ß√£o Recomendada |
|---------|-------------------------|
| Dataset pequeno (< 50 linhas) | `parallel_requests=1` (padr√£o) |
| Dataset m√©dio (50-500 linhas) | `parallel_requests=3` a `5` |
| Dataset grande (> 500 linhas) | `parallel_requests=5` a `10` |
| API com limite baixo | `parallel_requests=2` + `rate_limit_delay=1.0` |
| Tier pago com limites altos | `parallel_requests=10` ou mais |

## Processamento Incremental

Para grandes datasets, use `resume=True` para continuar de onde parou:

```python
# Primeira execu√ß√£o (processa 100 linhas e falha)
df_resultado = dataframeit(df, SuaClasse, TEMPLATE, resume=True)
df_resultado.to_excel('resultado_parcial.xlsx', index=False)

# Segunda execu√ß√£o (continua das linhas n√£o processadas)
df = pd.read_excel('resultado_parcial.xlsx')
df_resultado = dataframeit(df, SuaClasse, TEMPLATE, resume=True)
df_resultado.to_excel('resultado_completo.xlsx', index=False)
```

## Tracking de Tokens e Custos

O DataFrameIt pode rastrear automaticamente o uso de tokens para monitoramento de custos (dispon√≠vel com LangChain 1.0+).

### Como Usar

```python
# Habilitar tracking de tokens
df_resultado = dataframeit(
    df,
    SuaClasse,
    TEMPLATE,
    track_tokens=True  # Habilita tracking de tokens
)

# Ao final do processamento, exibe estat√≠sticas:
# ============================================================
# üìä ESTAT√çSTICAS DE USO DE TOKENS
# ============================================================
# Modelo: gemini-3.0-flash
# Total de tokens: 15,432
#   ‚Ä¢ Input:  12,345 tokens
#   ‚Ä¢ Output: 3,087 tokens
# ============================================================
```

### Colunas Adicionadas ao DataFrame

Quando `track_tokens=True`, o DataFrame incluir√° automaticamente:

- **`_input_tokens`**: Tokens de entrada (prompt) por linha
- **`_output_tokens`**: Tokens de sa√≠da (resposta) por linha
- **`_total_tokens`**: Total de tokens por linha

### Analisando Custos

```python
# Processar com tracking
df_resultado = dataframeit(df, SuaClasse, TEMPLATE, track_tokens=True)

# Analisar uso por linha
print(f"Linha mais cara: {df_resultado['_total_tokens'].max()} tokens")
print(f"M√©dia de tokens: {df_resultado['_total_tokens'].mean():.1f} tokens")

# Calcular custo estimado (exemplo: Gemini 3.0 Flash)
# Input: $0.075 por 1M tokens, Output: $0.30 por 1M tokens
custo_input = df_resultado['_input_tokens'].sum() * 0.075 / 1_000_000
custo_output = df_resultado['_output_tokens'].sum() * 0.30 / 1_000_000
custo_total = custo_input + custo_output
print(f"Custo estimado: ${custo_total:.4f}")
```

### Compatibilidade

- ‚úÖ **LangChain 1.0+** com `usage_metadata` (Gemini, Claude, GPT via LangChain)
- ‚ö†Ô∏è Vers√µes anteriores do LangChain n√£o incluem `usage_metadata`

## Exemplo Completo

Veja o diret√≥rio `example/` para um caso de uso completo com an√°lise de decis√µes judiciais, incluindo:
- Modelo Pydantic complexo com classes aninhadas
- Template detalhado com instru√ß√µes espec√≠ficas de dom√≠nio
- Uso de campos opcionais e tipos Literal
- Processamento de listas e tuplas

## Configura√ß√£o de Vari√°veis de Ambiente

### Para OpenAI
```bash
export OPENAI_API_KEY="sua-chave-openai"
```

### Para Google Gemini (LangChain)
```bash
export GOOGLE_API_KEY="sua-chave-google"
```

### Para Anthropic Claude (LangChain)
```bash
export ANTHROPIC_API_KEY="sua-chave-anthropic"
```

## Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Este √© um projeto em desenvolvimento inicial.

## Licen√ßa

Veja o arquivo LICENSE para detalhes.
