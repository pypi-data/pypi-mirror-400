{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training a Flow Matching Model for Weather Prediction\n",
        "\n",
        "This notebook demonstrates how to train a WeatherFlowMatch model for weather prediction using ERA5 data. We'll cover:\n",
        "\n",
        "1. Setting up the model architecture\n",
        "2. Configuring an effective training pipeline\n",
        "3. Incorporating physics constraints\n",
        "4. Implementing monitoring and visualization during training\n",
        "5. Evaluating model performance\n",
        "6. Saving and loading trained models\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add repository root to Python path to find weatherflow package\n",
        "import sys\n",
        "import os\n",
        "# Get absolute path to repo root\n",
        "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "repo_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
        "# Add to path if not already there\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "print(f\"Added {repo_root} to Python path\")\n",
        "\n",
        "# Add repository root to Python path\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
        "\n",
        "# Install WeatherFlow if needed\n",
        "try:\n",
        "    import weatherflow\n",
        "    print(f\"WeatherFlow version: {weatherflow.__version__}\")\n",
        "except ImportError:\n",
        "    !pip install -e ..\n",
        "    import weatherflow\n",
        "    print(f\"WeatherFlow installed, version: {weatherflow.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Mock dependencies that might not be available",
        "try:",
        "    import sys",
        "    notebook_dir = os.path.dirname(os.path.abspath('__file__'))",
        "    repo_root = os.path.abspath(os.path.join(notebook_dir, '..'))",
        "    mock_path = os.path.join(repo_root, 'mock_dependencies.py')",
        "    ",
        "    if os.path.exists(mock_path):",
        "        # Execute the mock dependencies script",
        "        with open(mock_path, 'r') as f:",
        "            mock_code = f.read()",
        "            # Add repo_root to sys.path if not already there",
        "            if repo_root not in sys.path:",
        "                sys.path.insert(0, repo_root)",
        "            # Execute the script",
        "            exec(mock_code)",
        "            # Call the function to install all mocks",
        "            exec(\"install_all_mocks()\")",
        "    else:",
        "        print(f\"Warning: Mock dependencies script not found at {mock_path}\")",
        "except Exception as e:",
        "    print(f\"Error loading mock dependencies: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress some warnings for cleaner output\n",
        "\n",
        "# Import WeatherFlow components\n",
        "from weatherflow.data import ERA5Dataset, create_data_loaders\n",
        "from weatherflow.models import WeatherFlowMatch, WeatherFlowODE\n",
        "from weatherflow.utils import WeatherVisualizer\n",
        "from weatherflow.training import FlowTrainer\n",
        "\n",
        "# Set up matplotlib\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Let's define our configuration parameters for the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define experiment configuration\n",
        "config = {\n",
        "    # Data parameters\n",
        "    \"variables\": ['z', 't', 'u', 'v'],  # Geopotential, temperature, u-wind, v-wind\n",
        "    \"pressure_levels\": [500],  # 500 hPa level\n",
        "    \"train_years\": ('2015', '2016'),  # Training period\n",
        "    \"val_years\": ('2017', '2017'),  # Validation period\n",
        "    \"batch_size\": 16,\n",
        "    \"num_workers\": 4,\n",
        "    \"normalize\": True,  # Apply normalization\n",
        "    \n",
        "    # Model parameters\n",
        "    \"hidden_dim\": 128,\n",
        "    \"n_layers\": 4,\n",
        "    \"use_attention\": True,\n",
        "    \"physics_informed\": True,\n",
        "    \n",
        "    # Training parameters\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"max_epochs\": 20,  # Reduced for demonstration\n",
        "    \"early_stopping_patience\": 5,\n",
        "    \"use_amp\": True,  # Use mixed precision training if available\n",
        "    \n",
        "    # Experiment tracking\n",
        "    \"exp_name\": f\"flow_match_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "    \"save_dir\": \"../models\",\n",
        "    \"save_checkpoint_freq\": 5,\n",
        "    \n",
        "    # Visualization\n",
        "    \"plot_freq\": 5,\n",
        "    \"vis_dir\": \"../visualizations\"\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(config[\"save_dir\"], exist_ok=True)\n",
        "os.makedirs(config[\"vis_dir\"], exist_ok=True)\n",
        "exp_vis_dir = os.path.join(config[\"vis_dir\"], config[\"exp_name\"])\n",
        "os.makedirs(exp_vis_dir, exist_ok=True)\n",
        "\n",
        "# Save config for reproducibility\n",
        "with open(os.path.join(config[\"save_dir\"], f\"{config['exp_name']}_config.json\"), 'w') as f:\n",
        "    json.dump(config, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Prepare Data\n",
        "\n",
        "Now let's load our ERA5 data for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading ERA5 data for training and validation...\")\n",
        "\n",
        "# Load training and validation data\n",
        "train_loader, val_loader = create_data_loaders(\n",
        "    variables=config[\"variables\"],\n",
        "    pressure_levels=config[\"pressure_levels\"],\n",
        "    train_slice=config[\"train_years\"],\n",
        "    val_slice=config[\"val_years\"],\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    num_workers=config[\"num_workers\"],\n",
        "    normalize=config[\"normalize\"]\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "\n",
        "# Examine the data structure by looking at a batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(\"\\nSample batch structure:\")\n",
        "for key, value in sample_batch.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  - {key}: {type(value)}\")\n",
        "        for subkey, subval in value.items():\n",
        "            print(f\"      {subkey}: {type(subval)}\")\n",
        "    else:\n",
        "        print(f\"  - {key}: {type(value)}, shape: {value.shape}\")\n",
        "\n",
        "# Extract grid size from the data\n",
        "input_channels = sample_batch['input'].shape[1]\n",
        "grid_size = sample_batch['input'].shape[2:]  # (lat, lon)\n",
        "print(f\"\\nInput channels: {input_channels}\")\n",
        "print(f\"Grid size: {grid_size}\")\n",
        "\n",
        "# Visualize a sample from the training data\n",
        "print(\"\\nVisualizing a sample from the training data:\")\n",
        "visualizer = WeatherVisualizer()\n",
        "\n",
        "# Extract the first sample for visualization\n",
        "input_data = sample_batch['input'][0].cpu()  # First batch item\n",
        "target_data = sample_batch['target'][0].cpu()\n",
        "var_names = config[\"variables\"]\n",
        "\n",
        "# Visualize each variable\n",
        "for i, var_name in enumerate(var_names):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    # Plot input state (current weather)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(input_data[i, 0].numpy(), cmap=visualizer.VAR_CMAPS.get(var_name, 'viridis'))\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"{var_name} - Current\")\n",
        "    \n",
        "    # Plot target state (future weather)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(target_data[i, 0].numpy(), cmap=visualizer.VAR_CMAPS.get(var_name, 'viridis'))\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"{var_name} - Future (6h later)\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define the Model\n",
        "\n",
        "Let's define our WeatherFlowMatch model architecture. Here, we'll use a configuration that includes attention mechanisms and physics-informed constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the WeatherFlowMatch model\n",
        "model = WeatherFlowMatch(\n",
        "    input_channels=input_channels,\n",
        "    hidden_dim=config['hidden_dim'],\n",
        "    n_layers=config['n_layers'],\n",
        "    use_attention=config['use_attention'],\n",
        "    physics_informed=config['physics_informed']\n",
        ")\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Setup\n",
        "\n",
        "Now, we'll set up our training components, including the optimizer, learning rate scheduler, and loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['learning_rate'],\n",
        "    weight_decay=config['weight_decay']\n",
        ")\n",
        "\n",
        "# Use a ReduceLROnPlateau learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    factor=0.5,  # Reduce LR by a factor of 0.5\n",
        "    patience=3,   # Reduce LR after 3 epochs with no improvement\n",
        "    verbose=True  # Print messages when LR is updated\n",
        ")\n",
        "\n",
        "# Define the loss function (MSE loss)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = FlowTrainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    scheduler=scheduler,\n",
        "    config=config,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Loop\n",
        "\n",
        "Let's start the training loop and monitor the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizations and Monitoring\n",
        "\n",
        "During training, we can create visualizations to monitor the model's progress.  For example, we can plot the loss over time or visualize model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training history\n",
        "history_path = os.path.join(config['save_dir'], f\"{config['exp_name']}_history.json\")\n",
        "with open(history_path, 'r') as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history['train_loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(exp_vis_dir, 'loss_curves.png'))\n",
        "plt.show()\n",
        "\n",
        "# Load a sample from the validation set\n",
        "sample_batch = next(iter(val_loader))\n",
        "x0, x1 = sample_batch['input'].to(device), sample_batch['target'].to(device)\n",
        "var_names = config['variables']\n",
        "\n",
        "# Generate predictions with the trained model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Create ODE solver\n",
        "    ode_model = WeatherFlowODE(\n",
        "        flow_model=model,\n",
        "        solver_method='dopri5',\n",
        "        rtol=1e-4,\n",
        "        atol=1e-4\n",
        "    )\n",
        "\n",
        "    t_eval = torch.tensor([1.0], device=device)  # Predict at t=1\n",
        "    predictions = ode_model(x0, t_eval)\n",
        "    y_pred = predictions[0].cpu()\n",
        "    x1 = x1.cpu()\n",
        "\n",
        "# Plotting predictions vs ground truth\n",
        "num_samples = min(4, x0.shape[0])  # Number of samples to visualize\n",
        "fig, axes = plt.subplots(num_samples, len(var_names) * 2, figsize=(20, 5 * num_samples))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    for j, var_name in enumerate(var_names):\n",
        "        # Ground truth plot\n",
        "        ax = axes[i, j*2]\n",
        "        im = ax.imshow(x1[i, j, 0].numpy(), cmap=visualizer.VAR_CMAPS.get(var_name, 'viridis'))\n",
        "        ax.set_title(f'{var_name} - Ground Truth (6h later)')\n",
        "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "        \n",
        "        # Prediction plot\n",
        "        ax = axes[i, j*2 + 1]\n",
        "        im = ax.imshow(y_pred[i, j, 0].numpy(), cmap=visualizer.VAR_CMAPS.get(var_name, 'viridis'))\n",
        "        ax.set_title(f'{var_name} - Prediction (6h later)')\n",
        "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(exp_vis_dir, 'predictions_vs_ground_truth.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Animate Predictions\n",
        "\n",
        "We can also create animations of the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating animation...\")\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "# Load a sample from the validation set\n",
        "sample_batch = next(iter(val_loader))\n",
        "x0 = sample_batch['input'][:1].to(device) # Single sample for animation\n",
        "\n",
        "# Define lead times for the animation\n",
        "lead_times = torch.linspace(0, 1.0, 100, device=device)\n",
        "\n",
        "# Generate predictions at multiple lead times\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Create ODE solver\n",
        "    ode_model = WeatherFlowODE(\n",
        "        flow_model=model,\n",
        "        solver_method='dopri5',\n",
        "        rtol=1e-4,\n",
        "        atol=1e-4\n",
        "    )\n",
        "    predictions = ode_model(x0, lead_times)\n",
        "\n",
        "# Animation function\n",
        "def animate(i):\n",
        "    imgs = []\n",
        "    for j, var_name in enumerate(config['variables']):\n",
        "        ax = axes[j]\n",
        "        ax.clear()\n",
        "        ax.set_title(f'{var_name} - Lead Time: {lead_times[i]:.2f}')\n",
        "        im = ax.imshow(predictions[i, 0, j, 0].cpu().numpy(), cmap=visualizer.VAR_CMAPS.get(var_name, 'viridis'), animated=True)\n",
        "        imgs.append([im])\n",
        "    return imgs\n",
        "\n",
        "# Create animation figure\n",
        "fig, axes = plt.subplots(1, len(config['variables']), figsize=(15, 5))\n",
        "fig.tight_layout()\n",
        "\n",
        "# Generate animation\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, animate, frames=len(lead_times), interval=100, blit=False, repeat=True\n",
        ")\n",
        "\n",
        "# Save animation (optional)\n",
        "animation_path = os.path.join(exp_vis_dir, 'weather_prediction_animation.gif')\n",
        "anim.save(animation_path, writer='imagemagick', fps=10)\n",
        "print(f\"Animation saved to {animation_path}\")\n",
        "\n",
        "# Display animation in the notebook\n",
        "from IPython.display import HTML\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 9. Calculate Evaluation Metrics\n",
        "\n",
        "print(\"Calculating evaluation metrics...\")\n",
        "\n",
        "# Function to calculate MSE, MAE, and bias for each variable\n",
        "def calculate_metrics(predictions, ground_truth, var_names):\n",
        "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Overall metrics\n",
        "    mse = ((predictions - ground_truth) ** 2).mean().item()\n",
        "    mae = torch.abs(predictions - ground_truth).mean().item()\n",
        "    bias = (predictions - ground_truth).mean().item()\n",
        "    \n",
        "    metrics['overall'] = {\n",
        "        'mse': mse,\n",
        "        'rmse': np.sqrt(mse),\n",
        "        'mae': mae,\n",
        "        'bias': bias\n",
        "    }\n",
        "    \n",
        "    # Per-variable metrics\n",
        "    for i, var in enumerate(var_names):\n",
        "        var_pred = predictions[:, i]\n",
        "        var_truth = ground_truth[:, i]\n",
        "        \n",
        "        var_mse = ((var_pred - var_truth) ** 2).mean().item()\n",
        "        var_mae = torch.abs(var_pred - var_truth).mean().item()\n",
        "        var_bias = (var_pred - var_truth).mean().item()\n",
        "        \n",
        "        metrics[var] = {\n",
        "            'mse': var_mse,\n",
        "            'rmse': np.sqrt(var_mse),\n",
        "            'mae': var_mae,\n",
        "            'bias': var_bias\n",
        "        }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Evaluate on the whole validation set\n",
        "def evaluate_model(model, val_loader, device, config):\n",
        "    \"\"\"Evaluate model on the validation set.\"\"\"\n",
        "    # Create ODE solver\n",
        "    ode_model = WeatherFlowODE(\n",
        "        flow_model=model,\n",
        "        solver_method='dopri5',\n",
        "        rtol=1e-4,\n",
        "        atol=1e-4\n",
        "    )\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Initialize metrics\n",
        "    all_metrics = []\n",
        "    \n",
        "    # Evaluate on batches\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
        "            # Get data\n",
        "            x0, x1 = batch['input'].to(device), batch['target'].to(device)\n",
        "            \n",
        "            # Generate predictions\n",
        "            t_eval = torch.tensor([1.0], device=device)  # Predict at t=1\n",
        "            predictions = ode_model(x0, t_eval)\n",
        "            \n",
        "            # Extract predictions at t=1\n",
        "            y_pred = predictions[0]  # Shape: [batch_size, channels, lat, lon]\n",
        "            \n",
        "            # Calculate metrics for this batch\n",
        "            batch_metrics = calculate_metrics(y_pred.flatten(1), x1.flatten(1), config['variables'])\n",
        "            all_metrics.append(batch_metrics)\n",
        "            \n",
        "            # Only process a subset of batches for speed if dataset is large\n",
        "            if batch_idx >= 10:\n",
        "                break\n",
        "    \n",
        "    # Aggregate metrics across batches\n",
        "    aggregated_metrics = {\n",
        "        'overall': {metric: np.mean([b['overall'][metric] for b in all_metrics]) \n",
        "                   for metric in ['mse', 'rmse', 'mae', 'bias']}\n",
        "    }\n",
        "    \n",
        "    for var in config['variables']:\n",
        "        aggregated_metrics[var] = {\n",
        "            metric: np.mean([b[var][metric] for b in all_metrics]) \n",
        "            for metric in ['mse', 'rmse', 'mae', 'bias']\n",
        "        }\n",
        "    \n",
        "    return aggregated_metrics\n",
        "\n",
        "# Run evaluation\n",
        "eval_metrics = evaluate_model(model, val_loader, device, config)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"Overall: RMSE = {eval_metrics['overall']['rmse']:.4f}, MAE = {eval_metrics['overall']['mae']:.4f}\")\n",
        "\n",
        "print(\"\\nPer-Variable Metrics:\")\n",
        "for var in config['variables']:\n",
        "    print(f\"{var}: RMSE = {eval_metrics[var]['rmse']:.4f}, MAE = {eval_metrics[var]['mae']:.4f}, Bias = {eval_metrics[var]['bias']:.4f}\")\n",
        "\n",
        "# Save metrics to file\n",
        "metrics_path = os.path.join(config['save_dir'], f\"{config['exp_name']}_metrics.json\")\n",
        "with open(metrics_path, 'w') as f:\n",
        "    # Convert numpy values to Python types for JSON serialization\n",
        "    json_metrics = {}\n",
        "    for key, val in eval_metrics.items():\n",
        "        json_metrics[key] = {k: float(v) for k, v in val.items()}\n",
        "    \n",
        "    json.dump(json_metrics, f, indent=2)\n",
        "\n",
        "print(f\"\\nMetrics saved to {metrics_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 10. Compare with Baseline Predictions\n",
        "\n",
        "print(\"\\nComparing with climatology and persistence baselines...\")\n",
        "\n",
        "# Function to create baseline predictions\n",
        "def create_baseline_predictions(val_loader, type='persistence'):\n",
        "    \"\"\"Create baseline predictions.\n",
        "    \n",
        "    Args:\n",
        "        val_loader: Validation data loader\n",
        "        type: Type of baseline ('persistence' or 'climatology')\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    all_metrics = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Evaluating {type} baseline\"):\n",
        "            # Get data\n",
        "            x0, x1 = batch['input'], batch['target']\n",
        "            \n",
        "            if type == 'persistence':\n",
        "                # Persistence baseline: use current state as prediction\n",
        "                y_pred = x0\n",
        "            elif type == 'climatology':\n",
        "                # Climatology baseline: use mean of training data\n",
        "                # For simplicity, we'll use zeros (assuming normalized data)\n",
        "                y_pred = torch.zeros_like(x1)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown baseline type: {type}\")\n",
        "            \n",
        "            # Calculate metrics\n",
        "            batch_metrics = calculate_metrics(y_pred.flatten(1), x1.flatten(1), config['variables'])\n",
        "            all_metrics.append(batch_metrics)\n",
        "            \n",
        "            # Only process a subset of batches for speed if dataset is large\n",
        "            if len(all_metrics) >= 10:\n",
        "                break\n",
        "    \n",
        "    # Aggregate metrics\n",
        "    aggregated_metrics = {\n",
        "        'overall': {metric: np.mean([b['overall'][metric] for b in all_metrics]) \n",
        "                   for metric in ['mse', 'rmse', 'mae', 'bias']}\n",
        "    }\n",
        "    \n",
        "    for var in config['variables']:\n",
        "        aggregated_metrics[var] = {\n",
        "            metric: np.mean([b[var][metric] for b in all_metrics]) \n",
        "            for metric in ['mse', 'rmse', 'mae', 'bias']\n",
        "        }\n",
        "    \n",
        "    return aggregated_metrics\n",
        "\n",
        "# Calculate baseline metrics\n",
        "persistence_metrics = create_baseline_predictions(val_loader, type='persistence')\n",
        "climatology_metrics = create_baseline_predictions(val_loader, type='climatology')\n",
        "\n",
        "# Print comparison results\n",
        "print(\"\\nModel vs Baseline RMSE Comparison:\")\n",
        "print(f\"{{'Variable':<10}} {{'Model':<10}} {{'Persistence':<15}} {{'Climatology':<15}} {{'Improvement %':<15}}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for var in config['variables']:\n",
        "    model_rmse = eval_metrics[var]['rmse']\n",
        "    persistence_rmse = persistence_metrics[var]['rmse']\n",
        "    climatology_rmse = climatology_metrics[[var]['rmse']\n",
        "    \n",
        "    # Calculate improvement over persistence\n",
        "    improvement = (persistence_rmse - model_rmse) / persistence_rmse * 100\n",
        "    \n",
        "    print(f\"{var:<10} {model_rmse:<10.4f} {persistence_rmse:<15.4f} {climatology_rmse:<15.4f} {improvement:<15.2f}\")\n",
        "\n",
        "# Create comparison plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Set up variables and metrics for plotting\n",
        "vars_for_plot = config['variables']\n",
        "model_rmse = [eval_metrics[var]['rmse'] for var in vars_for_plot]\n",
        "persistence_rmse = [persistence_metrics[var]['rmse'] for var in vars_for_plot]\n",
        "climatology_rmse = [climatology_metrics[var]['rmse'] for var in vars_for_plot]\n",
        "\n",
        "# Create bar positions\n",
        "x = np.arange(len(vars_for_plot))\n",
        "width = 0.25\n",
        "\n",
        "# Plot bars\n",
        "plt.bar(x - width, model_rmse, width, label='Flow Matching Model')\n",
        "plt.bar(x, persistence_rmse, width, label='Persistence Baseline')\n",
        "plt.bar(x + width, climatology_rmse, width, label='Climatology Baseline')\n",
        "\n",
        "# Customize plot\n",
        "plt.xlabel('Variable')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('Model vs Baseline RMSE Comparison')\n",
        "plt.xticks(x, vars_for_plot)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Save plot\n",
        "plt.savefig(os.path.join(exp_vis_dir, 'baseline_comparison.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 11. Model Usage Example\n",
        "\n",
        "print(\"\\nDemonstrating how to use the trained model for prediction...\")\n",
        "\n",
        "# Function to demonstrate model usage\n",
        "def demonstrate_model_usage(model, input_data, device):\n",
        "    \"\"\"Show how to use the trained model for prediction.\"\"\"\n",
        "    # Create ODE solver with the trained model\n",
        "    ode_model = WeatherFlowODE(\n",
        "        flow_model=model,\n",
        "        solver_method='dopri5',  # Options: 'euler', 'midpoint', 'rk4', 'dopri5'\n",
        "        rtol=1e-4,\n",
        "        atol=1e-4\n",
        "    )\n",
        "    \n",
        "    # Move data to device\n",
        "    x0 = input_data.to(device)\n",
        "    \n",
        "    # Define multiple lead times (in fractions of the prediction interval)\n",
        "    # 0.0 = current state, 1.0 = full prediction interval (e.g., 6 hours)\n",
        "    lead_times = torch.linspace(0, 1.0, 5, device=device)\n",
        "    \n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        predictions = ode_model(x0, lead_times)\n",
        "    \n",
        "    return predictions, lead_times\n",
        "\n",
        "# Get a sample from validation data\n",
        "sample_batch = next(iter(val_loader))\n",
        "input_data = sample_batch['input'][:1]  # Just use first sample\n",
        "\n",
        "# Run demonstration\n",
        "print(\"Generating predictions at multiple lead times...\")\n",
        "predictions, lead_times = demonstrate_model_usage(model, input_data, device)\n",
        "\n",
        "print(f\"Generated predictions with shape: {predictions.shape}\")\n",
        "print(f\"Lead times: {lead_times.cpu().numpy()}\")\n",
        "\n",
        "# Show code example\n",
        "print(\"\"\"\n",
        "## Example Code for Using the Trained Model\n",
        "\n",
        "```python\n",
        "from weatherflow.models import WeatherFlowMatch, WeatherFlowODE\n",
        "import torch\n",
        "\n",
        "# Load model\n",
        "model = WeatherFlowMatch(\n",
        "    input_channels=4,\n",
        "    hidden_dim=128,\n",
        "    n_layers=4,\n",
        "    use_attention=True,\n",
        "    physics_informed=True\n",
        ")\n",
        "\n",
        "# Load weights\n",
        "checkpoint = torch.load('path/to/model_checkpoint.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Create ODE solver\n",
        "ode_model = WeatherFlowODE(\n",
        "    flow_model=model,\n",
        "    solver_method='dopri5',\n",
        "    rtol=1e-4,\n",
        "    atol=1e-4\n",
        ")\n",
        "\n",
        "# Prepare input data\n",
        "input_data = torch.tensor(your_input_data)\n",
        "\n",
        "# Define lead times\n",
        "lead_times = torch.linspace(0, 1.0, 5)\n",
        "\n",
        "# Generate predictions\n",
        "with torch.no_grad():\n",
        "    predictions = ode_model(input_data, lead_times)\n",
        "\n",
        "# predictions.shape: [n_lead_times, batch_size, channels, lat, lon]\n",
        "```\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 12. Conclusion\n",
        "\n",
        "print(\"\"\"\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we've trained a Flow Matching model for weather prediction:\n",
        "\n",
        "1. We set up the WeatherFlowMatch model with physics-informed constraints\n",
        "2. We trained the model on ERA5 data and monitored its progress\n",
        "3. We evaluated the model's performance and compared it with baselines\n",
        "4. We demonstrated how to use the trained model for prediction\n",
        "\n",
        "Key findings:\n",
        "- The model outperforms persistence and climatology baselines\n",
        "- Physics-informed constraints help maintain physical consistency\n",
        "- The flow matching approach allows for predictions at arbitrary lead times\n",
        "\n",
        "Next steps:\n",
        "- Experiment with different model architectures and hyperparameters\n",
        "- Train on longer time periods and more variables\n",
        "- Evaluate on additional test sets and metrics\n",
        "- Compare with other state-of-the-art weather prediction models\n",
        "\n",
        "For detailed evaluation on the WeatherBench2 benchmark, see the next notebook.\n",
        "\"\"\")\n",
        "\n",
        "# Save trained model for future use\n",
        "final_model_path = os.path.join(config['save_dir'], f\"{config['exp_name']}_final.pt\")\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': config,\n",
        "    'metrics': eval_metrics\n",
        "}, final_model_path)\n",
        "\n",
        "print(f\"Final model saved to {final_model_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}