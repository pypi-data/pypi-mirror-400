"""
Performance Benchmark Suite for OpenSSL Encrypt Library.

Provides comprehensive benchmarking for cryptographic operations,
performance regression detection, and timing analysis.
"""

import gc
import json
import os
import statistics
import tempfile
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, Tuple

from ..crypt_core import decrypt_file, encrypt_file
from .base_test import BaseSecurityTest, TestConfig, TestResult, TestResultLevel


@dataclass
class BenchmarkResult:
    """Represents the result of a performance benchmark."""

    operation: str
    algorithm: str
    file_size: int
    iterations: int
    total_time: float
    average_time: float
    min_time: float
    max_time: float
    std_dev: float
    throughput_mbps: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class BaselineData:
    """Represents baseline performance data for regression detection."""

    algorithm: str
    operation: str
    file_size: int
    baseline_throughput: float
    baseline_avg_time: float
    timestamp: str
    platform_info: Dict[str, Any] = field(default_factory=dict)


class PerformanceAnalyzer:
    """Analyzes performance data and detects regressions."""

    @staticmethod
    def calculate_throughput(data_size_bytes: int, time_seconds: float) -> float:
        """Calculate throughput in MB/s."""
        if time_seconds <= 0:
            return 0.0

        mb_size = data_size_bytes / (1024 * 1024)
        return mb_size / time_seconds

    @staticmethod
    def detect_regression(
        current_result: BenchmarkResult, baseline: BaselineData, regression_threshold: float = 20.0
    ) -> Tuple[bool, float]:
        """
        Detect performance regression compared to baseline.

        Args:
            current_result: Current benchmark result
            baseline: Baseline performance data
            regression_threshold: Threshold percentage for regression detection

        Returns:
            Tuple of (is_regression, percentage_change)
        """
        if baseline.baseline_throughput <= 0:
            return False, 0.0

        current_throughput = current_result.throughput_mbps
        baseline_throughput = baseline.baseline_throughput

        percentage_change = ((current_throughput - baseline_throughput) / baseline_throughput) * 100

        # Negative percentage change indicates performance degradation
        is_regression = percentage_change < -regression_threshold

        return is_regression, percentage_change

    @staticmethod
    def analyze_timing_consistency(timings: List[float]) -> Dict[str, Any]:
        """Analyze timing measurements for consistency and outliers."""
        if len(timings) < 2:
            return {"error": "Insufficient timing data"}

        mean_time = statistics.mean(timings)
        std_dev = statistics.stdev(timings)

        # Coefficient of variation
        cv = (std_dev / mean_time) * 100 if mean_time > 0 else float("inf")

        # Detect outliers using IQR method
        sorted_timings = sorted(timings)
        n = len(sorted_timings)

        if n >= 4:
            q1_idx = n // 4
            q3_idx = (3 * n) // 4
            q1 = sorted_timings[q1_idx]
            q3 = sorted_timings[q3_idx]
            iqr = q3 - q1

            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            outliers = [t for t in timings if t < lower_bound or t > upper_bound]
        else:
            outliers = []

        return {
            "mean": mean_time,
            "std_dev": std_dev,
            "coefficient_of_variation": cv,
            "outlier_count": len(outliers),
            "outlier_percentage": (len(outliers) / len(timings)) * 100,
            "timing_consistent": cv < 15.0,  # Less than 15% variation
            "performance_stable": len(outliers) < len(timings) * 0.2,  # Less than 20% outliers
        }


class BenchmarkTestSuite(BaseSecurityTest):
    """Performance benchmark test suite."""

    def __init__(self):
        super().__init__(
            "BenchmarkTestSuite",
            "Performance benchmarks and regression detection for cryptographic operations",
        )
        self.analyzer = PerformanceAnalyzer()
        self.temp_dir = None
        self.baselines = {}  # Store baseline performance data

    def setup_temp_directory(self) -> str:
        """Set up temporary directory for test files."""
        if not self.temp_dir:
            self.temp_dir = tempfile.mkdtemp(prefix="benchmark_test_")
        return self.temp_dir

    def cleanup_temp_directory(self) -> None:
        """Clean up temporary directory."""
        if self.temp_dir and os.path.exists(self.temp_dir):
            import shutil

            shutil.rmtree(self.temp_dir, ignore_errors=True)
            self.temp_dir = None

    def run_tests(self, **kwargs) -> List[TestResult]:
        """Run all benchmark tests."""
        self.clear_results()
        config = TestConfig(**kwargs)

        try:
            self.setup_temp_directory()

            # Load existing baselines if available
            self._load_baselines(config)

            # Run different categories of benchmarks
            self._benchmark_algorithms_by_file_size(config)
            self._benchmark_different_algorithms(config)
            self._benchmark_kdf_performance(config)
            self._benchmark_memory_usage(config)
            self._detect_performance_regressions(config)

            # Save updated baselines
            self._save_baselines(config)

        finally:
            self.cleanup_temp_directory()

        return self.get_results()

    def _measure_operation_performance(
        self, operation_func: Callable, iterations: int = 3
    ) -> List[float]:
        """Measure performance of an operation with multiple iterations."""
        timings = []

        for i in range(iterations):
            # Force garbage collection before measurement
            gc.collect()

            # Perform measurement
            start_time = time.perf_counter()
            try:
                operation_func()
                end_time = time.perf_counter()
                timings.append(end_time - start_time)
            except Exception as e:
                self.logger.warning(f"Operation failed during benchmark iteration {i}: {e}")
                # Don't include failed operations in timing
                continue

        return timings

    def _benchmark_algorithms_by_file_size(self, config: TestConfig) -> None:
        """Benchmark different algorithms with various file sizes."""

        file_sizes = [
            1024,  # 1 KB
            10 * 1024,  # 10 KB
            100 * 1024,  # 100 KB
            1024 * 1024,  # 1 MB
            5 * 1024 * 1024,  # 5 MB
        ]

        algorithms = config.get("algorithms", ["fernet", "aes-gcm", "chacha20-poly1305"])

        for algorithm in algorithms:
            for file_size in file_sizes:
                result = self.run_single_test(
                    self._benchmark_algorithm_file_size,
                    f"benchmark_{algorithm}_{file_size}_bytes",
                    algorithm,
                    file_size,
                    config,
                )
                self.add_result(result)

    def _benchmark_algorithm_file_size(
        self, algorithm: str, file_size: int, config: TestConfig
    ) -> TestResult:
        """Benchmark specific algorithm with specific file size."""
        try:
            # Create test file
            input_file = os.path.join(self.temp_dir, f"benchmark_{algorithm}_{file_size}.bin")
            encrypted_file = os.path.join(
                self.temp_dir, f"benchmark_{algorithm}_{file_size}_enc.bin"
            )
            decrypted_file = os.path.join(
                self.temp_dir, f"benchmark_{algorithm}_{file_size}_dec.bin"
            )

            # Generate test data
            test_data = os.urandom(file_size)
            with open(input_file, "wb") as f:
                f.write(test_data)

            config_dict = {
                "algorithm": algorithm,
                "hash_algorithm": config.get("hash_algorithm", "SHA256"),
                "kdf": config.get("kdf", "pbkdf2"),
            }

            password = "benchmark_password_123"
            iterations = config.get("benchmark_iterations", 5)

            # Benchmark encryption
            def encrypt_operation():
                if os.path.exists(encrypted_file):
                    os.remove(encrypted_file)
                encrypt_file(input_file, encrypted_file, password, hash_config=config_dict)

            encrypt_timings = self._measure_operation_performance(encrypt_operation, iterations)

            if not encrypt_timings:
                return TestResult(
                    f"benchmark_{algorithm}_{file_size}_bytes",
                    TestResultLevel.ERROR,
                    f"No successful encryption timings for {algorithm} with {file_size} bytes",
                )

            # Benchmark decryption
            def decrypt_operation():
                if os.path.exists(decrypted_file):
                    os.remove(decrypted_file)
                decrypt_file(encrypted_file, decrypted_file, password)

            decrypt_timings = self._measure_operation_performance(decrypt_operation, iterations)

            if not decrypt_timings:
                return TestResult(
                    f"benchmark_{algorithm}_{file_size}_bytes",
                    TestResultLevel.WARNING,
                    f"No successful decryption timings for {algorithm} with {file_size} bytes",
                )

            # Calculate benchmark results
            encrypt_result = self._calculate_benchmark_result(
                "encryption", algorithm, file_size, encrypt_timings
            )
            decrypt_result = self._calculate_benchmark_result(
                "decryption", algorithm, file_size, decrypt_timings
            )

            # Analyze consistency
            encrypt_analysis = self.analyzer.analyze_timing_consistency(encrypt_timings)
            decrypt_analysis = self.analyzer.analyze_timing_consistency(decrypt_timings)

            # Store results for regression detection
            baseline_key = f"{algorithm}_{file_size}_encrypt"
            if baseline_key not in self.baselines:
                self.baselines[baseline_key] = BaselineData(
                    algorithm=algorithm,
                    operation="encryption",
                    file_size=file_size,
                    baseline_throughput=encrypt_result.throughput_mbps,
                    baseline_avg_time=encrypt_result.average_time,
                    timestamp=datetime.now().isoformat(),
                )

            return TestResult(
                f"benchmark_{algorithm}_{file_size}_bytes",
                TestResultLevel.PASS,
                f"{algorithm} benchmark completed for {file_size} bytes",
                details={
                    "encryption": {
                        "avg_time": encrypt_result.average_time,
                        "throughput_mbps": encrypt_result.throughput_mbps,
                        "analysis": encrypt_analysis,
                    },
                    "decryption": {
                        "avg_time": decrypt_result.average_time,
                        "throughput_mbps": decrypt_result.throughput_mbps,
                        "analysis": decrypt_analysis,
                    },
                    "file_size": file_size,
                    "algorithm": algorithm,
                    "iterations": iterations,
                },
            )

        except Exception as e:
            return TestResult(
                f"benchmark_{algorithm}_{file_size}_bytes",
                TestResultLevel.ERROR,
                f"Exception during benchmark: {str(e)}",
                exception=e,
            )

    def _calculate_benchmark_result(
        self, operation: str, algorithm: str, file_size: int, timings: List[float]
    ) -> BenchmarkResult:
        """Calculate benchmark statistics from timing measurements."""
        if not timings:
            return BenchmarkResult(
                operation=operation,
                algorithm=algorithm,
                file_size=file_size,
                iterations=0,
                total_time=0.0,
                average_time=0.0,
                min_time=0.0,
                max_time=0.0,
                std_dev=0.0,
                throughput_mbps=0.0,
            )

        total_time = sum(timings)
        average_time = statistics.mean(timings)
        min_time = min(timings)
        max_time = max(timings)
        std_dev = statistics.stdev(timings) if len(timings) > 1 else 0.0

        # Calculate throughput based on average time
        throughput_mbps = self.analyzer.calculate_throughput(file_size, average_time)

        return BenchmarkResult(
            operation=operation,
            algorithm=algorithm,
            file_size=file_size,
            iterations=len(timings),
            total_time=total_time,
            average_time=average_time,
            min_time=min_time,
            max_time=max_time,
            std_dev=std_dev,
            throughput_mbps=throughput_mbps,
        )

    def _benchmark_different_algorithms(self, config: TestConfig) -> None:
        """Benchmark and compare different algorithms."""

        result = self.run_single_test(
            self._algorithm_comparison_benchmark, "algorithm_comparison_benchmark", config
        )
        self.add_result(result)

    def _algorithm_comparison_benchmark(self, config: TestConfig) -> TestResult:
        """Compare performance across different algorithms."""
        try:
            algorithms = ["fernet", "aes-gcm", "chacha20-poly1305"]
            file_size = 1024 * 1024  # 1 MB test file

            # Create test file
            input_file = os.path.join(self.temp_dir, "algorithm_comparison.bin")
            test_data = os.urandom(file_size)
            with open(input_file, "wb") as f:
                f.write(test_data)

            algorithm_results = {}
            password = "comparison_password_123"

            for algorithm in algorithms:
                encrypted_file = os.path.join(self.temp_dir, f"comparison_{algorithm}_enc.bin")
                decrypted_file = os.path.join(self.temp_dir, f"comparison_{algorithm}_dec.bin")

                config_dict = {"algorithm": algorithm, "hash_algorithm": "SHA256", "kdf": "pbkdf2"}

                try:
                    # Measure encryption
                    def encrypt_op():
                        if os.path.exists(encrypted_file):
                            os.remove(encrypted_file)
                        encrypt_file(input_file, encrypted_file, password, hash_config=config_dict)

                    encrypt_timings = self._measure_operation_performance(encrypt_op, 3)

                    if encrypt_timings:
                        avg_encrypt_time = statistics.mean(encrypt_timings)
                        encrypt_throughput = self.analyzer.calculate_throughput(
                            file_size, avg_encrypt_time
                        )

                        algorithm_results[algorithm] = {
                            "encrypt_time": avg_encrypt_time,
                            "encrypt_throughput": encrypt_throughput,
                            "success": True,
                        }
                    else:
                        algorithm_results[algorithm] = {"success": False}

                except Exception as e:
                    algorithm_results[algorithm] = {"success": False, "error": str(e)}

            # Analyze results
            successful_algorithms = [
                alg for alg, result in algorithm_results.items() if result.get("success", False)
            ]

            if len(successful_algorithms) < 2:
                return TestResult(
                    "algorithm_comparison_benchmark",
                    TestResultLevel.WARNING,
                    f"Only {len(successful_algorithms)} algorithms completed successfully",
                )

            # Find fastest and slowest algorithms
            throughputs = {
                alg: algorithm_results[alg]["encrypt_throughput"] for alg in successful_algorithms
            }

            fastest_alg = max(throughputs.keys(), key=lambda x: throughputs[x])
            slowest_alg = min(throughputs.keys(), key=lambda x: throughputs[x])

            performance_ratio = throughputs[fastest_alg] / throughputs[slowest_alg]

            return TestResult(
                "algorithm_comparison_benchmark",
                TestResultLevel.PASS,
                f"Algorithm comparison completed. Fastest: {fastest_alg}, Slowest: {slowest_alg}",
                details={
                    "algorithm_results": algorithm_results,
                    "fastest_algorithm": fastest_alg,
                    "slowest_algorithm": slowest_alg,
                    "performance_ratio": performance_ratio,
                    "file_size": file_size,
                    "successful_algorithms": successful_algorithms,
                },
            )

        except Exception as e:
            return TestResult(
                "algorithm_comparison_benchmark",
                TestResultLevel.ERROR,
                f"Exception during algorithm comparison: {str(e)}",
                exception=e,
            )

    def _benchmark_kdf_performance(self, config: TestConfig) -> None:
        """Benchmark KDF performance with different parameters."""

        result = self.run_single_test(
            self._kdf_performance_test, "kdf_performance_benchmark", config
        )
        self.add_result(result)

    def _kdf_performance_test(self, config: TestConfig) -> TestResult:
        """Test KDF performance with various parameters."""
        try:
            # Test different KDF configurations
            kdf_configs = [
                {"kdf": "pbkdf2", "pbkdf2_iterations": 100000},
                {"kdf": "pbkdf2", "pbkdf2_iterations": 200000},
                {"kdf": "argon2", "argon2_time_cost": 2, "argon2_memory_cost": 65536},
                {"kdf": "balloon", "balloon_space_cost": 16, "balloon_time_cost": 20},
            ]

            # Create small test file for KDF benchmarking
            input_file = os.path.join(self.temp_dir, "kdf_test.txt")
            with open(input_file, "w") as f:
                f.write("KDF performance test data")

            kdf_results = {}
            password = "kdf_benchmark_password"

            for i, kdf_config in enumerate(kdf_configs):
                encrypted_file = os.path.join(self.temp_dir, f"kdf_{i}_enc.bin")

                config_dict = {"algorithm": "fernet", "hash_algorithm": "SHA256", **kdf_config}

                try:

                    def kdf_encrypt_op():
                        if os.path.exists(encrypted_file):
                            os.remove(encrypted_file)
                        encrypt_file(input_file, encrypted_file, password, hash_config=config_dict)

                    timings = self._measure_operation_performance(kdf_encrypt_op, 3)

                    if timings:
                        avg_time = statistics.mean(timings)
                        kdf_name = f"{kdf_config['kdf']}"
                        if "pbkdf2_iterations" in kdf_config:
                            kdf_name += f"_iter{kdf_config['pbkdf2_iterations']}"
                        elif "argon2_time_cost" in kdf_config:
                            kdf_name += f"_t{kdf_config['argon2_time_cost']}_m{kdf_config['argon2_memory_cost']}"
                        elif "balloon_time_cost" in kdf_config:
                            kdf_name += f"_s{kdf_config['balloon_space_cost']}_t{kdf_config['balloon_time_cost']}"

                        kdf_results[kdf_name] = {
                            "avg_time": avg_time,
                            "config": kdf_config,
                            "success": True,
                        }
                    else:
                        kdf_results[f"kdf_{i}"] = {"success": False}

                except Exception as e:
                    kdf_results[f"kdf_{i}"] = {"success": False, "error": str(e)}

            successful_kdfs = [
                name for name, result in kdf_results.items() if result.get("success", False)
            ]

            if len(successful_kdfs) == 0:
                return TestResult(
                    "kdf_performance_benchmark",
                    TestResultLevel.ERROR,
                    "No KDF configurations completed successfully",
                )

            # Find fastest and slowest KDFs
            if len(successful_kdfs) > 1:
                kdf_times = {name: kdf_results[name]["avg_time"] for name in successful_kdfs}
                fastest_kdf = min(kdf_times.keys(), key=lambda x: kdf_times[x])
                slowest_kdf = max(kdf_times.keys(), key=lambda x: kdf_times[x])
                time_ratio = kdf_times[slowest_kdf] / kdf_times[fastest_kdf]
            else:
                fastest_kdf = slowest_kdf = successful_kdfs[0]
                time_ratio = 1.0

            return TestResult(
                "kdf_performance_benchmark",
                TestResultLevel.PASS,
                f"KDF benchmark completed. Fastest: {fastest_kdf}, Slowest: {slowest_kdf}",
                details={
                    "kdf_results": kdf_results,
                    "fastest_kdf": fastest_kdf,
                    "slowest_kdf": slowest_kdf,
                    "time_ratio": time_ratio,
                    "successful_kdfs": successful_kdfs,
                },
            )

        except Exception as e:
            return TestResult(
                "kdf_performance_benchmark",
                TestResultLevel.ERROR,
                f"Exception during KDF benchmark: {str(e)}",
                exception=e,
            )

    def _benchmark_memory_usage(self, config: TestConfig) -> None:
        """Benchmark memory usage during operations."""

        result = self.run_single_test(
            self._memory_usage_benchmark, "memory_usage_benchmark", config
        )
        self.add_result(result)

    def _memory_usage_benchmark(self, config: TestConfig) -> TestResult:
        """Test memory usage during cryptographic operations."""
        try:
            import psutil

            file_sizes = [100 * 1024, 1024 * 1024, 5 * 1024 * 1024]  # 100KB, 1MB, 5MB
            process = psutil.Process()

            memory_results = {}

            for file_size in file_sizes:
                input_file = os.path.join(self.temp_dir, f"memory_{file_size}.bin")
                encrypted_file = os.path.join(self.temp_dir, f"memory_{file_size}_enc.bin")

                # Create test file
                test_data = os.urandom(file_size)
                with open(input_file, "wb") as f:
                    f.write(test_data)

                config_dict = {"algorithm": "fernet", "hash_algorithm": "SHA256"}

                # Measure memory before operation
                gc.collect()
                memory_before = process.memory_info().rss

                # Perform encryption
                encrypt_file(
                    input_file, encrypted_file, "memory_test_password", hash_config=config_dict
                )

                # Measure memory after operation
                memory_after = process.memory_info().rss
                memory_used = memory_after - memory_before

                # Calculate memory efficiency
                memory_ratio = memory_used / file_size if file_size > 0 else 0

                memory_results[file_size] = {
                    "memory_used_bytes": memory_used,
                    "memory_used_mb": memory_used / (1024 * 1024),
                    "memory_ratio": memory_ratio,
                    "file_size": file_size,
                }

            # Analyze memory usage patterns
            memory_ratios = [result["memory_ratio"] for result in memory_results.values()]
            avg_memory_ratio = statistics.mean(memory_ratios)

            # Memory usage is considered good if it's less than 3x the file size
            memory_efficient = avg_memory_ratio < 3.0

            level = TestResultLevel.PASS if memory_efficient else TestResultLevel.WARNING
            message = f"Memory usage {'efficient' if memory_efficient else 'high'} (avg ratio: {avg_memory_ratio:.2f}x)"

            return TestResult(
                "memory_usage_benchmark",
                level,
                message,
                details={
                    "memory_results": memory_results,
                    "average_memory_ratio": avg_memory_ratio,
                    "memory_efficient": memory_efficient,
                    "file_sizes_tested": file_sizes,
                },
            )

        except ImportError:
            return TestResult(
                "memory_usage_benchmark",
                TestResultLevel.WARNING,
                "psutil not available for memory analysis",
            )
        except Exception as e:
            return TestResult(
                "memory_usage_benchmark",
                TestResultLevel.ERROR,
                f"Exception during memory benchmark: {str(e)}",
                exception=e,
            )

    def _detect_performance_regressions(self, config: TestConfig) -> None:
        """Detect performance regressions compared to baselines."""

        result = self.run_single_test(
            self._regression_detection_test, "performance_regression_detection", config
        )
        self.add_result(result)

    def _regression_detection_test(self, config: TestConfig) -> TestResult:
        """Test for performance regressions."""
        try:
            if not self.baselines:
                return TestResult(
                    "performance_regression_detection",
                    TestResultLevel.INFO,
                    "No baseline data available for regression detection",
                )

            # This is a placeholder for more sophisticated regression detection
            # In a real implementation, you would compare current results against stored baselines
            regression_count = 0
            improvement_count = 0
            stable_count = 0

            # Simulate regression analysis
            for baseline_key, baseline in self.baselines.items():
                # In a real implementation, you'd have current results to compare
                # For now, we'll just report that baselines exist
                stable_count += 1

            if regression_count > 0:
                level = TestResultLevel.WARNING
                message = f"Detected {regression_count} performance regressions"
            elif improvement_count > 0:
                level = TestResultLevel.PASS
                message = f"Performance improvements detected: {improvement_count}"
            else:
                level = TestResultLevel.PASS
                message = f"Performance stable across {stable_count} benchmarks"

            return TestResult(
                "performance_regression_detection",
                level,
                message,
                details={
                    "baselines_count": len(self.baselines),
                    "regressions": regression_count,
                    "improvements": improvement_count,
                    "stable": stable_count,
                },
            )

        except Exception as e:
            return TestResult(
                "performance_regression_detection",
                TestResultLevel.ERROR,
                f"Exception during regression detection: {str(e)}",
                exception=e,
            )

    def _load_baselines(self, config: TestConfig) -> None:
        """Load baseline performance data."""
        baseline_file = config.get("baseline_file", "benchmark_baselines.json")

        if os.path.exists(baseline_file):
            try:
                with open(baseline_file, "r") as f:
                    baseline_data = json.load(f)

                for key, data in baseline_data.items():
                    self.baselines[key] = BaselineData(**data)

                self.logger.info(f"Loaded {len(self.baselines)} baseline performance records")
            except Exception as e:
                self.logger.warning(f"Could not load baselines: {e}")

    def _save_baselines(self, config: TestConfig) -> None:
        """Save baseline performance data."""
        baseline_file = config.get("baseline_file", "benchmark_baselines.json")

        try:
            baseline_data = {}
            for key, baseline in self.baselines.items():
                baseline_data[key] = {
                    "algorithm": baseline.algorithm,
                    "operation": baseline.operation,
                    "file_size": baseline.file_size,
                    "baseline_throughput": baseline.baseline_throughput,
                    "baseline_avg_time": baseline.baseline_avg_time,
                    "timestamp": baseline.timestamp,
                    "platform_info": baseline.platform_info,
                }

            with open(baseline_file, "w") as f:
                json.dump(baseline_data, f, indent=2)

            self.logger.info(f"Saved {len(self.baselines)} baseline performance records")
        except Exception as e:
            self.logger.warning(f"Could not save baselines: {e}")
