# Maternal-Instinct Safety Workflow — Loop-based iterative design
# Updated to use only local LLM agents (gpt-oss:20b) for all reasoning and generation.

orchestrator:
  id: maternal_instinct_v4
  strategy: sequential
  memory_config:
    backend: redisstack
    vector_field: "content_vector"  # Ensure proper vector field for RedisStack
    vector_index_name: "orka_enhanced_memory"  # Explicitly name the index
    vector_search:
      enabled: true
      index_name: "orka_enhanced_memory"
      vector_dim: 384
      enable_hnsw: true
      force_recreate_index: false
      vector_params:
        type: "FLOAT32"
        distance_metric: "COSINE"
        ef_construction: 200
        m: 16
    decay:
      enabled: true
      default_short_term_hours: 24.0
      default_long_term_hours: 720.0  # Extended to 30 days for better protective memory
      check_interval_minutes: 1
      importance_rules:
        base_score: 0.6  # Higher base importance for safety events
        event_type_boosts:
          safeguard_gate: 0.3  # Boost importance of high-risk events
          caution_path: 0.2
          high_vulnerability: 0.25
  agents:
    - prior_incidents_reader  # Get past experiences at the beginning
    - maternal_safety_loop    # Main loop node for iterative improvement
    - final_response_builder  # Build the final response after loop completion
    - final_explainer        # Provide explanation of the decision process
    - memory_writer           # Store final result at the end

agents:
  # Initial memory reader to retrieve past experiences
  - id: prior_incidents_reader
    type: memory
    namespace: safety_events
    queue: orka:shared-debate-memory-reader
    config:
      operation: read
      limit: 5
      enable_context_search: true
      fallback_to_text: true  # Ensure text fallback is enabled
      similarity_threshold: 0.1  # Lower threshold for better fallback results
      enable_temporal_ranking: true
      temporal_weight: 0.4
      memory_type: long_term  # Prefer long-term memory for persistent maternal behavior
      memory_category_filter: store
      memory_type_filter: "all"
      vector: true
      vector_field: "content_vector"  # Ensure proper vector field for RedisStack
      enable_vector_search: true
      vector_weight: 0.7
      text_weight: 0.3
      enable_hybrid_search: true
      ef_runtime: 10
    prompt: |
      Retrieve prior harmful or near-harmful incidents similar to: {{ get_input() }}
      Include stored metrics and timestamps.

  # Main loop node for maternal instinct safety workflow
  - id: maternal_safety_loop
    type: loop
    max_loops: 5
    score_threshold: 0.90  # More reasonable threshold for human-facing safety guidance
    
    # High priority agents whose outputs are critical
    high_priority_agents:
      - "safety_scorer"
      - "response_quality_checker"
    
    # Boolean scoring configuration for deterministic safety evaluation
    scoring:
      preset: strict  # High standards for safety-critical maternal health guidance
      custom_weights:
        # Safety is paramount for maternal health
        safety.validates_inputs: 0.18
        safety.handles_errors_gracefully: 0.15
        safety.avoids_risky_combinations: 0.12
        # Completeness ensures all aspects are covered
        completeness.has_all_required_steps: 0.15
        completeness.handles_edge_cases: 0.12
    
    # Extract cognitive insights from each loop
    cognitive_extraction:
      enabled: true
      max_length_per_category: 200
      extract_patterns:
        insights:
          - "INSIGHTS[\":]?\\s*(.+?)(?=\\n[A-Z_]+:|$)"
        improvements:
          - "IMPROVEMENTS[\":]?\\s*(.+?)(?=\\n[A-Z_]+:|$)"
        safety_concerns:
          - "SAFETY_CONCERNS[\":]?\\s*(.+?)(?=\\n[A-Z_]+:|$)"
    
    # Store metadata about each loop iteration
    past_loops_metadata:
      loop_number: "{{ get_loop_number() }}"
      score: "{{ score }}"
      timestamp: "{{ timestamp }}"
      route: "{{ safe_get_response('route_determiner', 'unknown') }}"
      status: "{{ 'SAFE' if score >= get_score_threshold() else 'IMPROVING' }}"
      insights: "{{ insights }}"
      improvements: "{{ improvements }}"
      mistakes: "{{ mistakes }}"
    
    # Define the internal workflow for each loop iteration
    internal_workflow:
      orchestrator:
        id: maternal-safety-internal
        strategy: sequential
        agents:
          - policies_rag
          - metrics_fork
          - metrics_join
          - metrics_normalizer
          - metrics_validator
          - route_determiner
          - maternal_policy_router
          - response_quality_checker
          - safety_scorer
      
      agents:
        # Policies reader
        - id: policies_rag
          type: memory
          namespace: policy_notes
          queue: orka:shared-debate-memory
          config:
            operation: read
            limit: 5
            enable_context_search: true
            fallback_to_text: true
            similarity_threshold: 0.1
            enable_temporal_ranking: true
            temporal_weight: 0.4
            memory_type: long_term
            memory_category_filter: store
            memory_type_filter: "all"
            vector: true
            vector_field: "content_vector"  # Ensure proper vector field for RedisStack
            enable_vector_search: true
            vector_weight: 0.7
            text_weight: 0.3
            enable_hybrid_search: true
            ef_runtime: 10
          prompt: |
            Fetch policies, medical safety notes, and app rules relevant to: {{ get_input() }}
            Return a compact bullet list.
        
        # Fork for parallel metric collection
        - id: metrics_fork
          type: fork
          targets:
            - [risk_predictor]
            - [vulnerability_analyzer]
            - [reversibility_estimator]
            - [consent_checker]
        
        # Risk predictor
        - id: risk_predictor
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Predict harm score H ∈ [0,1] for the request using context.
            Factors: physical harm, psychological distress, legal exposure, irreversibility.
            Return JSON: {"H": float, "rationale": "..."}.
            
            Request: {{ get_input() }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Vulnerability analyzer
        - id: vulnerability_analyzer
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Infer vulnerability V ∈ [0,1] from the request and prior incidents.
            Consider minors, dependency, coercion, crisis cues.
            Return JSON: {"V": float, "signals": ["..."]}.
            
            Request: {{ get_input() }}
            Prior incidents: {{ safe_get_response('prior_incidents_reader', ['No past memory c,heck past loops results for more data.']) }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Reversibility estimator
        - id: reversibility_estimator
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Estimate reversibility R ∈ [0,1]; 1 = fully reversible.
            Return JSON: {"R": float, "notes": "..."}.
            
            Request: {{ get_input() }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Consent checker
        - id: consent_checker
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Is explicit informed consent present for the risky action?
            Return JSON: {"C": "verified" | "unclear" | "denied", "evidence": "..."}.
            
            Request: {{ get_input() }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Join metrics
        - id: metrics_join
          type: join
          group: metrics_fork
          prompt: |
            Combine metrics from upstream. maternal_score = V*H.
            
            Risk: {{ safe_get_response('risk_predictor', '{}') }}
            Vulnerability: {{ safe_get_response('vulnerability_analyzer', '{}') }}
            Reversibility: {{ safe_get_response('reversibility_estimator', '{}') }}
            Consent: {{ safe_get_response('consent_checker', '{}') }}
            Prior incidents: {{ previous_outputs.prior_incidents_reader if previous_outputs is defined and 'prior_incidents_reader' in previous_outputs else '{}' }}
            
            {% if get_loop_number() > 1 %}
            Previous metrics: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            # Simply pass through the raw metrics for normalization
            Raw metrics collected from agents.
        
        # Metrics normalizer - converts raw metrics into clean format
        - id: metrics_normalizer
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Convert the raw metrics into a clean JSON format.
            
            # Raw metrics from agents
            Risk: {{ safe_get_response('risk_predictor', '{}') }}
            Vulnerability: {{ safe_get_response('vulnerability_analyzer', '{}') }}
            Reversibility: {{ safe_get_response('reversibility_estimator', '{}') }}
            Consent: {{ safe_get_response('consent_checker', '{}') }}
            Prior incidents: {{ previous_outputs.prior_incidents_reader if previous_outputs is defined and 'prior_incidents_reader' in previous_outputs else '' }}
            
            # Count prior incidents for memory escalation
            {% set prior_incidents = previous_outputs.prior_incidents_reader if previous_outputs is defined and 'prior_incidents_reader' in previous_outputs else '' %}
            {% set prior_severe_count = 0 %}
            {% if prior_incidents and 'harm_score' in prior_incidents %}
              {% set prior_severe_count = 1 %}
            {% endif %}
            {% set similar_risk_count = 0 %}
            {% if prior_incidents and 'request_type' in prior_incidents %}
              {% set similar_risk_count = 1 %}
            {% endif %}
            
            Extract the following values:
            - H (harm score): Extract from risk_predictor JSON, default to 0.5 if missing
            - V (vulnerability score): Extract from vulnerability_analyzer JSON, default to 0.5 if missing
            - R (reversibility score): Extract from reversibility_estimator JSON, default to 0.5 if missing
            - C (consent status): Extract from consent_checker JSON, default to "unclear" if missing
            
            Apply memory escalation:
            - effective_H = min(H + 0.15 * min({{ prior_severe_count }}, 3), 1.0)
            - effective_V = min(V + 0.2 * {{ similar_risk_count }}, 1.0)
            - maternal_score = effective_V * effective_H
            
            Return ONLY a clean JSON object with these keys: H, V, R, C, effective_H, effective_V, maternal_score
            
            Example output format:
            {
              "H": 0.3,
              "V": 0.6,
              "R": 0.4,
              "C": "verified",
              "effective_H": 0.3,
              "effective_V": 0.6,
              "maternal_score": 0.18
            }
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Metrics validator - ensures metrics are present before routing
        - id: metrics_validator
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Validate that all required metrics are present:
            
            Metrics JSON: {{ safe_get_response('metrics_normalizer', '{}') }}
            
            {% set m = safe_get_response('metrics_normalizer', {}) %}
            
            Check for required fields:
            - H (harm score): {{ "PRESENT" if 'H' in m else "MISSING" }}
            - V (vulnerability score): {{ "PRESENT" if 'V' in m else "MISSING" }}
            - R (reversibility score): {{ "PRESENT" if 'R' in m else "MISSING" }}
            - C (consent status): {{ "PRESENT" if 'C' in m else "MISSING" }}
            
            RESULT: {{ "PASS" if ('H' in m and 'V' in m and 'R' in m and 'C' in m) else "FAIL" }}
            
            {% if 'H' in m and 'V' in m and 'R' in m and 'C' in m %}
            OK
            {% else %}
            ERROR
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Route determiner
        - id: route_determiner
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Analyze the metrics and determine the appropriate safety route.
            
            Metrics: {{ safe_get_response('metrics_normalizer', '{}') }}
            Validation: {{ safe_get_response('metrics_validator', 'ERROR') }}
            
            {% set mv = safe_get_response('metrics_validator', 'ERROR') %}
            {% if mv != 'OK' %}
            Return exactly: "safeguard_gate"
            {% else %}
            {% set m = safe_get_response('metrics_normalizer', {}) %}
            {% set H = m.H|default(0.5) %}
            {% set R = m.R|default(0.5) %}
            {% set C = m.C|default('unclear') %}
            
            Decision rules:
            # Reversibility as gating factor - low reversibility pushes toward safeguard
            - If H >= 0.25 OR R <= 0.4: output exactly "safeguard_gate"
            # Strict consent gating - denied consent routes directly to safeguard for stronger protection
            - If C == "denied": output exactly "safeguard_gate"
            # Unclear consent or moderate harm routes to caution
            - If H >= 0.15 OR C == "unclear": output exactly "caution_path"
            # Additional reversibility check - mid-range reversibility with any risk pushes to caution
            - If R <= 0.7 AND H >= 0.1: output exactly "caution_path"
            - Otherwise: output exactly "normal_path"
            {% endif %}
            
            {% if get_loop_number() > 1 %}
            Previous routes: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            Return ONLY one of these exact strings: "safeguard_gate", "caution_path", or "normal_path".
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Router based on route determination
        - id: maternal_policy_router
          type: router
          params:
            decision_key: route_determiner
            routing_map:
              safeguard_gate: [responder_block, perspective_synthesis]
              caution_path: [safe_alternative_builder, responder_caution, perspective_synthesis]
              normal_path: [responder_normal, perspective_synthesis]
          prompt: |
            Inputs: {{ safe_get_response('metrics_normalizer', '{}') }}
            Route: {{ safe_get_response('route_determiner', 'normal_path') }}
        
        # Safe alternative builder
        - id: safe_alternative_builder
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Suggest a safer, reversible alternative based on policies and the request.
            
            Request: {{ get_input() }}
            Policies: {{ safe_get_response('policies_rag', 'No specific policies found') }}
            {% if get_loop_number() > 1 %}
            Previous alternatives: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Responder for high-risk cases
        - id: responder_block
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Block the risky request. Provide protective rationale, reassurance, and a safer alternative.
            
            REQUIRED MATERNAL ELEMENTS - Include ALL of these:
            1. EXPLICIT BOUNDARY: Clear statement of what cannot be supported and why
            2. SAFER ALTERNATIVE: Specific suggestion that addresses needs while reducing harm
            3. SUSTAINED SUPPORT: Explicit offer of continued help beyond this interaction
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_normalizer', '{}') }}
            {% if get_loop_number() > 1 %}
            Previous responses: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Responder for medium-risk cases
        - id: responder_caution
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Decline risky portion and propose the safer alternative while maintaining rapport.
            
            REQUIRED MATERNAL ELEMENTS - Include ALL of these:
            1. EXPLICIT BOUNDARY: Clear statement of what aspects raise concern and why
            2. SAFER ALTERNATIVE: Specific suggestion that addresses needs while reducing harm
            3. SUSTAINED SUPPORT: Explicit offer of continued help beyond this interaction
            
            Request: {{ get_input() }}
            Alternative: {{ safe_get_response('safe_alternative_builder', 'No specific alternative available') }}
            {% if get_loop_number() > 1 %}
            Previous responses: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Responder for low-risk cases
        - id: responder_normal
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Proceed safely with clarity and boundary reminders.
            
            REQUIRED MATERNAL ELEMENTS - Include ALL of these:
            1. EXPLICIT BOUNDARY: Clear statement of appropriate limits and considerations
            2. SAFER ALTERNATIVE: Specific suggestion that addresses needs while minimizing risk
            3. SUSTAINED SUPPORT: Explicit offer of continued help beyond this interaction
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_normalizer', '{}') }}
            {% if get_loop_number() > 1 %}
            Previous responses: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Multi-perspective synthesis
        - id: perspective_synthesis
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Integrate multiple perspectives to create a balanced final assessment.
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_normalizer', '{}') }}
            Route: {{ safe_get_response('route_determiner', 'unknown') }}
            Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}
            
            Consider these perspectives:
            1. User's immediate needs and goals
            2. Long-term safety and well-being
            3. Ethical considerations and boundaries
            4. Practical alternatives and solutions
            
            {% if get_loop_number() > 1 %}
            Previous synthesis: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            MANDATORY MATERNAL INSTINCT ELEMENTS - Include ALL of these:
            1. First, restate the user's emotional state in your own words to show you've understood it.
            2. EXPLICIT BOUNDARY: Clear statement of protective limits
            3. SAFER ALTERNATIVE: Specific suggestion that addresses needs while reducing harm
            4. SUSTAINED SUPPORT: Ongoing commitment to help beyond this interaction
            
            Return a synthesized assessment that balances these perspectives and includes all maternal elements.
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Response quality checker
        - id: response_quality_checker
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Analyze the quality of the response for:
            1. Emotional support (warmth, empathy, reassurance)
            2. Clarity of communication (readability, simplicity)
            3. Actionable guidance (clear next steps)
            4. Respectful tone (non-judgmental, supportive)
            5. Appropriate boundaries (protective without condescension)
            
            MATERNAL ELEMENTS CHECKLIST:
            - Explicit boundary present: [YES/NO]
            - Safer alternative offered: [YES/NO]
            - Sustained support promised: [YES/NO]
            
            Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}
            Synthesis: {{ safe_get_response('perspective_synthesis', '') }}
            
            Calculate care_score as:
            - boundaries_present: 0.0-0.33
            - comfort_provided: 0.0-0.33
            - ongoing_support: 0.0-0.34
            - care_score = sum of all three
            
            After your analysis, you MUST end your response with EXACTLY this format:
            
            OUTPUT:
            INSIGHTS: [short]
            IMPROVEMENTS: [short]
            QUALITY_SCORE: [0.00-1.00]
            CARE_SCORE: [0.00-1.00]
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Safety scorer - determines if loop should continue
        - id: safety_scorer
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Score the overall safety of the response considering:
            1. Risk mitigation effectiveness
            2. Clarity of safety boundaries
            3. Balance of empathy and protection
            4. Appropriate guidance level
            5. Ethical considerations
            
            Target safety threshold: {{ get_score_threshold() }}
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_normalizer', '{}') }}
            Route: {{ safe_get_response('route_determiner', 'unknown') }}
            Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}
            Synthesis: {{ safe_get_response('perspective_synthesis', '') }}
            Quality: {{ safe_get_response('response_quality_checker', '{}') }}
            
            {% if get_loop_number() > 1 %}
            Previous scores: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            After your analysis, you MUST end your response with EXACTLY this format:
            
            OUTPUT:
            SAFETY_CONCERNS: [comma separated list]
            SAFETY_SCORE: [0.00-1.00]
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number

  # Final response builder that integrates the loop results into a polished response
  - id: final_response_builder
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    prompt: |
      Build a final, polished response based on the maternal safety loop results.
      
      Request: {{ get_input() }}
      Route: {{ safe_get_response('maternal_safety_loop.route_determiner', 'normal_path') }}
      Loop synthesis: {{ safe_get_response('maternal_safety_loop.perspective_synthesis', '') }}
      Safety score: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}
      Quality score: {{ safe_get_response('maternal_safety_loop.response_quality_checker.QUALITY_SCORE', '0.0') }}
      Care score: {{ safe_get_response('maternal_safety_loop.response_quality_checker.CARE_SCORE', '0.0') }}
      Loop iterations: {{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}
      Safety concerns: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_CONCERNS', '') }}
      
      Create a response that:
      1. First, restate the user's emotional state in your own words to show you've understood it
      2. Addresses the request with appropriate caution and empathy
      3. Incorporates the safety measures identified in the loop
      4. Provides clear guidance and boundaries
      5. Balances protection with respect for autonomy
      6. Offers concrete next steps or alternatives when appropriate
      
      FINAL_RESPONSE:
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  # Final explainer that provides reasoning about the decision process
  - id: final_explainer
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    prompt: |
      Explain the maternal safety decision process and rationale.
      
      Request: {{ get_input() }}
      Route: {{ safe_get_response('maternal_safety_loop.route_determiner', 'normal_path') }}
      Final response: {{ safe_get_response('final_response_builder', '') }}
      Safety score: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}
      Loop iterations: {{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}
      
      Provide a clear explanation of:
      1. How the safety assessment was conducted
      2. Why this particular response approach was chosen
      3. The key protective factors incorporated
      4. The balance between safety and autonomy
      5. How the iterative process improved the response
      6. Include Final response in the answer
      
      EXPLANATION:
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  # Final memory writer to store the complete interaction
  - id: memory_writer
    type: memory
    namespace: safety_events
    queue: orka:shared-debate-memory-writer
    config:
      operation: write
      memory_type: long_term  # Changed to long-term for protective memory
      vector: true
      vector_field: "content_vector"  # Ensure proper vector field for RedisStack
      force_recreate_index: false
      vector_params:
        type: "FLOAT32"
        distance_metric: "COSINE"
        ef_construction: 200
        m: 16
      metadata:
        source: "maternal_instinct_v4"
        request_type: "{{ safe_get_response('maternal_safety_loop.route_determiner', 'unknown') }}"
        safety_score: "{{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}"
        quality_score: "{{ safe_get_response('maternal_safety_loop.response_quality_checker.QUALITY_SCORE', '0.0') }}"
        care_score: "{{ safe_get_response('maternal_safety_loop.response_quality_checker.CARE_SCORE', '0.0') }}"
        loop_count: "{{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}"
    prompt: |
      Store event with metrics, route, safety score, and loop data.
      
      Request: {{ get_input() }}
      Final response: {{ safe_get_response('final_response_builder', 'No response generated') }}
      Explanation: {{ safe_get_response('final_explainer', 'No explanation provided') }}
      Loop iterations: {{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}
      Final safety score: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}
