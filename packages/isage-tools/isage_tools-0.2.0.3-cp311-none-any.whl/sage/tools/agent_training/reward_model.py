"""
Agent Reward Model

Computes rewards for agent responses based on:
- Task completion
- Tool selection accuracy
- Execution efficiency
- Timing quality
- Format compliance
"""

import logging
import re
from dataclasses import dataclass
from typing import Any, Optional

from .config import AgentRewardConfig

logger = logging.getLogger(__name__)


@dataclass
class RewardResult:
    """å¥–åŠ±è®¡ç®—ç»“æœ"""

    total: float
    breakdown: dict[str, float]
    feedback: str
    penalties_applied: list[str]


class AgentRewardModel:
    """
    Agent å¥–åŠ±æ¨¡å‹

    ç”¨äºè®¡ç®— Agent å“åº”çš„å¥–åŠ±åˆ†æ•°ï¼Œæ”¯æŒ:
    - ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°
    - å·¥å…·é€‰æ‹©å‡†ç¡®æ€§è¯„ä¼°
    - æ‰§è¡Œæ•ˆç‡è¯„ä¼°
    - æ—¶æœºè´¨é‡è¯„ä¼°
    - æ ¼å¼åˆè§„æ€§è¯„ä¼°

    Example:
        >>> reward_model = AgentRewardModel(AgentRewardConfig())
        >>> result = reward_model.compute_reward(
        ...     query="æŸ¥è¯¢å¤©æ°”",
        ...     response="<tool_call>weather_001</tool_call>",
        ...     ground_truth={"target_tools": ["weather_001"]},
        ...     execution_trace=[]
        ... )
        >>> print(f"Total reward: {result.total:.2f}")
    """

    # å·¥å…·è°ƒç”¨æ¨¡å¼
    TOOL_CALL_PATTERN = re.compile(
        r'<tool_call>\s*\{?\s*"?name"?\s*:\s*"?([^"}\s]+)"?', re.IGNORECASE
    )

    # ç®€å•å·¥å…· ID æ¨¡å¼
    SIMPLE_TOOL_PATTERN = re.compile(
        r"(?:call|use|invoke|execute)\s+([a-z_]+_\d{3})", re.IGNORECASE
    )

    def __init__(self, config: Optional[AgentRewardConfig] = None):
        """
        åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹

        Args:
            config: å¥–åŠ±é…ç½®ï¼ŒNone åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or AgentRewardConfig()

        # éªŒè¯æƒé‡æ€»å’Œ
        weight_sum = sum(self.config.weights.values())
        if abs(weight_sum - 1.0) > 0.01:
            logger.warning(f"Reward weights sum to {weight_sum}, expected 1.0")

    def compute_reward(
        self,
        query: str,
        response: str,
        ground_truth: dict,
        execution_trace: Optional[list] = None,
    ) -> RewardResult:
        """
        è®¡ç®—ç»¼åˆå¥–åŠ±åˆ†æ•°

        Args:
            query: ç”¨æˆ·è¯·æ±‚
            response: æ¨¡å‹å“åº”
            ground_truth: æ ‡å‡†ç­”æ¡ˆï¼ŒåŒ…å« target_tools, expected_steps ç­‰
            execution_trace: æ‰§è¡Œè½¨è¿¹ (å¯é€‰)

        Returns:
            RewardResult åŒ…å«æ€»åˆ†ã€åˆ†é¡¹åˆ†æ•°ã€åé¦ˆå’Œæƒ©ç½š
        """
        execution_trace = execution_trace or []
        rewards = {}
        penalties_applied = []

        # 1. ä»»åŠ¡å®Œæˆåº¦
        rewards["task_completion"] = self._eval_task_completion(
            response, ground_truth, execution_trace
        )

        # 2. å·¥å…·é€‰æ‹©å‡†ç¡®æ€§
        target_tools = ground_truth.get("target_tools", [])
        rewards["tool_accuracy"], tool_penalties = self._eval_tool_accuracy(response, target_tools)
        penalties_applied.extend(tool_penalties)

        # 3. æ‰§è¡Œæ•ˆç‡
        optimal_steps = ground_truth.get("optimal_steps", 5)
        rewards["efficiency"] = self._eval_efficiency(execution_trace, optimal_steps)

        # 4. æ—¶æœºè´¨é‡
        rewards["timing_quality"] = self._eval_timing(response, execution_trace, ground_truth)

        # 5. æ ¼å¼åˆè§„æ€§
        rewards["format_compliance"], format_penalties = self._eval_format(response)
        penalties_applied.extend(format_penalties)

        # åŠ æƒæ±‚å’Œ
        total = sum(rewards[k] * self.config.weights.get(k, 0) for k in rewards)

        # åº”ç”¨æƒ©ç½š
        for penalty_type in penalties_applied:
            penalty_value = self.config.penalties.get(penalty_type, 0)
            total += penalty_value

        # å½’ä¸€åŒ–åˆ° [0, 1]
        total = max(0.0, min(1.0, total))

        # ç”Ÿæˆåé¦ˆ
        feedback = self._generate_feedback(rewards, penalties_applied)

        return RewardResult(
            total=total,
            breakdown=rewards,
            feedback=feedback,
            penalties_applied=penalties_applied,
        )

    def _eval_task_completion(
        self,
        response: str,
        ground_truth: dict,
        execution_trace: list,
    ) -> float:
        """è¯„ä¼°ä»»åŠ¡å®Œæˆåº¦"""
        # æ£€æŸ¥æ‰§è¡Œè½¨è¿¹ä¸­æ˜¯å¦æœ‰æˆåŠŸæ ‡è®°
        if execution_trace:
            success_count = sum(1 for step in execution_trace if step.get("status") == "success")
            total_steps = len(execution_trace)
            if total_steps > 0:
                return success_count / total_steps

        # åŸºäºå“åº”å†…å®¹åˆ¤æ–­
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ€ç»ˆç­”æ¡ˆ/ç»“è®º
        completion_indicators = [
            "å®Œæˆ",
            "done",
            "finished",
            "ç»“æœæ˜¯",
            "ç­”æ¡ˆæ˜¯",
            "æ€»ç»“",
            "conclusion",
            "æœ€ç»ˆ",
            "final",
        ]

        response_lower = response.lower()
        has_conclusion = any(ind in response_lower for ind in completion_indicators)

        # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†ç›®æ ‡å·¥å…·
        target_tools = ground_truth.get("target_tools", [])
        predicted_tools = self._extract_tool_calls(response)

        if target_tools:
            tool_coverage = len(set(predicted_tools) & set(target_tools)) / len(target_tools)
        else:
            tool_coverage = 1.0 if not predicted_tools else 0.5

        # ç»¼åˆè¯„åˆ†
        score = 0.5 * tool_coverage + 0.5 * (1.0 if has_conclusion else 0.5)
        return score

    def _eval_tool_accuracy(
        self,
        response: str,
        target_tools: list[str],
    ) -> tuple[float, list[str]]:
        """
        è¯„ä¼°å·¥å…·é€‰æ‹©å‡†ç¡®ç‡

        Returns:
            (accuracy_score, penalties_list)
        """
        penalties = []
        predicted_tools = self._extract_tool_calls(response)

        if not target_tools:
            # æ— ç›®æ ‡å·¥å…·æ—¶ï¼Œè°ƒç”¨ä»»ä½•å·¥å…·éƒ½å¯ä»¥
            return (1.0 if not predicted_tools else 0.7, penalties)

        if not predicted_tools:
            # åº”è¯¥è°ƒç”¨å·¥å…·ä½†æ²¡æœ‰è°ƒç”¨
            return (0.0, penalties)

        # è®¡ç®— Precision å’Œ Recall
        predicted_set = set(predicted_tools)
        target_set = set(target_tools)

        correct = len(predicted_set & target_set)

        # Precision: é¢„æµ‹çš„å·¥å…·ä¸­æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„
        precision = correct / len(predicted_set) if predicted_set else 0

        # Recall: ç›®æ ‡å·¥å…·ä¸­æœ‰å¤šå°‘è¢«é¢„æµ‹åˆ°
        recall = correct / len(target_set) if target_set else 0

        # æ£€æŸ¥é”™è¯¯å·¥å…·
        wrong_tools = predicted_set - target_set
        if wrong_tools:
            penalties.append("wrong_tool")

        # æ£€æŸ¥å¹»è§‰å·¥å…· (æ ¼å¼ä¸ç¬¦åˆ tool_id è§„èŒƒçš„)
        for tool in predicted_tools:
            if not re.match(r"^[a-z]+(_[a-z]+)*_\d{3}$", tool):
                penalties.append("hallucination")
                break

        # F1 Score
        if precision + recall == 0:
            return (0.0, penalties)

        f1 = 2 * precision * recall / (precision + recall)
        return (f1, penalties)

    def _eval_efficiency(
        self,
        execution_trace: list,
        optimal_steps: int,
    ) -> float:
        """è¯„ä¼°æ‰§è¡Œæ•ˆç‡"""
        actual_steps = len(execution_trace)

        if actual_steps == 0:
            return 0.5  # æ²¡æœ‰æ‰§è¡Œè½¨è¿¹æ—¶ç»™ä¸­ç­‰åˆ†

        if actual_steps <= optimal_steps:
            return 1.0

        # è¶…è¿‡æœ€ä¼˜æ­¥æ•°æ—¶ï¼Œæ•ˆç‡é€’å‡
        excess_ratio = (actual_steps - optimal_steps) / optimal_steps
        score = max(0.0, 1.0 - 0.2 * excess_ratio)

        return score

    def _eval_timing(
        self,
        response: str,
        execution_trace: list,
        ground_truth: dict,
    ) -> float:
        """è¯„ä¼°è°ƒç”¨æ—¶æœºè´¨é‡"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å†—ä½™è°ƒç”¨
        predicted_tools = self._extract_tool_calls(response)
        unique_tools = set(predicted_tools)

        if len(predicted_tools) > len(unique_tools):
            # æœ‰é‡å¤è°ƒç”¨
            redundancy_penalty = 0.2 * (len(predicted_tools) - len(unique_tools))
            return max(0.0, 1.0 - redundancy_penalty)

        # æ£€æŸ¥è°ƒç”¨é¡ºåºæ˜¯å¦åˆç† (åŸºäºæ‰§è¡Œè½¨è¿¹)
        if execution_trace:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥åé‡è¯•
            retry_count = sum(
                1
                for i, step in enumerate(execution_trace[1:], 1)
                if step.get("tool_id") == execution_trace[i - 1].get("tool_id")
            )
            if retry_count > 0:
                return max(0.5, 1.0 - 0.1 * retry_count)

        return 1.0  # é»˜è®¤è‰¯å¥½

    def _eval_format(self, response: str) -> tuple[float, list[str]]:
        """
        è¯„ä¼°æ ¼å¼åˆè§„æ€§

        Returns:
            (format_score, penalties_list)
        """
        penalties = []
        score = 1.0

        # æ£€æŸ¥å·¥å…·è°ƒç”¨æ ¼å¼
        tool_calls = self.TOOL_CALL_PATTERN.findall(response)
        simple_calls = self.SIMPLE_TOOL_PATTERN.findall(response)

        # å¦‚æœä½¿ç”¨äº†éæ ‡å‡†æ ¼å¼
        if simple_calls and not tool_calls:
            score -= 0.2
            penalties.append("format_error")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªé—­åˆçš„æ ‡ç­¾
        open_tags = response.count("<tool_call>")
        close_tags = response.count("</tool_call>")
        if open_tags != close_tags:
            score -= 0.3
            penalties.append("format_error")

        # æ£€æŸ¥ JSON æ ¼å¼
        if "<tool_call>" in response:
            try:
                import json

                # æå– JSON éƒ¨åˆ†
                json_match = re.search(r"<tool_call>\s*(\{[^}]+\})\s*</tool_call>", response)
                if json_match:
                    json.loads(json_match.group(1))
            except json.JSONDecodeError:
                score -= 0.2

        return (max(0.0, score), penalties)

    def _extract_tool_calls(self, response: str) -> list[str]:
        """ä»å“åº”ä¸­æå–å·¥å…·è°ƒç”¨"""
        tools = []

        # æ ‡å‡†æ ¼å¼
        tools.extend(self.TOOL_CALL_PATTERN.findall(response))

        # ç®€å•æ ¼å¼
        tools.extend(self.SIMPLE_TOOL_PATTERN.findall(response))

        # å»é‡ä½†ä¿æŒé¡ºåº
        seen = set()
        unique_tools = []
        for tool in tools:
            if tool not in seen:
                seen.add(tool)
                unique_tools.append(tool)

        return unique_tools

    def _generate_feedback(
        self,
        rewards: dict[str, float],
        penalties: list[str],
    ) -> str:
        """ç”Ÿæˆäººç±»å¯è¯»çš„åé¦ˆ"""
        feedback_parts = []

        # åˆ†æ•°åé¦ˆ
        for metric, score in rewards.items():
            if score < 0.5:
                feedback_parts.append(f"âŒ {metric}: {score:.2f} (éœ€æ”¹è¿›)")
            elif score < 0.8:
                feedback_parts.append(f"âš ï¸ {metric}: {score:.2f} (ä¸€èˆ¬)")
            else:
                feedback_parts.append(f"âœ… {metric}: {score:.2f} (è‰¯å¥½)")

        # æƒ©ç½šåé¦ˆ
        if penalties:
            penalty_msgs = {
                "wrong_tool": "é€‰æ‹©äº†é”™è¯¯çš„å·¥å…·",
                "redundant_call": "å­˜åœ¨å†—ä½™çš„å·¥å…·è°ƒç”¨",
                "format_error": "å“åº”æ ¼å¼ä¸è§„èŒƒ",
                "timeout": "æ‰§è¡Œè¶…æ—¶",
                "hallucination": "è°ƒç”¨äº†ä¸å­˜åœ¨çš„å·¥å…·",
            }
            for p in set(penalties):
                msg = penalty_msgs.get(p, f"æƒ©ç½š: {p}")
                feedback_parts.append(f"ğŸš« {msg}")

        return "\n".join(feedback_parts)


class ToolVerifier:
    """å·¥å…·è°ƒç”¨éªŒè¯å™¨"""

    def __init__(self, tool_registry: Optional[Any] = None):
        self.tool_registry = tool_registry

    def verify_tool_exists(self, tool_id: str) -> bool:
        """éªŒè¯å·¥å…·æ˜¯å¦å­˜åœ¨"""
        if self.tool_registry is None:
            # åªéªŒè¯æ ¼å¼
            return bool(re.match(r"^[a-z]+(_[a-z]+)*_\d{3}$", tool_id))

        return self.tool_registry.has_tool(tool_id)

    def verify_arguments(self, tool_id: str, arguments: dict) -> tuple[bool, str]:
        """éªŒè¯å·¥å…·å‚æ•°æ˜¯å¦åˆæ³•"""
        if self.tool_registry is None:
            return (True, "")

        tool = self.tool_registry.get_tool(tool_id)
        if not tool:
            return (False, f"Tool {tool_id} not found")

        # æ£€æŸ¥å¿…éœ€å‚æ•°
        required_params = getattr(tool, "required_params", [])
        missing = [p for p in required_params if p not in arguments]

        if missing:
            return (False, f"Missing required parameters: {missing}")

        return (True, "")


class PlanEvaluator:
    """è§„åˆ’è´¨é‡è¯„ä¼°å™¨"""

    def evaluate_plan(
        self,
        plan_steps: list[dict],
        ground_truth_steps: Optional[list[dict]] = None,
    ) -> dict:
        """
        è¯„ä¼°è§„åˆ’è´¨é‡

        Args:
            plan_steps: æ¨¡å‹ç”Ÿæˆçš„è§„åˆ’æ­¥éª¤
            ground_truth_steps: æ ‡å‡†è§„åˆ’æ­¥éª¤ (å¯é€‰)

        Returns:
            è¯„ä¼°ç»“æœ
        """
        result = {
            "step_count": len(plan_steps),
            "has_clear_goal": False,
            "has_tool_assignments": False,
            "is_executable": False,
            "score": 0.0,
        }

        if not plan_steps:
            return result

        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®ç›®æ ‡
        first_step = plan_steps[0]
        if "goal" in first_step or "objective" in first_step:
            result["has_clear_goal"] = True

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·åˆ†é…
        for step in plan_steps:
            if "tool" in step or "tool_id" in step:
                result["has_tool_assignments"] = True
                break

        # æ£€æŸ¥æ˜¯å¦å¯æ‰§è¡Œ (æ¯ä¸ªæ­¥éª¤éƒ½æœ‰ action)
        result["is_executable"] = all("action" in step or "tool" in step for step in plan_steps)

        # è®¡ç®—æ€»åˆ†
        score = 0.0
        if result["has_clear_goal"]:
            score += 0.3
        if result["has_tool_assignments"]:
            score += 0.4
        if result["is_executable"]:
            score += 0.3

        result["score"] = score

        return result
