{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computation\n",
    "\n",
    "This notebook describes how to use LightCurveLynx to perform parallel computation. \n",
    "\n",
    "The core simulation function of the model can take a `concurrent.futures.Executor` object and use that to distribute the computation over multiple processes. This object can be a built in parallelization method, such as `ThreadPoolExecutor` or `ProcessPoolExecutor`, or other libraries, such as Dask.\n",
    "\n",
    "Note:\n",
    "  * Each process will load a full version of all the data, so they may be memory intensive.\n",
    "  * Not all subpackages work with distributed computation yet. If you get an error about not being able to pickle an object, please let the LightCurveLynx team know so we can investigate. We are aware the PZFlowNodes are currently failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightcurvelynx.astro_utils.passbands import PassbandGroup\n",
    "from lightcurvelynx.models.basic_models import ConstantSEDModel\n",
    "from lightcurvelynx.obstable.opsim import OpSim\n",
    "from lightcurvelynx.simulate import simulate_lightcurves\n",
    "\n",
    "# Usually we would not hardcode the path to the passband files, but for this demo we will use a relative path\n",
    "# to the test data directory so that we do not have to download the files.\n",
    "table_dir = \"../../tests/lightcurvelynx/data/passbands\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Data\n",
    "\n",
    "We start by loading the standard information that we need for any simulation:\n",
    "\n",
    "  * An `ObsTable` that includes the surveyâ€™s pointing and noise information.\n",
    "  * A `PassbandGroup` for that survey.\n",
    "\n",
    "We start by creating a toy survey that includes pointings at two locations (0.0, 10.0) and (180.0, -10.0) in the \"g\" and \"r\" bands and loading the passband group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdata1 = {\n",
    "    \"time\": [0.0, 1.0, 2.0, 3.0],\n",
    "    \"ra\": [0.0, 0.0, 180.0, 180.0],\n",
    "    \"dec\": [10.0, 10.0, -10.0, -10.0],\n",
    "    \"filter\": [\"g\", \"r\", \"g\", \"r\"],\n",
    "    \"zp\": [5.0, 6.0, 7.0, 8.0],\n",
    "    \"seeing\": [1.12, 1.12, 1.12, 1.12],\n",
    "    \"skybrightness\": [20.0, 20.0, 20.0, 20.0],\n",
    "    \"exptime\": [29.2, 29.2, 29.2, 29.2],\n",
    "    \"nexposure\": [2, 2, 2, 2],\n",
    "}\n",
    "obstable1 = OpSim(obsdata1)\n",
    "\n",
    "passband_group1 = PassbandGroup.from_preset(\n",
    "    preset=\"LSST\",\n",
    "    table_dir=table_dir,\n",
    "    filters=[\"g\", \"r\", \"i\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Next we create a model from which to simulate observations. We define a model and its parameters as we would with any other simulation.  Here we use a constant SED model (same value for all times and wavelengths). We place the object at (0.0, 10.0) so it is observed by some of the pointings from each survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConstantSEDModel(brightness=100.0, t0=0.0, ra=0.0, dec=10.0, redshift=0.0, node_label=\"my_star\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "The only change in running the simulation in parallel is that we create a `ProcessPoolExecutor` object and pass that to the simulation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    results = simulate_lightcurves(\n",
    "        model=model,\n",
    "        num_samples=10_000,\n",
    "        obstable=obstable1,\n",
    "        passbands=passband_group1,\n",
    "        obstable_save_cols=[\"zp_nJy\"],\n",
    "        executor=executor,\n",
    "        batch_size=100,\n",
    "    )\n",
    "\n",
    "print(f\"Generated {len(results)} light curves\")\n",
    "print(results[\"lightcurve\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not provide an executor object, but rather a number of jobs, we automatically create and manage the `ProcessPoolExecutor`. Here we run the simulation on 4 processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = simulate_lightcurves(\n",
    "    model=model,\n",
    "    num_samples=10_000,\n",
    "    obstable=obstable1,\n",
    "    passbands=passband_group1,\n",
    "    obstable_save_cols=[\"zp_nJy\"],\n",
    "    num_jobs=4,\n",
    "    batch_size=100,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(results)} light curves\")\n",
    "print(results[\"lightcurve\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "We can parallelize the computation via Dask by using dask.distributed.\n",
    "\n",
    "**Note:** Dask is not installed by default, so users will need to install dask (`pip install 'dask[distributed]'`) to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dask.distributed\n",
    "\n",
    "    with dask.distributed.Client() as client:\n",
    "        results = simulate_lightcurves(\n",
    "            model=model,\n",
    "            num_samples=100,\n",
    "            obstable=obstable1,\n",
    "            passbands=passband_group1,\n",
    "            obstable_save_cols=[\"zp_nJy\"],\n",
    "            executor=client,\n",
    "        )\n",
    "    print(f\"Generated {len(results)} light curves\")\n",
    "    print(results[\"lightcurve\"][0])\n",
    "except ImportError:\n",
    "    print(\"Dask is not installed, skipping Dask example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray\n",
    "\n",
    "We can parallelize the computation via Ray by using ray.util.multiprocessing.Pool \n",
    "\n",
    "**Note:** Ray is not installed by default, so users will need to install dask (`pip install -U \"ray[default]\"`) to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ray\n",
    "    from ray.util.multiprocessing import Pool\n",
    "\n",
    "    with Pool(processes=4) as executor:\n",
    "        results = simulate_lightcurves(\n",
    "            model=model,\n",
    "            num_samples=100,\n",
    "            obstable=obstable1,\n",
    "            passbands=passband_group1,\n",
    "            obstable_save_cols=[\"zp_nJy\"],\n",
    "            executor=executor,\n",
    "        )\n",
    "    print(f\"Generated {len(results)} light curves\")\n",
    "    print(results[\"lightcurve\"][0])\n",
    "\n",
    "    ray.shutdown()\n",
    "except ImportError:\n",
    "    print(\"Ray is not installed, skipping Ray example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to Files\n",
    "\n",
    "Depending on the size of the simulated results, you might not want to load the full set into memory as a single table. The `simulate_lightcurves` has a function to save each shard (the result of each process) to a unique file. Instead of returning the NestedFrames, the function returns the list of file paths containing the data. Users can then analyze or load these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = simulate_lightcurves(\n",
    "    model=model,\n",
    "    num_samples=10_000,\n",
    "    obstable=obstable1,\n",
    "    passbands=passband_group1,\n",
    "    num_jobs=4,\n",
    "    batch_size=1000,\n",
    "    obstable_save_cols=[\"zp_nJy\"],\n",
    "    output_file_path=\"./scratch/nb_results.parquet\",\n",
    ")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the results are broken up into ten different files.\n",
    "\n",
    "## Handling Randomness\n",
    "\n",
    "By default LightCurveLynx creates a new random number generator (without a fixed seed) so that the parameters will vary from run to run. However it also all allows the users to control the randomness by passing in their own random number generator, which may have a fixed seed. In both these cases the ``simulate_lightcurves()`` ensures the correct behavior during parallel runs.\n",
    "\n",
    "The initial random number generator (provided or default) is used to create a new random seed for each processing shard. If the user had provided a seeded random number generator, the new list of seeds will be predefined. Thus each shard will get a predefined (but different) random number generator. If the initial random number generator was not seeded, the seeds for each shard will themselves vary from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Fixed Sets of Data\n",
    "\n",
    "Several of the sampling nodes, such as the `TableSampler`, `GivenValueSampler`, or `GivenValueList` work with predefined lists of data. If the data is drawn randomly from the lists, such as with the `GivenValueSampler`, this randomness will be handled as described above. Each shard will use a different random seed to start from a different part of the sampling space.\n",
    "\n",
    "In contrast there are a few nodes whose behavior is designed to be deterministic:\n",
    "  * `GivenValueList` is meant for testing only and will fail if you try to use it in a parallel run.\n",
    "  * `TableSampler` (with `in_order=True`) automatically handles the coordination so that each worker uses a disjoint range of rows. For example, if we are sampling 100 values using batches of size 10, the first shard will use rows [0, 9]. The second shard will use rows [10, 19]. And so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightcurvelynx.math_nodes.given_sampler import TableSampler\n",
    "\n",
    "table_data = {\n",
    "    \"ra\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"dec\": [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "}\n",
    "table_node = TableSampler(table_data, in_order=True, node_label=\"table_node\")\n",
    "\n",
    "model2 = ConstantSEDModel(\n",
    "    brightness=100.0,\n",
    "    t0=0.0,\n",
    "    ra=table_node.ra,\n",
    "    dec=table_node.dec,\n",
    "    redshift=0.0,\n",
    "    node_label=\"my_star\",\n",
    ")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    results = simulate_lightcurves(\n",
    "        model=model2,\n",
    "        num_samples=5,\n",
    "        obstable=obstable1,\n",
    "        passbands=passband_group1,\n",
    "        executor=executor,\n",
    "        batch_size=2,\n",
    "    )\n",
    "\n",
    "for idx in range(5):\n",
    "    print(f\"{idx}: RA={results['ra'][idx]}, DEC={results['dec'][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead\n",
    "\n",
    "As with any distributed computation, there will be per-batch overhead. All of the input data (model, obstable, etc.) are pickled and sent to the new processes. It takes time to pack and unpack this information. So care must be taken to ensure the parallelization is worth it.\n",
    "\n",
    "The user can provide a `batch_size` parameter to control the target batch size for each process. This allows the user to ensure that each process has enough data to be worth it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightcurvelynx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
