#!/usr/bin/env python3
"""
Finetune CLI - Core Logic
Ê†∏ÂøÉÈÄªËæëÔºöÊï∞ÊçÆÂáÜÂ§á„ÄÅÈÖçÁΩÆÁîüÊàê
"""

import json
from pathlib import Path

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

from .models import FinetuneTask
from .utils import collect_sage_code_files

console = Console()


def prepare_training_data(
    task_type: FinetuneTask,
    root_dir: Path,
    output_dir: Path,
    format: str = "alpaca",
    custom_data_path: Path | None = None,
    **kwargs,
) -> Path:
    """ÂáÜÂ§áËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºàÊîØÊåÅÂ§öÁßç‰ªªÂä°Á±ªÂûãÔºâ"""
    output_dir.mkdir(parents=True, exist_ok=True)

    training_data = []

    if task_type == FinetuneTask.CODE_UNDERSTANDING:
        # ‰ª£Á†ÅÁêÜËß£‰ªªÂä° - Êî∂ÈõÜ‰ª£Á†ÅÊñá‰ª∂
        extensions = kwargs.get("extensions", [".py", ".yaml", ".yml", ".toml", ".md", ".rst"])
        files = collect_sage_code_files(root_dir, extensions=extensions)

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task(
                f"üìù ÂáÜÂ§á‰ª£Á†ÅÁêÜËß£Êï∞ÊçÆ ({len(files)} ‰∏™Êñá‰ª∂)...", total=len(files)
            )

            for file_path in files:
                try:
                    content = file_path.read_text(encoding="utf-8")
                    rel_path = file_path.relative_to(root_dir)

                    if format == "alpaca":
                        # Êñá‰ª∂ÂäüËÉΩËß£Èáä
                        training_data.append(
                            {
                                "instruction": f"ËØ∑Ëß£ÈáäÈ°πÁõÆ‰∏≠ {rel_path} Êñá‰ª∂ÁöÑÂäüËÉΩÂíåÂÆûÁé∞",
                                "input": "",
                                "output": content,
                            }
                        )

                        # ‰ª£Á†ÅÈóÆÁ≠î
                        if file_path.suffix == ".py":
                            training_data.append(
                                {
                                    "instruction": f"È°πÁõÆ‰∏≠ {rel_path} Êñá‰ª∂ÂåÖÂê´Âì™‰∫õÁ±ªÂíåÂáΩÊï∞ÔºüËØ∑ËØ¶ÁªÜËØ¥Êòé„ÄÇ",
                                    "input": content[:1500],
                                    "output": f"ËøôÊòØ {rel_path} ÁöÑ‰ª£Á†ÅÂÆûÁé∞ÔºåÂåÖÂê´‰∫ÜÊ†∏ÂøÉÂäüËÉΩ„ÄÇËÆ©Êàë‰∏∫‰Ω†ËØ¶ÁªÜÂàÜÊûê...",
                                }
                            )

                            # ‰ª£Á†Å‰øÆÊîπÂª∫ËÆÆ
                            training_data.append(
                                {
                                    "instruction": f"Â¶Ç‰ΩïÊîπËøõ {rel_path} ‰∏≠ÁöÑ‰ª£Á†ÅÔºü",
                                    "input": content[:1000],
                                    "output": f"Âü∫‰∫é {rel_path} ÁöÑ‰ª£Á†ÅÂàÜÊûêÔºåÊàëÂª∫ËÆÆ‰ªé‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢ÊîπËøõ...",
                                }
                            )

                    progress.update(task, advance=1)
                except Exception as e:
                    console.print(f"[yellow]‚ö†Ô∏è  Ë∑≥ËøáÊñá‰ª∂ {file_path}: {e}[/yellow]")
                    continue

    elif task_type == FinetuneTask.QA_PAIRS:
        # ÈóÆÁ≠îÂØπ‰ªªÂä°
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, encoding="utf-8") as f:
                qa_data = json.load(f)

            for item in qa_data:
                if format == "alpaca":
                    training_data.append(
                        {
                            "instruction": item.get("question", ""),
                            "input": item.get("context", ""),
                            "output": item.get("answer", ""),
                        }
                    )
        else:
            console.print("[yellow]‚ö†Ô∏è  ÈúÄË¶ÅÊèê‰æõÈóÆÁ≠îÊï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/yellow]")

    elif task_type == FinetuneTask.INSTRUCTION:
        # Êåá‰ª§ÂæÆË∞É‰ªªÂä°
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, encoding="utf-8") as f:
                instruction_data = json.load(f)

            for item in instruction_data:
                if format == "alpaca":
                    training_data.append(
                        {
                            "instruction": item.get("instruction", ""),
                            "input": item.get("input", ""),
                            "output": item.get("output", ""),
                        }
                    )
        else:
            console.print("[yellow]‚ö†Ô∏è  ÈúÄË¶ÅÊèê‰æõÊåá‰ª§Êï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/yellow]")

    elif task_type == FinetuneTask.CHAT:
        # ÂØπËØùÂæÆË∞É‰ªªÂä°
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, encoding="utf-8") as f:
                chat_data = json.load(f)

            for item in chat_data:
                if format == "chat":
                    training_data.append({"conversations": item.get("conversations", [])})
                elif format == "alpaca":
                    # ËΩ¨Êç¢‰∏∫alpacaÊ†ºÂºè
                    conversations = item.get("conversations", [])
                    if len(conversations) >= 2:
                        training_data.append(
                            {
                                "instruction": conversations[0].get("content", ""),
                                "input": "",
                                "output": conversations[1].get("content", ""),
                            }
                        )
        else:
            console.print("[yellow]‚ö†Ô∏è  ÈúÄË¶ÅÊèê‰æõÂØπËØùÊï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/yellow]")

    elif task_type == FinetuneTask.CUSTOM:
        # Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜ
        if custom_data_path and custom_data_path.exists():
            with open(custom_data_path, encoding="utf-8") as f:
                training_data = json.load(f)
            console.print(f"‚úÖ Â∑≤Âä†ËΩΩËá™ÂÆö‰πâÊï∞ÊçÆÈõÜ: {len(training_data)} Êù°")
        else:
            console.print("[red]‚ùå Ëá™ÂÆö‰πâ‰ªªÂä°ÈúÄË¶ÅÊèê‰æõÊï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ[/red]")
            import typer

            raise typer.Exit(1)

    # ‰øùÂ≠òËÆ≠ÁªÉÊï∞ÊçÆ
    output_file = output_dir / f"training_data_{task_type.value}_{format}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)

    console.print(f"\n‚úÖ ËÆ≠ÁªÉÊï∞ÊçÆÂ∑≤ÁîüÊàê: [cyan]{output_file}[/cyan]")
    console.print(f"   üìä ÂÖ± {len(training_data)} Êù°ËÆ≠ÁªÉÊ†∑Êú¨")

    return output_file


def generate_training_config(
    model_name: str,
    dataset_path: Path,
    output_dir: Path,
    framework: str = "llama-factory",
) -> Path:
    """ÁîüÊàêËÆ≠ÁªÉÈÖçÁΩÆÊñá‰ª∂"""

    config = {}

    if framework == "llama-factory":
        config = {
            "model_name_or_path": model_name,
            "stage": "sft",
            "do_train": True,
            "finetuning_type": "lora",
            "lora_target": "all",
            "dataset": str(dataset_path),
            "template": "qwen",
            "cutoff_len": 4096,
            "max_samples": 10000,
            "overwrite_cache": True,
            "preprocessing_num_workers": 16,
            "output_dir": str(output_dir / "checkpoints"),
            "logging_steps": 10,
            "save_steps": 500,
            "plot_loss": True,
            "overwrite_output_dir": True,
            "per_device_train_batch_size": 2,
            "gradient_accumulation_steps": 8,
            "learning_rate": 5e-5,
            "num_train_epochs": 3,
            "lr_scheduler_type": "cosine",
            "warmup_ratio": 0.1,
            "fp16": True,
            "lora_rank": 8,
            "lora_alpha": 16,
            "lora_dropout": 0.05,
        }
    elif framework == "unsloth":
        config = {
            "model_name": model_name,
            "max_seq_length": 4096,
            "load_in_4bit": True,
            "dataset": str(dataset_path),
            "dataset_text_field": "text",
            "packing": False,
            "output_dir": str(output_dir / "checkpoints"),
            "num_train_epochs": 3,
            "per_device_train_batch_size": 2,
            "gradient_accumulation_steps": 4,
            "warmup_steps": 10,
            "learning_rate": 2e-4,
            "fp16": True,
            "logging_steps": 1,
            "optim": "adamw_8bit",
            "weight_decay": 0.01,
            "lr_scheduler_type": "linear",
            "seed": 3407,
        }

    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)

    config_path = output_dir / f"{framework}_config.json"
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)

    return config_path
