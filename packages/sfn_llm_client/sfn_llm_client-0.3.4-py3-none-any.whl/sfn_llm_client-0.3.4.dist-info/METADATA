Metadata-Version: 2.4
Name: sfn_llm_client
Version: 0.3.4
Summary: SDK for using LLM
Author-email: Rajesh Darak <rajesh@stepfuction.ai>
License-Expression: MIT
Project-URL: Homepage, https://github.com/iamrajeshdaraksfn/llm-client-sdk.git
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: aiohttp
Requires-Dist: openai
Requires-Dist: tiktoken
Requires-Dist: anthropic
Requires-Dist: snowflake-connector-python>=3.17.0
Requires-Dist: snowflake-snowpark-python>=1.26.0
Requires-Dist: snowflake-ml-python>=1.9.0
Requires-Dist: transformers>=4.46.2
Requires-Dist: langchain-openai
Requires-Dist: langchain-community
Requires-Dist: pydantic
Requires-Dist: langchain-core
Requires-Dist: StrEnum
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: pytest-aiohttp; extra == "test"
Requires-Dist: pytest-asyncio; extra == "test"
Requires-Dist: pytest-mock; extra == "test"
Requires-Dist: aioresponses; extra == "test"
Provides-Extra: openai
Requires-Dist: openai>=1.54.3; extra == "openai"
Requires-Dist: tiktoken>=0.3.3; extra == "openai"
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.39.0; extra == "anthropic"
Provides-Extra: cortex-langchain
Requires-Dist: snowflake-snowpark-python>=1.0.0; extra == "cortex-langchain"
Requires-Dist: langchain-community>=0.0.30; extra == "cortex-langchain"
Requires-Dist: pydantic>=2.6.0; extra == "cortex-langchain"
Requires-Dist: langchain-core>=0.3; extra == "cortex-langchain"
Provides-Extra: google
Requires-Dist: google-generativeai>=0.1.0; extra == "google"
Provides-Extra: api
Requires-Dist: sfn_llm_client[anthropic,google,openai]; extra == "api"
Provides-Extra: local
Requires-Dist: transformers>=4.0.0; extra == "local"
Provides-Extra: sync
Requires-Dist: async_to_sync>=0.2.0; extra == "sync"
Provides-Extra: all
Requires-Dist: sfn_llm_client[api,local,sync]; extra == "all"

# SFN_LLM_Client

This is an enhanced and improved version with latest llm provider chat completion feature The `sfn_llm_client` now includes:

- **Updated to the latest version of OpenAI**.
- **Integrated Cortex LLM provider support**.
- **Latest improvements and updates to the codebase** for better performance and compatibility.

## Features

- Supports multiple LLM providers, including OpenAI and Cortex.
- Easily extensible to include new LLM providers by implementing base client classes.
- Well-documented and tested.

### Adding a New LLM Client

To add a new LLM client, follow these steps:

1. **Implement `BaseLLMClient` or `BaseLLMAPIClient`:**  
   If you're adding a new LLM provider, you'll need to implement either the `BaseLLMClient` or `BaseLLMAPIClient` interfaces.
   
2. **Register in `LLMAPIClientFactory`:**  
   If you're adding a client based on `BaseLLMAPIClient`, don't forget to register it in the `LLMAPIClientFactory` so that it's available for use.

### Adding Dependencies

If your LLM client requires additional dependencies, you can add them to the `pyproject.toml` file under the appropriate section.

## Contributing
Contributions are welcome! If you'd like to help improve this SDK, please check out the todos or open an issue or pull request.

### Credits
the core forked functionality taken from `llm-client-sdk` created by uripeled2.

## Contact:
For any queries or issues, please contact the maintainer at: `rajesh@stepfunction.ai`
