# Copyright (c) 2023-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import builtins
from typing import Literal, Optional, Union, overload
from typing import Any, List, Sequence

from nvidia.dali.types import DALIDataType, DALIImageType, DALIInterpType

from nvidia.dali._typing import TensorLikeIn, TensorLikeArg
from nvidia.dali.data_node import DataNode
from . import readers as readers
from . import decoders as decoders
from . import inputs as inputs

@overload
def audio_resample(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    in_rate: Union[DataNode, TensorLikeArg, float, None] = None,
    out_length: Union[DataNode, TensorLikeArg, int, None] = None,
    out_rate: Union[DataNode, TensorLikeArg, float, None] = None,
    preserve: Optional[bool] = False,
    quality: Optional[float] = 50.0,
    scale: Union[DataNode, TensorLikeArg, float, None] = None,
) -> DataNode:
    """
    .. warning::

       This operator is now deprecated. Use :meth:`audio_resample` instead.

       This operator was moved out from the experimental phase, and is now a regular DALI operator. This is just an deprecated alias kept for backward compatibility.

    Legacy alias for :meth:`audio_resample`.

    """
    ...

@overload
def audio_resample(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    in_rate: Union[DataNode, TensorLikeArg, float, None] = None,
    out_length: Union[DataNode, TensorLikeArg, int, None] = None,
    out_rate: Union[DataNode, TensorLikeArg, float, None] = None,
    preserve: Optional[bool] = False,
    quality: Optional[float] = 50.0,
    scale: Union[DataNode, TensorLikeArg, float, None] = None,
) -> Union[DataNode, List[DataNode]]:
    """
    .. warning::

       This operator is now deprecated. Use :meth:`audio_resample` instead.

       This operator was moved out from the experimental phase, and is now a regular DALI operator. This is just an deprecated alias kept for backward compatibility.

    Legacy alias for :meth:`audio_resample`.

    """
    ...

@overload
def debayer(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    algorithm: Optional[str] = None,
    blue_position: Union[DataNode, TensorLikeArg, Sequence[int], int],
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    preserve: Optional[bool] = False,
) -> DataNode:
    """
    Performs image demosaicing/debayering.

    Converts single-channel image to RGB using specified color filter array.

    The supported input types are ``uint8_t`` and ``uint16_t``.
    The input images must be 2D tensors (``HW``) or 3D tensors (``HWC``) where the number of channels is 1.
    The operator supports sequence of images/video-like inputs (layout ``FHW``).
    The output of the operator is always ``HWC`` (or ``FHWC`` for sequences).

    For example, the following snippet presents debayering of batch of image sequences::

      def bayered_sequence(sample_info):
        # some actual source of video inputs with corresponding pattern
        # as opencv-style string
        video, bayer_pattern = get_sequence(sample_info)
        if bayer_pattern == "bggr":
            blue_position = [0, 0]
        elif bayer_pattern == "gbrg":
            blue_position = [0, 1]
        elif bayer_pattern == "grbg":
            blue_position = [1, 0]
        else:
            assert bayer_pattern == "rggb"
            blue_position = [1, 1]
        return video, np.array(blue_position, dtype=np.int32)

      @pipeline_def
      def debayer_pipeline():
        bayered_sequences, blue_positions = fn.external_source(
          source=bayered_sequence, batch=False, num_outputs=2,
          layout=["FHW", None])  # note the "FHW" layout, for plain images it would be "HW"
        debayered_sequences = fn.experimental.debayer(
          bayered_sequences.gpu(), blue_position=blue_positions, algorithm='default_npp')
        return debayered_sequences



    This operator allows sequence inputs.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHW', 'FHWC')
        Input to the operator.


    Keyword args
    ------------
    algorithm : str, optional
        The algorithm to be used when inferring missing colours for any given pixel.
        Different algorithms are supported on the GPU and CPU.

        **GPU Algorithms:**

         - ``default_npp`` - default - bilinear interpolation with chroma correlation for green values.

        **CPU Algorithms:**

         - ``bilinear_ocv`` - default - bilinear interpolation.
         - ``edgeaware_ocv`` edge-aware interpolation.
         - ``vng_ocv`` Variable Number of Gradients (VNG) interpolation (only ``uint8_t`` supported).
         - ``gray_ocv`` converts the image to grayscale with bilinear interpolation.
    blue_position : int or list of int or TensorList of int
        The layout of color filter array/bayer tile.

        A position of the blue value in the 2x2 bayer tile.
        The supported values correspond to the following OpenCV bayer layouts:

        * ``(0, 0)`` - ``BG``/``BGGR``
        * ``(0, 1)`` - ``GB``/``GBRG``
        * ``(1, 0)`` - ``GR``/``GRBG``
        * ``(1, 1)`` - ``RG``/``RGGB``

        The argument follows OpenCV's convention of referring to a 2x2 tile that starts
        in the second row and column of the sensors' matrix.

        For example, the ``(0, 0)``/``BG``/``BGGR`` corresponds to the following matrix of sensors:

        .. list-table::
           :header-rows: 0

           * - R
             - G
             - R
             - G
             - R
           * - G
             - **B**
             - **G**
             - B
             - G
           * - R
             - **G**
             - **R**
             - G
             - R
           * - G
             - B
             - G
             - B
             - G

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def debayer(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    algorithm: Optional[str] = None,
    blue_position: Union[DataNode, TensorLikeArg, Sequence[int], int],
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    preserve: Optional[bool] = False,
) -> Union[DataNode, List[DataNode]]:
    """
    Performs image demosaicing/debayering.

    Converts single-channel image to RGB using specified color filter array.

    The supported input types are ``uint8_t`` and ``uint16_t``.
    The input images must be 2D tensors (``HW``) or 3D tensors (``HWC``) where the number of channels is 1.
    The operator supports sequence of images/video-like inputs (layout ``FHW``).
    The output of the operator is always ``HWC`` (or ``FHWC`` for sequences).

    For example, the following snippet presents debayering of batch of image sequences::

      def bayered_sequence(sample_info):
        # some actual source of video inputs with corresponding pattern
        # as opencv-style string
        video, bayer_pattern = get_sequence(sample_info)
        if bayer_pattern == "bggr":
            blue_position = [0, 0]
        elif bayer_pattern == "gbrg":
            blue_position = [0, 1]
        elif bayer_pattern == "grbg":
            blue_position = [1, 0]
        else:
            assert bayer_pattern == "rggb"
            blue_position = [1, 1]
        return video, np.array(blue_position, dtype=np.int32)

      @pipeline_def
      def debayer_pipeline():
        bayered_sequences, blue_positions = fn.external_source(
          source=bayered_sequence, batch=False, num_outputs=2,
          layout=["FHW", None])  # note the "FHW" layout, for plain images it would be "HW"
        debayered_sequences = fn.experimental.debayer(
          bayered_sequences.gpu(), blue_position=blue_positions, algorithm='default_npp')
        return debayered_sequences



    This operator allows sequence inputs.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHW', 'FHWC')
        Input to the operator.


    Keyword args
    ------------
    algorithm : str, optional
        The algorithm to be used when inferring missing colours for any given pixel.
        Different algorithms are supported on the GPU and CPU.

        **GPU Algorithms:**

         - ``default_npp`` - default - bilinear interpolation with chroma correlation for green values.

        **CPU Algorithms:**

         - ``bilinear_ocv`` - default - bilinear interpolation.
         - ``edgeaware_ocv`` edge-aware interpolation.
         - ``vng_ocv`` Variable Number of Gradients (VNG) interpolation (only ``uint8_t`` supported).
         - ``gray_ocv`` converts the image to grayscale with bilinear interpolation.
    blue_position : int or list of int or TensorList of int
        The layout of color filter array/bayer tile.

        A position of the blue value in the 2x2 bayer tile.
        The supported values correspond to the following OpenCV bayer layouts:

        * ``(0, 0)`` - ``BG``/``BGGR``
        * ``(0, 1)`` - ``GB``/``GBRG``
        * ``(1, 0)`` - ``GR``/``GRBG``
        * ``(1, 1)`` - ``RG``/``RGGB``

        The argument follows OpenCV's convention of referring to a 2x2 tile that starts
        in the second row and column of the sensors' matrix.

        For example, the ``(0, 0)``/``BG``/``BGGR`` corresponds to the following matrix of sensors:

        .. list-table::
           :header-rows: 0

           * - R
             - G
             - R
             - G
             - R
           * - G
             - **B**
             - **G**
             - B
             - G
           * - R
             - **G**
             - **R**
             - G
             - R
           * - G
             - B
             - G
             - B
             - G

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def dilate(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    anchor: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [-1, -1],
    border_mode: Optional[str] = "constant",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    iterations: Optional[int] = 1,
    mask_size: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [3, 3],
    preserve: Optional[bool] = False,
) -> DataNode:
    """
    Performs a dilation operation on the input image.

    This operator allows sequence inputs.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.


    Keyword args
    ------------
    anchor : int or list of int or TensorList of int, optional, default = `[-1, -1]`
        Sets the anchor point of the structuring element. Default value (-1, -1) uses the element's center as the anchor point.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    border_mode : str, optional, default = `'constant'`
        Border mode to be used when accessing elements outside input image.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    iterations : int, optional, default = `1`
        Number of times to execute the operation, typically set to 1. Setting to a value higher than 1 is equivelent to increasing the mask size by (mask_width - 1, mask_height -1) for every additional iteration.
    mask_size : int or list of int or TensorList of int, optional, default = `[3, 3]`
        Size of the structuring element.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def dilate(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    anchor: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [-1, -1],
    border_mode: Optional[str] = "constant",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    iterations: Optional[int] = 1,
    mask_size: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [3, 3],
    preserve: Optional[bool] = False,
) -> Union[DataNode, List[DataNode]]:
    """
    Performs a dilation operation on the input image.

    This operator allows sequence inputs.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.


    Keyword args
    ------------
    anchor : int or list of int or TensorList of int, optional, default = `[-1, -1]`
        Sets the anchor point of the structuring element. Default value (-1, -1) uses the element's center as the anchor point.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    border_mode : str, optional, default = `'constant'`
        Border mode to be used when accessing elements outside input image.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    iterations : int, optional, default = `1`
        Number of times to execute the operation, typically set to 1. Setting to a value higher than 1 is equivelent to increasing the mask size by (mask_width - 1, mask_height -1) for every additional iteration.
    mask_size : int or list of int or TensorList of int, optional, default = `[3, 3]`
        Size of the structuring element.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def equalize(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    preserve: Optional[bool] = False,
) -> DataNode:
    """
    Performs grayscale/per-channel histogram equalization.

    The supported inputs are images and videos of uint8_t type.

    This operator allows sequence inputs.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'CHW', 'FHW', 'FHWC', 'FCHW')
        Input to the operator.


    Keyword args
    ------------
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def equalize(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    preserve: Optional[bool] = False,
) -> Union[DataNode, List[DataNode]]:
    """
    Performs grayscale/per-channel histogram equalization.

    The supported inputs are images and videos of uint8_t type.

    This operator allows sequence inputs.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'CHW', 'FHW', 'FHWC', 'FCHW')
        Input to the operator.


    Keyword args
    ------------
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def erode(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    anchor: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [-1, -1],
    border_mode: Optional[str] = "constant",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    iterations: Optional[int] = 1,
    mask_size: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [3, 3],
    preserve: Optional[bool] = False,
) -> DataNode:
    """
    Performs an erosion operation on the input image.

    This operator allows sequence inputs.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.


    Keyword args
    ------------
    anchor : int or list of int or TensorList of int, optional, default = `[-1, -1]`
        Sets the anchor point of the structuring element. Default value (-1, -1) uses the element's center as the anchor point.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    border_mode : str, optional, default = `'constant'`
        Border mode to be used when accessing elements outside input image.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    iterations : int, optional, default = `1`
        Number of times to execute the operation, typically set to 1. Setting to a value higher than 1 is equivelent to increasing the mask size by (mask_width - 1, mask_height -1) for every additional iteration.
    mask_size : int or list of int or TensorList of int, optional, default = `[3, 3]`
        Size of the structuring element.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def erode(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    anchor: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [-1, -1],
    border_mode: Optional[str] = "constant",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    iterations: Optional[int] = 1,
    mask_size: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [3, 3],
    preserve: Optional[bool] = False,
) -> Union[DataNode, List[DataNode]]:
    """
    Performs an erosion operation on the input image.

    This operator allows sequence inputs.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.


    Keyword args
    ------------
    anchor : int or list of int or TensorList of int, optional, default = `[-1, -1]`
        Sets the anchor point of the structuring element. Default value (-1, -1) uses the element's center as the anchor point.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    border_mode : str, optional, default = `'constant'`
        Border mode to be used when accessing elements outside input image.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    iterations : int, optional, default = `1`
        Number of times to execute the operation, typically set to 1. Setting to a value higher than 1 is equivelent to increasing the mask size by (mask_width - 1, mask_height -1) for every additional iteration.
    mask_size : int or list of int or TensorList of int, optional, default = `[3, 3]`
        Size of the structuring element.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def filter(
    data: Union[DataNode, TensorLikeIn],
    filter: Union[DataNode, TensorLikeIn],
    fill_value: Union[DataNode, TensorLikeIn, None] = None,
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    anchor: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [-1],
    border: Optional[str] = "reflect_101",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    mode: Optional[str] = "same",
    preserve: Optional[bool] = False,
) -> DataNode:
    """
    Convolves the image with the provided filter.

    .. note::
      In fact, the operator computes a correlation, not a convolution,
      i.e. the order of filter elements is not flipped when computing the product of
      the filter and the image.



    This operator allows sequence inputs.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    data : TensorList
        Batch of input samples.

        Sample can be an image, a video or volumetric (3D) data.
        Samples can contain channels: channel-first and channel-last layouts are supported.
        In case of video/sequences, the frame extent must preced the channels extent, i.e.,
        for example, a video with ``"FCHW"`` layout is supported, but ``"CFHW"`` samples are not.

        Samples with the following types are supported:
        int8, int16, uint8, uint16, float16, float32.

        Please note that the intermediate type used for the computation is always float32.

        .. note::
          The CPU variant does not support volumetric (3D) data, nor inputs of types: int8 and float16.

    filter : TensorList
        Batch of filters.

        For inputs with two spatial dimensions (images or video), each filter must be a 2D array
        (or a sequence of 2D arrays to be applied
        :func:`per-frame<nvidia.dali.fn.per_frame>` to a video input).
        For volumetric inputs, the filter must be a 3D array.
        The filter values must have float32 type.
    fill_value : TensorList, optional
        Batch of scalars used for padding.

        If ``"border"`` is set to ``"constant"``, the input samples will be padded with
        the corresponding scalars when convolved with the filter.
        The scalars must be of the same type as the input samples.
        For video/sequence input, an array of scalars can be specified to be applied
        :func:`per-frame<nvidia.dali.fn.per_frame>`.


    Keyword args
    ------------
    anchor : int or list of int or TensorList of int, optional, default = `[-1]`
        Specifies the position of the filter over the input.

        If the filter size is ``(r, s)`` and the anchor is ``(a, b)``, the output
        at position ``(x, y)`` is a product of the filter and the input rectangle spanned between the
        corners: top-left ``(x - a, y - b)`` and bottom-right ``(x - a + r - 1, x - b + s - 1)``.

        If the -1 (the default) is specifed, the middle (rounded down to integer)
        of the filter extents is used, which, for odd sized filters, results in the filter
        centered over the input.

        The anchor must be, depending on the input dimensionality, a 2D or 3D point whose each extent lies
        within filter boundaries (``[0, ..., filter_extent - 1]``). The ordering of anchor's extents
        corresponds to the order of filter's extents.

        The parameter is ignored in ``"valid"`` mode.
        .

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    border : str, optional, default = `'reflect_101'`
        Controls how to handle out-of-bound filter positions over the sample.

        Supported values are: ``"reflect_101"``, ``"reflect_1001"``, ``"wrap"``,
        ``"clamp"``, ``"constant"``.

        - ``"reflect_101"`` (default), reflects the input but does not repeat the outermost
          values (``dcb|abcdefghi|hgf``).
        - ``"reflect_1001"``: reflects the input including outermost values (``cba|abcdefghi|ihg``)
        - ``"wrap"``: wraps the input (``ghi|abcdefghi|abc``).
        - ``"clamp"``: the input is padded with outermost values (``aaa|abcdefghi|iii``).
        - ``"constant"``: the input is padded with the user-provided scalar (zeros by default).
          within the sample.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional
        Output data type.
        The output type can either be float or must be same as input type.
        If not set, the input type is used.

        .. note::
          The intermediate type used for actual computation is float32. If the output is of integral type,
          the values will be clamped to the output type range.
    mode : str, optional, default = `'same'`
        Supported values are: ``"same"`` and ``"valid"``.

        - ``"same"`` (default): The input and output sizes are the same and `border` is used
          to handle out-of-bound filter positions.
        - ``"valid"``: the output sample is cropped (by ``filter_extent - 1``) so that all
          filter positions lie fully within the input sample.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def filter(
    data: Union[List[DataNode], DataNode, TensorLikeIn],
    filter: Union[List[DataNode], DataNode, TensorLikeIn],
    fill_value: Union[List[DataNode], DataNode, TensorLikeIn, None] = None,
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    anchor: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [-1],
    border: Optional[str] = "reflect_101",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    mode: Optional[str] = "same",
    preserve: Optional[bool] = False,
) -> Union[DataNode, List[DataNode]]:
    """
    Convolves the image with the provided filter.

    .. note::
      In fact, the operator computes a correlation, not a convolution,
      i.e. the order of filter elements is not flipped when computing the product of
      the filter and the image.



    This operator allows sequence inputs.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    data : TensorList
        Batch of input samples.

        Sample can be an image, a video or volumetric (3D) data.
        Samples can contain channels: channel-first and channel-last layouts are supported.
        In case of video/sequences, the frame extent must preced the channels extent, i.e.,
        for example, a video with ``"FCHW"`` layout is supported, but ``"CFHW"`` samples are not.

        Samples with the following types are supported:
        int8, int16, uint8, uint16, float16, float32.

        Please note that the intermediate type used for the computation is always float32.

        .. note::
          The CPU variant does not support volumetric (3D) data, nor inputs of types: int8 and float16.

    filter : TensorList
        Batch of filters.

        For inputs with two spatial dimensions (images or video), each filter must be a 2D array
        (or a sequence of 2D arrays to be applied
        :func:`per-frame<nvidia.dali.fn.per_frame>` to a video input).
        For volumetric inputs, the filter must be a 3D array.
        The filter values must have float32 type.
    fill_value : TensorList, optional
        Batch of scalars used for padding.

        If ``"border"`` is set to ``"constant"``, the input samples will be padded with
        the corresponding scalars when convolved with the filter.
        The scalars must be of the same type as the input samples.
        For video/sequence input, an array of scalars can be specified to be applied
        :func:`per-frame<nvidia.dali.fn.per_frame>`.


    Keyword args
    ------------
    anchor : int or list of int or TensorList of int, optional, default = `[-1]`
        Specifies the position of the filter over the input.

        If the filter size is ``(r, s)`` and the anchor is ``(a, b)``, the output
        at position ``(x, y)`` is a product of the filter and the input rectangle spanned between the
        corners: top-left ``(x - a, y - b)`` and bottom-right ``(x - a + r - 1, x - b + s - 1)``.

        If the -1 (the default) is specifed, the middle (rounded down to integer)
        of the filter extents is used, which, for odd sized filters, results in the filter
        centered over the input.

        The anchor must be, depending on the input dimensionality, a 2D or 3D point whose each extent lies
        within filter boundaries (``[0, ..., filter_extent - 1]``). The ordering of anchor's extents
        corresponds to the order of filter's extents.

        The parameter is ignored in ``"valid"`` mode.
        .

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    border : str, optional, default = `'reflect_101'`
        Controls how to handle out-of-bound filter positions over the sample.

        Supported values are: ``"reflect_101"``, ``"reflect_1001"``, ``"wrap"``,
        ``"clamp"``, ``"constant"``.

        - ``"reflect_101"`` (default), reflects the input but does not repeat the outermost
          values (``dcb|abcdefghi|hgf``).
        - ``"reflect_1001"``: reflects the input including outermost values (``cba|abcdefghi|ihg``)
        - ``"wrap"``: wraps the input (``ghi|abcdefghi|abc``).
        - ``"clamp"``: the input is padded with outermost values (``aaa|abcdefghi|iii``).
        - ``"constant"``: the input is padded with the user-provided scalar (zeros by default).
          within the sample.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional
        Output data type.
        The output type can either be float or must be same as input type.
        If not set, the input type is used.

        .. note::
          The intermediate type used for actual computation is float32. If the output is of integral type,
          the values will be clamped to the output type range.
    mode : str, optional, default = `'same'`
        Supported values are: ``"same"`` and ``"valid"``.

        - ``"same"`` (default): The input and output sizes are the same and `border` is used
          to handle out-of-bound filter positions.
        - ``"valid"``: the output sample is cropped (by ``filter_extent - 1``) so that all
          filter positions lie fully within the input sample.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def inflate(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    algorithm: Optional[str] = "LZ4",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    chunk_offsets: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = None,
    chunk_sizes: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = None,
    dtype: Optional[DALIDataType] = DALIDataType.UINT8,
    layout: Optional[str] = "",
    preserve: Optional[bool] = False,
    sequence_axis_name: Optional[str] = "F",
    shape: Union[DataNode, TensorLikeArg, Sequence[int], int],
) -> DataNode:
    """
    Inflates/decompresses the input using specified decompression algorithm.

    The input must be a 1D tensor of bytes (uint8). Passing the `shape` and `dtype` of the
    decompressed samples is required.

    Each input sample can either be a single compressed chunk or consist of multiple
    compressed chunks that have the same shape and type when inflated, so that they can be
    be merged into a single tensor where the outermost extent of the tensor corresponds
    to the number of the chunks.

    If the sample is comprised of multiple chunks, the `chunk_offsets` or `chunk_sizes`
    must be specified. In that case, the `shape` must describe the shape of a single inflated
    (output) chunk. The number of the chunks will automatically be added as the outermost extent
    to the output tensors.

    For example, the following snippet presents decompression of a video-like sequences.
    Each video sequence was deflated by, first, compressing each frame separately and then
    concatenating compressed frames from the corresponding sequences.::

      @pipeline_def
      def inflate_sequence_pipeline():
        compres_seq, uncompres_hwc_shape, compres_chunk_sizes = fn.external_source(...)
        sequences = fn.experimental.inflate(
            compres_seq.gpu(),
            chunk_sizes=compres_chunk_sizes,  # refers to sizes in ``compres_seq``
            shape=uncompres_hwc_shape,
            layout="HWC",
            sequence_axis_name="F")
        return sequences



    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList
        Input to the operator.


    Keyword args
    ------------
    algorithm : str, optional, default = `'LZ4'`
        Algorithm to be used to decode the data.

        Currently only ``LZ4`` is supported.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    chunk_offsets : int or list of int or TensorList of int, optional
        A list of offsets within the input sample
        describing where the consecutive chunks begin.

        If the `chunk_sizes` is not specified, it is assumed that the chunks are densely packed
        in the input tensor and the last chunk ends with the sample's end.
    chunk_sizes : int or list of int or TensorList of int, optional
        A list of sizes of corresponding input chunks.

        If the `chunk_offsets` is not specified, it is assumed that the chunks are densely packed
        in the input tensor and the first chunk starts at the beginning of the sample.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional, default = `DALIDataType.UINT8`
        The output (inflated) data type.
    layout : :ref:`layout str<layout_str_doc>`, optional, default = `''`
        Layout of the output (inflated) chunk.

        If the samples consist of multiple chunks, additionally, the `sequence_axis_name` extent
        will be added to the beginning of the specified layout.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    sequence_axis_name : :ref:`layout str<layout_str_doc>`, optional, default = `'F'`
        The name for the sequence axis.

        If the samples consist of multiple chunks, an extra outer dimension will be added to
        the output tensor. By default, it is assumed to be video frames, hence the default label 'F'

        The value is ignored if the `layout` is not specified or the input is not a sequence
        ( neither `chunk_offsets` nor `chunk_sizes` is specified).
    shape : int or list of int or TensorList of int
        The shape of the output (inflated) chunk.

    """
    ...

@overload
def inflate(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    algorithm: Optional[str] = "LZ4",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    chunk_offsets: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = None,
    chunk_sizes: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = None,
    dtype: Optional[DALIDataType] = DALIDataType.UINT8,
    layout: Optional[str] = "",
    preserve: Optional[bool] = False,
    sequence_axis_name: Optional[str] = "F",
    shape: Union[DataNode, TensorLikeArg, Sequence[int], int],
) -> Union[DataNode, List[DataNode]]:
    """
    Inflates/decompresses the input using specified decompression algorithm.

    The input must be a 1D tensor of bytes (uint8). Passing the `shape` and `dtype` of the
    decompressed samples is required.

    Each input sample can either be a single compressed chunk or consist of multiple
    compressed chunks that have the same shape and type when inflated, so that they can be
    be merged into a single tensor where the outermost extent of the tensor corresponds
    to the number of the chunks.

    If the sample is comprised of multiple chunks, the `chunk_offsets` or `chunk_sizes`
    must be specified. In that case, the `shape` must describe the shape of a single inflated
    (output) chunk. The number of the chunks will automatically be added as the outermost extent
    to the output tensors.

    For example, the following snippet presents decompression of a video-like sequences.
    Each video sequence was deflated by, first, compressing each frame separately and then
    concatenating compressed frames from the corresponding sequences.::

      @pipeline_def
      def inflate_sequence_pipeline():
        compres_seq, uncompres_hwc_shape, compres_chunk_sizes = fn.external_source(...)
        sequences = fn.experimental.inflate(
            compres_seq.gpu(),
            chunk_sizes=compres_chunk_sizes,  # refers to sizes in ``compres_seq``
            shape=uncompres_hwc_shape,
            layout="HWC",
            sequence_axis_name="F")
        return sequences



    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList
        Input to the operator.


    Keyword args
    ------------
    algorithm : str, optional, default = `'LZ4'`
        Algorithm to be used to decode the data.

        Currently only ``LZ4`` is supported.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    chunk_offsets : int or list of int or TensorList of int, optional
        A list of offsets within the input sample
        describing where the consecutive chunks begin.

        If the `chunk_sizes` is not specified, it is assumed that the chunks are densely packed
        in the input tensor and the last chunk ends with the sample's end.
    chunk_sizes : int or list of int or TensorList of int, optional
        A list of sizes of corresponding input chunks.

        If the `chunk_offsets` is not specified, it is assumed that the chunks are densely packed
        in the input tensor and the first chunk starts at the beginning of the sample.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional, default = `DALIDataType.UINT8`
        The output (inflated) data type.
    layout : :ref:`layout str<layout_str_doc>`, optional, default = `''`
        Layout of the output (inflated) chunk.

        If the samples consist of multiple chunks, additionally, the `sequence_axis_name` extent
        will be added to the beginning of the specified layout.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    sequence_axis_name : :ref:`layout str<layout_str_doc>`, optional, default = `'F'`
        The name for the sequence axis.

        If the samples consist of multiple chunks, an extra outer dimension will be added to
        the output tensor. By default, it is assumed to be video frames, hence the default label 'F'

        The value is ignored if the `layout` is not specified or the input is not a sequence
        ( neither `chunk_offsets` nor `chunk_sizes` is specified).
    shape : int or list of int or TensorList of int
        The shape of the output (inflated) chunk.

    """
    ...

@overload
def median_blur(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    preserve: Optional[bool] = False,
    window_size: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [3, 3],
) -> DataNode:
    """

    Median blur performs smoothing of an image or sequence of images by replacing each pixel
    with the median color of a surrounding rectangular region.


    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.


    Keyword args
    ------------
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    window_size : int or list of int or TensorList of int, optional, default = `[3, 3]`
        The size of the window over which the smoothing is performed

    """
    ...

@overload
def median_blur(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    preserve: Optional[bool] = False,
    window_size: Union[DataNode, TensorLikeArg, Sequence[int], int, None] = [3, 3],
) -> Union[DataNode, List[DataNode]]:
    """

    Median blur performs smoothing of an image or sequence of images by replacing each pixel
    with the median color of a surrounding rectangular region.


    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.


    Keyword args
    ------------
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    window_size : int or list of int or TensorList of int, optional, default = `[3, 3]`
        The size of the window over which the smoothing is performed

    """
    ...

@overload
def peek_image_shape(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["cpu"]] = None,
    name: Optional[str] = None,
    adjust_orientation: Optional[bool] = True,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = DALIDataType.INT64,
    image_type: Optional[DALIImageType] = DALIImageType.RGB,
    preserve: Optional[bool] = False,
) -> DataNode:
    """
    Obtains the shape of the encoded image.

    This operator returns the shape that an image would have after decoding.

    .. note::
        In most cases the optimal solution is to call :meth:`nvidia.dali.pipeline.DataNode.shape()`
        on the decoded images. Use this operator if you either do not intend to decode the image
        in your pipeline, or do not use the default execution model (i.e., explicitly set
        ``exec_dynamic=False``).


    Supported backends
     * 'cpu'


    Args
    ----
    input : TensorList
        Input to the operator.


    Keyword args
    ------------
    adjust_orientation : bool, optional, default = `True`
        Use the EXIF orientation metadata when calculating the shape.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional, default = `DALIDataType.INT64`
        Data type, to which the sizes are converted.
    image_type : :class:`nvidia.dali.types.DALIImageType`, optional, default = `DALIImageType.RGB`
        Color format of the image.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def peek_image_shape(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["cpu"]] = None,
    name: Optional[str] = None,
    adjust_orientation: Optional[bool] = True,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = DALIDataType.INT64,
    image_type: Optional[DALIImageType] = DALIImageType.RGB,
    preserve: Optional[bool] = False,
) -> Union[DataNode, List[DataNode]]:
    """
    Obtains the shape of the encoded image.

    This operator returns the shape that an image would have after decoding.

    .. note::
        In most cases the optimal solution is to call :meth:`nvidia.dali.pipeline.DataNode.shape()`
        on the decoded images. Use this operator if you either do not intend to decode the image
        in your pipeline, or do not use the default execution model (i.e., explicitly set
        ``exec_dynamic=False``).


    Supported backends
     * 'cpu'


    Args
    ----
    input : TensorList
        Input to the operator.


    Keyword args
    ------------
    adjust_orientation : bool, optional, default = `True`
        Use the EXIF orientation metadata when calculating the shape.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional, default = `DALIDataType.INT64`
        Data type, to which the sizes are converted.
    image_type : :class:`nvidia.dali.types.DALIImageType`, optional, default = `DALIImageType.RGB`
        Color format of the image.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def remap(
    input: Union[DataNode, TensorLikeIn],
    mapx: Union[DataNode, TensorLikeIn],
    mapy: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    interp: Optional[DALIInterpType] = DALIInterpType.INTERP_LINEAR,
    pixel_origin: Optional[str] = "corner",
    preserve: Optional[bool] = False,
) -> DataNode:
    """

    The remap operation applies a generic geometrical transformation to an image. In other words,
    it takes pixels from one place in the input image and puts them in another place in
    the output image. The transformation is described by ``mapx`` and ``mapy`` parameters, where:

        output(x,y) = input(mapx(x,y),mapy(x,y))

    The type of the output tensor will match the type of the input tensor.

    Handles only HWC layout.

    Currently picking border policy is not supported.
    The ``DALIBorderType`` will always be ``CONSTANT`` with the value ``0``.


    This operator allows sequence inputs.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HWC', 'FHWC')
        Input data. Must be a 1- or 3-channel HWC image.
    mapx : TensorList of float ('HWC', 'HW', 'FHWC', 'FHW', 'F***', 'F**')
        Defines the remap transformation for x coordinates.
    mapy : TensorList of float ('HWC', 'HW', 'FHWC', 'FHW', 'F***', 'F**')
        Defines the remap transformation for y coordinates.


    Keyword args
    ------------
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    interp : :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Interpolation type.
    pixel_origin : str, optional, default = `'corner'`

        Pixel origin. Possible values: ``"corner"``, ``"center"``.

        Defines which part of the pixel (upper-left corner or center) is interpreted as its origin.
        This value impacts the interpolation result. To match OpenCV, please pick ``"center"``.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def remap(
    input: Union[List[DataNode], DataNode, TensorLikeIn],
    mapx: Union[List[DataNode], DataNode, TensorLikeIn],
    mapy: Union[List[DataNode], DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    interp: Optional[DALIInterpType] = DALIInterpType.INTERP_LINEAR,
    pixel_origin: Optional[str] = "corner",
    preserve: Optional[bool] = False,
) -> Union[DataNode, List[DataNode]]:
    """

    The remap operation applies a generic geometrical transformation to an image. In other words,
    it takes pixels from one place in the input image and puts them in another place in
    the output image. The transformation is described by ``mapx`` and ``mapy`` parameters, where:

        output(x,y) = input(mapx(x,y),mapy(x,y))

    The type of the output tensor will match the type of the input tensor.

    Handles only HWC layout.

    Currently picking border policy is not supported.
    The ``DALIBorderType`` will always be ``CONSTANT`` with the value ``0``.


    This operator allows sequence inputs.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HWC', 'FHWC')
        Input data. Must be a 1- or 3-channel HWC image.
    mapx : TensorList of float ('HWC', 'HW', 'FHWC', 'FHW', 'F***', 'F**')
        Defines the remap transformation for x coordinates.
    mapy : TensorList of float ('HWC', 'HW', 'FHWC', 'FHW', 'F***', 'F**')
        Defines the remap transformation for y coordinates.


    Keyword args
    ------------
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    interp : :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Interpolation type.
    pixel_origin : str, optional, default = `'corner'`

        Pixel origin. Possible values: ``"corner"``, ``"center"``.

        Defines which part of the pixel (upper-left corner or center) is interpreted as its origin.
        This value impacts the interpolation result. To match OpenCV, please pick ``"center"``.
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.

    """
    ...

@overload
def resize(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    antialias: Optional[bool] = True,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    interp_type: Union[
        DataNode, TensorLikeArg, DALIInterpType, None
    ] = DALIInterpType.INTERP_LINEAR,
    mag_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    max_size: Union[Sequence[float], float, None] = None,
    min_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    minibatch_size: Optional[int] = 32,
    mode: Optional[str] = "default",
    preserve: Optional[bool] = False,
    resize_longer: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_shorter: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_x: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_y: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_z: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    roi_end: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    roi_relative: Optional[bool] = False,
    roi_start: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    save_attrs: Optional[bool] = False,
    size: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    subpixel_scale: Optional[bool] = True,
    temp_buffer_hint: Optional[int] = 0,
) -> Union[DataNode, Sequence[DataNode], None]:
    """
    Resize images.

    This operator allows sequence inputs and supports volumetric data.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HWC', 'FHWC', 'CHW', 'FCHW', 'CFHW', 'DHWC', 'FDHWC', 'CDHW', 'FCDHW', 'CFDHW')
        Input to the operator.


    Keyword args
    ------------
    antialias : bool, optional, default = `True`
        If enabled, it applies an antialiasing filter when scaling down.

        .. note::
          Nearest neighbor interpolation does not support antialiasing.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional
        Output data type.

        Must be same as input type or ``float``. If not set, input type is used.
    image_type : :class:`nvidia.dali.types.DALIImageType`
        .. warning::

            The argument `image_type` is no longer used and will be removed in a future release.
    interp_type : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Type of interpolation to be used.

        Use `min_filter` and `mag_filter` to specify different filtering for downscaling and upscaling.

        .. note::
          Usage of INTERP_TRIANGULAR is now deprecated and it should be replaced by a combination of
        INTERP_LINEAR with `antialias` enabled.
    mag_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling up.
    max_size : float or list of float, optional
        Limit of the output size.

        When the operator is configured to keep aspect ratio and only the smaller dimension is specified,
        the other(s) can grow very large. This can happen when using `resize_shorter` argument
        or "not_smaller" mode or when some extents are left unspecified.

        This parameter puts a limit to how big the output can become. This value can be specified per-axis
        or uniformly for all axes.

        .. note::
          When used with "not_smaller" mode or `resize_shorter` argument, `max_size` takes
          precedence and the aspect ratio is kept - for example, resizing with
          ``mode="not_smaller", size=800, max_size=1400`` an image of size 1200x600 would be resized to
          1400x700.
    min_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling down.
    minibatch_size : int, optional, default = `32`
        Maximum number of images that are processed in
        a kernel call.
    mode : str, optional, default = `'default'`
        Resize mode.

          Here is a list of supported modes:

          * | ``"default"`` - image is resized to the specified size.
            | Missing extents are scaled with the average scale of the provided ones.
          * | ``"stretch"`` - image is resized to the specified size.
            | Missing extents are not scaled at all.
          * | ``"not_larger"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image exceeds the specified size.
            | For example, a 1280x720, with a desired output size of 640x480, actually produces
              a 640x360 output.
          * | ``"not_smaller"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image is smaller than specified.
            | For example, a 640x480 image with a desired output size of 1920x1080, actually produces
              a 1920x1440 output.

            This argument is mutually exclusive with `resize_longer` and `resize_shorter`
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    resize_longer : float or TensorList of float, optional, default = `0.0`
        The length of the longer dimension of the resized image.

        This option is mutually exclusive with `resize_shorter` and explicit size arguments, and
        the operator keeps the aspect ratio of the original image.
        This option is equivalent to specifying the same size for all dimensions and ``mode="not_larger"``.
    resize_shorter : float or TensorList of float, optional, default = `0.0`
        The length of the shorter dimension of the resized image.

        This option is mutually exclusive with `resize_longer` and explicit size arguments, and
        the operator keeps the aspect ratio of the original image.
        This option is equivalent to specifying the same size for all dimensions and ``mode="not_smaller"``.
        The longer dimension can be bounded by setting the `max_size` argument.
        See `max_size` argument doc for more info.
    resize_x : float or TensorList of float, optional, default = `0.0`
        The length of the X dimension of the resized image.

        This option is mutually exclusive with `resize_shorter`, `resize_longer` and `size`.
        If the `resize_y` is unspecified or 0, the operator keeps the aspect ratio of the original image.
        A negative value flips the image.
    resize_y : float or TensorList of float, optional, default = `0.0`
        The length of the Y dimension of the resized image.

        This option is mutually exclusive with `resize_shorter`, `resize_longer` and `size`.
        If the `resize_x` is unspecified or 0, the operator keeps the aspect ratio of the original image.
        A negative value flips the image.
    resize_z : float or TensorList of float, optional, default = `0.0`
        The length of the Z dimension of the resized volume.

        This option is mutually exclusive with `resize_shorter`, `resize_longer` and `size`.
        If the `resize_x` and `resize_y` are left unspecified or 0, then the op will keep
        the aspect ratio of the original volume. Negative value flips the volume.
    roi_end : float or list of float or TensorList of float, optional
        End of the input region of interest (ROI).

        Must be specified together with `roi_start`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    roi_relative : bool, optional, default = `False`
        If true, ROI coordinates are relative to the input size,
        where 0 denotes top/left and 1 denotes bottom/right
    roi_start : float or list of float or TensorList of float, optional
        Origin of the input region of interest (ROI).

        Must be specified together with `roi_end`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    save_attrs : bool, optional, default = `False`
        Save reshape attributes for testing.
    size : float or list of float or TensorList of float, optional
        The desired output size.

        Must be a list/tuple with one entry per spatial dimension, excluding video frames and channels.
        Dimensions with a 0 extent are treated as absent, and the output size will be calculated based on
        other extents and `mode` argument.
    subpixel_scale : bool, optional, default = `True`
        If True, fractional sizes, directly specified or
        calculated, will cause the input ROI to be adjusted to keep the scale factor.

        Otherwise, the scale factor will be adjusted so that the source image maps to
        the rounded output size.
    temp_buffer_hint : int, optional, default = `0`
        Initial size in bytes, of a temporary buffer for resampling.

        .. note::
          This argument is ignored for the CPU variant.

    """
    ...

@overload
def resize(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["gpu"]] = None,
    name: Optional[str] = None,
    antialias: Optional[bool] = True,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    interp_type: Union[
        DataNode, TensorLikeArg, DALIInterpType, None
    ] = DALIInterpType.INTERP_LINEAR,
    mag_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    max_size: Union[Sequence[float], float, None] = None,
    min_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    minibatch_size: Optional[int] = 32,
    mode: Optional[str] = "default",
    preserve: Optional[bool] = False,
    resize_longer: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_shorter: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_x: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_y: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    resize_z: Union[DataNode, TensorLikeArg, float, None] = 0.0,
    roi_end: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    roi_relative: Optional[bool] = False,
    roi_start: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    save_attrs: Optional[bool] = False,
    size: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    subpixel_scale: Optional[bool] = True,
    temp_buffer_hint: Optional[int] = 0,
) -> Union[DataNode, Sequence[DataNode], List[DataNode], List[Sequence[DataNode]], None]:
    """
    Resize images.

    This operator allows sequence inputs and supports volumetric data.

    Supported backends
     * 'gpu'


    Args
    ----
    input : TensorList ('HWC', 'FHWC', 'CHW', 'FCHW', 'CFHW', 'DHWC', 'FDHWC', 'CDHW', 'FCDHW', 'CFDHW')
        Input to the operator.


    Keyword args
    ------------
    antialias : bool, optional, default = `True`
        If enabled, it applies an antialiasing filter when scaling down.

        .. note::
          Nearest neighbor interpolation does not support antialiasing.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional
        Output data type.

        Must be same as input type or ``float``. If not set, input type is used.
    image_type : :class:`nvidia.dali.types.DALIImageType`
        .. warning::

            The argument `image_type` is no longer used and will be removed in a future release.
    interp_type : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Type of interpolation to be used.

        Use `min_filter` and `mag_filter` to specify different filtering for downscaling and upscaling.

        .. note::
          Usage of INTERP_TRIANGULAR is now deprecated and it should be replaced by a combination of
        INTERP_LINEAR with `antialias` enabled.
    mag_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling up.
    max_size : float or list of float, optional
        Limit of the output size.

        When the operator is configured to keep aspect ratio and only the smaller dimension is specified,
        the other(s) can grow very large. This can happen when using `resize_shorter` argument
        or "not_smaller" mode or when some extents are left unspecified.

        This parameter puts a limit to how big the output can become. This value can be specified per-axis
        or uniformly for all axes.

        .. note::
          When used with "not_smaller" mode or `resize_shorter` argument, `max_size` takes
          precedence and the aspect ratio is kept - for example, resizing with
          ``mode="not_smaller", size=800, max_size=1400`` an image of size 1200x600 would be resized to
          1400x700.
    min_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling down.
    minibatch_size : int, optional, default = `32`
        Maximum number of images that are processed in
        a kernel call.
    mode : str, optional, default = `'default'`
        Resize mode.

          Here is a list of supported modes:

          * | ``"default"`` - image is resized to the specified size.
            | Missing extents are scaled with the average scale of the provided ones.
          * | ``"stretch"`` - image is resized to the specified size.
            | Missing extents are not scaled at all.
          * | ``"not_larger"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image exceeds the specified size.
            | For example, a 1280x720, with a desired output size of 640x480, actually produces
              a 640x360 output.
          * | ``"not_smaller"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image is smaller than specified.
            | For example, a 640x480 image with a desired output size of 1920x1080, actually produces
              a 1920x1440 output.

            This argument is mutually exclusive with `resize_longer` and `resize_shorter`
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    resize_longer : float or TensorList of float, optional, default = `0.0`
        The length of the longer dimension of the resized image.

        This option is mutually exclusive with `resize_shorter` and explicit size arguments, and
        the operator keeps the aspect ratio of the original image.
        This option is equivalent to specifying the same size for all dimensions and ``mode="not_larger"``.
    resize_shorter : float or TensorList of float, optional, default = `0.0`
        The length of the shorter dimension of the resized image.

        This option is mutually exclusive with `resize_longer` and explicit size arguments, and
        the operator keeps the aspect ratio of the original image.
        This option is equivalent to specifying the same size for all dimensions and ``mode="not_smaller"``.
        The longer dimension can be bounded by setting the `max_size` argument.
        See `max_size` argument doc for more info.
    resize_x : float or TensorList of float, optional, default = `0.0`
        The length of the X dimension of the resized image.

        This option is mutually exclusive with `resize_shorter`, `resize_longer` and `size`.
        If the `resize_y` is unspecified or 0, the operator keeps the aspect ratio of the original image.
        A negative value flips the image.
    resize_y : float or TensorList of float, optional, default = `0.0`
        The length of the Y dimension of the resized image.

        This option is mutually exclusive with `resize_shorter`, `resize_longer` and `size`.
        If the `resize_x` is unspecified or 0, the operator keeps the aspect ratio of the original image.
        A negative value flips the image.
    resize_z : float or TensorList of float, optional, default = `0.0`
        The length of the Z dimension of the resized volume.

        This option is mutually exclusive with `resize_shorter`, `resize_longer` and `size`.
        If the `resize_x` and `resize_y` are left unspecified or 0, then the op will keep
        the aspect ratio of the original volume. Negative value flips the volume.
    roi_end : float or list of float or TensorList of float, optional
        End of the input region of interest (ROI).

        Must be specified together with `roi_start`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    roi_relative : bool, optional, default = `False`
        If true, ROI coordinates are relative to the input size,
        where 0 denotes top/left and 1 denotes bottom/right
    roi_start : float or list of float or TensorList of float, optional
        Origin of the input region of interest (ROI).

        Must be specified together with `roi_end`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    save_attrs : bool, optional, default = `False`
        Save reshape attributes for testing.
    size : float or list of float or TensorList of float, optional
        The desired output size.

        Must be a list/tuple with one entry per spatial dimension, excluding video frames and channels.
        Dimensions with a 0 extent are treated as absent, and the output size will be calculated based on
        other extents and `mode` argument.
    subpixel_scale : bool, optional, default = `True`
        If True, fractional sizes, directly specified or
        calculated, will cause the input ROI to be adjusted to keep the scale factor.

        Otherwise, the scale factor will be adjusted so that the source image maps to
        the rounded output size.
    temp_buffer_hint : int, optional, default = `0`
        Initial size in bytes, of a temporary buffer for resampling.

        .. note::
          This argument is ignored for the CPU variant.

    """
    ...

@overload
def tensor_resize(
    input: Union[DataNode, TensorLikeIn],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    alignment: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = [0.5],
    antialias: Optional[bool] = True,
    axes: Union[Sequence[int], int, None] = None,
    axis_names: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    interp_type: Union[
        DataNode, TensorLikeArg, DALIInterpType, None
    ] = DALIInterpType.INTERP_LINEAR,
    mag_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    max_size: Union[Sequence[float], float, None] = None,
    min_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    minibatch_size: Optional[int] = 32,
    mode: Optional[str] = "default",
    preserve: Optional[bool] = False,
    roi_end: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    roi_relative: Optional[bool] = False,
    roi_start: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    scales: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    size_rounding: Optional[str] = "round",
    sizes: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    subpixel_scale: Optional[bool] = True,
    temp_buffer_hint: Optional[int] = 0,
) -> DataNode:
    """
    Resize tensors.

    This operator allows sequence inputs and supports volumetric data.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList
        Input to the operator.


    Keyword args
    ------------
    alignment : float or list of float or TensorList of float, optional, default = `[0.5]`
        Determines the position of the ROI
        when using scales (provided or calculated).

        The real output size must be integral and may differ from "ideal" output size calculated as input
        (or ROI) size multiplied by the scale factor. In that case, the output size is rounded (according
        to `size_rounding` policy) and the input ROI needs to be adjusted to maintain the scale factor.
        This parameter defines which relative point of the ROI should retain its position in the output.

        This point is calculated as ``center = (1 - alignment) * roi_start + alignment * roi_end``.
        Alignment 0.0 denotes alignment with the start of the ROI, 0.5 with the center of the region, and 1.0 with the end.
        Note that when ROI is not specified, roi_start=0 and roi_end=input_size is assumed.

        When using 0.5 (default), the resize operation has flip invariant properties (flipping after resizing is
        mathematically equivalent to resizing after flipping).

        The value of this argument contains as many elements as dimensions provided for
        sizes/scales. If only one value is provided, it is applied to all dimensions.
    antialias : bool, optional, default = `True`
        If enabled, it applies an antialiasing filter when scaling down.

        .. note::
          Nearest neighbor interpolation does not support antialiasing.
    axes : int or list of int, optional
        Indices of dimensions that `sizes`, `scales`, `max_size`, `roi_start`, `roi_end` refer to.

        Accepted range is [-ndim, ndim-1]. Negative indices are counted from the back.

        By default, all dimensions are assumed. The `axis_names` and `axes` arguments are mutually exclusive.
    axis_names : :ref:`layout str<layout_str_doc>`, optional
        Names of the axes that `sizes`, `scales`, `max_size`, `roi_start`, `roi_end` refer to.

        By default, all dimensions are assumed. The `axis_names` and `axes` arguments are mutually exclusive.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional
        Output data type.

        Must be same as input type or ``float``. If not set, input type is used.
    interp_type : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Type of interpolation to be used.

        Use `min_filter` and `mag_filter` to specify different filtering for downscaling and upscaling.

        .. note::
          Usage of INTERP_TRIANGULAR is now deprecated and it should be replaced by a combination of
        INTERP_LINEAR with `antialias` enabled.
    mag_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling up.
    max_size : float or list of float, optional
        Limit of the output size.

        When the operator is configured to keep aspect ratio and only the smaller dimension is specified,
        the other(s) can grow very large. This can happen when using `resize_shorter` argument
        or "not_smaller" mode or when some extents are left unspecified.

        This parameter puts a limit to how big the output can become. This value can be specified per-axis
        or uniformly for all axes.

        .. note::
          When used with "not_smaller" mode or `resize_shorter` argument, `max_size` takes
          precedence and the aspect ratio is kept - for example, resizing with
          ``mode="not_smaller", size=800, max_size=1400`` an image of size 1200x600 would be resized to
          1400x700.
    min_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling down.
    minibatch_size : int, optional, default = `32`
        Maximum number of images that are processed in
        a kernel call.
    mode : str, optional, default = `'default'`
        Resize mode.

          Here is a list of supported modes:

          * | ``"default"`` - image is resized to the specified size.
            | Missing extents are scaled with the average scale of the provided ones.
          * | ``"stretch"`` - image is resized to the specified size.
            | Missing extents are not scaled at all.
          * | ``"not_larger"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image exceeds the specified size.
            | For example, a 1280x720, with a desired output size of 640x480, actually produces
              a 640x360 output.
          * | ``"not_smaller"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image is smaller than specified.
            | For example, a 640x480 image with a desired output size of 1920x1080, actually produces
              a 1920x1440 output.

            This argument is mutually exclusive with `resize_longer` and `resize_shorter`
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    roi_end : float or list of float or TensorList of float, optional
        End of the input region of interest (ROI).

        Must be specified together with `roi_start`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    roi_relative : bool, optional, default = `False`
        If true, ROI coordinates are relative to the input size,
        where 0 denotes top/left and 1 denotes bottom/right
    roi_start : float or list of float or TensorList of float, optional
        Origin of the input region of interest (ROI).

        Must be specified together with `roi_end`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    scales : float or list of float or TensorList of float, optional
        Scale factors.

        The resulting output size is calculated as
        ``out_size = size_rounding(scale_factor * original_size)``.
        See `size_rounding` for a list of supported rounding policies.

        When `axes` is provided, the scale factor values refer to the axes specified.
        Note: Arguments `sizes` and `scales` are mutually exclusive.
    size_rounding : str, optional, default = `'round'`
        Determines the rounding policy when using scales.

        Possible values are:
        * | ``"round"`` - Rounds the resulting size to the nearest integer value, with halfway cases rounded away from zero.
        * | ``"truncate"`` - Discards the fractional part of the resulting size.
        * | ``"ceil"`` - Rounds up the resulting size to the next integer value.
    sizes : float or list of float or TensorList of float, optional
        Output sizes.

        When `axes` is provided, the size values refer to the axes specified.
        Note: Arguments `sizes` and `scales` are mutually exclusive.
    subpixel_scale : bool, optional, default = `True`
        If True, fractional sizes, directly specified or
        calculated, will cause the input ROI to be adjusted to keep the scale factor.

        Otherwise, the scale factor will be adjusted so that the source image maps to
        the rounded output size.
    temp_buffer_hint : int, optional, default = `0`
        Initial size in bytes, of a temporary buffer for resampling.

        .. note::
          This argument is ignored for the CPU variant.

    """
    ...

@overload
def tensor_resize(
    input: List[DataNode],
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    alignment: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = [0.5],
    antialias: Optional[bool] = True,
    axes: Union[Sequence[int], int, None] = None,
    axis_names: Optional[str] = None,
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    dtype: Optional[DALIDataType] = None,
    interp_type: Union[
        DataNode, TensorLikeArg, DALIInterpType, None
    ] = DALIInterpType.INTERP_LINEAR,
    mag_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    max_size: Union[Sequence[float], float, None] = None,
    min_filter: Union[DataNode, TensorLikeArg, DALIInterpType, None] = DALIInterpType.INTERP_LINEAR,
    minibatch_size: Optional[int] = 32,
    mode: Optional[str] = "default",
    preserve: Optional[bool] = False,
    roi_end: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    roi_relative: Optional[bool] = False,
    roi_start: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    scales: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    size_rounding: Optional[str] = "round",
    sizes: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = None,
    subpixel_scale: Optional[bool] = True,
    temp_buffer_hint: Optional[int] = 0,
) -> Union[DataNode, List[DataNode]]:
    """
    Resize tensors.

    This operator allows sequence inputs and supports volumetric data.

    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList
        Input to the operator.


    Keyword args
    ------------
    alignment : float or list of float or TensorList of float, optional, default = `[0.5]`
        Determines the position of the ROI
        when using scales (provided or calculated).

        The real output size must be integral and may differ from "ideal" output size calculated as input
        (or ROI) size multiplied by the scale factor. In that case, the output size is rounded (according
        to `size_rounding` policy) and the input ROI needs to be adjusted to maintain the scale factor.
        This parameter defines which relative point of the ROI should retain its position in the output.

        This point is calculated as ``center = (1 - alignment) * roi_start + alignment * roi_end``.
        Alignment 0.0 denotes alignment with the start of the ROI, 0.5 with the center of the region, and 1.0 with the end.
        Note that when ROI is not specified, roi_start=0 and roi_end=input_size is assumed.

        When using 0.5 (default), the resize operation has flip invariant properties (flipping after resizing is
        mathematically equivalent to resizing after flipping).

        The value of this argument contains as many elements as dimensions provided for
        sizes/scales. If only one value is provided, it is applied to all dimensions.
    antialias : bool, optional, default = `True`
        If enabled, it applies an antialiasing filter when scaling down.

        .. note::
          Nearest neighbor interpolation does not support antialiasing.
    axes : int or list of int, optional
        Indices of dimensions that `sizes`, `scales`, `max_size`, `roi_start`, `roi_end` refer to.

        Accepted range is [-ndim, ndim-1]. Negative indices are counted from the back.

        By default, all dimensions are assumed. The `axis_names` and `axes` arguments are mutually exclusive.
    axis_names : :ref:`layout str<layout_str_doc>`, optional
        Names of the axes that `sizes`, `scales`, `max_size`, `roi_start`, `roi_end` refer to.

        By default, all dimensions are assumed. The `axis_names` and `axes` arguments are mutually exclusive.
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    dtype : :class:`nvidia.dali.types.DALIDataType`, optional
        Output data type.

        Must be same as input type or ``float``. If not set, input type is used.
    interp_type : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Type of interpolation to be used.

        Use `min_filter` and `mag_filter` to specify different filtering for downscaling and upscaling.

        .. note::
          Usage of INTERP_TRIANGULAR is now deprecated and it should be replaced by a combination of
        INTERP_LINEAR with `antialias` enabled.
    mag_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling up.
    max_size : float or list of float, optional
        Limit of the output size.

        When the operator is configured to keep aspect ratio and only the smaller dimension is specified,
        the other(s) can grow very large. This can happen when using `resize_shorter` argument
        or "not_smaller" mode or when some extents are left unspecified.

        This parameter puts a limit to how big the output can become. This value can be specified per-axis
        or uniformly for all axes.

        .. note::
          When used with "not_smaller" mode or `resize_shorter` argument, `max_size` takes
          precedence and the aspect ratio is kept - for example, resizing with
          ``mode="not_smaller", size=800, max_size=1400`` an image of size 1200x600 would be resized to
          1400x700.
    min_filter : :class:`nvidia.dali.types.DALIInterpType` or TensorList of :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Filter used when scaling down.
    minibatch_size : int, optional, default = `32`
        Maximum number of images that are processed in
        a kernel call.
    mode : str, optional, default = `'default'`
        Resize mode.

          Here is a list of supported modes:

          * | ``"default"`` - image is resized to the specified size.
            | Missing extents are scaled with the average scale of the provided ones.
          * | ``"stretch"`` - image is resized to the specified size.
            | Missing extents are not scaled at all.
          * | ``"not_larger"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image exceeds the specified size.
            | For example, a 1280x720, with a desired output size of 640x480, actually produces
              a 640x360 output.
          * | ``"not_smaller"`` - image is resized, keeping the aspect ratio, so that no extent of the
              output image is smaller than specified.
            | For example, a 640x480 image with a desired output size of 1920x1080, actually produces
              a 1920x1440 output.

            This argument is mutually exclusive with `resize_longer` and `resize_shorter`
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    roi_end : float or list of float or TensorList of float, optional
        End of the input region of interest (ROI).

        Must be specified together with `roi_start`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    roi_relative : bool, optional, default = `False`
        If true, ROI coordinates are relative to the input size,
        where 0 denotes top/left and 1 denotes bottom/right
    roi_start : float or list of float or TensorList of float, optional
        Origin of the input region of interest (ROI).

        Must be specified together with `roi_end`. The coordinates follow the tensor shape order, which is
        the same as `size`. The coordinates can be either absolute (in pixels, which is the default) or
        relative (0..1), depending on the value of ``relative_roi`` argument. If the ROI origin is greater
        than the ROI end in any dimension, the region is flipped in that dimension.
    scales : float or list of float or TensorList of float, optional
        Scale factors.

        The resulting output size is calculated as
        ``out_size = size_rounding(scale_factor * original_size)``.
        See `size_rounding` for a list of supported rounding policies.

        When `axes` is provided, the scale factor values refer to the axes specified.
        Note: Arguments `sizes` and `scales` are mutually exclusive.
    size_rounding : str, optional, default = `'round'`
        Determines the rounding policy when using scales.

        Possible values are:
        * | ``"round"`` - Rounds the resulting size to the nearest integer value, with halfway cases rounded away from zero.
        * | ``"truncate"`` - Discards the fractional part of the resulting size.
        * | ``"ceil"`` - Rounds up the resulting size to the next integer value.
    sizes : float or list of float or TensorList of float, optional
        Output sizes.

        When `axes` is provided, the size values refer to the axes specified.
        Note: Arguments `sizes` and `scales` are mutually exclusive.
    subpixel_scale : bool, optional, default = `True`
        If True, fractional sizes, directly specified or
        calculated, will cause the input ROI to be adjusted to keep the scale factor.

        Otherwise, the scale factor will be adjusted so that the source image maps to
        the rounded output size.
    temp_buffer_hint : int, optional, default = `0`
        Initial size in bytes, of a temporary buffer for resampling.

        .. note::
          This argument is ignored for the CPU variant.

    """
    ...

@overload
def warp_perspective(
    input: Union[DataNode, TensorLikeIn],
    matrix_input: Union[DataNode, TensorLikeIn, None] = None,
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    border_mode: Optional[str] = "constant",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    fill_value: Union[Sequence[float], float, None] = [],
    interp_type: Optional[DALIInterpType] = DALIInterpType.INTERP_LINEAR,
    inverse_map: Optional[bool] = True,
    matrix: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = [],
    pixel_origin: Optional[str] = "corner",
    preserve: Optional[bool] = False,
    size: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = [],
) -> DataNode:
    """

    Performs a perspective transform on the images.


    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList of uint8, uint16, int16 or float ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.
    matrix_input : 2D TensorList of float, optional
        3x3 Perspective transform matrix for per sample homography, same device as input.


    Keyword args
    ------------
    border_mode : str, optional, default = `'constant'`
        Border mode to be used when accessing elements outside input image.
        Supported values are: "constant", "replicate", "reflect", "reflect_101", "wrap".
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    fill_value : float or list of float, optional, default = `[]`
        Value used to fill areas that are outside the source image when the "constant" border_mode is chosen.
    interp_type : :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Type of interpolation used.
    inverse_map : bool, optional, default = `True`
        If set to true (default), the matrix is interpreted as destination to source coordinates mapping. Otherwise it's interpreted as source to destination coordinates mapping.
    matrix : float or list of float or TensorList of float, optional, default = `[]`

          3x3 Perspective transform matrix of destination to source coordinates.
          If `inverse_map` argument is set to false, the matrix is interpreted
          as a source to destination coordinates mapping.

        It is equivalent to OpenCV's ``warpPerspective`` operation with the `inverse_map` argument being
        analog to the ``WARP_INVERSE_MAP`` flag.

        .. note::
          Instead of this argument, the operator can take a second positional input, in which
          case the matrix can be placed on the GPU.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    pixel_origin : str, optional, default = `'corner'`
        Pixel origin. Possible values: "corner", "center".

        Determines the meaning of (0, 0) coordinates - "corner" places the origin at the top-left corner of
        the top-left pixel (like in OpenGL); "center" places (0, 0) in the center of
        the top-left pixel (like in OpenCV).)
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    size : float or list of float or TensorList of float, optional, default = `[]`
        Output size, in pixels/points.

        The channel dimension should be excluded (for example, for RGB images,
        specify ``(480,640)``, not ``(480,640,3)``.

    """
    ...

@overload
def warp_perspective(
    input: Union[List[DataNode], DataNode, TensorLikeIn],
    matrix_input: Union[List[DataNode], DataNode, TensorLikeIn, None] = None,
    /,
    *,
    device: Optional[Literal["cpu", "gpu"]] = None,
    name: Optional[str] = None,
    border_mode: Optional[str] = "constant",
    bytes_per_sample_hint: Union[Sequence[int], int, None] = [0],
    fill_value: Union[Sequence[float], float, None] = [],
    interp_type: Optional[DALIInterpType] = DALIInterpType.INTERP_LINEAR,
    inverse_map: Optional[bool] = True,
    matrix: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = [],
    pixel_origin: Optional[str] = "corner",
    preserve: Optional[bool] = False,
    size: Union[DataNode, TensorLikeArg, Sequence[float], float, None] = [],
) -> Union[DataNode, List[DataNode]]:
    """

    Performs a perspective transform on the images.


    Supported backends
     * 'cpu'
     * 'gpu'


    Args
    ----
    input : TensorList of uint8, uint16, int16 or float ('HW', 'HWC', 'FHWC', 'CHW', 'FCHW')
        Input data. Must be images in HWC or CHW layout, or a sequence of those.
    matrix_input : 2D TensorList of float, optional
        3x3 Perspective transform matrix for per sample homography, same device as input.


    Keyword args
    ------------
    border_mode : str, optional, default = `'constant'`
        Border mode to be used when accessing elements outside input image.
        Supported values are: "constant", "replicate", "reflect", "reflect_101", "wrap".
    bytes_per_sample_hint : int or list of int, optional, default = `[0]`
        Output size hint, in bytes per sample.

        If specified, the operator's outputs residing in GPU or page-locked host memory will be preallocated
        to accommodate a batch of samples of this size.
    fill_value : float or list of float, optional, default = `[]`
        Value used to fill areas that are outside the source image when the "constant" border_mode is chosen.
    interp_type : :class:`nvidia.dali.types.DALIInterpType`, optional, default = `DALIInterpType.INTERP_LINEAR`
        Type of interpolation used.
    inverse_map : bool, optional, default = `True`
        If set to true (default), the matrix is interpreted as destination to source coordinates mapping. Otherwise it's interpreted as source to destination coordinates mapping.
    matrix : float or list of float or TensorList of float, optional, default = `[]`

          3x3 Perspective transform matrix of destination to source coordinates.
          If `inverse_map` argument is set to false, the matrix is interpreted
          as a source to destination coordinates mapping.

        It is equivalent to OpenCV's ``warpPerspective`` operation with the `inverse_map` argument being
        analog to the ``WARP_INVERSE_MAP`` flag.

        .. note::
          Instead of this argument, the operator can take a second positional input, in which
          case the matrix can be placed on the GPU.

        Supports :func:`per-frame<nvidia.dali.fn.per_frame>` inputs.
    pixel_origin : str, optional, default = `'corner'`
        Pixel origin. Possible values: "corner", "center".

        Determines the meaning of (0, 0) coordinates - "corner" places the origin at the top-left corner of
        the top-left pixel (like in OpenGL); "center" places (0, 0) in the center of
        the top-left pixel (like in OpenCV).)
    preserve : bool, optional, default = `False`
        Prevents the operator from being removed from the
        graph even if its outputs are not used.
    size : float or list of float or TensorList of float, optional, default = `[]`
        Output size, in pixels/points.

        The channel dimension should be excluded (for example, for RGB images,
        specify ``(480,640)``, not ``(480,640,3)``.

    """
    ...
