{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERA5 Flow Matching Pipeline End-to-End\n",
    "\n",
    "Welcome to the all-in-one WeatherFlow workflow! This notebook shows how to go from ERA5-style atmospheric data to a trained flow matching model, generate forecasts with an ODE solver, and visualise the resulting weather fields. The walkthrough is designed to surface the \"aha!\" moment where everything clicks together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "We'll work through the complete lifecycle:\n",
    "\n",
    "1. Configure the experiment for your set of ERA5 variables and pressure levels.\n",
    "2. Load data either from WeatherBench2/ERA5 or a fast synthetic fallback that mimics the structure of the real dataset.\n",
    "3. Build a `WeatherFlowMatch` vector field model and review its architecture.\n",
    "4. Train the model with `FlowTrainer`, monitoring flow matching and physics-aware losses.\n",
    "5. Wrap the trained model with `WeatherFlowODE` for inference at arbitrary lead times.\n",
    "6. Visualise true vs. predicted fields using the rich `WeatherVisualizer` toolkit.\n",
    "\n",
    "Each section includes compact code you can re-use in scripts or other notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the repository root to the Python path so imports resolve correctly\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "REPO_ROOT = (NOTEBOOK_DIR / '..').resolve()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "print(f'Repository root: {REPO_ROOT}')\n",
    "\n",
    "try:\n",
    "    import weatherflow\n",
    "    print(f'WeatherFlow version: {weatherflow.__version__}')\n",
    "except Exception as exc:\n",
    "    print(f'WeatherFlow import warning: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lightweight mocks for optional dependencies (cartopy, torchdiffeq) when missing\n",
    "try:\n",
    "    from mock_dependencies import install_all_mocks\n",
    "    install_all_mocks()\n",
    "except Exception as exc:\n",
    "    print(f'Mock dependency installation skipped: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for the workflow\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from weatherflow.data import ERA5Dataset, create_data_loaders\n",
    "except Exception as exc:\n",
    "    print(f'ERA5 data utilities unavailable: {exc}')\n",
    "    ERA5Dataset = None\n",
    "    create_data_loaders = None\n",
    "\n",
    "from weatherflow.models import WeatherFlowMatch, WeatherFlowODE\n",
    "from weatherflow.training import FlowTrainer\n",
    "\n",
    "try:\n",
    "    from weatherflow.utils import WeatherVisualizer\n",
    "    HAS_VISUALIZER = True\n",
    "except Exception as exc:\n",
    "    print(f'WeatherVisualizer import warning: {exc}')\n",
    "    WeatherVisualizer = None\n",
    "    HAS_VISUALIZER = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_device = torch.device(device)\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure the experiment\n",
    "\n",
    "Customise the configuration dictionary below to match the ERA5 variables, pressure levels, and training schedule you want to explore. The synthetic fallback shares the same interface so you can switch to real data without touching the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'variables': ['z', 't', 'u', 'v'],\n",
    "    'pressure_levels': [500, 300],\n",
    "    'resolution': (32, 64),\n",
    "    'train_samples': 96,\n",
    "    'val_samples': 24,\n",
    "    'batch_size': 8,\n",
    "    'hidden_dim': 160,\n",
    "    'n_layers': 4,\n",
    "    'use_attention': True,\n",
    "    'physics_informed': True,\n",
    "    'learning_rate': 3e-4,\n",
    "    'epochs': 4,\n",
    "    'loss_type': 'mse',\n",
    "    'use_synthetic_data': True,  # Flip to False to connect with actual ERA5/WeatherBench2 data\n",
    "    'era5_data_path': None,       # Optional local path or GCS URL when use_synthetic_data=False\n",
    "    'train_years': ('2016', '2016'),\n",
    "    'val_years': ('2017', '2017'),\n",
    "    'solver_method': 'dopri5',\n",
    "    'rtol': 1e-4,\n",
    "    'atol': 1e-4,\n",
    "    'visualize_variable': 'z'\n",
    "}\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ERA5-style data\n",
    "\n",
    "To make the notebook runnable anywhere, we provide a physically-inspired synthetic dataset that mirrors the shape and metadata of ERA5. When you have direct access to the WeatherBench2 ERA5 store (or a local subset), set `use_synthetic_data=False` and the same code path will wrap the real dataset into channel-first tensors expected by the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticERA5Dataset(Dataset):\n",
    "    \"\"\"Smooth, repeatable toy dataset that mimics ERA5 tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, num_steps, variables, pressure_levels, resolution=(32, 64), seed=0):\n",
    "        self.num_steps = int(num_steps)\n",
    "        self.variables = list(variables)\n",
    "        self.pressure_levels = list(pressure_levels)\n",
    "        self.resolution = tuple(resolution)\n",
    "        self.channels = len(self.variables) * len(self.pressure_levels)\n",
    "        self.generator = torch.Generator().manual_seed(int(seed))\n",
    "\n",
    "        lat_radians = torch.linspace(-math.pi / 2, math.pi / 2, self.resolution[0])\n",
    "        lon_radians = torch.linspace(0.0, 2 * math.pi, self.resolution[1])\n",
    "        self.lat_grid, self.lon_grid = torch.meshgrid(lat_radians, lon_radians, indexing='ij')\n",
    "        self.latitudes = torch.linspace(-90.0, 90.0, self.resolution[0])\n",
    "        self.longitudes = torch.linspace(0.0, 360.0, self.resolution[1])\n",
    "\n",
    "        self._states = self._generate_sequence()\n",
    "\n",
    "    def _base_wave(self, var_idx, level_idx, phase):\n",
    "        lat_component = torch.sin(self.lat_grid * (level_idx + 1))\n",
    "        lon_component = torch.cos(self.lon_grid * (var_idx + 1))\n",
    "        standing_wave = torch.sin(self.lon_grid * 0.5 + phase) * torch.cos(self.lat_grid + 0.5 * phase * (level_idx + 1))\n",
    "        rotational = torch.sin(self.lat_grid * 0.3 + phase) * torch.sin(self.lon_grid * 0.7 - phase)\n",
    "        amplitude = 1.0 + 0.25 * level_idx + 0.35 * var_idx\n",
    "        return amplitude * (lat_component + lon_component) + 0.6 * standing_wave + 0.1 * rotational\n",
    "\n",
    "    def _generate_state(self, step):\n",
    "        phase = 2 * math.pi * step / max(self.num_steps, 1)\n",
    "        channels = []\n",
    "        for v_idx, _ in enumerate(self.variables):\n",
    "            for l_idx, _ in enumerate(self.pressure_levels):\n",
    "                wave = self._base_wave(v_idx, l_idx, phase)\n",
    "                drift = torch.sin(self.lon_grid + phase) * torch.cos(self.lat_grid - 0.5 * phase)\n",
    "                noise = 0.02 * torch.randn(self.resolution, generator=self.generator)\n",
    "                channels.append(wave + 0.05 * step * drift / max(self.num_steps, 1) + noise)\n",
    "        return torch.stack(channels).float()\n",
    "\n",
    "    def _generate_sequence(self):\n",
    "        states = []\n",
    "        for step in range(self.num_steps + 1):\n",
    "            states.append(self._generate_state(step))\n",
    "        return torch.stack(states)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_state = self._states[idx]\n",
    "        target_state = self._states[idx + 1]\n",
    "        metadata = {\n",
    "            't0_index': idx,\n",
    "            't1_index': idx + 1,\n",
    "            'variables': self.variables,\n",
    "            'pressure_levels': self.pressure_levels\n",
    "        }\n",
    "        return {'input': input_state, 'target': target_state, 'metadata': metadata}\n",
    "\n",
    "    def get_coords(self):\n",
    "        return {\n",
    "            'latitude': self.latitudes.numpy(),\n",
    "            'longitude': self.longitudes.numpy()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_first_wrapper(base_dataset, variables, pressure_levels):\n",
    "    \"\"\"Flatten variable/level dimensions into channels for FlowTrainer.\"\"\"\n",
    "\n",
    "    class _WrappedDataset(Dataset):\n",
    "        def __init__(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            self.variables = list(variables)\n",
    "            self.pressure_levels = list(pressure_levels)\n",
    "            self.channels = len(self.variables) * len(self.pressure_levels)\n",
    "            coords = getattr(dataset, 'get_coords', lambda: None)() if hasattr(dataset, 'get_coords') else None\n",
    "            if coords:\n",
    "                self.latitudes = torch.tensor(coords['latitude'], dtype=torch.float32)\n",
    "                self.longitudes = torch.tensor(coords['longitude'], dtype=torch.float32)\n",
    "            else:\n",
    "                spatial_shape = getattr(dataset, 'shape', (len(self.variables), len(self.pressure_levels), 32, 64))\n",
    "                self.latitudes = torch.linspace(-90.0, 90.0, spatial_shape[-2])\n",
    "                self.longitudes = torch.linspace(0.0, 360.0, spatial_shape[-1])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sample = self.dataset[idx]\n",
    "            x0 = sample['input']\n",
    "            x1 = sample['target']\n",
    "            if x0.ndim == 4:\n",
    "                x0 = x0.view(-1, *x0.shape[-2:])\n",
    "            if x1.ndim == 4:\n",
    "                x1 = x1.view(-1, *x1.shape[-2:])\n",
    "            metadata = dict(sample.get('metadata', {}))\n",
    "            metadata.setdefault('variables', self.variables)\n",
    "            metadata.setdefault('pressure_levels', self.pressure_levels)\n",
    "            return {'input': x0.float(), 'target': x1.float(), 'metadata': metadata}\n",
    "\n",
    "        def get_coords(self):\n",
    "            return {\n",
    "                'latitude': self.latitudes.numpy(),\n",
    "                'longitude': self.longitudes.numpy()\n",
    "            }\n",
    "\n",
    "    return _WrappedDataset(base_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_samples(batch):\n",
    "    inputs = torch.stack([item['input'] for item in batch])\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    metadata = {}\n",
    "    for key in batch[0]['metadata']:\n",
    "        metadata[key] = [item['metadata'][key] for item in batch]\n",
    "    return {'input': inputs, 'target': targets, 'metadata': metadata}\n",
    "\n",
    "if config['use_synthetic_data']:\n",
    "    train_dataset = SyntheticERA5Dataset(\n",
    "        num_steps=config['train_samples'],\n",
    "        variables=config['variables'],\n",
    "        pressure_levels=config['pressure_levels'],\n",
    "        resolution=config['resolution'],\n",
    "        seed=42\n",
    "    )\n",
    "    val_dataset = SyntheticERA5Dataset(\n",
    "        num_steps=config['val_samples'],\n",
    "        variables=config['variables'],\n",
    "        pressure_levels=config['pressure_levels'],\n",
    "        resolution=config['resolution'],\n",
    "        seed=123\n",
    "    )\n",
    "else:\n",
    "    if ERA5Dataset is None:\n",
    "        raise RuntimeError('ERA5Dataset is unavailable. Install required dependencies or use synthetic data.')\n",
    "    train_base = ERA5Dataset(\n",
    "        variables=config['variables'],\n",
    "        pressure_levels=config['pressure_levels'],\n",
    "        data_path=config['era5_data_path'],\n",
    "        time_slice=config['train_years'],\n",
    "        normalize=True,\n",
    "        cache_data=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    val_base = ERA5Dataset(\n",
    "        variables=config['variables'],\n",
    "        pressure_levels=config['pressure_levels'],\n",
    "        data_path=config['era5_data_path'],\n",
    "        time_slice=config['val_years'],\n",
    "        normalize=True,\n",
    "        cache_data=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    train_dataset = channel_first_wrapper(train_base, config['variables'], config['pressure_levels'])\n",
    "    val_dataset = channel_first_wrapper(val_base, config['variables'], config['pressure_levels'])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_samples\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_samples\n",
    ")\n",
    "\n",
    "coords = train_dataset.get_coords() if hasattr(train_dataset, 'get_coords') else {'latitude': train_dataset.latitudes.numpy(), 'longitude': train_dataset.longitudes.numpy()}\n",
    "latitudes = torch.tensor(coords['latitude'], dtype=torch.float32)\n",
    "longitudes = torch.tensor(coords['longitude'], dtype=torch.float32)\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "input_channels = sample_batch[\"input\"].shape[1]\n",
    "grid_shape = sample_batch[\"input\"].shape[2:]\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f\"Batch tensor shape: {sample_batch[\"input\"].shape}\")\n",
    "print(f'Channels feeding the model: {input_channels}')\n",
    "print(f'Spatial grid: {grid_shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapped loaders now emit tensors with shape `[batch, channels, lat, lon]`, exactly what the convolutional backbone expects. Metadata (such as the time indices and pressure levels) is preserved for analysis and visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_channels(tensor, variables, pressure_levels):\n",
    "    vars_count = len(variables)\n",
    "    levels_count = len(pressure_levels)\n",
    "    if tensor.ndim == 3:\n",
    "        channels, lat, lon = tensor.shape\n",
    "        return tensor.view(vars_count, levels_count, lat, lon)\n",
    "    elif tensor.ndim == 4:\n",
    "        batch, channels, lat, lon = tensor.shape\n",
    "        return tensor.view(batch, vars_count, levels_count, lat, lon)\n",
    "    else:\n",
    "        raise ValueError('Expected a 3D or 4D tensor.')\n",
    "\n",
    "def describe_batch(batch):\n",
    "    print('Batch keys:', list(batch.keys()))\n",
    "    print('Input shape:', batch['input'].shape)\n",
    "    print('Target shape:', batch['target'].shape)\n",
    "    print('Metadata example:', {k: v[0] for k, v in batch['metadata'].items()})\n",
    "\n",
    "describe_batch(sample_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the flow matching model\n",
    "\n",
    "`WeatherFlowMatch` provides a spatio-temporal vector field with optional attention and physics-informed divergence control. We configure it with the number of input channels discovered above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WeatherFlowMatch(\n",
    "    input_channels=input_channels,\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    n_layers=config['n_layers'],\n",
    "    use_attention=config['use_attention'],\n",
    "    physics_informed=config['physics_informed']\n",
    ")\n",
    "model = model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(model)\n",
    "print(f'Total parameters: {total_params:,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train with flow matching objectives\n",
    "\n",
    "`FlowTrainer` handles the stochastic time sampling, automatic mixed precision (disabled here for clarity), and optional physics regularisation. The loop below records training and validation losses so you can inspect convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\ntrainer = FlowTrainer(\n    model=model,\n    optimizer=optimizer,\n    device=device,\n    use_amp=False,\n    physics_regularization=config['physics_informed'],\n    physics_lambda=0.1,\n    loss_type=config['loss_type']\n)\n\nhistory = {\n    'train_loss': [],\n    'train_flow_loss': [],\n    'val_loss': [],\n    'val_flow_loss': []\n}\n\nfor epoch in range(1, config['epochs'] + 1):\n    print(f\"\\nEpoch {epoch}/{config['epochs']}\")\n    train_metrics = trainer.train_epoch(train_loader)\n    val_metrics = trainer.validate(val_loader)\n\n    history[\"train_loss\"].append(train_metrics[\"loss\"])\n    history[\"train_flow_loss\"].append(train_metrics.get(\"flow_loss\", train_metrics[\"loss\"]))\n    history[\"val_loss\"].append(val_metrics[\"val_loss\"])\n    history[\"val_flow_loss\"].append(val_metrics.get(\"val_flow_loss\", val_metrics[\"val_loss\"]))\n\n    print(f\"Train loss: {train_metrics['loss']:.4f} | Flow: {train_metrics.get('flow_loss', train_metrics['loss']):.4f}\")\n    print(f\"Val loss:   {val_metrics['val_loss']:.4f} | Flow: {val_metrics.get('val_flow_loss', val_metrics['val_loss']):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_flow_loss'], label='Train flow loss')\n",
    "plt.plot(history['val_flow_loss'], label='Validation flow loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Flow matching convergence')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run inference with `WeatherFlowODE`\n",
    "\n",
    "Once the vector field is trained, the differentiable ODE solver can evaluate forecasts at any intermediate time. We'll request five lead times between the initial state and the six-hour target (t=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "ode_model = WeatherFlowODE(\n",
    "    flow_model=trainer.model,\n",
    "    solver_method=config['solver_method'],\n",
    "    rtol=config['rtol'],\n",
    "    atol=config['atol']\n",
    ")\n",
    "\n",
    "val_iter = iter(val_loader)\n",
    "batch_for_eval = next(val_iter)\n",
    "x0 = batch_for_eval['input'].to(device)\n",
    "x1 = batch_for_eval['target']\n",
    "lead_times = torch.linspace(0.0, 1.0, steps=5, device=torch_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    trajectory = ode_model(x0, lead_times)\n",
    "\n",
    "forecast = trajectory[-1].cpu()\n",
    "truth = x1\n",
    "mse_per_sample = torch.mean((forecast - truth) ** 2, dim=(1, 2, 3))\n",
    "\n",
    "print(f'Trajectory shape: {trajectory.shape}')\n",
    "print(f'MSE per sample: {mse_per_sample.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualise predictions versus truth\n",
    "\n",
    "With the forecasts in hand, we can inspect individual variables and pressure levels. The `WeatherVisualizer` offers map projections and difference plots; when it isn't available the notebook falls back to standard Matplotlib heatmaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = config['visualize_variable']\n",
    "var_index = config['variables'].index(var_name)\n",
    "level_index = 0\n",
    "truth_fields = unflatten_channels(truth[0], config['variables'], config['pressure_levels'])\n",
    "forecast_fields = unflatten_channels(forecast[0], config['variables'], config['pressure_levels'])\n",
    "true_slice = truth_fields[var_index, level_index]\n",
    "pred_slice = forecast_fields[var_index, level_index]\n",
    "\n",
    "if HAS_VISUALIZER and WeatherVisualizer is not None:\n",
    "    visualizer = WeatherVisualizer(figsize=(16, 6))\n",
    "    try:\n",
    "        fig, axes = visualizer.plot_comparison(\n",
    "            true_data={var_name: true_slice},\n",
    "            pred_data={var_name: pred_slice},\n",
    "            var_name=var_name,\n",
    "            level_idx=0,\n",
    "            title=f\"{var_name} comparison at {config['pressure_levels'][level_index]} hPa\"\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(f'WeatherVisualizer plotting failed ({exc}). Falling back to Matplotlib.')\n",
    "        HAS_VISUALIZER = False\n",
    "\n",
    "if not HAS_VISUALIZER or WeatherVisualizer is None:\n",
    "    lon_vals = longitudes.numpy() if isinstance(longitudes, torch.Tensor) else np.asarray(longitudes)\n",
    "    lat_vals = latitudes.numpy() if isinstance(latitudes, torch.Tensor) else np.asarray(latitudes)\n",
    "    extent = [lon_vals.min(), lon_vals.max(), lat_vals.min(), lat_vals.max()]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    im0 = axes[0].imshow(true_slice.numpy(), extent=extent, origin='lower', aspect='auto', cmap='viridis')\n",
    "    axes[0].set_title('Truth')\n",
    "    fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    im1 = axes[1].imshow(pred_slice.numpy(), extent=extent, origin='lower', aspect='auto', cmap='viridis')\n",
    "    axes[1].set_title('Prediction')\n",
    "    fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    diff = (pred_slice - true_slice).numpy()\n",
    "    im2 = axes[2].imshow(diff, extent=extent, origin='lower', aspect='auto', cmap='RdBu_r')\n",
    "    axes[2].set_title('Difference')\n",
    "    fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "\n",
    "    plt.suptitle(f\"{var_name} comparison at {config['pressure_levels'][level_index]} hPa\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Where to go next\n",
    "\n",
    "* Swap in the real ERA5 dataset by setting `use_synthetic_data=False` and pointing `era5_data_path` to a WeatherBench2 store or local Zarr.\n",
    "* Increase the resolution or add humidity variables to stress-test the architecture.\n",
    "* Integrate the training history with Weights & Biases or TensorBoard for richer monitoring.\n",
    "* Extend the visual analysis with the WeatherFlow dashboard (`frontend/`) or the animation utilities in `weatherflow.utils.visualization`.\n",
    "\n",
    "Happy forecasting!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}