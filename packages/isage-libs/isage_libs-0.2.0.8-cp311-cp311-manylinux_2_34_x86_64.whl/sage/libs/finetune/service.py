#!/usr/bin/env python3
"""
Finetune CLI - Service Management
æœåŠ¡ç®¡ç†ï¼šè®­ç»ƒæ‰§è¡Œã€æ¨¡å‹åˆå¹¶

Note: vLLM æœåŠ¡åŠŸèƒ½å·²ç§»è‡³ç»Ÿä¸€çš„ sage.llm.LLMService
      è¯·ä½¿ç”¨ `sage llm serve` å‘½ä»¤å¯åŠ¨æ¨¡å‹æœåŠ¡
"""

import json
import subprocess
from pathlib import Path

from rich.console import Console

console = Console()


def start_training(config_path: Path, use_native: bool = True):
    """å¯åŠ¨è®­ç»ƒè¿‡ç¨‹

    Args:
        config_path: è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
        use_native: æ˜¯å¦ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒæ¨¡å—ï¼ˆæ¨èï¼‰

    Raises:
        FileNotFoundError: å½“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶
    """
    # Validate config file exists
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    try:
        if use_native:
            # ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒæ¨¡å—
            console.print("[cyan]ä½¿ç”¨ SAGE åŸç”Ÿè®­ç»ƒæ¨¡å—[/cyan]\n")

            # å¯¼å…¥è®­ç»ƒæ¨¡å—
            from sage.libs.finetune.trainer import train_from_meta

            # è¯»å–é…ç½®æ–‡ä»¶è·å–è¾“å‡ºç›®å½•
            with open(config_path) as f:
                config = json.load(f)

            # output_dir æ˜¯ checkpoints ç›®å½•ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯å…¶çˆ¶ç›®å½•
            checkpoint_dir = Path(config.get("output_dir", "finetune_output"))
            if checkpoint_dir.name == "checkpoints":
                output_dir = checkpoint_dir.parent
            else:
                output_dir = checkpoint_dir

            # æ‰§è¡Œè®­ç»ƒ
            train_from_meta(output_dir)

        else:
            # å°è¯•ä½¿ç”¨ LLaMA-Factory (å¯èƒ½ä¸å…¼å®¹)
            cmd = ["llamafactory-cli", "train", str(config_path)]
            console.print("[yellow]âš ï¸  ä½¿ç”¨ LLaMA-Factory (å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜)[/yellow]")
            console.print(f"[cyan]æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}[/cyan]\n")

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
            )

            for line in process.stdout:
                console.print(line, end="")

            process.wait()

            if process.returncode == 0:
                console.print("\n[green]âœ… è®­ç»ƒå®Œæˆï¼[/green]")
            else:
                console.print(f"\n[red]âŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {process.returncode}[/red]")

    except ImportError as e:
        console.print(f"[red]âŒ å¯¼å…¥é”™è¯¯: {e}[/red]")
        console.print("[yellow]æç¤º:[/yellow]")
        console.print(
            "  â€¢ ç¡®ä¿å·²å®‰è£…å¾®è°ƒä¾èµ–: [cyan]pip install -e packages/sage-libs[finetune][/cyan]"
        )
    except FileNotFoundError as e:
        # Only catch FileNotFoundError for commands, not for config file
        if "config" not in str(e).lower():
            console.print(f"[red]âŒ æ‰¾ä¸åˆ°å‘½ä»¤: {e}[/red]")
            console.print("[yellow]æç¤º:[/yellow]")
            console.print("  â€¢ ä½¿ç”¨ SAGE åŸç”Ÿè„šæœ¬ (æ¨è): [cyan]--use-native[/cyan]")
            console.print("  â€¢ æˆ–å®‰è£… LLaMA-Factory: [cyan]pip install llmtuner[/cyan]")
        else:
            raise


def merge_lora_weights(checkpoint_path: Path, base_model: str, output_path: Path) -> bool:
    """åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹

    Args:
        checkpoint_path: LoRA checkpoint è·¯å¾„
        base_model: åŸºç¡€æ¨¡å‹åç§°
        output_path: è¾“å‡ºè·¯å¾„

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        from peft import PeftModel
        from transformers import AutoModelForCausalLM, AutoTokenizer
    except ImportError:
        console.print("[red]âŒ ç¼ºå°‘ä¾èµ–ï¼Œè¯·å®‰è£…:[/red]")
        console.print("[cyan]pip install transformers peft[/cyan]")
        return False

    try:
        console.print("[cyan]â³ åŠ è½½åŸºç¡€æ¨¡å‹...[/cyan]")
        base = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype="auto",
            device_map="cpu",  # åœ¨CPUä¸Šåˆå¹¶ï¼ŒèŠ‚çœæ˜¾å­˜
        )

        console.print("[cyan]â³ åŠ è½½ LoRA æƒé‡...[/cyan]")
        model = PeftModel.from_pretrained(base, str(checkpoint_path))

        console.print("[cyan]â³ åˆå¹¶æƒé‡...[/cyan]")
        merged_model = model.merge_and_unload()  # type: ignore[operator]

        console.print("[cyan]â³ ä¿å­˜åˆå¹¶æ¨¡å‹...[/cyan]")
        merged_model.save_pretrained(str(output_path))

        # ä¿å­˜tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        tokenizer.save_pretrained(str(output_path))

        console.print("\n[green]âœ… åˆå¹¶å®Œæˆï¼[/green]")
        console.print(f"ğŸ“ åˆå¹¶æ¨¡å‹å·²ä¿å­˜åˆ°: [cyan]{output_path}[/cyan]")
        return True

    except Exception as e:
        console.print(f"[red]âŒ åˆå¹¶å¤±è´¥: {e}[/red]")
        return False
