Metadata-Version: 2.4
Name: scalexi_llm
Version: 0.1.11
Summary: A comprehensive multi-provider LLM proxy library with unified interface
Home-page: https://github.com/scalexi/scalexi_llm
Author: scalex_innovation
Author-email: scalex_innovation@gmail.com
Keywords: llm,ai,openai,anthropic,gemini,groq,deepseek,grok,qwen,ollama,exa,proxy,api
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: openai
Requires-Dist: anthropic
Requires-Dist: google-genai
Requires-Dist: groq
Requires-Dist: pymupdf
Requires-Dist: xai-sdk
Requires-Dist: python-docx
Requires-Dist: pydantic
Requires-Dist: python-dotenv
Requires-Dist: exa-py
Requires-Dist: google-search-results
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ScaleXI LLM

A production-ready, multi-provider LLM proxy that gives you **one unified API** for many different model providers.

- **9 Providers**: OpenAI, Anthropic (Claude), Google (Gemini), Groq, DeepSeek, Alibaba/Qwen, Grok, local **Ollama**, and **RunPod (native API)** 
- **60+ Model Configurations**: Pricing, limits, and capabilities encoded in a single model registry
- **Structured Outputs**: Pydantic schemas with intelligent fallbacks and validation
- **Vision & Files**: Image analysis, PDF/DOCX/TXT/JSON handling, and automatic vision fallbacks
- **Web Search**: Exa + SERP (Google) integration for retrieval-augmented generation
- **Fallbacks & Reliability**: Provider-best and global-standard fallbacks, plus detailed error logging

This package is ideal when you want a **single, consistent interface** to multiple LLM vendors, with:

- Centralized configuration for models and costs
- Unified ask function (`ask_llm`) that works across providers
- Built-in support for web search, files, and images
- Optional local-only workflows via Ollama

## Installation

```bash
pip install scalexi_llm
```

## Quick Example

```python
from scalexi_llm import LLMProxy

llm = LLMProxy()

response, execution_time, token_usage, cost = llm.ask_llm(
    model_name="chatgpt-4o-latest",
    system_prompt="You are a helpful assistant.",
    user_prompt="Explain quantum computing in simple terms."
)

print(response)
```

## Features at a Glance

- One `LLMProxy` class for all providers
- Unified `ask_llm` API for text, files, images, and web search (with file/image fallbacks for providers like RunPod/Ollama)
- Pydantic-based structured outputs with retry and model fallbacks
- Vision fallback when a chosen model doesnâ€™t support images
- Token and cost accounting for every call
- Comprehensive test suite (`provider_test.py`, `ollama_test.py`, `combined_test.py`)


