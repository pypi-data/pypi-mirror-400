{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive OSFT Training Tutorial\n",
        "\n",
        "This notebook provides a comprehensive guide to Orthogonal Subspace Fine-Tuning (OSFT) using the training_hub library. We'll cover:\n",
        "\n",
        "- **All available parameters** and their detailed explanations\n",
        "- **Single-node and multi-node training** configurations\n",
        "- **Popular model examples** (Qwen 2.5 7B Instruct, Llama 3.1 8B Instruct, Phi 4 Mini, etc.)\n",
        "- **Best practices and troubleshooting**\n",
        "\n",
        "OSFT (Orthogonal Subspace Fine-Tuning) is an algorithm based on [Nayak et al. (2025), arXiv:2504.07097](https://arxiv.org/abs/2504.07097) that enables continual training of pre-trained or instruction-tuned models **without** catastrophic forgetting and **without** needing replay buffers or supplementary datasets.\n",
        "\n",
        "This tutorial serves as both a learning resource and a template you can adapt for your specific continual learning needs.\n",
        "\n",
        "**Note:** For production workflows, we also provide focused example scripts for popular models: `scripts/osft_qwen_example.py`, `scripts/osft_llama_example.py`, and `scripts/osft_phi_example.py` with better logging consistency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is OSFT?\n",
        "\n",
        "OSFT (Orthogonal Subspace Fine-Tuning) is a continual learning algorithm that allows you to adapt pre-trained or instruction-tuned models to new domains **without catastrophic forgetting**. Based on [Nayak et al. (2025), arXiv:2504.07097](https://arxiv.org/abs/2504.07097), OSFT fundamentally changes how we approach model adaptation.\n",
        "\n",
        "### Key Innovation\n",
        "\n",
        "Traditional fine-tuning updates all model parameters, which can overwrite previously learned knowledge. OSFT instead:\n",
        "1. **Identifies orthogonal subspaces** in the model's weight matrices\n",
        "2. **Restricts updates to these subspaces**, preserving existing knowledge\n",
        "3. **Eliminates the need for replay buffers** or supplementary datasets\n",
        "\n",
        "### OSFT vs Traditional Fine-Tuning\n",
        "\n",
        "| Aspect | Traditional SFT | OSFT |\n",
        "|--------|----------------|------|\n",
        "| **Catastrophic Forgetting** | Common problem | Prevented by design |\n",
        "| **Data Requirements** | Needs replay/mixed data | Only new domain data |\n",
        "| **Preservation Method** | Data mixing ratios | Algorithm (math guarantees) |\n",
        "| **Memory Usage** | Similar | Similar |\n",
        "| **Complexity** | Complex data pipelines | Simple, direct |\n",
        "\n",
        "### When to Use OSFT\n",
        "\n",
        "**Perfect for:**\n",
        "- Adding domain-specific knowledge (medical, legal, technical)\n",
        "- Adapting to new languages or dialects\n",
        "- Customizing instruction formats\n",
        "- Continual learning across multiple domains\n",
        "- Any scenario where you need to preserve existing capabilities\n",
        "\n",
        "**Not needed for:**\n",
        "- Training from scratch\n",
        "- Base model pre-training\n",
        "- When you want to completely replace model behavior\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Key Parameter: `unfreeze_rank_ratio`\n",
        "\n",
        "The `unfreeze_rank_ratio` is the most important OSFT-specific parameter. It controls the balance between preservation and adaptation.\n",
        "\n",
        "### What Does It Do?\n",
        "\n",
        "- Controls **how much of each weight matrix** can be updated during training\n",
        "- Range: `0.0` to `1.0`\n",
        "- Lower values = more preservation, slower adaptation\n",
        "- Higher values = more adaptation, slightly less preservation\n",
        "\n",
        "### Visual Intuition\n",
        "\n",
        "Think of a weight matrix as a building:\n",
        "- `unfreeze_rank_ratio = 0.1`: You can only renovate 10% of the rooms\n",
        "- `unfreeze_rank_ratio = 0.3`: You can renovate 30% of the rooms\n",
        "- `unfreeze_rank_ratio = 1.0`: You can renovate the entire building (standard fine-tuning)\n",
        "\n",
        "The \"rooms\" you renovate are carefully chosen to be orthogonal to existing knowledge, preventing damage to what's already there.\n",
        "\n",
        "### Recommended Settings by Use Case\n",
        "\n",
        "| Use Case | Recommended Ratio | Why? |\n",
        "|----------|-------------------|------|\n",
        "| **Minor format adjustments** | 0.1-0.15 | Minimal changes needed |\n",
        "| **Domain vocabulary addition** | 0.15-0.25 | Add terms without losing general knowledge |\n",
        "| **Domain specialization** | 0.25-0.35 | Balance preservation and new expertise |\n",
        "| **Major capability expansion** | 0.35-0.5 | Significant new learning required |\n",
        "| **Complete repurposing** | >0.5 | Rarely needed, approaching standard fine-tuning |\n",
        "\n",
        "### Practical Guidelines\n",
        "\n",
        "```python\n",
        "# Conservative: Maximum preservation\n",
        "unfreeze_rank_ratio = 0.2  # Great for adding specialized knowledge\n",
        "\n",
        "# Balanced: Good for most use cases  \n",
        "unfreeze_rank_ratio = 0.3  # Ideal default for domain adaptation\n",
        "\n",
        "# Aggressive: When you need significant changes\n",
        "unfreeze_rank_ratio = 0.4  # Use when preservation is less critical\n",
        "```\n",
        "\n",
        "**Pro tip:** Start conservative (0.2-0.3) and increase only if needed. It's easier to train again with higher ratio than to recover lost capabilities!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The `target_patterns` Parameter (Advanced Users Only)\n",
        "\n",
        "There's an optional `target_patterns` parameter that allows targeting specific model layers for OSFT:\n",
        "\n",
        "```python\n",
        "target_patterns = None  # Default: applies OSFT to all appropriate layers (RECOMMENDED)\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è Important:** This is an expert-level parameter. Unless you have deep knowledge of model architecture and a specific reason to limit OSFT to certain layers, **leave it as `None`**.\n",
        "\n",
        "If you do need to use it, it performs simple substring matching on module names:\n",
        "- `target_patterns = [\"attention\"]` ‚Üí Targets modules with \"attention\" in the name\n",
        "- `target_patterns = [\"mlp\"]` ‚Üí Targets modules with \"mlp\" in the name\n",
        "\n",
        "**For 99% of users:** Just use the default (`None`) and let OSFT handle layer selection automatically. The algorithm knows what it's doing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import training_hub for OSFT training\n",
        "from training_hub import osft\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Format Requirements\n",
        "\n",
        "Before configuring your training, ensure your data is in the correct format. OSFT uses the mini-trainer backend, which supports both standard messages format and pre-processed datasets.\n",
        "\n",
        "### Required Format: JSONL with Messages\n",
        "\n",
        "Your training data must be a **JSON Lines (.jsonl)** file where each line contains a conversation sample:\n",
        "\n",
        "```json\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello, how are you?\"}, {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you! How can I help you today?\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}, {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"}]}\n",
        "```\n",
        "\n",
        "### Message Structure\n",
        "\n",
        "Each conversation contains a `messages` array with message objects having:\n",
        "- **`role`**: One of `\"system\"`, `\"user\"`, `\"assistant\"`, or `\"pretraining\"`\n",
        "- **`content`**: The text content of the message\n",
        "- **`reasoning_content`** (optional): Additional reasoning traces\n",
        "\n",
        "### Masking Control with `unmask_messages` Parameter\n",
        "\n",
        "Control which parts of the conversation are used for training loss:\n",
        "\n",
        "#### Standard Instruction Tuning (default)\n",
        "```python\n",
        "osft(..., unmask_messages=False)  # Only assistant responses used for loss\n",
        "```\n",
        "- **Trains only on assistant responses** (standard instruction-following)\n",
        "- System messages are always masked (ignored for loss)\n",
        "- User messages are masked\n",
        "- Assistant messages are unmasked (used for loss calculation)\n",
        "\n",
        "#### Pretraining Mode\n",
        "```python\n",
        "osft(..., unmask_messages=True)   # All content except system messages used for loss\n",
        "```\n",
        "- **Trains on all content except system messages**\n",
        "- System messages are always masked\n",
        "- User and assistant messages are both unmasked\n",
        "- Useful for pretraining-style data where the model should learn from all text\n",
        "\n",
        "### Pre-processed Dataset Option\n",
        "\n",
        "If you have pre-processed data with `input_ids` and `labels` fields:\n",
        "\n",
        "```json\n",
        "{\"input_ids\": [1, 2, 3, ...], \"labels\": [1, 2, 3, ...]}\n",
        "```\n",
        "\n",
        "Use with:\n",
        "```python\n",
        "osft(..., use_processed_dataset=True)\n",
        "```\n",
        "\n",
        "### Data Path Configuration\n",
        "\n",
        "When configuring your training, point to your JSONL file:\n",
        "\n",
        "```python\n",
        "data_path = \"/path/to/your/training_data.jsonl\"  # Your messages-format JSONL file\n",
        "```\n",
        "\n",
        "The training pipeline will automatically:\n",
        "1. Load and validate your JSONL data\n",
        "2. Apply chat templates based on your model\n",
        "3. Handle masking according to the `unmask_messages` setting\n",
        "4. Process the data for efficient training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Configuration Examples\n",
        "\n",
        "Here are configuration examples for popular models. These serve as starting points - adjust based on your specific hardware and continual learning requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL CONFIGURATION EXAMPLES FOR OSFT\n",
        "# These are example configurations - adjust based on your hardware and requirements\n",
        "# =============================================================================\n",
        "\n",
        "# Example 1: Qwen 2.5 7B Instruct\n",
        "qwen_example = {\n",
        "    \"model_name\": \"Qwen 2.5 7B Instruct\",\n",
        "    \"model_path\": \"Qwen/Qwen2.5-7B-Instruct\",  # HuggingFace model name or local path\n",
        "    \"example_unfreeze_rank_ratio\": 0.25,  # Conservative for preserving multilingual capabilities\n",
        "    \"example_max_tokens_per_gpu\": 10000,\n",
        "    \"example_max_seq_len\": 8196,  # Qwen 2.5 supports long context\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 5e-6, \n",
        "    \"notes\": \"Excellent for domain adaptation while preserving multilingual capabilities\"\n",
        "}\n",
        "\n",
        "# Example 2: Llama 3.1 8B Instruct\n",
        "llama_example = {\n",
        "    \"model_name\": \"Llama 3.1 8B Instruct\",\n",
        "    \"model_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",  # HuggingFace model name or local path\n",
        "    \"example_unfreeze_rank_ratio\": 0.3,  # Slightly higher for more adaptation freedom\n",
        "    \"example_max_tokens_per_gpu\": 10000,\n",
        "    \"example_max_seq_len\": 8192,  # Supports up to 128K but 8K is common\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 5e-6,\n",
        "    \"notes\": \"Ideal for adding specialized knowledge without losing general capabilities\"\n",
        "}\n",
        "\n",
        "# Example 3: Phi 4 Mini\n",
        "phi_example = {\n",
        "    \"model_name\": \"Phi 4 Mini\",\n",
        "    \"model_path\": \"microsoft/Phi-4-mini-instruct\",  # HuggingFace model name or local path\n",
        "    \"example_unfreeze_rank_ratio\": 0.25,  # Conservative for smaller model\n",
        "    \"example_max_tokens_per_gpu\": 8192,\n",
        "    \"example_max_seq_len\": 4096,\n",
        "    \"example_batch_size\": 64,\n",
        "    \"example_learning_rate\": 5e-6,\n",
        "    \"notes\": \"Efficient for edge deployment with continual adaptation\"\n",
        "}\n",
        "\n",
        "# Example 4: Generic 7B Base Model\n",
        "generic_7b_example = {\n",
        "    \"model_name\": \"Generic 7B Base\",\n",
        "    \"model_path\": \"/path/to/your-7b-model\",  # Local path to model directory\n",
        "    \"example_unfreeze_rank_ratio\": 0.3,  # Balanced preservation vs adaptation\n",
        "    \"example_max_tokens_per_gpu\": 10000,\n",
        "    \"example_max_seq_len\": 4096,\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 5e-6,\n",
        "    \"notes\": \"Good baseline for most 7B instruction-tuned models\"\n",
        "}\n",
        "\n",
        "# Example 5: Smaller Model (1B-3B)\n",
        "small_model_example = {\n",
        "    \"model_name\": \"Small Model (1B-3B)\",\n",
        "    \"model_path\": \"/path/to/small-model\",  # Local path or HuggingFace name\n",
        "    \"example_unfreeze_rank_ratio\": 0.4,  # Higher ratio for smaller models\n",
        "    \"example_max_tokens_per_gpu\": 16_000,\n",
        "    \"example_max_seq_len\": 4096,\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 3e-5,\n",
        "    \"notes\": \"Smaller models can handle more aggressive adaptation\"\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# SELECT YOUR CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Choose one of the examples above as a starting point\n",
        "selected_example = qwen_example  # Change this to your preferred example\n",
        "\n",
        "print(f\"Selected Example: {selected_example['model_name']}\")\n",
        "print(f\"Model Path: {selected_example['model_path']}\")\n",
        "print(f\"OSFT Unfreeze Rank Ratio: {selected_example['example_unfreeze_rank_ratio']}\")\n",
        "print(f\"Example Max Tokens per GPU: {selected_example['example_max_tokens_per_gpu']:,}\")\n",
        "print(f\"Example Max Sequence Length: {selected_example['example_max_seq_len']:,}\")\n",
        "print(f\"Example Batch Size: {selected_example['example_batch_size']:,}\")\n",
        "print(f\"Example Learning Rate: {selected_example['example_learning_rate']}\")\n",
        "print(f\"Notes: {selected_example['notes']}\")\n",
        "print(\"\\nüí° Remember: OSFT preserves original capabilities without needing replay buffers!\")\n",
        "print(\"   Adjust unfreeze_rank_ratio based on preservation vs adaptation needs.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Parameter Reference\n",
        "\n",
        "Let's configure all available OSFT parameters with detailed explanations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE OSFT PARAMETER CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Experiment identification\n",
        "experiment_name = \"osft_comprehensive_example\"\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "full_experiment_name = f\"{experiment_name}_{timestamp}\"\n",
        "\n",
        "# =============================================================================\n",
        "# REQUIRED PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# TODO: revert these overrides after we've concluded training\n",
        "model_path = selected_example[\"model_path\"]  # HuggingFace model name or local path\n",
        "data_path = \"/path/to/your/training_data.jsonl\"  # Path to training data in JSONL format\n",
        "ckpt_output_dir = f\"/path/to/checkpoints/{full_experiment_name}\"  # Where to save checkpoints\n",
        "unfreeze_rank_ratio = selected_example[\"example_unfreeze_rank_ratio\"]  # OSFT-specific parameter\n",
        "effective_batch_size = selected_example[\"example_batch_size\"]  # Effective batch size for training\n",
        "max_tokens_per_gpu = selected_example[\"example_max_tokens_per_gpu\"]  # Maximum tokens per GPU (memory limit)\n",
        "max_seq_len = selected_example[\"example_max_seq_len\"]  # Maximum sequence length\n",
        "learning_rate = selected_example[\"example_learning_rate\"]  # Learning rate for training\n",
        "\n",
        "print(\"üìã Required Parameters (all must be specified):\")\n",
        "print(f\"  ‚Ä¢ model_path: {model_path}\")\n",
        "print(f\"  ‚Ä¢ data_path: {data_path}\")\n",
        "print(f\"  ‚Ä¢ ckpt_output_dir: {ckpt_output_dir}\")\n",
        "print(f\"  ‚Ä¢ unfreeze_rank_ratio: {unfreeze_rank_ratio}\")\n",
        "print(f\"  ‚Ä¢ effective_batch_size: {effective_batch_size}\")\n",
        "print(f\"  ‚Ä¢ max_tokens_per_gpu: {max_tokens_per_gpu:,}\")\n",
        "print(f\"  ‚Ä¢ max_seq_len: {max_seq_len:,}\")\n",
        "print(f\"  ‚Ä¢ learning_rate: {learning_rate}\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# OSFT-SPECIFIC PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "target_patterns = None  # Optional: Patterns to match specific modules for OSFT\n",
        "# Example: [\"*attention*\", \"*mlp*\"] to target attention and MLP layers\n",
        "\n",
        "print(\"üîß OSFT-Specific Parameters:\")\n",
        "print(f\"  unfreeze_rank_ratio: {unfreeze_rank_ratio} - Controls how much of each matrix is unfrozen\")\n",
        "print(f\"    ‚Ä¢ 0.1-0.3: Conservative, maximum preservation\")\n",
        "print(f\"    ‚Ä¢ 0.3-0.5: Balanced adaptation\")\n",
        "print(f\"    ‚Ä¢ >0.5: Rarely needed for typical use cases\")\n",
        "print(f\"  target_patterns: {target_patterns} - Optional patterns for selecting specific modules\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING HYPERPARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# num_epochs = 3  # Number of training epochs\n",
        "num_epochs = 1  # Number of training epochs\n",
        "seed = 42  # Random seed for reproducibility\n",
        "lr_scheduler = \"cosine\"  # Learning rate scheduler\n",
        "lr_scheduler_kwargs = {}  # Scheduler parameters\n",
        "warmup_steps = 0  # Number of warmup steps\n",
        "\n",
        "print(\"üéØ Training Hyperparameters:\")\n",
        "print(f\"  effective_batch_size: {effective_batch_size} - Effective batch size for training\")\n",
        "print(f\"  learning_rate: {learning_rate} - Learning rate for model updates\")\n",
        "print(f\"  num_epochs: {num_epochs} - Number of training epochs\")\n",
        "print(f\"  lr_scheduler: '{lr_scheduler}' - Learning rate scheduler type\")\n",
        "print(f\"  lr_scheduler_kwargs: {lr_scheduler_kwargs} - Scheduler parameters\")\n",
        "print(f\"  warmup_steps: {warmup_steps} - Number of warmup steps\")\n",
        "print(f\"  seed: {seed} - Random seed for reproducibility\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# MEMORY AND PERFORMANCE PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "use_liger = True  # Use Liger kernels for efficiency\n",
        "\n",
        "print(\"‚ö° Memory and Performance Parameters:\")\n",
        "print(f\"  max_tokens_per_gpu: {max_tokens_per_gpu:,} - Maximum tokens per GPU (hard-cap for memory)\")\n",
        "print(f\"  max_seq_len: {max_seq_len:,} - Maximum sequence length\")\n",
        "print(f\"  use_liger: {use_liger} - Use Liger kernels for efficiency\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# DATA PROCESSING PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "data_output_dir = \"/dev/shm/osft_data\"  # Directory for processed data (RAM disk for speed)\n",
        "use_processed_dataset = False  # Whether data is pre-processed\n",
        "unmask_messages = False  # Whether to unmask all messages for pretraining-style learning\n",
        "\n",
        "print(\"üíæ Data Processing Parameters:\")\n",
        "print(f\"  data_path: '{data_path}' - Path to training data (JSONL format)\")\n",
        "print(f\"  data_output_dir: '{data_output_dir}' - Directory to save processed data\")\n",
        "print(f\"  use_processed_dataset: {use_processed_dataset} - Whether to use pre-processed data\")\n",
        "print(f\"  unmask_messages: {unmask_messages} - Whether to unmask all messages\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# CHECKPOINTING PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "checkpoint_at_epoch = True  # Whether to checkpoint at each epoch\n",
        "save_final_checkpoint = True  # Whether to save final checkpoint\n",
        "\n",
        "print(\"üíæ Checkpointing Parameters:\")\n",
        "print(f\"  ckpt_output_dir: '{ckpt_output_dir}' - Directory to save checkpoints\")\n",
        "print(f\"  checkpoint_at_epoch: {checkpoint_at_epoch} - Whether to checkpoint at each epoch\")\n",
        "print(f\"  save_final_checkpoint: {save_final_checkpoint} - Whether to save final checkpoint\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distributed Training Configuration\n",
        "\n",
        "Configure distributed training for both single-node and multi-node setups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DISTRIBUTED TRAINING PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration options for different setups\n",
        "distributed_configs = {\n",
        "    \"single_gpu_dev\": {\n",
        "        \"nproc_per_node\": 1,\n",
        "        \"nnodes\": 1,\n",
        "        \"node_rank\": 0,\n",
        "        \"rdzv_id\": 1,\n",
        "        \"rdzv_endpoint\": \"127.0.0.1:29500\",\n",
        "        \"description\": \"Development setup with single GPU\"\n",
        "    },\n",
        "    \"single_node_8gpu\": {\n",
        "        \"nproc_per_node\": 8,\n",
        "        \"nnodes\": 1,\n",
        "        \"node_rank\": 0,\n",
        "        \"rdzv_id\": 100,\n",
        "        \"rdzv_endpoint\": \"127.0.0.1:29500\",\n",
        "        \"description\": \"Single node with 8 GPUs\"\n",
        "    },\n",
        "    \"multi_node_master\": {\n",
        "        \"nproc_per_node\": 8,\n",
        "        \"nnodes\": 2,  # 2 nodes\n",
        "        \"node_rank\": 0,\n",
        "        \"rdzv_id\": 42,\n",
        "        # master node IP\n",
        "        \"rdzv_endpoint\": \"10.241.128.23:1738\",  # Replace with actual master IP\n",
        "        \"description\": \"Multi-node master (rank 0) - 4 nodes total\"\n",
        "    },\n",
        "    \"multi_node_worker\": {\n",
        "        \"nproc_per_node\": 8,\n",
        "        \"nnodes\": 2,  # 2 nodes\n",
        "        \"node_rank\": 1,  # Change this for each worker node (1, 2, 3, ...)\n",
        "        \"rdzv_id\": 42,\n",
        "        \"rdzv_endpoint\": \"10.241.128.23:1738\",  # Same as master\n",
        "        \"description\": \"Multi-node worker (rank 1) - change rank for each worker\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Select your distributed configuration\n",
        "selected_distributed = \"single_node_8gpu\"  # Change this to match your setup\n",
        "dist_config = distributed_configs[selected_distributed]\n",
        "\n",
        "# Extract distributed training parameters\n",
        "nproc_per_node = dist_config[\"nproc_per_node\"]  # Number of processes (GPUs) per node\n",
        "nnodes = dist_config[\"nnodes\"]  # Total number of nodes\n",
        "node_rank = dist_config[\"node_rank\"]  # Rank of this node (0 to nnodes-1)\n",
        "rdzv_id = dist_config[\"rdzv_id\"]  # Unique job ID for rendezvous\n",
        "rdzv_endpoint = dist_config[\"rdzv_endpoint\"]  # Master node endpoint for multi-node training\n",
        "\n",
        "# Calculate total resources\n",
        "total_gpus = nproc_per_node * nnodes\n",
        "per_gpu_batch_size = effective_batch_size // total_gpus\n",
        "\n",
        "print(\"üñ•Ô∏è  Distributed Training Parameters:\")\n",
        "print(f\"  Configuration: {dist_config['description']}\")\n",
        "print(f\"  nproc_per_node: {nproc_per_node} - Number of processes (GPUs) per node\")\n",
        "print(f\"  nnodes: {nnodes} - Total number of nodes\")\n",
        "print(f\"  node_rank: {node_rank} - Rank of this node (0 to nnodes-1)\")\n",
        "print(f\"  rdzv_id: {rdzv_id} - Unique job ID for rendezvous\")\n",
        "print(f\"  rdzv_endpoint: '{rdzv_endpoint}' - Master node endpoint for multi-node training\")\n",
        "print()\n",
        "print(f\"üìä Resource Calculation:\")\n",
        "print(f\"  Total GPUs: {total_gpus} ({nproc_per_node} √ó {nnodes})\")\n",
        "print(f\"  Effective batch size: {effective_batch_size}\")\n",
        "print(f\"  Approximate per-GPU batch size: {per_gpu_batch_size}\")\n",
        "print(f\"  (Actual micro-batch size determined automatically by gradient accumulation)\")\n",
        "print()\n",
        "\n",
        "# Multi-node setup instructions\n",
        "if nnodes > 1:\n",
        "    print(\"üîß Multi-Node Setup Instructions:\")\n",
        "    print(f\"  1. Ensure all nodes can reach the master at {rdzv_endpoint}\")\n",
        "    print(f\"  2. Use the same rdzv_id ({rdzv_id}) on all nodes\")\n",
        "    print(f\"  3. Set node_rank to 0 for master, 1,2,3... for workers\")\n",
        "    print(f\"  4. Start training on ALL nodes simultaneously\")\n",
        "    print()\n",
        "\n",
        "# OSFT-specific multi-node considerations\n",
        "print(\"üìù OSFT Multi-Node Considerations:\")\n",
        "print(\"  ‚Ä¢ OSFT works seamlessly across multiple nodes\")\n",
        "print(\"  ‚Ä¢ No special replay buffer coordination needed (unlike SFT)\")\n",
        "print(\"  ‚Ä¢ Each node processes its data portion with the same unfreeze_rank_ratio\")\n",
        "print(\"  ‚Ä¢ Gradients are synchronized automatically across all nodes\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute Training\n",
        "\n",
        "Now let's run the actual OSFT training with all our configured parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üöÄ Starting OSFT Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Experiment: {full_experiment_name}\")\n",
        "print(f\"Model: {selected_example['model_name']}\")\n",
        "print(f\"Total GPUs: {total_gpus} ({nproc_per_node} per node √ó {nnodes} nodes)\")\n",
        "print(f\"Configuration: {dist_config['description']}\")\n",
        "print(f\"Unfreeze Rank Ratio: {unfreeze_rank_ratio}\")\n",
        "print()\n",
        "print(\"‚ú® OSFT Advantages:\")\n",
        "print(\"  ‚Ä¢ No catastrophic forgetting\")\n",
        "print(\"  ‚Ä¢ No replay buffer needed\")\n",
        "print(\"  ‚Ä¢ Preserves original model capabilities\")\n",
        "print()\n",
        "\n",
        "# Prepare all training parameters\n",
        "training_params = {\n",
        "    # Required parameters\n",
        "    'model_path': model_path,\n",
        "    'data_path': data_path,\n",
        "    'ckpt_output_dir': ckpt_output_dir,\n",
        "    'unfreeze_rank_ratio': unfreeze_rank_ratio,\n",
        "    'effective_batch_size': effective_batch_size,\n",
        "    'max_tokens_per_gpu': max_tokens_per_gpu,\n",
        "    'max_seq_len': max_seq_len,\n",
        "    'learning_rate': learning_rate,\n",
        "    \n",
        "    # Optional OSFT-specific parameters\n",
        "    'target_patterns': target_patterns,\n",
        "    \n",
        "    # Training duration\n",
        "    'num_epochs': num_epochs,\n",
        "    \n",
        "    # Data processing parameters\n",
        "    'data_output_dir': data_output_dir,\n",
        "    'use_processed_dataset': use_processed_dataset,\n",
        "    'unmask_messages': unmask_messages,\n",
        "    'warmup_steps': warmup_steps,\n",
        "    \n",
        "    # Optimization parameters\n",
        "    'use_liger': use_liger,\n",
        "    'seed': seed,\n",
        "    'lr_scheduler': lr_scheduler,\n",
        "    'lr_scheduler_kwargs': lr_scheduler_kwargs,\n",
        "    \n",
        "    # Checkpointing parameters\n",
        "    'checkpoint_at_epoch': checkpoint_at_epoch,\n",
        "    'save_final_checkpoint': save_final_checkpoint,\n",
        "    \n",
        "    # Distributed training parameters\n",
        "    'nproc_per_node': nproc_per_node,\n",
        "    'nnodes': nnodes,\n",
        "    'node_rank': node_rank,\n",
        "    'rdzv_id': rdzv_id,\n",
        "    'rdzv_endpoint': rdzv_endpoint,\n",
        "}\n",
        "\n",
        "# Display final configuration summary\n",
        "print(\"üìã Final Training Configuration:\")\n",
        "for key, value in training_params.items():\n",
        "    if value is not None:  # Only show non-None values\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚è≥ Training starting...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Execute training\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    result = osft(**training_params)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ OSFT Training completed successfully!\")\n",
        "    print(f\"‚è±Ô∏è  Total duration: {duration/3600:.2f} hours ({duration/60:.1f} minutes)\")\n",
        "    print(f\"üìÅ Checkpoints saved to: {ckpt_output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    print(\"üéØ What you've achieved with OSFT:\")\n",
        "    print(\"  ‚Ä¢ Model adapted to new domain/task\")\n",
        "    print(\"  ‚Ä¢ Original capabilities preserved\")\n",
        "    print(\"  ‚Ä¢ No catastrophic forgetting occurred\")\n",
        "    print(\"  ‚Ä¢ Ready for deployment without regression testing!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"‚ùå Training failed after {duration/60:.1f} minutes\")\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(\"\\nüîç Quick Troubleshooting Checklist:\")\n",
        "    print(\"  ‚ñ° Check that model_path exists or is a valid HuggingFace model name\")\n",
        "    print(\"  ‚ñ° Verify data_path points to valid JSONL file\")\n",
        "    print(\"  ‚ñ° Ensure ckpt_output_dir parent directory exists and is writable\")\n",
        "    print(\"  ‚ñ° Try reducing max_tokens_per_gpu if you see OOM errors\")\n",
        "    print(\"  ‚ñ° Try adjusting unfreeze_rank_ratio (lower = more preservation)\")\n",
        "    print(\"  ‚ñ° For multi-node: verify network connectivity and endpoints\")\n",
        "    print(\"  ‚ñ° Check that mini-trainer backend dependencies are installed\")\n",
        "    \n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Training Analysis\n",
        "\n",
        "After training completes, let's analyze the results and provide guidance for next steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# POST-TRAINING ANALYSIS AND NEXT STEPS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üìä Post-Training Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check for saved checkpoints\n",
        "checkpoint_dir = ckpt_output_dir\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = [d for d in os.listdir(checkpoint_dir) \n",
        "                  if os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
        "    \n",
        "    if checkpoints:\n",
        "        print(f\"‚úÖ Found {len(checkpoints)} checkpoint(s):\")\n",
        "        for ckpt in sorted(checkpoints):\n",
        "            ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
        "            print(f\"  üìÅ {ckpt}\")\n",
        "        \n",
        "        # Identify the final checkpoint\n",
        "        final_checkpoint = sorted(checkpoints)[-1]\n",
        "        final_checkpoint_path = os.path.join(checkpoint_dir, final_checkpoint)\n",
        "        \n",
        "        print(f\"\\nüéØ Final model checkpoint: {final_checkpoint_path}\")\n",
        "        \n",
        "        # Provide model loading example\n",
        "        print(f\"\\nüíª Model Loading Example:\")\n",
        "        print(f\"```python\")\n",
        "        print(f\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
        "        print(f\"\")\n",
        "        print(f\"# Load your OSFT-adapted model\")\n",
        "        print(f\"model = AutoModelForCausalLM.from_pretrained('{final_checkpoint_path}')\")\n",
        "        print(f\"tokenizer = AutoTokenizer.from_pretrained('{final_checkpoint_path}')\")\n",
        "        print(f\"\")\n",
        "        print(f\"# Test the model - it should maintain original capabilities\")\n",
        "        print(f\"# while excelling at your new domain/task\")\n",
        "        print(f\"inputs = tokenizer('Your domain-specific prompt:', return_tensors='pt')\")\n",
        "        print(f\"outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True)\")\n",
        "        print(f\"response = tokenizer.decode(outputs[0], skip_special_tokens=True)\")\n",
        "        print(f\"print(response)\")\n",
        "        print(f\"```\")\n",
        "    else:\n",
        "        print(f\"‚ùå No checkpoints found in {checkpoint_dir}\")\n",
        "else:\n",
        "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_dir}\")\n",
        "\n",
        "# Training summary\n",
        "print(f\"\\nüìà Training Summary:\")\n",
        "print(f\"  Model: {selected_example['model_name']}\")\n",
        "print(f\"  Algorithm: OSFT (Orthogonal Subspace Fine-Tuning)\")\n",
        "print(f\"  Unfreeze Rank Ratio: {unfreeze_rank_ratio}\")\n",
        "print(f\"  Epochs: {num_epochs}\")\n",
        "print(f\"  Global Batch Size: {effective_batch_size}\")\n",
        "print(f\"  Learning Rate: {learning_rate}\")\n",
        "print(f\"  Max Tokens per GPU: {max_tokens_per_gpu:,}\")\n",
        "print(f\"  Max Sequence Length: {max_seq_len:,}\")\n",
        "print(f\"  Total GPUs: {total_gpus}\")\n",
        "print(f\"  Distributed Config: {dist_config['description']}\")\n",
        "\n",
        "# OSFT-specific validation recommendations\n",
        "print(f\"\\nüß™ OSFT-Specific Validation Steps:\")\n",
        "print(f\"  1. **Test Original Capabilities**: Verify the model still performs well on\")\n",
        "print(f\"     general tasks it was originally trained for\")\n",
        "print(f\"  2. **Test New Domain**: Confirm improved performance on your target domain\")\n",
        "print(f\"  3. **No Regression Testing Needed**: Unlike SFT, OSFT preserves capabilities\")\n",
        "print(f\"     by design, reducing validation overhead\")\n",
        "print(f\"  4. **Compare with Base Model**: Run side-by-side comparisons to see\")\n",
        "print(f\"     improvements without degradation\")\n",
        "\n",
        "# Next steps recommendations\n",
        "print(f\"\\nüöÄ Recommended Next Steps:\")\n",
        "print(f\"  1. üéØ Test on domain-specific evaluation sets\")\n",
        "print(f\"  2. üìä Compare performance with base model on both general and domain tasks\")\n",
        "print(f\"  3. üîÑ If more adaptation needed, slightly increase unfreeze_rank_ratio\")\n",
        "print(f\"  4. üí° If too much change occurred, reduce unfreeze_rank_ratio\")\n",
        "print(f\"  5. üìù Document the unfreeze_rank_ratio that works best for your use case\")\n",
        "print(f\"  6. üö¢ Deploy with confidence - no catastrophic forgetting!\")\n",
        "\n",
        "# Performance optimization tips\n",
        "print(f\"\\n‚ö° OSFT-Specific Optimization Tips:\")\n",
        "print(f\"  ‚Ä¢ Current unfreeze_rank_ratio ({unfreeze_rank_ratio}):\")\n",
        "if unfreeze_rank_ratio < 0.2:\n",
        "    print(f\"    Very conservative - great preservation, slower adaptation\")\n",
        "    print(f\"    Consider increasing to 0.25-0.3 if need more adaptation\")\n",
        "elif unfreeze_rank_ratio < 0.35:\n",
        "    print(f\"    Balanced - good preservation with reasonable adaptation\")\n",
        "    print(f\"    This is ideal for most use cases\")\n",
        "else:\n",
        "    print(f\"    Aggressive - faster adaptation, slightly less preservation\")\n",
        "    print(f\"    Consider reducing if seeing any capability degradation\")\n",
        "\n",
        "print(f\"  ‚Ä¢ Memory usage is similar to SFT - adjust max_tokens_per_gpu as needed\")\n",
        "print(f\"  ‚Ä¢ For production: use the script version for better logging and resumption\")\n",
        "\n",
        "print(f\"\\n‚ú® OSFT Training Complete!\")\n",
        "print(f\"Your model has been successfully adapted without forgetting!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Reference Summary\n",
        "\n",
        "Quick reference for all OSFT parameters and their purposes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Required Parameters\n",
        "\n",
        "| Parameter | Description | Example Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `model_path` | Path to the model to fine-tune | `\"Qwen/Qwen2.5-7B\"`, `\"/path/to/model\"` |\n",
        "| `data_path` | Path to the training data | `\"/path/to/train.jsonl\"` |\n",
        "| `ckpt_output_dir` | Directory to save checkpoints | `\"/path/to/checkpoints\"` |\n",
        "| `unfreeze_rank_ratio` | **OSFT-specific**: Controls preservation vs adaptation | `0.25`, `0.3`, `0.4` |\n",
        "| `effective_batch_size` | Effective batch size for training | `64`, `128`, `256` |\n",
        "| `max_tokens_per_gpu` | Maximum tokens per GPU (memory limit) | `16384`, `25000`, `40000` |\n",
        "| `max_seq_len` | Maximum sequence length | `2048`, `8192`, `32768` |\n",
        "| `learning_rate` | Learning rate for training | `1e-5`, `2e-5`, `5e-6` |\n",
        "\n",
        "### OSFT-Specific Parameters\n",
        "\n",
        "| Parameter | Description | Recommended Values | Use Case |\n",
        "|-----------|-------------|-------------------|----------|\n",
        "| `unfreeze_rank_ratio` | Controls how much of each matrix is unfrozen | `0.1-0.3` | Conservative preservation |\n",
        "|           |             | `0.3-0.5` | Balanced adaptation |\n",
        "|           |             | `>0.5` | Rarely needed |\n",
        "| `target_patterns` | Optional patterns to match specific modules | `None` | Default (all modules) |\n",
        "\n",
        "### Training Configuration Parameters\n",
        "\n",
        "| Parameter | Description | Default/Example |\n",
        "|-----------|-------------|-----------------|\n",
        "| `num_epochs` | Number of training epochs | `1` |\n",
        "| `seed` | Random seed for reproducibility | `42` |\n",
        "| `use_liger` | Enable Liger kernels for efficiency | `False` |\n",
        "| `warmup_steps` | Number of warmup steps | `0` |\n",
        "| `lr_scheduler` | Learning rate scheduler | `\"cosine\"` |\n",
        "| `lr_scheduler_kwargs` | Additional scheduler parameters | `{\"eta_min\": 1e-6}` |\n",
        "\n",
        "### Data Processing Parameters\n",
        "\n",
        "| Parameter | Description | Default/Example |\n",
        "|-----------|-------------|-----------------|\n",
        "| `data_output_dir` | Directory to save processed data | Defaults to `f\"{ckpt_output_dir}/_internal_data_processing\"`, Recommended value is `\"/dev/shm\"` (shared memory) |\n",
        "| `use_processed_dataset` | Use pre-processed data with input_ids/labels | `False` |\n",
        "| `unmask_messages` | Unmask all messages for pretraining-style learning | `False` |\n",
        "\n",
        "### Checkpointing Parameters\n",
        "\n",
        "| Parameter | Description | Recommended |\n",
        "|-----------|-------------|-------------|\n",
        "| `checkpoint_at_epoch` | Whether to checkpoint at each epoch | `True` |\n",
        "| `save_final_checkpoint` | Whether to save final checkpoint | `True` |\n",
        "\n",
        "### Distributed Training Parameters\n",
        "\n",
        "| Parameter | Description | Example Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `nproc_per_node` | Number of processes (GPUs) per node | `1`, `4`, `8` |\n",
        "| `nnodes` | Total number of nodes | `1`, `2`, `4` |\n",
        "| `node_rank` | Rank of this node (0 to nnodes-1) | `0` (master), `1`, `2`... |\n",
        "| `rdzv_id` | Unique job ID for rendezvous | `42`, `100` |\n",
        "| `rdzv_endpoint` | Master node endpoint for multi-node training | `\"127.0.0.1:29500\"` |\n",
        "\n",
        "### Unfreeze Rank Ratio Guidelines\n",
        "\n",
        "| Use Case | Recommended Ratio | Rationale |\n",
        "|----------|-------------------|-----------|\n",
        "| **Minor format changes** | 0.1-0.15 | Maximum preservation, minimal changes |\n",
        "| **Domain vocabulary addition** | 0.15-0.25 | Add specialized terms without losing general knowledge |\n",
        "| **Domain specialization** | 0.25-0.35 | Balance between preservation and adaptation |\n",
        "| **Major capability expansion** | 0.35-0.5 | More freedom for significant new capabilities |\n",
        "| **Complete repurposing** | >0.5 | Rarely needed, approaching standard fine-tuning |\n",
        "\n",
        "### OSFT vs SFT Key Differences\n",
        "\n",
        "| Aspect | OSFT | SFT |\n",
        "|--------|------|-----|\n",
        "| **Catastrophic Forgetting** | Prevented by design | Requires replay buffers |\n",
        "| **Data Requirements** | Only new domain data | Needs mixed/replay data |\n",
        "| **Memory Usage** | Similar to SFT | Similar to OSFT |\n",
        "| **Key Parameter** | `unfreeze_rank_ratio` | N/A |\n",
        "| **Backend** | mini-trainer | instructlab-training |\n",
        "| **Best For** | Continual learning, domain adaptation | Initial fine-tuning |\n",
        "\n",
        "### Popular Model Examples for OSFT\n",
        "\n",
        "| Model | HuggingFace Path | Recommended `unfreeze_rank_ratio` | `max_tokens_per_gpu` |\n",
        "|-------|------------------|-----------------------------------|----------------------|\n",
        "| Qwen 2.5 7B | `Qwen/Qwen2.5-7B-Instruct` | 0.25 | 10000 |\n",
        "| Llama 3.1 8B | `meta-llama/Meta-Llama-3.1-8B-Instruct` | 0.3 | 10000 |\n",
        "| Phi 4 Mini | `microsoft/Phi-4-mini-instruct` | 0.25 | 15000 |\n",
        "\n",
        "### Script Alternative\n",
        "\n",
        "For production workloads or long-running training, use the script version:\n",
        "\n",
        "```bash\n",
        "# Qwen example\n",
        "python scripts/osft_qwen_example.py \\\n",
        "  --data-path /path/to/data.jsonl \\\n",
        "  --ckpt-output-dir /path/to/checkpoints \\\n",
        "  --unfreeze-rank-ratio 0.25\n",
        "\n",
        "# Llama example\n",
        "python scripts/osft_llama_example.py \\\n",
        "  --data-path /path/to/data.jsonl \\\n",
        "  --ckpt-output-dir /path/to/checkpoints \\\n",
        "  --unfreeze-rank-ratio 0.3\n",
        "\n",
        "# Phi example\n",
        "python scripts/osft_phi_example.py \\\n",
        "  --data-path /path/to/data.jsonl \\\n",
        "  --ckpt-output-dir /path/to/checkpoints \\\n",
        "  --unfreeze-rank-ratio 0.25\n",
        "```\n",
        "\n",
        "### When to Use OSFT vs SFT\n",
        "\n",
        "**Use OSFT when:**\n",
        "- Adding domain-specific knowledge to an already-trained model\n",
        "- Need to preserve original capabilities without regression\n",
        "- Don't have access to original training data for replay\n",
        "- Want to avoid catastrophic forgetting\n",
        "- Performing continual learning across multiple domains\n",
        "\n",
        "**Use SFT when:**\n",
        "- Training a model from scratch or base model\n",
        "- Have comprehensive training data covering all desired capabilities  \n",
        "- Don't need to preserve specific prior behaviors\n",
        "- Performing initial instruction tuning\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
