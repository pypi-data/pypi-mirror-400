{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ERA5 Data Exploration for Weather Flow Matching\n",
        "\n",
        "This notebook demonstrates how to load, explore, and visualize ERA5 data for weather prediction using the WeatherFlow library. We'll cover:\n",
        "\n",
        "1. Loading data from WeatherBench2\n",
        "2. Exploring the data structure\n",
        "3. Visualizing different variables\n",
        "4. Preparing data for model training\n",
        "5. Computing statistics and climatology\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "First, let's make sure we have WeatherFlow and all dependencies installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add repository root to Python path to find weatherflow package\n",
        "import sys\n",
        "import os\n",
        "# Get absolute path to repo root\n",
        "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "repo_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
        "# Add to path if not already there\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "print(f\"Added {repo_root} to Python path\")\n",
        "\n",
        "# Add repository root to Python path\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
        "\n",
        "# Install WeatherFlow if needed\n",
        "try:\n",
        "    import weatherflow\n",
        "    print(f\"WeatherFlow version: {weatherflow.__version__}\")\n",
        "except ImportError:\n",
        "    !pip install -e ..\n",
        "    import weatherflow\n",
        "    print(f\"WeatherFlow installed, version: {weatherflow.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Mock dependencies that might not be available",
        "try:",
        "    import sys",
        "    notebook_dir = os.path.dirname(os.path.abspath('__file__'))",
        "    repo_root = os.path.abspath(os.path.join(notebook_dir, '..'))",
        "    mock_path = os.path.join(repo_root, 'mock_dependencies.py')",
        "    ",
        "    if os.path.exists(mock_path):",
        "        # Execute the mock dependencies script",
        "        with open(mock_path, 'r') as f:",
        "            mock_code = f.read()",
        "            # Add repo_root to sys.path if not already there",
        "            if repo_root not in sys.path:",
        "                sys.path.insert(0, repo_root)",
        "            # Execute the script",
        "            exec(mock_code)",
        "            # Call the function to install all mocks",
        "            exec(\"install_all_mocks()\")",
        "    else:",
        "        print(f\"Warning: Mock dependencies script not found at {mock_path}\")",
        "except Exception as e:",
        "    print(f\"Error loading mock dependencies: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ADDED FOR COMPATIBILITY: Mock data when real ERA5 data is not available\n",
        "def create_mock_era5_data():\n",
        "    \"\"\"Create mock data for notebooks when actual data is not available.\"\"\"\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    \n",
        "    class MockERA5Dataset:\n",
        "        \"\"\"Mock implementation of ERA5Dataset.\"\"\"\n",
        "        \n",
        "        def __init__(self, data_path=None, variables=None, pressure_levels=None, time_slice=None):\n",
        "            self.variables = variables or ['z', 't']\n",
        "            self.pressure_levels = pressure_levels or [500]\n",
        "            self.n_lat, self.n_lon = 32, 64\n",
        "            self.time_steps = 100\n",
        "            print(f\"Created mock dataset with variables: {self.variables}, levels: {self.pressure_levels}\")\n",
        "        \n",
        "        def __len__(self):\n",
        "            return self.time_steps - 1\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            # Create random tensors for input and target\n",
        "            input_data = torch.randn(len(self.variables), len(self.pressure_levels), self.n_lat, self.n_lon)\n",
        "            target_data = torch.randn(len(self.variables), len(self.pressure_levels), self.n_lat, self.n_lon)\n",
        "            \n",
        "            return {\n",
        "                'input': input_data,\n",
        "                'target': target_data,\n",
        "                'metadata': {\n",
        "                    't0': '2015-01-01',\n",
        "                    't1': '2015-01-02',\n",
        "                    'variables': self.variables,\n",
        "                    'pressure_levels': self.pressure_levels\n",
        "                }\n",
        "            }\n",
        "    \n",
        "    def create_mock_data_loaders(variables=None, pressure_levels=None,\n",
        "                              train_slice=None, val_slice=None, batch_size=4):\n",
        "        \"\"\"Create mock data loaders for training and validation.\"\"\"\n",
        "        import torch\n",
        "        from torch.utils.data import DataLoader, Subset\n",
        "        \n",
        "        # Create mock dataset\n",
        "        dataset = MockERA5Dataset(variables=variables, pressure_levels=pressure_levels)\n",
        "        \n",
        "        # Split into train and validation\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        \n",
        "        train_indices = list(range(train_size))\n",
        "        val_indices = list(range(train_size, train_size + val_size))\n",
        "        \n",
        "        train_dataset = Subset(dataset, train_indices)\n",
        "        val_dataset = Subset(dataset, val_indices)\n",
        "        \n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "        \n",
        "        print(f\"Created mock data loaders with {len(train_dataset)} training and {len(val_dataset)} validation samples\")\n",
        "        return train_loader, val_loader\n",
        "    \n",
        "    # Monkey patch the actual functions\n",
        "    try:\n",
        "        from weatherflow.data.era5 import ERA5Dataset, create_data_loaders\n",
        "        global ERA5Dataset, create_data_loaders\n",
        "        ERA5Dataset = MockERA5Dataset\n",
        "        create_data_loaders = create_mock_data_loaders\n",
        "        print(\"Patched ERA5Dataset and create_data_loaders with mock versions\")\n",
        "    except ImportError:\n",
        "        print(\"Could not patch actual ERA5Dataset - mock data will need to be used manually\")\n",
        "        pass\n",
        "    \n",
        "    return MockERA5Dataset, create_mock_data_loaders\n",
        "\n",
        "# Execute the function to create mock data\n",
        "try:\n",
        "    MockERA5Dataset, mock_create_data_loaders = create_mock_era5_data()\n",
        "    print(\"Mock data utilities created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not create mock data: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import xarray as xr\n",
        "import cartopy.crs as ccrs\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress some warnings for cleaner output\n",
        "\n",
        "# Import WeatherFlow modules\n",
        "from weatherflow.data import ERA5Dataset, create_data_loaders\n",
        "from weatherflow.utils import WeatherVisualizer\n",
        "\n",
        "# Set up matplotlib larger figures\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['figure.dpi'] = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading ERA5 Data\n",
        "\n",
        "WeatherFlow supports loading ERA5 data from multiple sources:\n",
        "\n",
        "1. WeatherBench2 on Google Cloud Storage\n",
        "2. Local NetCDF files\n",
        "3. Custom Zarr datasets\n",
        "\n",
        "Let's use the WeatherBench2 dataset which contains preprocessed global ERA5 reanalysis data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define variables and pressure levels we're interested in\n",
        "variables = ['z', 't', 'u', 'v']  # Geopotential, temperature, u-wind, v-wind\n",
        "pressure_levels = [500]  # 500 hPa level\n",
        "years = ('2016', '2016')  # Load just one year for faster exploration\n",
        "\n",
        "# Detailed explanation of variables:\n",
        "variable_details = {\n",
        "    'z': 'Geopotential (m\u00b2/s\u00b2) - Represents atmospheric pressure levels',\n",
        "    't': 'Temperature (K) - Air temperature',\n",
        "    'u': 'U-component of wind (m/s) - Eastward wind',\n",
        "    'v': 'V-component of wind (m/s) - Northward wind',\n",
        "    'q': 'Specific humidity (kg/kg) - Mass of water vapor per unit mass of air',\n",
        "    'r': 'Relative humidity (%) - Amount of water vapor relative to maximum possible'\n",
        "}\n",
        "\n",
        "# Print selected variables and their descriptions\n",
        "print(\"Selected variables:\")\n",
        "for var in variables:\n",
        "    print(f\"  - {var}: {variable_details.get(var, 'Unknown variable')}\")\n",
        "print(f\"\\nPressure level: {pressure_levels[0]} hPa\")\n",
        "print(f\"Time period: {years[0]} to {years[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ERA5 data with progress information\n",
        "print(\"Loading ERA5 data from WeatherBench2...\")\n",
        "try:\n",
        "    # Try loading with default settings\n",
        "    era5_data = ERA5Dataset(\n",
        "        variables=variables,\n",
        "        pressure_levels=pressure_levels,\n",
        "        time_slice=years,\n",
        "        normalize=False,  # Keep original values for exploration\n",
        "        verbose=True\n",
        "    )\n",
        "    print(f\"Successfully loaded data with {len(era5_data)} time steps\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {str(e)}\")\n",
        "    print(\"\\nTrying alternative loading method...\")\n",
        "    \n",
        "    # If default method fails, try with explicit storage options\n",
        "    era5_data = ERA5Dataset(\n",
        "        variables=variables,\n",
        "        pressure_levels=pressure_levels,\n",
        "        time_slice=years,\n",
        "        normalize=False,\n",
        "        verbose=True,\n",
        "        add_physics_features=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploring Data Structure\n",
        "\n",
        "Let's examine the structure of the loaded data to better understand what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get basic dataset information\n",
        "print(f\"Dataset shape information:\")\n",
        "print(f\"  - Number of time steps: {len(era5_data)}\")\n",
        "print(f\"  - Variables: {era5_data.variables}\")\n",
        "print(f\"  - Pressure levels: {era5_data.pressure_levels}\")\n",
        "print(f\"  - Spatial grid size: {era5_data.ds.latitude.size} \u00d7 {era5_data.ds.longitude.size}\")\n",
        "\n",
        "# Look at the first sample to understand its structure\n",
        "sample = era5_data[0]\n",
        "print(\"\\nSample data structure:\")\n",
        "for key, value in sample.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  - {key}: {type(value)}\")\n",
        "        for subkey, subvalue in value.items():\n",
        "            print(f\"      {subkey}: {type(subvalue)}\")\n",
        "    else:\n",
        "        print(f\"  - {key}: {type(value)}, shape: {value.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract coordinate information\n",
        "coords = era5_data.get_coords()\n",
        "lats = coords['latitude']\n",
        "lons = coords['longitude']\n",
        "\n",
        "print(f\"Latitude range: {lats.min():.2f}\u00b0 to {lats.max():.2f}\u00b0, {len(lats)} points\")\n",
        "print(f\"Longitude range: {lons.min():.2f}\u00b0 to {lons.max():.2f}\u00b0, {len(lons)} points\")\n",
        "\n",
        "# Show coordinate spacing (important for certain physical calculations)\n",
        "lat_spacing = np.mean(np.diff(lats))\n",
        "lon_spacing = np.mean(np.diff(lons))\n",
        "print(f\"Grid resolution: {lat_spacing:.2f}\u00b0 latitude \u00d7 {lon_spacing:.2f}\u00b0 longitude\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizing Weather Variables\n",
        "\n",
        "Now let's visualize each of our variables to get a feel for the data. We'll use the WeatherVisualizer class from WeatherFlow for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize visualizer\n",
        "visualizer = WeatherVisualizer(figsize=(14, 8))\n",
        "\n",
        "# Extract first sample (current state)\n",
        "sample_data = era5_data[0]['input']\n",
        "\n",
        "# Create a dictionary for visualization\n",
        "data_dict = {}\n",
        "for i, var in enumerate(variables):\n",
        "    # Each variable has shape [levels, lat, lon], select first level\n",
        "    data_dict[var] = sample_data[i, 0].numpy()  # Convert tensor to numpy\n",
        "\n",
        "# Plot each variable\n",
        "for i, var_name in enumerate(variables):\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    fig, ax = visualizer.plot_field(\n",
        "        data_dict[var_name],\n",
        "        title=f\"{var_name} at {pressure_levels[0]} hPa\",\n",
        "        var_name=var_name,\n",
        "        coastlines=True,\n",
        "        grid=True\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Wind Vector Visualization\n",
        "\n",
        "Since we have both U and V wind components, we can visualize the vector field to see wind patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract U and V wind components\n",
        "u_index = variables.index('u')\n",
        "v_index = variables.index('v')\n",
        "u_wind = sample_data[u_index, 0].numpy()\n",
        "v_wind = sample_data[v_index, 0].numpy()\n",
        "\n",
        "# For background, use geopotential height\n",
        "z_index = variables.index('z')\n",
        "geopotential = sample_data[z_index, 0].numpy()\n",
        "\n",
        "# Calculate wind speed (magnitude)\n",
        "wind_speed = np.sqrt(u_wind**2 + v_wind**2)\n",
        "\n",
        "# Plot wind field with geopotential height as background\n",
        "fig, ax = visualizer.plot_flow_vectors(\n",
        "    u_wind, v_wind, \n",
        "    background=geopotential, \n",
        "    var_name='z',\n",
        "    title=f\"Wind Field at {pressure_levels[0]} hPa\",\n",
        "    scale=1.0, \n",
        "    density=1.0\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot wind speed\n",
        "plt.figure(figsize=(14, 8))\n",
        "fig, ax = visualizer.plot_field(\n",
        "    wind_speed,\n",
        "    title=f\"Wind Speed at {pressure_levels[0]} hPa\",\n",
        "    cmap='YlOrRd',\n",
        "    coastlines=True,\n",
        "    grid=True\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Temporal Evolution\n",
        "\n",
        "Let's look at how variables change over time by extracting and visualizing a sequence of states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of time steps to visualize\n",
        "n_steps = 5\n",
        "\n",
        "# Extract a sequence of states for one variable (geopotential)\n",
        "var_index = 0  # Index of variable to visualize (geopotential)\n",
        "level_index = 0  # First pressure level\n",
        "\n",
        "# Collect time sequence\n",
        "time_sequence = []\n",
        "time_stamps = []\n",
        "\n",
        "for i in range(n_steps):\n",
        "    if i < len(era5_data):\n",
        "        sample = era5_data[i]\n",
        "        # Extract the variable\n",
        "        time_sequence.append(sample['input'][var_index, level_index].numpy())\n",
        "        # Extract timestamp from metadata\n",
        "        time_stamps.append(sample['metadata']['t0'])\n",
        "\n",
        "# Create animation\n",
        "print(f\"Creating animation for {variables[var_index]} at {pressure_levels[0]} hPa...\")\n",
        "anim = visualizer.create_prediction_animation(\n",
        "    time_sequence,\n",
        "    var_name=variables[var_index],\n",
        "    title=f\"{variables[var_index]} Evolution\",\n",
        "    interval=800  # Slower animation for better viewing\n",
        ")\n",
        "\n",
        "# Display animation\n",
        "from IPython.display import HTML\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Statistics and Climatology\n",
        "\n",
        "Understanding the statistical properties of each variable is important for normalization and model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics for each variable\n",
        "stats = {}\n",
        "\n",
        "# Number of samples to use for statistics (limit for memory efficiency)\n",
        "n_samples = min(50, len(era5_data))\n",
        "print(f\"Computing statistics from {n_samples} samples...\")\n",
        "\n",
        "# Initialize arrays to collect data\n",
        "var_data = {var: [] for var in variables}\n",
        "\n",
        "# Collect data\n",
        "for i in tqdm(range(n_samples)):\n",
        "    sample = era5_data[i]\n",
        "    for j, var in enumerate(variables):\n",
        "        var_data[var].append(sample['input'][j].numpy().flatten())\n",
        "\n",
        "# Compute statistics\n",
        "for var in variables:\n",
        "    # Concatenate all samples for this variable\n",
        "    all_data = np.concatenate(var_data[var])\n",
        "    \n",
        "    # Calculate statistics\n",
        "    stats[var] = {\n",
        "        'mean': np.mean(all_data),\n",
        "        'std': np.std(all_data),\n",
        "        'min': np.min(all_data),\n",
        "        'max': np.max(all_data),\n",
        "        '5th_percentile': np.percentile(all_data, 5),\n",
        "        '95th_percentile': np.percentile(all_data, 95)\n",
        "    }\n",
        "\n",
        "# Display statistics\n",
        "print(\"\\nVariable Statistics:\")\n",
        "for var in variables:\n",
        "    print(f\"\\n{var} ({variable_details.get(var, '')})\")\n",
        "    for stat_name, stat_value in stats[var].items():\n",
        "        print(f\"  - {stat_name}: {stat_value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, var in enumerate(variables):\n",
        "    # Get data for histograms\n",
        "    all_data = np.concatenate(var_data[var])\n",
        "    \n",
        "    # Plot histogram\n",
        "    axes[i].hist(all_data, bins=50, alpha=0.7, density=True)\n",
        "    axes[i].set_title(f\"{var} Distribution\")\n",
        "    axes[i].set_xlabel(variable_details.get(var, var))\n",
        "    axes[i].set_ylabel(\"Density\")\n",
        "    \n",
        "    # Add vertical lines for mean and std range\n",
        "    mean = stats[var]['mean']\n",
        "    std = stats[var]['std']\n",
        "    axes[i].axvline(mean, color='r', linestyle='--', label=f\"Mean: {mean:.2f}\")\n",
        "    axes[i].axvline(mean + std, color='g', linestyle=':', label=f\"\u00b11 Std: {std:.2f}\")\n",
        "    axes[i].axvline(mean - std, color='g', linestyle=':')\n",
        "    axes[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Normalization and Preparation for Training\n",
        "\n",
        "Based on the statistics we calculated, let's create properly normalized data for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create normalized data loaders for training\n",
        "print(\"Creating data loaders with normalization...\")\n",
        "\n",
        "# Split data into training and validation\n",
        "train_years = ('2016', '2016-06')  # First half of 2016\n",
        "val_years = ('2016-07', '2016-12')  # Second half of 2016\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader = create_data_loaders(\n",
        "    variables=variables,\n",
        "    pressure_levels=pressure_levels,\n",
        "    train_slice=train_years,\n",
        "    val_slice=val_years,\n",
        "    batch_size=16,\n",
        "    num_workers=4,\n",
        "    normalize=True  # Apply normalization\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize normalized data\n",
        "# Get a batch from the training loader\n",
        "sample_batch = next(iter(train_loader))\n",
        "\n",
        "# Plot normalized fields for each variable\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, var in enumerate(variables):\n",
        "    # Extract normalized field\n",
        "    normalized_field = sample_batch['input'][0, i, 0].numpy()\n",
        "    \n",
        "    # Plot\n",
        "    im = axes[i].imshow(normalized_field, cmap=visualizer.VAR_CMAPS.get(var, 'viridis'))\n",
        "    axes[i].set_title(f\"Normalized {var}\")\n",
        "    plt.colorbar(im, ax=axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Temporal Patterns and Lag Correlation\n",
        "\n",
        "Understanding the temporal correlation in weather data is crucial for flow matching. Let's examine how variables evolve over short time periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a specific location (grid point) to examine\n",
        "lat_idx = len(lats) // 2  # Middle latitude (roughly equator)\n",
        "lon_idx = len(lons) // 2  # Middle longitude\n",
        "\n",
        "print(f\"Selected location: Latitude {lats[lat_idx]:.2f}\u00b0, Longitude {lons[lon_idx]:.2f}\u00b0\")\n",
        "\n",
        "# Extract time series for each variable at this location\n",
        "n_samples = min(100, len(era5_data))\n",
        "time_series = {var: [] for var in variables}\n",
        "timestamps = []\n",
        "\n",
        "for i in tqdm(range(n_samples)):\n",
        "    sample = era5_data[i]\n",
        "    timestamps.append(sample['metadata']['t0'])\n",
        "    \n",
        "    for j, var in enumerate(variables):\n",
        "        # Extract value at the selected location\n",
        "        value = sample['input'][j, 0, lat_idx, lon_idx].item()\n",
        "        time_series[var].append(value)\n",
        "\n",
        "# Convert timestamps to datetime objects for better plotting\n",
        "import pandas as pd\n",
        "datetimes = pd.to_datetime(timestamps)\n",
        "\n",
        "# Plot time series\n",
        "fig, axes = plt.subplots(len(variables), 1, figsize=(14, 12), sharex=True)\n",
        "\n",
        "for i, var in enumerate(variables):\n",
        "    axes[i].plot(datetimes, time_series[var], '-o', markersize=4)\n",
        "    axes[i].set_title(f\"{var} - {variable_details.get(var, '')}\")\n",
        "    axes[i].set_ylabel(var)\n",
        "    axes[i].grid(True)\n",
        "\n",
        "axes[-1].set_xlabel(\"Time\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate lag correlations to understand predictability\n",
        "max_lag = 10  # Maximum lag in time steps\n",
        "lag_corrs = {var: [] for var in variables}\n",
        "\n",
        "for var in variables:\n",
        "    # Get the time series data\n",
        "    ts = np.array(time_series[var])\n",
        "    \n",
        "    # Calculate autocorrelation for different lags\n",
        "    for lag in range(max_lag + 1):\n",
        "        if lag == 0:\n",
        "            # Correlation with itself is always 1\n",
        "            lag_corrs[var].append(1.0)\n",
        "        else:\n",
        "            # Compute correlation between original series and lagged series\n",
        "            corr = np.corrcoef(ts[lag:], ts[:-lag])[0, 1]\n",
        "            lag_corrs[var].append(corr)\n",
        "\n",
        "# Plot lag correlations\n",
        "plt.figure(figsize=(12, 6))\n",
        "lags = range(max_lag + 1)\n",
        "\n",
        "for var in variables:\n",
        "    plt.plot(lags, lag_corrs[var], 'o-', label=var)\n",
        "    \n",
        "plt.xlabel('Lag (time steps)')\n",
        "plt.ylabel('Autocorrelation')\n",
        "plt.title('Temporal Autocorrelation by Variable')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate cross-correlations between variables\n",
        "plt.figure(figsize=(14, 10))\n",
        "var_data = {}\n",
        "for var in variables:\n",
        "    var_data[var] = np.array(time_series[var])\n",
        "\n",
        "# Create a correlation matrix\n",
        "corr_matrix = np.zeros((len(variables), len(variables)))\n",
        "for i, var1 in enumerate(variables):\n",
        "    for j, var2 in enumerate(variables):\n",
        "        corr_matrix[i, j] = np.corrcoef(var_data[var1], var_data[var2])[0, 1]\n",
        "\n",
        "# Plot correlation matrix as a heatmap\n",
        "plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.colorbar(label='Correlation Coefficient')\n",
        "plt.xticks(range(len(variables)), variables)\n",
        "plt.yticks(range(len(variables)), variables)\n",
        "plt.title('Cross-Correlation Between Variables')\n",
        "\n",
        "# Add correlation values as text\n",
        "for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "        plt.text(j, i, f'{corr_matrix[i, j]:.2f}', \n",
        "                 ha='center', va='center', \n",
        "                 color='white' if abs(corr_matrix[i, j]) > 0.5 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compute derived quantities for physics constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Computing some important derived physical quantities for flow matching...\")\n",
        "\n",
        "# Extract geopotential, temperature, and wind components\n",
        "z_index = variables.index('z')\n",
        "t_index = variables.index('t')\n",
        "u_index = variables.index('u')\n",
        "v_index = variables.index('v')\n",
        "\n",
        "sample = era5_data[0]\n",
        "z = sample['input'][z_index, 0].numpy()\nt = sample['input'][t_index, 0].numpy()\nu = sample['input'][u_index, 0].numpy()\nv = sample['input'][v_index, 0].numpy()\n\n# Calculate wind speed\nwind_speed = np.sqrt(u**2 + v**2)\n\n# Calculate vorticity (curl of wind field)\n# Simplified calculation using finite differences\ndy = 111000 * np.mean(np.diff(lats))  # Convert degrees to meters\ndx = 111000 * np.mean(np.diff(lons)) * np.cos(np.radians(np.mean(lats)))\n\n# Compute derivatives\ndudy = np.zeros_like(u)\ndvdx = np.zeros_like(v)\n\ndudy[1:-1, :] = (u[2:, :] - u[:-2, :]) / (2 * dy)\ndvdx[:, 1:-1] = (v[:, 2:] - v[:, :-2]) / (2 * dx)\n\n# Relative vorticity (curl of velocity)\nvorticity = dvdx - dudy\n\n# Calculate divergence (measure of mass continuity)\ndudx = np.zeros_like(u)\ndvdy = np.zeros_like(v)\n\ndudx[:, 1:-1] = (u[:, 2:] - u[:, :-2]) / (2 * dx)\ndvdy[1:-1, :] = (v[2:, :] - v[:-2, :]) / (2 * dy)\n\ndivergence = dudx + dvdy\n\n# Visualize these derived quantities\nfig, axes = plt.subplots(2, 2, figsize=(18, 12))\n\n# Plot original wind field\nvisualizer.plot_flow_vectors(u, v, background=z, var_name='z', \n                           title=\"Wind Field and Geopotential Height\", \n                           ax=axes[0, 0])\n\n# Plot vorticity\ncmap = 'RdBu_r'\nim = axes[0, 1].imshow(vorticity, cmap=cmap, origin='lower')\naxes[0, 1].set_title(\"Vorticity (1/s)\")\nplt.colorbar(im, ax=axes[0, 1])\n\n# Plot divergence\nim = axes[1, 0].imshow(divergence, cmap=cmap, origin='lower')\naxes[1, 0].set_title(\"Divergence (1/s)\")\nplt.colorbar(im, ax=axes[1, 0])\n\n# Plot wind speed\nim = axes[1, 1].imshow(wind_speed, cmap='viridis', origin='lower')\naxes[1, 1].set_title(\"Wind Speed (m/s)\")\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualize data suitable for flow matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Visualizing Sequential Data for Flow Matching\")\n",
        "\n",
        "# Extract consecutive states to visualize flow matching targets\n",
        "n_steps = 4  # Number of consecutive steps\n",
        "first_sample_idx = 0\n",
        "\n",
        "states = []\n",
        "for i in range(first_sample_idx, first_sample_idx + n_steps):\n",
        "    if i < len(era5_data):\n",
        "        sample = era5_data[i]\n",
        "        states.append(sample['input'][z_index, 0].numpy())  # Using geopotential\n",
        "\n",
        "# Compute the \"flow\" between consecutive states\n",
        "flows = []\n",
        "for i in range(len(states) - 1):\n",
        "    flow = states[i+1] - states[i]\n",
        "    flows.append(flow)\n",
        "\n",
        "# Visualize states and flows\n",
        "fig, axes = plt.subplots(2, n_steps-1, figsize=(18, 10))\n",
        "\n",
        "# Plot consecutive states\n",
        "for i in range(n_steps-1):\n",
        "    # Plot current state\n",
        "    im = axes[0, i].imshow(states[i], cmap='viridis', origin='lower')\n",
        "    axes[0, i].set_title(f\"State at t={i}\")\n",
        "    plt.colorbar(im, ax=axes[0, i])\n",
        "    \n",
        "    # Plot flow to next state\n",
        "    im = axes[1, i].imshow(flows[i], cmap='RdBu_r', origin='lower')\n",
        "    axes[1, i].set_title(f\"Flow t={i} \u2192 t={i+1}\")\n",
        "    plt.colorbar(im, ax=axes[1, i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save statistics for model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a clean dictionary of normalization statistics\n",
        "norm_stats = {}\n",
        "for var in variables:\n",
        "    norm_stats[var] = {\n",
        "        'mean': stats[var]['mean'],\n",
        "        'std': stats[var]['std']\n",
        "    }\n",
        "    \n",
        "print(\"Normalization statistics for model training:\")\n",
        "for var, var_stats in norm_stats.items():\n",
        "    print(f\"  {var}: mean = {var_stats['mean']:.4f}, std = {var_stats['std']:.4f}\")\n",
        "\n",
        "# Save statistics to file\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs('../data', exist_ok=True)\n",
        "\n",
        "# Save as JSON\n",
        "with open('../data/normalization_stats.json', 'w') as f:\n",
        "    json.dump(norm_stats, f, indent=2)\n",
        "    \n",
        "print(f\"\\nStatistics saved to '../data/normalization_stats.json'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "In this notebook, we've explored ERA5 data for weather prediction using flow matching. We:\n",
        "\n",
        "1. Loaded and inspected ERA5 data from WeatherBench2\n",
        "2. Visualized different weather variables and their relationships\n",
        "3. Analyzed temporal patterns and correlations\n",
        "4. Computed physics-relevant derived quantities\n",
        "5. Prepared data for flow matching model training\n",
        "\n",
        "Next steps would be to train a flow matching model using this data, which we'll cover in the next notebook.\n",
        "\"\"\")"
      ]
    }
  ]
}