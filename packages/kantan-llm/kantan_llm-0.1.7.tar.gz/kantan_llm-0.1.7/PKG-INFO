Metadata-Version: 2.4
Name: kantan-llm
Version: 0.1.7
Summary: Minimal LLM client getter for OpenAI Responses + OpenAI-compatible Chat Completions.
Project-URL: Repository, https://github.com/kitfactory/kantan-llm
Keywords: llm,openai,openrouter,lmstudio,ollama,gemini
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: openai<3,>=2
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Dynamic: license-file

# kantan-llm ğŸ˜ºâœ¨

A tiny Python library that removes the boring boilerplate (keys/URLs/provider selection) so you can call LLMs with a single `get_llm()` ğŸ’¨

**Big idea:** set env vars for the providers/models you use, then just do `get_llm("model-name")` and it â€œjust connectsâ€ ğŸ˜ºâœ¨

## Supported providers (roughly) ğŸŒ

- OpenAI (Responses)
- Anthropic (Claude via OpenAI-compatible SDK)
- OpenRouter (OpenAI-compatible Chat)
- Google (Gemini via OpenAI-compatible Chat)
- LMStudio / Ollama / any OpenAI-compatible Chat

## Install ğŸ“¦

```bash
pip install kantan-llm
```

## Quickstart ğŸš€

### OpenAI (Responses API is the source of truth)

```bash
export OPENAI_API_KEY="sk-..."
```

```python
from kantan_llm import get_llm

llm = get_llm("gpt-4.1-mini")
res = llm.responses.create(input="Say hi in one short line.")
print(res.output_text)
```

`llm` is OpenAI SDK compatible (unknown attributes delegate to the underlying client).

### OpenAI-compatible (Chat Completions is the source of truth)

#### LMStudio (example: `openai/gpt-oss-20b`)

```bash
export LMSTUDIO_BASE_URL="http://192.168.11.16:1234"  # `/v1` is optional
```

```python
from kantan_llm import get_llm

llm = get_llm("openai/gpt-oss-20b", provider="lmstudio")
cc = llm.chat.completions.create(messages=[{"role": "user", "content": "Return exactly: OK"}], max_tokens=16)
print(cc.choices[0].message.content)
```

#### Ollama (example)

```bash
export OLLAMA_BASE_URL="http://localhost:11434"  # `/v1` is optional
```

```python
from kantan_llm import get_llm

llm = get_llm("llama3.2", provider="ollama")
cc = llm.chat.completions.create(messages=[{"role": "user", "content": "Return exactly: OK"}], max_tokens=16)
print(cc.choices[0].message.content)
```

#### Anthropic (Claude via OpenAI-compatible SDK)

```bash
export CLAUDE_API_KEY="sk-ant-..."
```

```python
from kantan_llm import get_llm

llm = get_llm("claude-3-5-sonnet-latest")  # if `CLAUDE_API_KEY` exists -> provider=anthropic (inferred)
cc = llm.chat.completions.create(messages=[{"role": "user", "content": "Return exactly: OK"}], max_tokens=16)
print(cc.choices[0].message.content)
```

#### OpenRouter (includes Claude, etc.)

```bash
export OPENROUTER_API_KEY="..."
```

```python
from kantan_llm import get_llm

llm = get_llm("anthropic/claude-3.5-sonnet", provider="openrouter")  # explicit is recommended (Anthropic takes precedence)
cc = llm.chat.completions.create(messages=[{"role": "user", "content": "Return exactly: OK"}], max_tokens=16)
print(cc.choices[0].message.content)
```

#### Google (Gemini via an OpenAI-compatible endpoint)

```bash
export GOOGLE_API_KEY="..."
```

```python
from kantan_llm import get_llm

llm = get_llm("gemini-2.0-flash")
cc = llm.chat.completions.create(messages=[{"role": "user", "content": "Return exactly: OK"}], max_tokens=16)
print(cc.choices[0].message.content)
```

## Provider rules ğŸ§­

- `gpt-*` â†’ `openai`
- `gemini-*` â†’ `google`
- `claude-*` â†’ `anthropic` (if `CLAUDE_API_KEY` is set) â†’ `openrouter` (if `OPENROUTER_API_KEY` is set) â†’ otherwise `compat`
- If the model name is not recognizable, it picks the first available provider by env vars: `lmstudio` â†’ `ollama` â†’ `openrouter` â†’ `anthropic` â†’ `google`

## Explicit provider ğŸ¯

```python
from kantan_llm import get_llm

llm = get_llm("gpt-4.1-mini", provider="openai")
```

## Fallback (order = priority) ğŸ§¯

```python
from kantan_llm import get_llm

llm = get_llm("gpt-4.1-mini", providers=["openai", "lmstudio", "openrouter"])
```

## Tracing / Tracer ğŸ§µ

By default, `get_llm()` enables a simple tracer that prints input/output (colorized) for each LLM call.

```python
from kantan_llm import get_llm
from kantan_llm.tracing import trace

llm = get_llm("gpt-4.1-mini")
with trace("workflow"):
    llm.responses.create(input="Say hi.")
```

More: `docs/tracing.md`

## Async (ASGI) support
ASGIï¼ˆFastAPI/Starletteï¼‰ã§ event loop ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ãŸã‚ã€async å°ç·šã‚’æä¾›ã—ã¾ã™ã€‚

### get_async_llm()ï¼ˆæ¨å¥¨ï¼‰
- kantan-llm ã®ä¿è¨¼ï¼ˆæ­£è¦åŒ–/ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯/ã‚¬ãƒ¼ãƒ‰/ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰ã‚’ async ã§ã‚‚ç¶­æŒã—ã¾ã™ã€‚

### Async streaming (KantanAsyncLLM)
KantanAsyncLLM ã§ã¯ streaming API ã‚’æä¾›ã—ã€æœ€çµ‚å¿œç­”ã§ã¾ã¨ã‚ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¾ã™ã€‚

```python
from kantan_llm import get_async_llm

llm = get_async_llm("gpt-4.1-mini")
async with llm.responses.stream(input="Say hi.") as stream:
    async for _ in stream:
        pass
    final = await stream.get_final_response()
print(final.output_text)
```

Note: Some models (e.g. `gpt-5-mini`) may emit only `response.output_item.*` events without `output_text`/text deltas.
KantanAsyncLLM tries `output_text` first, then stream deltas, then `output_item` text; if none exists, the stream completes but the traced output can be empty.

### get_async_llm_client()ï¼ˆEscape hatchï¼‰
- `AsyncOpenAI` ã® raw client ã‚’è¿”ã—ã¾ã™ï¼ˆäº’æ›æ€§æœ€å¤§åŒ–ã€Agents SDK æ³¨å…¥å‘ã‘ï¼‰ã€‚
- **æ³¨æ„:** raw client è¿”å´ã§ã¯ API ã‚¬ãƒ¼ãƒ‰ / è‡ªå‹•ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¯è¡Œã„ã¾ã›ã‚“ã€‚
- ä»£ã‚ã‚Šã« `model/provider/base_url` ã‚’å«ã‚€ bundle ã‚’è¿”ã—ã€æ­£è¦åŒ–æ¸ˆã¿ model åã‚’ä¸‹æµã¸æ¸¡ã›ã¾ã™ã€‚

## OpenAI Agents SDK integration
Agents SDK ã¯ AsyncOpenAI client ã‚’å·®ã—æ›¿ãˆå¯èƒ½ã§ã™ã€‚

- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ client ã‚’å·®ã—æ›¿ãˆã‚‹:
  - `set_default_openai_client(AsyncOpenAI(...))`
- ãƒ¢ãƒ‡ãƒ«å˜ä½ã§ client ã‚’æ¸¡ã™:
  - `OpenAIResponsesModel(..., openai_client=AsyncOpenAI(...))`

### In kantan-agents
kantan-agents (Agents SDK wrapper) uses the same two entry points:

- `set_default_openai_client(...)`
- `OpenAIResponsesModel(..., openai_client=...)`

kantan-llm ã§ Agents SDK ã‚’ä½¿ã†å ´åˆã®æ¨å¥¨:

- äº’æ›æ€§å„ªå…ˆ: `bundle = get_async_llm_client(...)`
  - `bundle.client` ã‚’ Agents SDK ã«æ¸¡ã™
  - `bundle.model`ï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰ã‚’ Agent/Model å´ã¸æ¸¡ã™
- kantan ã®ã‚¬ãƒ¼ãƒ‰/ãƒˆãƒ¬ãƒ¼ã‚¹ã‚‚ä½¿ã„ãŸã„: `llm = get_async_llm(...)`
  - ãŸã ã— Agents SDK å´ã¨äºŒé‡ãƒˆãƒ¬ãƒ¼ã‚¹ã«ãªã‚Šå¾—ã‚‹ãŸã‚ã€ã©ã¡ã‚‰ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹ã‹æ–¹é‡ã‚’æ±ºã‚ã‚‹ï¼ˆä¸‹è¨˜ï¼‰ã€‚

### Tracingï¼ˆäºŒé‡è¨ˆæ¸¬ã‚’é¿ã‘ã‚‹ï¼‰
Agents SDK å´ã«ã¯ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç„¡åŠ¹åŒ–ã®å°ç·šãŒã‚ã‚Šã¾ã™ï¼ˆä¾‹: `set_tracing_disabled(True)` ã‚„ç’°å¢ƒå¤‰æ•°ï¼‰ã€‚
é‹ç”¨ã§ã¯ä»¥ä¸‹ã®ã©ã¡ã‚‰ã‹ã‚’é¸ã³ã¾ã™ã€‚

- A) Agents SDK ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã€kantan å´ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ç„¡åŠ¹ï¼ˆã¾ãŸã¯ raw client ã‚’ä½¿ã†ï¼‰
- B) kantan ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã€Agents SDK å´ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ç„¡åŠ¹

## Search (SQLite) ğŸ”

Use `SQLiteTracer` as a lightweight search backend for traces/spans.

```python
from kantan_llm.tracing import SpanQuery, TraceQuery
from kantan_llm.tracing.processors import SQLiteTracer

tracer = SQLiteTracer("traces.sqlite3")
traces = tracer.search_traces(query=TraceQuery(keywords=["hello"], limit=10))
spans = tracer.search_spans(query=SpanQuery(keywords=["hello"], limit=10))
```

More: `docs/search.md`
Tutorial: `docs/tutorial_trace_analysis.md`

## Examples ğŸ“š

- `examples/tracing_basic.py`
- `examples/search_sqlite.py`

## Environment variables ğŸ”

- OpenAI
  - `OPENAI_API_KEY` (required)
  - `OPENAI_BASE_URL` (optional)
- Generic compatible (`compat`)
  - `KANTAN_LLM_BASE_URL` (required)
  - `KANTAN_LLM_API_KEY` (optional; falls back to a dummy value)
- LMStudio
  - `LMSTUDIO_BASE_URL` (required)
- Ollama
  - `OLLAMA_BASE_URL` (required)
- OpenRouter
  - `OPENROUTER_API_KEY` (required)
- Anthropic
  - `CLAUDE_API_KEY` (required)
  - `CLAUDE_BASE_URL` (optional)
- Google
  - `GOOGLE_API_KEY` (required)
  - `GOOGLE_BASE_URL` (optional)

## Error example ğŸ’¥

- Missing OpenAI key: `python -c 'from kantan_llm import get_llm; get_llm(\"gpt-4.1-mini\")'` â†’ `[kantan-llm][E2] Missing OPENAI_API_KEY for provider: openai`

## Tests ğŸ§ª

Live integration tests (real APIs) are opt-in:

```bash
KANTAN_LLM_RUN_LIVE_TESTS=1 pytest -q -m integration
```
