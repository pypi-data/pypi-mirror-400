{"id": "nvidia_cuda_cuda-programming-guide_00000", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA C cudaMemAdvise and cudaMemPrefetchAsync Example", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory", "content": "### CUDA C cudaMemAdvise and cudaMemPrefetchAsync Example\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory\n\nDemonstrates how to use `cudaMemAdvise` and `cudaMemPrefetchAsync` to guide data movement between host and device memory. It shows encouraging data to move to the GPU before use and back to the CPU after.\n\n```cuda-c\nvoid test_prefetch_managed(cudaStream_t s) {\n  init_data(data, dataSizeBytes);\n  cudaMemLocation location = {.type = cudaMemLocationTypeDevice, .id = myGpuId};\n\n  // encourage data to move to GPU before use\n  const unsigned int flags = 0;\n  cudaMemPrefetchAsync(data, dataSizeBytes, location, flags, s);\n\n  // use data on GPU\n  const uinsigned num_blocks = (dataSizeBytes + threadsPerBlock - 1) / threadsPerBlock;\n  mykernel<<<num_blocks, threadsPerBlock, 0, s>>>(data, dataSizeBytes);\n\n  // encourage data to move back to CPU\n  location = {.type = cudaMemLocationTypeHost};\n  cudaMemPrefetchAsync(data, dataSizeBytes, location, flags, s);\n\n  cudaStreamSynchronize(s);\n\n  // use data on CPU\n  use_data(data, dataSizeBytes);\n  cudaFree(data);\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00001", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA Compilation and Execution Example", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/intro-to-cuda-cpp", "content": "### CUDA Compilation and Execution Example\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/intro-to-cuda-cpp\n\nThis section provides command-line examples for compiling CUDA code using nvcc and executing the resulting binaries. It shows how to compile both unified memory and explicit memory management examples and run them with different vector lengths.\n\n```bash\n$ nvcc vecAdd_unifiedMemory.cu -o vecAdd_unifiedMemory\n$ ./vecAdd_unifiedMemory\nUnified Memory: CPU and GPU answers match\n$ ./vecAdd_unifiedMemory 4096\nUnified Memory: CPU and GPU answers match\n\n\n$ nvcc vecAdd_explicitMemory.cu -o vecAdd_explicitMemory\n$ ./vecAdd_explicitMemory\nExplicit Memory: CPU and GPU answers match\n$ ./vecAdd_explicitMemory 4096\nExplicit Memory: CPU and GPU answers match\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00002", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "L2 Persistence Example", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/l2-cache-control", "content": "### L2 Persistence Example\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/l2-cache-control\n\nProvides a practical C++ example demonstrating how to set aside L2 cache for persistent accesses, use this set-aside cache within CUDA kernels via a CUDA Stream, and then reset the L2 cache.\n\n```APIDOC\n## L2 Persistence Example\n\nThe following example shows how to set-aside L2 cache for persistent accesses, use the set-aside L2 cache in CUDA kernels via CUDA Stream and then reset the L2 cache.\n\n```c++\n// Include necessary CUDA headers\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <algorithm>\n\n// Assuming grid_size, block_size, data1, data2, and device_id are defined elsewhere\n// extern dim3 grid_size;\n// extern dim3 block_size;\n// extern void* data1;\n// extern void* data2;\n// extern int device_id;\n\n// Function to execute the example\nvoid executeL2PersistenceExample() {\n    cudaStream_t stream;\n    cudaStreamCreate(&stream); // Create CUDA stream\n\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device_id); // Query GPU properties\n\n    // Calculate size for L2 cache persistence\n    size_t size = std::min(static_cast<size_t>(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);\n    cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); // set-aside 3/4 of L2 cache for persisting accesses or the max allowed\n\n    size_t window_size = std::min(prop.accessPolicyMaxWindowSize, num_bytes); // Select minimum of user defined num_bytes and max window size.\n\n    cudaStreamAttrValue stream_attribute;\n    stream_attribute.accessPolicyWindow.base_ptr = reinterpret_cast<void*>(data1);\n    stream_attribute.accessPolicyWindow.num_bytes = window_size;\n    stream_attribute.accessPolicyWindow.hitRatio = 0.6;\n    stream_attribute.accessPolicyWindow.hitProp = cudaAccessPropertyPersisting; // Persistence Property\n    stream_attribute.accessPolicyWindow.missProp = cudaAccessPropertyStreaming; // Type of access property on cache miss\n\n    // Set the attributes to a CUDA Stream\n    cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);\n\n    // Kernel launches benefiting from L2 persistence\n    for (int i = 0; i < 10; ++i) {\n        cuda_kernelA<<<grid_size, block_size, 0, stream>>>(data1); // This data1 is used by a kernel multiple times\n    }\n    // [data1 + num_bytes) benefits from L2 persistence\n    cuda_kernelB<<<grid_size, block_size, 0, stream>>>(data1); // A different kernel in the same stream can also benefit from the persistence of data1\n\n    // Reset L2 cache persistence for the stream\n    stream_attribute.accessPolicyWindow.num_bytes = 0; // Setting the window size to 0 disables it\n    cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute); // Overwrite the access policy attribute to a CUDA Stream\n    cudaCtxResetPersistingL2Cache(); // Remove any persistent lines in L2\n\n    // Kernel launch using L2 cache in normal mode\n    cuda_kernelC<<<grid_size, block_size, 0, stream>>>(data2); // data2 can now benefit from full L2 in normal mode\n\n    cudaStreamDestroy(stream); // Destroy CUDA stream\n}\n\n// Placeholder for kernel definitions (replace with actual kernel calls)\n__global__ void cuda_kernelA(void* data) { /* ... */ }\n__global__ void cuda_kernelB(void* data) { /* ... */ }\n__global__ void cuda_kernelC(void* data) { /* ... */ }\n\n// Example usage (assuming main function context)\n// int main() {\n//     // Initialize device, data pointers, etc.\n//     executeL2PersistenceExample();\n//     // Cleanup\n//     return 0;\n// }\n```\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00003", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA Source File Example", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/nvcc", "content": "### CUDA Source File Example\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/nvcc\n\nAn example CUDA C++ source file (`.cu`) containing both host code (in `main` and `kernel_launcher`) and device code (in `kernel`). This file demonstrates a simple CUDA kernel launch and synchronization.\n\n```cuda\n// ----- example.cu -----\n#include <stdio.h>\n__global__ void kernel() {\n    printf(\"Hello from kernel\\n\");\n}\n\nvoid kernel_launcher() {\n    kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n}\n\nint main() {\n    kernel_launcher();\n    return 0;\n}\n\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00004", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Dynamic Parallelism Hello World Example", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/dynamic-parallelism", "content": "### Dynamic Parallelism Hello World Example\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/dynamic-parallelism\n\nA basic 'Hello World' example demonstrating dynamic parallelism in CUDA. It includes a parent kernel that launches a child kernel and a tail kernel, showcasing inter-kernel communication and stream management.\n\n```cuda-cpp\n#include <stdio.h>\n\n__global__ void childKernel()\n{\n    printf(\"Hello \");\n}\n\n__global__ void tailKernel()\n{\n    printf(\"World!\\n\");\n}\n\n__global__ void parentKernel()\n{\n    // launch child\n    childKernel<<<1,1>>>();\n    if (cudaSuccess != cudaGetLastError()) {\n        return;\n    }\n\n    // launch tail into cudaStreamTailLaunch stream\n    // implicitly synchronizes: waits for child to complete\n    tailKernel<<<1,1,0,cudaStreamTailLaunch>>>();\n\n}\n\nint main(int argc, char *argv[])\n{\n    // launch parent\n    parentKernel<<<1,1>>>();\n    if (cudaSuccess != cudaGetLastError()) {\n        return 1;\n    }\n\n    // wait for parent to complete\n    if (cudaSuccess != cudaDeviceSynchronize()) {\n        return 2;\n    }\n\n    return 0;\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00005", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Example: Creating thread_block_tile instances (C++)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis", "content": "### Example: Creating thread_block_tile instances (C++)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis\n\nThis example demonstrates creating `thread_block_tile` instances with different sizes and parent types. `tile32` is created with the size encoded in the type, while `tile4` explicitly encodes provenance in the type.\n\n```cpp\n/// The following code will create two sets of tiled groups, of size 32 and 4 respectively:\n/// The latter has the provenance encoded in the type, while the first stores it in the handle\nthread_block block = this_thread_block();\nthread_block_tile<32> tile32 = tiled_partition<32>(block);\nthread_block_tile<4, thread_block> tile4 = tiled_partition<4>(block);\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00006", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Vulkan-CUDA Interoperability Setup", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/graphics-interop", "content": "### Vulkan-CUDA Interoperability Setup\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/graphics-interop\n\nIllustrates the C++ data structures and member variables used in a Vulkan-CUDA interoperability example. This includes Vulkan buffer and memory handles, CUDA stream and external semaphore/memory handles, and mapped CUDA device pointers for efficient data exchange between graphics and compute workloads.\n\n```c++\nclass VulkanCudaSineWave : public VulkanBaseApp {\n  typedef struct UniformBufferObject_st {\n    mat4x4 modelViewProj;\n  } UniformBufferObject;\n\n  VkBuffer m_heightBuffer, m_xyBuffer, m_indexBuffer;\n  VkDeviceMemory m_heightMemory, m_xyMemory, m_indexMemory;\n  UniformBufferObject m_ubo;\n  VkSemaphore m_vkWaitSemaphore, m_vkSignalSemaphore;\n  SineWaveSimulation m_sim;\n  cudaStream_t m_stream;\n  cudaExternalSemaphore_t m_cudaWaitSemaphore, m_cudaSignalSemaphore, m_cudaTimelineSemaphore;\n  cudaExternalMemory_t m_cudaVertMem;\n  float *m_cudaHeightMap;\n  // ...\n};\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00007", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA C++ Data Prefetching Example (System Allocator)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory", "content": "### CUDA C++ Data Prefetching Example (System Allocator)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory\n\nDemonstrates using `cudaMemPrefetchAsync` with system-allocated memory. It shows prefetching data to the GPU before kernel execution and back to the CPU afterwards.\n\n```cuda\nvoid test_prefetch_sam(const cudaStream_t& s) {\n  // initialize data on CPU\n  char *data = (char*)malloc(dataSizeBytes);\n  init_data(data, dataSizeBytes);\n  cudaMemLocation location = {.type = cudaMemLocationTypeDevice, .id = myGpuId};\n\n  // encourage data to move to GPU before use\n  const unsigned int flags = 0;\n  cudaMemPrefetchAsync(data, dataSizeBytes, location, flags, s);\n\n  // use data on GPU\n  const unsigned num_blocks = (dataSizeBytes + threadsPerBlock - 1) / threadsPerBlock;\n  mykernel<<<num_blocks, threadsPerBlock, 0, s>>>(data, dataSizeBytes);\n\n  // encourage data to move back to CPU\n  location = {.type = cudaMemLocationTypeHost};\n  cudaMemPrefetchAsync(data, dataSizeBytes, location, flags, s);\n\n  cudaStreamSynchronize(s);\n\n  // use data on CPU\n  use_data(data, dataSizeBytes);\n  free(data);\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00008", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Step 1: Get available GPU resources (SM)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts", "content": "### Step 1: Get available GPU resources (SM)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts\n\nDemonstrates how to retrieve the available Streaming Multiprocessor (SM) resources for a given GPU device using `cudaDeviceGetDevResource`.\n\n```APIDOC\n## GET /cudaDeviceGetDevResource (SM)\n\n### Description\nRetrieves the available SM resources for a specified GPU device.\n\n### Method\nCUDA Runtime API Call\n\n### Endpoint\n`cudaDeviceGetDevResource(int device, cudaDevResource* resource, cudaDevResourceType type)`\n\n### Parameters\n#### Path Parameters\nNone\n\n#### Query Parameters\nNone\n\n#### Request Body\nNone\n\n### Request Example\n```cpp\nint current_device = 0; // assume device ordinal of 0\nCUDA_CHECK(cudaSetDevice(current_device));\n\ncudaDevResource initial_SM_resources = {};\nCUDA_CHECK(cudaDeviceGetDevResource(current_device /* GPU device */,\n                                   &initial_SM_resources /* device resource to populate */,\n                                   cudaDevResourceTypeSm /* resource type*/));\n\nstd::cout << \"Initial SM resources: \" << initial_SM_resources.sm.smCount << \" SMs\" << std::endl; // number of available SMs\n\n// Special fields relevant for partitioning (see Step 3 below)\nstd::cout << \"Min. SM partition size: \" <<  initial_SM_resources.sm.minSmPartitionSize << \" SMs\" << std::endl;\nstd::cout << \"SM co-scheduled alignment: \" <<  initial_SM_resources.sm.smCoscheduledAlignment << \" SMs\" << std::endl;\n```\n\n### Response\n#### Success Response (cudaSuccess)\n- **initial_SM_resources** (`cudaDevResource`*) - Pointer to a structure populated with SM resource details.\n  - **smCount** (int) - The number of available SMs.\n  - **minSmPartitionSize** (int) - The minimum partition size for SMs.\n  - **smCoscheduledAlignment** (int) - The co-scheduled alignment for SMs.\n\n#### Response Example\n```\nInitial SM resources: 80 SMs\nMin. SM partition size: 4 SMs\nSM co-scheduled alignment: 2 SMs\n```\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00009", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Launching Cooperative Kernels", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis", "content": "### Launching Cooperative Kernels\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis\n\nExplains the methods for launching kernels cooperatively using `cudaLaunchCooperativeKernel`. This is required for grid synchronization and involves using the CUDA runtime API instead of the standard `<<<...>>>` syntax. Examples are provided for launching a fixed number of blocks and maximizing GPU occupancy.\n\n```APIDOC\n## POST /kernel/launch/cooperative\n\n### Description\nLaunches a CUDA kernel cooperatively, enabling grid-wide synchronization. This API should be used instead of the standard `<<<...>>>` syntax when cooperative features are required.\n\n### Method\nPOST\n\n### Endpoint\n`/kernel/launch/cooperative`\n\n### Parameters\n#### Path Parameters\nNone\n\n#### Query Parameters\nNone\n\n#### Request Body\n- **kernel_function** (function pointer) - Required - Pointer to the kernel function to launch.\n- **grid_dim** (dim3) - Required - The dimensions of the grid of thread blocks.\n- **block_dim** (dim3) - Required - The dimensions of the thread block.\n- **args** (array of void pointers) - Optional - Arguments to be passed to the kernel.\n\n### Request Example\n```cuda\n// Example 1: Launching a fixed number of blocks (e.g., matching SM count)\nint dev = 0;\ncudaDeviceProp deviceProp;\ncudaGetDeviceProperties(&deviceProp, dev);\n// initialize, then launch\ncudaLaunchCooperativeKernel((void*)my_kernel, deviceProp.multiProcessorCount, numThreads, args);\n\n// Example 2: Maximizing GPU occupancy\nint numBlocksPerSm = 0;\nint numThreads = 128;\ncudaDeviceProp deviceProp;\ncudaGetDeviceProperties(&deviceProp, dev);\ncudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocksPerSm, my_kernel, numThreads, 0);\nvoid *kernelArgs[] = { /* add kernel args */ };\ndim3 dimBlock(numThreads, 1, 1);\ndim3 dimGrid(deviceProp.multiProcessorCount*numBlocksPerSm, 1, 1);\ncudaLaunchCooperativeKernel((void*)my_kernel, dimGrid, dimBlock, kernelArgs);\n```\n\n### Response\n#### Success Response (200)\n- **status** (string) - Indicates the cooperative kernel launch was initiated successfully.\n\n#### Response Example\n```json\n{\n  \"status\": \"cooperative_launch_initiated\"\n}\n```\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00010", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Configure Workqueue Resource", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts", "content": "### Configure Workqueue Resource\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts\n\nExample demonstrating how to create a workqueue configuration resource for a specific device with balanced sharing scope and a concurrency limit.\n\n```APIDOC\n## POST /cuda/device/resource/workqueue\n\n### Description\nConfigures a workqueue resource for a specific CUDA device.\n\n### Method\nPOST\n\n### Endpoint\n/cuda/device/resource/workqueue\n\n### Parameters\n#### Request Body\n- **device** (integer) - Required - The ordinal of the CUDA device.\n- **sharingScope** (enum) - Required - The sharing scope of the workqueue (e.g., `cudaDevWorkqueueConfigScopeGreenCtxBalanced`).\n- **wqConcurrencyLimit** (integer) - Required - The maximum number of concurrent stream-ordered workloads expected.\n\n### Request Example\n```json\n{\n  \"device\": 0,\n  \"sharingScope\": \"cudaDevWorkqueueConfigScopeGreenCtxBalanced\",\n  \"wqConcurrencyLimit\": 4\n}\n```\n\n### Response\n#### Success Response (200)\n- **resource** (object) - The configured workqueue resource.\n  - **type** (enum) - The type of the resource, should be `cudaDevResourceTypeWorkqueueConfig`.\n  - **wqConfig** (object) - Workqueue configuration details.\n    - **device** (integer) - The device ordinal.\n    - **sharingScope** (enum) - The sharing scope.\n    - **wqConcurrencyLimit** (integer) - The concurrency limit.\n\n#### Response Example\n```json\n{\n  \"resource\": {\n    \"type\": \"cudaDevResourceTypeWorkqueueConfig\",\n    \"wqConfig\": {\n      \"device\": 0,\n      \"sharingScope\": \"cudaDevWorkqueueConfigScopeGreenCtxBalanced\",\n      \"wqConcurrencyLimit\": 4\n    }\n  }\n}\n```\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00011", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Step 1: Get available GPU resources (Workqueue Config)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts", "content": "### Step 1: Get available GPU resources (Workqueue Config)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts\n\nDemonstrates how to retrieve the available workqueue configuration resources for a given GPU device using `cudaDeviceGetDevResource`.\n\n```APIDOC\n## GET /cudaDeviceGetDevResource (Workqueue Config)\n\n### Description\nRetrieves the available workqueue configuration resources for a specified GPU device.\n\n### Method\nCUDA Runtime API Call\n\n### Endpoint\n`cudaDeviceGetDevResource(int device, cudaDevResource* resource, cudaDevResourceType type)`\n\n### Parameters\n#### Path Parameters\nNone\n\n#### Query Parameters\nNone\n\n#### Request Body\nNone\n\n### Request Example\n```cpp\nint current_device = 0; // assume device ordinal of 0\nCUDA_CHECK(cudaSetDevice(current_device));\n\ncudaDevResource initial_WQ_config_resources = {};\nCUDA_CHECK(cudaDeviceGetDevResource(current_device /* GPU device */,\n                                   &initial_WQ_config_resources /* device resource to populate */,\n                                   cudaDevResourceTypeWorkqueueConfig /* resource type*/));\n\nstd::cout << \"Initial WQ config. resources: \" << std::endl;\nstd::cout << \"  - WQ concurrency limit: \" << initial_WQ_config_resources.wqConfig.wqConcurrencyLimit << std::endl;\nstd::cout << \"  - WQ sharing scope: \" << initial_WQ_config_resources.wqConfig.sharingScope << std::endl;\n```\n\n### Response\n#### Success Response (cudaSuccess)\n- **initial_WQ_config_resources** (`cudaDevResource`*) - Pointer to a structure populated with workqueue configuration resource details.\n  - **wqConcurrencyLimit** (int) - The concurrency limit for the workqueue.\n  - **sharingScope** (int) - The sharing scope of the workqueue.\n\n#### Response Example\n```\nInitial WQ config. resources: \n  - WQ concurrency limit: 1024\n  - WQ sharing scope: 0\n```\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00012", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA Kernel Parameter Buffer Setup", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/03-advanced/driver-api", "content": "### CUDA Kernel Parameter Buffer Setup\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/03-advanced/driver-api\n\nThis C/C++ code demonstrates how to set up a parameter buffer for launching a CUDA kernel using `cuLaunchKernel` with the `CU_LAUNCH_PARAM_BUFFER_POINTER` option. It includes macros for aligning offsets and adding parameters to the buffer, ensuring correct alignment for various data types including built-in vectors and `CUdeviceptr`. This is crucial for passing parameters efficiently and correctly when using the driver API.\n\n```c++\n#define ALIGN_UP(offset, alignment) \\\n      (offset) = ((offset) + (alignment) - 1) & ~((alignment) - 1)\n\nchar paramBuffer[1024];\nsize_t paramBufferSize = 0;\n\n#define ADD_TO_PARAM_BUFFER(value, alignment)                   \\\n    do {                                                        \\\n        paramBufferSize = ALIGN_UP(paramBufferSize, alignment); \\\n        memcpy(paramBuffer + paramBufferSize,                   \\\n               &(value), sizeof(value));                        \\\n        paramBufferSize += sizeof(value);                       \\\n    } while (0)\n\nint i;\nADD_TO_PARAM_BUFFER(i, __alignof__(i));\nfloat4 f4;\nADD_TO_PARAM_BUFFER(f4, 16); // float4's alignment is 16\nchar c;\nADD_TO_PARAM_BUFFER(c, __alignof__(c));\nfloat f;\nADD_TO_PARAM_BUFFER(f, __alignof__(f));\nCUdeviceptr devPtr;\nADD_TO_PARAM_BUFFER(devPtr, __alignof__(devPtr));\nfloat2 f2;\nADD_TO_PARAM_BUFFER(f2, 8); // float2's alignment is 8\n\nvoid* extra[] = {\n    CU_LAUNCH_PARAM_BUFFER_POINTER, paramBuffer,\n    CU_LAUNCH_PARAM_BUFFER_SIZE,    &paramBufferSize,\n    CU_LAUNCH_PARAM_END\n};\ncuLaunchKernel(cuFunction,\n               blockWidth, blockHeight, blockDepth,\n               gridWidth, gridHeight, gridDepth,\n               0, 0, 0, extra);\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00013", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Setup and Teardown for Extern Variable", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory", "content": "### Setup and Teardown for Extern Variable\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory\n\nProvides setup and teardown functions for managing an externally declared global variable. This includes allocating and freeing memory for the variable.\n\n```c++\n/** This may be a non-CUDA file */\nchar* ext_data;\nstatic const char global_string[] = \"Hello World\";\n\nvoid __attribute__ ((constructor)) setup(void) {\n  ext_data = (char*)malloc(sizeof(global_string));\n  strncpy(ext_data, global_string, sizeof(global_string));\n}\n\nvoid __attribute__ ((destructor)) tear_down(void) {\n  free(ext_data);\n}\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00014", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "nvstd::function Usage Example", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-support", "content": "### nvstd::function Usage Example\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-support\n\nDemonstrates the usage of nvstd::function in host, device, and host-device code with lambda expressions and device functions.\n\n```APIDOC\n## nvstd::function Usage Example\n\n### Description\nThis example showcases how `nvstd::function` can store, copy, and invoke various callable targets, including lambda expressions and functions, within different CUDA execution spaces (host, device, host-device).\n\n### Method\nN/A (Code Example)\n\n### Endpoint\nN/A (Code Example)\n\n### Parameters\nN/A\n\n### Request Example\n```cpp\n#include <nvfunctional>\n\n__host__            int host_function()        { return 1; } \n__device__          int device_function()      { return 2; } \n__host__ __device__ int host_device_function() { return 3; } \n\n__global__ void kernel(int* result) {\n    nvstd::function<int()> fn1 = device_function;\n    nvstd::function<int()> fn2 = host_device_function;\n    nvstd::function<int()> fn3 = [](){ return 10; };\n    *result                    = fn1() + fn2() + fn3();\n}\n\n__host__ __device__ void host_device_test(int* result) {\n    nvstd::function<int()> fn1 = host_device_function;\n    nvstd::function<int()> fn2 = [](){ return 10; };\n    *result                    = fn1() + fn2();\n}\n\n__host__ void host_test(int* result) {\n    nvstd::function<int()> fn1 = host_function;\n    nvstd::function<int()> fn2 = host_device_function;\n    nvstd::function<int()> fn3 = [](){ return 10; };\n    *result                    = fn1() + fn2() + fn3();\n}\n```\n\n### Response\nN/A (Code Example)\n\n### Response Example\nN/A (Code Example)\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00015", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Diagnosing cuGetProcAddress Failures with CUDA", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access", "content": "### Diagnosing cuGetProcAddress Failures with CUDA\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access\n\nThis C code example illustrates how to interpret the CUdriverProcAddressQueryResult returned by cuGetProcAddress. It differentiates between API usage errors and cases where the driver API symbol is not found or the driver version is insufficient for the requested API version.\n\n```c\n#include <cuda.h>\n#include <stdio.h>\n\n// Assume cudaVersion is defined and set appropriately.\n// Assume pfn is a function pointer type compatible with the symbol.\n\nCUdriverProcAddressQueryResult driverStatus;\ncudaVersion = ...; // Example: 11030 or 11040\nvoid* pfn;\nCUresult status;\n\n// Example: cuDeviceGetExecAffinitySupport was introduced in CUDA 11.4\nstatus = cuGetProcAddress(\"cuDeviceGetExecAffinitySupport\", &pfn, cudaVersion, 0, &driverStatus);\n\nif (CUDA_SUCCESS == status) {\n    if (CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT == driverStatus) {\n        printf(\"Symbol found, but cudaVersion < 11.4. Upgrade cudaVersion to 11.4 for this feature.\\n\");\n        // Indicates cudaVersion was < 11.4 but run against a CUDA driver >= 11.4\n    }\n    else if (CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND == driverStatus) {\n        printf(\"Symbol not found. Please update both CUDA driver and cudaVersion to at least 11.4.\\n\");\n        // Indicates driver is < 11.4 or a typo in the symbol name.\n    }\n    else if (CU_GET_PROC_ADDRESS_SUCCESS == driverStatus && pfn) {\n        printf(\"cudaVersion and CUDA driver are >= 11.4. Using the new feature.\\n\");\n        // Cast pfn to the correct function pointer type and call it.\n        // ((your_function_pointer_type)pfn)();\n    }\n}\nelse {\n    printf(\"cuGetProcAddress failed with status: %d\\n\", status);\n}\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00016", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA C++ Data Prefetching Example (Managed Memory)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory", "content": "### CUDA C++ Data Prefetching Example (Managed Memory)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/unified-memory\n\nIllustrates using `cudaMemPrefetchAsync` with CUDA managed memory. It mirrors the system allocator example by prefetching data to the GPU and then back to the CPU.\n\n```cuda\nvoid test_prefetch_managed(const cudaStream_t& s) {\n  // initialize data on CPU\n  char *data;\n  cudaMallocManaged(&data, dataSizeBytes);\n  init_data(data, dataSizeBytes);\n  cudaMemLocation location = {.type = cudaMemLocationTypeDevice, .id = myGpuId};\n\n  // encourage data to move to GPU before use\n  const unsigned int flags = 0;\n  cudaMemPrefetchAsync(data, dataSizeBytes, location, flags, s);\n\n  // use data on GPU\n  const uinsigned num_blocks = (dataSizeBytes + threadsPerBlock - 1) / threadsPerBlock;\n  mykernel<<<num_blocks, threadsPerBlock, 0, s>>>(data, dataSizeBytes);\n\n  // encourage data to move back to CPU\n  location = {.type = cudaMemLocationTypeHost};\n  cudaMemPrefetchAsync(data, dataSizeBytes, location, flags, s);\n\n  cudaStreamSynchronize(s);\n\n  // use data on CPU\n  use_data(data, dataSizeBytes);\n  cudaFree(data);\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00017", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Query GPU Name and Compute Capability using nvidia-smi", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/compute-capabilities", "content": "### Query GPU Name and Compute Capability using nvidia-smi\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/compute-capabilities\n\nThis command-line utility, provided with the NVIDIA Driver, retrieves the names and compute capabilities of GPUs installed on the system. It's a straightforward way to get hardware information without writing code.\n\n```bash\nnvidia-smi --query-gpu=name,compute_cap\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00018", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Get Driver Entry Point with CUDA Runtime API (C)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access", "content": "### Get Driver Entry Point with CUDA Runtime API (C)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access\n\nDemonstrates using `cudaGetDriverEntryPoint` to retrieve a function pointer for a CUDA driver API. This method obtains the API for the current CUDA Runtime version, which may differ from the installed driver version, leading to potential mismatches.\n\n```c\n#include <cuda.h>\n#include <cudaTypedefs.h>\n#include <cuda_runtime.h>\n\nCUresult status;\ncudaError_t error;\nint driverVersion, runtimeVersion;\nCUdriverProcAddressQueryResult driverStatus;\n\n// Ask the runtime for the function\nPFN_cuDeviceGetUuid pfn_cuDeviceGetUuidRuntime;\nerror = cudaGetDriverEntryPoint (\"cuDeviceGetUuid\", &pfn_cuDeviceGetUuidRuntime, cudaEnableDefault, &driverStatus);\nif(cudaSuccess == error && pfn_cuDeviceGetUuidRuntime) {\n    // pfn_cuDeviceGetUuid points to ???\n}\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00019", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Basic CUDA Kernel Launch with printf", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-support", "content": "### Basic CUDA Kernel Launch with printf\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-support\n\nDemonstrates a simple CUDA kernel launch and the use of printf within the kernel. The output depends on conditional execution within the kernel.\n\n```cuda\n#include <stdio.h>\n\n__global__ void helloCUDA(float value) {\n    if (threadIdx.x == 0)\n        printf(\"Hello thread %d, value=%f\\n\", threadIdx.x, value);\n}\n\nint main() {\n    helloCUDA<<<1, 5>>>(1.2345f);\n    cudaDeviceSynchronize();\n    return 0;\n}\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00020", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Get UUID using Direct API Call (C)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access", "content": "### Get UUID using Direct API Call (C)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access\n\nThis example shows how to directly call the `cuDeviceGetUuid` API to retrieve the UUID of a CUDA device. This method is straightforward but relies on the CUDA toolkit version to provide the correct API implementation. Ensure proper error handling for the `cuDeviceGet` and `cuDeviceGetUuid` calls.\n\n```c\n#include <cuda.h>\n\nint main() {\n    CUuuid uuid;\n    CUdevice dev;\n    CUresult status;\n\n    status = cuDeviceGet(&dev, 0); // Get device 0\n    // handle status\n\n    status = cuDeviceGetUuid(&uuid, dev); // Get uuid of device 0\n    // handle status\n\n    return 0;\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00021", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA Graph Launch and Stream Synchronization", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/cuda-graphs", "content": "### CUDA Graph Launch and Stream Synchronization\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/cuda-graphs\n\nIllustrates launching CUDA graphs and synchronizing streams using events. This example shows how to establish dependencies between streams and graph executions to ensure correct ordering and data availability.\n\n```c++\ncudaGraphLaunch(allocGraphExec, allocStream);\n\n// establish the dependency of stream2 on the event node satisfies the ordering requirement\ncudaStreamWaitEvent(stream2, allocEvent);\nkernel<<< ..., stream2 >>> (dptr, ...);\ncudaStreamRecordEvent(streamUseDoneEvent, stream2);\n\n// the event wait node in the waitAndFreeGraphExec establishes the dependency on the \"readyForFreeEvent\" that is needed to prevent the kernel running in stream two from accessing the allocation after the free node in execution order.\ncudaGraphLaunch(waitAndFreeGraphExec, stream3);\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00022", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA printf() Example Kernel", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-support", "content": "### CUDA printf() Example Kernel\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-support\n\nAn example of using the printf() function within a CUDA kernel to print thread-specific information and a float value to the host console. It demonstrates basic formatting.\n\n```c\n#include <stdio.h>\n\n__global__ void helloCUDA(float value) {\n    printf(\"Hello thread %d, value=%f\\n\", threadIdx.x, value);\n}\n\nint main() {\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00023", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Device-Side Kernel Launch APIs", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis", "content": "### Device-Side Kernel Launch APIs\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis\n\nAPIs for launching kernels from device code, including parameter buffer creation and device launch execution.\n\n```APIDOC\n## Device-Side Kernel Launch APIs\n\n### Description\nProvides functions for launching kernels directly from device code, mimicking the host-side triple chevron syntax. These APIs are also accessible from PTX.\n\n### Method\n- `cudaGetParameterBuffer`\n- `cudaLaunchDevice`\n\n### Endpoint\nN/A (Device Runtime APIs)\n\n### Parameters\n\n#### `cudaGetParameterBuffer`\n\n- **params** (void **) - Output parameter to receive a pointer to the parameter buffer.\n\n#### `cudaLaunchDevice`\n\n- **kernel** (void *) - Pointer to the kernel function to be launched.\n- **params** (void *) - Pointer to the parameter buffer populated by `cudaGetParameterBuffer`.\n- **gridDim** (dim3) - The dimensions of the grid.\n- **blockDim** (dim3) - The dimensions of the block.\n- **sharedMemSize** (unsigned int) - Amount of dynamically allocated shared memory. Defaults to 0.\n- **stream** (cudaStream_t) - The stream to launch the kernel on. Defaults to 0.\n\n### Request Example\n\n```c\nvoid **params;\ncudaGetParameterBuffer(&params);\n// Populate parameter buffer here...\ncudaLaunchDevice(myKernel, params, dim3(10, 10), dim3(128), 0, 0);\n```\n\n### Response\n\n#### Success Response (cudaError_t)\n- **cudaSuccess** (cudaError_t) - Indicates the function executed successfully.\n\n#### Error Response (cudaError_t)\n- **cudaErrorInvalidValue** (cudaError_t) - If any of the arguments are invalid.\n- **cudaErrorLaunchFailure** (cudaError_t) - If the kernel launch fails.\n\n#### Response Example\n```c\n// cudaError_t result;\n// Check result for specific error codes\n```\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00024", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Launching Kernel on Green Context Stream", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts", "content": "### Launching Kernel on Green Context Stream\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/green-contexts\n\nDemonstrates creating a CUDA stream for a green context and launching a kernel on that stream, ensuring the kernel uses only the resources allocated to that stream's execution context.\n\n```APIDOC\n## POST /cudaExecutionCtxStreamCreate\n\n### Description\nCreates a CUDA stream associated with a specific green execution context. Kernels launched on this stream will be restricted to the resources (SMs, work queues) available to that context.\n\n### Method\n`cudaExecutionCtxStreamCreate` (API Call)\n\n### Parameters\n#### Path Parameters\nNone\n\n#### Query Parameters\nNone\n\n#### Request Body\n- **`green_ctx_stream`** (cudaStream_t*) - Output parameter to store the created stream.\n- **`green_ctx`** (cudaExecutionCtx_t) - The green execution context to associate the stream with.\n- **`stream_flags`** (cudaStreamFlags_t) - Flags for stream creation (e.g., `cudaStreamDefault`, `cudaStreamNonBlocking`).\n- **`priority`** (int) - Priority of the stream.\n\n### Request Example\n```c++\n// Create green_ctx_stream CUDA stream for previously created green_ctx green context\ncudaStream_t green_ctx_stream;\nint priority = 0;\nCUDA_CHECK(cudaExecutionCtxStreamCreate(&green_ctx_stream,\n                                        green_ctx,\n                                        cudaStreamDefault,\n                                        priority));\n\n// Kernel my_kernel will only use the resources (SMs, work queues, as applicable) available to green_ctx_stream's execution context\nmy_kernel<<<grid_dim, block_dim, 0, green_ctx_stream>>>();\nCUDA_CHECK(cudaGetLastError());\n```\n\n### Response\n#### Success Response (0)\n- **`cudaError_t`** (int) - Returns 0 on success.\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00025", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA C++: Stream Compaction Example using exclusive_scan", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis", "content": "### CUDA C++: Stream Compaction Example using exclusive_scan\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis\n\nThis C++ code demonstrates stream compaction using the `cg::exclusive_scan` collective operation. It filters data from an input array based on a predicate function, copying only the passing elements to an output array. The `exclusive_scan` is used to calculate the correct starting index for each thread in the output array, ensuring contiguous placement of compacted data.\n\n```cpp\n#include <cooperative_groups.h>\n#include <cooperative_groups/scan.h>\nnamespace cg = cooperative_groups;\n\n// put data from input into output only if it passes test_fn predicate\ntemplate<typename Group, typename Data, typename TyFn>\n__device__ int stream_compaction(Group &g, Data *input, int count, TyFn&& test_fn, Data *output) {\n    int per_thread = count / g.num_threads();\n    int thread_start = min(g.thread_rank() * per_thread, count);\n    int my_count = min(per_thread, count - thread_start);\n\n    // get all passing items from my part of the input\n    //  into a contagious part of the array and count them.\n    int i = thread_start;\n    while (i < my_count + thread_start) {\n        if (test_fn(input[i])) {\n            i++;\n        }\n        else {\n            my_count--;\n            input[i] = input[my_count + thread_start];\n        }\n    }\n\n    // scan over counts from each thread to calculate my starting\n    //  index in the output\n    int my_idx = cg::exclusive_scan(g, my_count);\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00026", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Complete CUDA Graph Creation, Instantiation, and Execution", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/cuda-graphs", "content": "### Complete CUDA Graph Creation, Instantiation, and Execution\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/cuda-graphs\n\nA comprehensive example demonstrating the full lifecycle of a CUDA graph: capturing operations into a graph, instantiating it for execution, and then launching the instantiated graph. This illustrates how to create, prepare, and run a graph sequentially.\n\n```c++\ncudaGraph_t graph;\n\ncudaStreamBeginCapture(stream);\n\nkernel_A<<< ..., stream >>>(...);\nkernel_B<<< ..., stream >>>(...);\nlibraryCall(stream);\nkernel_C<<< ..., stream >>>(...);\n\ncudaStreamEndCapture(stream, &graph);\n\ncudaGraphExec_t graphExec;\ncudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\ncudaGraphLaunch(graphExec, stream);\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00027", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Dynamically Obtain CUDA Function Pointer with Runtime Version Check (C)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access", "content": "### Dynamically Obtain CUDA Function Pointer with Runtime Version Check (C)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access\n\nDemonstrates how to dynamically obtain a CUDA function pointer using `cuGetProcAddress` by querying the driver version at runtime. This example addresses potential undefined behavior when the runtime driver version differs from the compile-time assumed version, especially after driver upgrades. It includes error handling for `cuDeviceGet` and `cuDriverGetVersion`.\n\n```c\n#include <cudaTypedefs.h>\n\nCUuuid uuid;\nCUdevice dev;\nCUresult status;\nint cudaVersion;\nCUdriverProcAddressQueryResult driverStatus;\n\nstatus = cuDeviceGet(&dev, 0); // Get device 0\n// handle status\n\nstatus = cuDriverGetVersion(&cudaVersion);\n// handle status\n\nPFN_cuDeviceGetUuid pfn_cuDeviceGetUuid;\nstatus = cuGetProcAddress(\"cuDeviceGetUuid\", &pfn_cuDeviceGetUuid, cudaVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);\nif(CUDA_SUCCESS == status && pfn_cuDeviceGetUuid) {\n    // pfn_cuDeviceGetUuid points to ???\n}\n\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00028", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Handle API Version Bumps with Explicit Runtime Checks (C)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access", "content": "### Handle API Version Bumps with Explicit Runtime Checks (C)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/driver-entry-point-access\n\nIllustrates a scenario where API functions have been updated across CUDA minor versions, requiring explicit runtime checks to ensure compatibility. This example shows how to use `cuGetProcAddress` with version-specific typedefs and conditional logic based on the runtime driver version to select the correct function pointer. It warns that without updating typedefs and case handling, applications may encounter undefined behavior.\n\n```c\nCUresult cuFoo(int bar); // Introduced in CUDA 11.4\nCUresult cuFoo_v2(int bar); // Introduced in CUDA 11.5\nCUresult cuFoo_v3(int bar, void* jazz); // Introduced in CUDA 11.6\n\ntypedef CUresult (CUDAAPI *PFN_cuFoo_v11040)(int bar);\ntypedef CUresult (CUDAAPI *PFN_cuFoo_v11050)(int bar);\ntypedef CUresult (CUDAAPI *PFN_cuFoo_v11060)(int bar, void* jazz);\n\n#include <cuda.h>\n#include <cudaTypedefs.h>\n\nCUresult status;\nint cudaVersion;\nCUdriverProcAddressQueryResult driverStatus;\n\nstatus = cuDriverGetVersion(&cudaVersion);\n// handle status\n\nPFN_cuFoo_v11040 pfn_cuFoo_v11040;\nPFN_cuFoo_v11050 pfn_cuFoo_v11050;\nif(cudaVersion < 11050 ) {\n    // We know to get the CUDA 11.4 version\n    status = cuGetProcAddress(\"cuFoo\", &pfn_cuFoo_v11040, cudaVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);\n    // Handle status and validating pfn_cuFoo_v11040\n}\nelse {\n    // Assume >= CUDA 11.5 version we can use the second version\n    status = cuGetProcAddress(\"cuFoo\", &pfn_cuFoo_v11050, cudaVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);\n    // Handle status and validating pfn_cuFoo_v11050\n}\n\n\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00029", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "WMMA API Overview", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions", "content": "### WMMA API Overview\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions\n\nProvides an overview of the WMMA API, including the `nvcuda::wmma` namespace and experimental features.\n\n```APIDOC\n## WMMA API Overview\n\nAll functions and types in the WMMA API are defined within the `nvcuda::wmma` namespace. Sub-byte operations are considered preview features and are located in the `nvcuda::wmma::experimental` namespace, with the potential for changes in future releases.\n\n**Key Components:**\n\n*   **`fragment` class**: Represents a tile of a matrix distributed across threads in a warp.\n*   **`load_matrix_sync`**: Loads matrix data from memory into a fragment.\n*   **`store_matrix_sync`**: Stores matrix data from a fragment back to memory.\n*   **`fill_fragment`**: Fills a fragment with a constant value.\n*   **`mma_sync`**: Performs a warp-synchronous matrix multiply-accumulate operation.\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00030", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA Device Atomic Load Function Example", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions", "content": "### CUDA Device Atomic Load Function Example\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions\n\nAn example of a CUDA device atomic load function signature, demonstrating how to load a value atomically with specified memory order and thread scope. This function can only be used in device functions.\n\n```cuda\n__device__ T __nv_atomic_load_n(T*  pointer,\n                                int memory_order,\n                                int thread_scope = __NV_THREAD_SCOPE_SYSTEM);\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00031", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "DPX Example: Min of Two Unsigned Integers with Predicate (CUDA C)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions", "content": "### DPX Example: Min of Two Unsigned Integers with Predicate (CUDA C)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions\n\nExample demonstrating the `__vibmin_u32` function in CUDA C. It finds the minimum of two unsigned 32-bit integers and outputs a boolean predicate indicating which value was smaller.\n\n```CUDA C\nunsigned a = 9;\nunsigned b = 6;\nbool     smaller_value;\nunsigned min_value = __vibmin_u32(a, b, &smaller_value); // min_value is 6, smaller_value is true\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00032", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA Kernel Launch Example with Child Grid and Tail Launch", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/dynamic-parallelism", "content": "### CUDA Kernel Launch Example with Child Grid and Tail Launch\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/dynamic-parallelism\n\nDemonstrates launching a child kernel (`child_launch`) and a tail launch kernel (`tail_launch`) from a parent kernel (`parent_launch`). The child kernel executes on the default stream, while the tail launch uses `cudaStreamTailLaunch` to ensure modifications are visible after the child grid exits. Synchronization primitives like `__syncthreads()` are used to manage memory visibility between threads within a block.\n\n```cuda\n__global__ void tail_launch(int *data) {\n   data[threadIdx.x] = data[threadIdx.x]+1;\n}\n\n__global__ void child_launch(int *data) {\n   data[threadIdx.x] = data[threadIdx.x]+1;\n}\n\n__global__ void parent_launch(int *data) {\n   data[threadIdx.x] = threadIdx.x;\n\n   __syncthreads();\n\n   if (threadIdx.x == 0) {\n       child_launch<<< 1, 256 >>>(data);\n       tail_launch<<< 1, 256, 0, cudaStreamTailLaunch >>>(data);\n   }\n}\n\nvoid host_launch(int *data) {\n    parent_launch<<< 1, 256 >>>(data);\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00033", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA thread_block Example: Global to Shared Memory Load", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis", "content": "### CUDA thread_block Example: Global to Shared Memory Load\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis\n\nAn example kernel demonstrating how to use `thread_block` to load data from global memory into shared memory. It shows thread synchronization after the load to ensure all threads can access the shared data.\n\n```cuda-c++\n__global__ void kernel(int *globalInput) {\n    __shared__ int x;\n    thread_block g = this_thread_block();\n    // Choose a leader in the thread block\n    if (g.thread_rank() == 0) {\n        // load from global into shared for all threads to work with\n        x = (*globalInput);\n    }\n    // After loading data into shared memory, you want to synchronize\n    // if all threads in your thread block need to see it\n    g.sync(); // equivalent to __syncthreads();\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00034", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Batched Memory Transfer with Location Hints (C++)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/03-advanced/advanced-host-programming", "content": "### Batched Memory Transfer with Location Hints (C++)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/03-advanced/advanced-host-programming\n\nDemonstrates setting source and destination location hints for batched memory transfers using cudaMemcpyBatchAsync. This example utilizes managed memory and prefetching to optimize data staging between device and host NUMA nodes.\n\n```cpp\n// Allocate the source and destination buffers\nstd::vector<void *> srcs(batch_size);\nstd::vector<void *> dsts(batch_size);\nstd::vector<void *> sizes(batch_size);\n\n// cudaMemLocation structures we will use tp provide location hints\n// Device device_id\ncudaMemLocation srcLoc = {cudaMemLocationTypeDevice, dev_id};\n\n// Host with numa Node numa_id\ncudaMemLocation dstLoc = {cudaMemLocationTypeHostNuma, numa_id};\n\n// Allocate the src and dst buffers\nfor (size_t i = 0; i < batch_size; i++) {\n    cudaMallocManaged(&srcs[i], sizes[i]);\n    cudaMallocManaged(&dsts[i], sizes[i]);\n\n    cudaMemPrefetchAsync(srcs[i], sizes[i], srcLoc, 0, stream);\n    cudaMemPrefetchAsync(dsts[i], sizes[i], dstLoc, 0, stream);\n    cudaMemsetAsync(srcs[i], sizes[i], stream);\n}\n\n// Setup attributes for this batch of copies\ncudaMemcpyAttributes attrs = {};\n\n// These are managed memory pointers so Stream Order is appropriate\nattrs.srcAccessOrder = cudaMemcpySrcAccessOrderStream;\n\n// Now we can specify the location hints here.\nattrs.srcLocHint = srcLoc;\nattrs.dstlocHint = dstLoc;\n\n// All copies in the batch have same copy attributes.\nsize_t attrsIdxs = 0;\n\n// Launch the batched memory transfer\ncudaMemcpyBatchAsync(&dsts[0], &srcs[0], &sizes[0], batch_size,\n    &attrs, &attrsIdxs, 1 /*numAttrs*/, nullptr /*failIdx*/, stream);\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00035", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "CUDA __managed__ Memory Restrictions and Usage Examples (C++)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions", "content": "### CUDA __managed__ Memory Restrictions and Usage Examples (C++)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/cpp-language-extensions\n\nDemonstrates legal and illegal uses of __managed__ variables in CUDA C++. It covers restrictions related to static initialization, references, const-qualification, and usage with decltype. The example includes kernel launches and host-side operations.\n\n```cpp\n#include <cassert>\n\n__device__ __managed__ int global_var = 10; // OK\n\nint* ptr = &global_var;                     // ERROR: use of a managed variable in static initialization\n\nstruct MyStruct1 {\n    int field;\n    MyStruct1() : field(global_var) {};\n};\n\nstruct MyStruct2 {\n    ~MyStruct2() { global_var = 10; }\n};\n\nMyStruct1 temp1; // ERROR: use of managed variable in dynamic initialization\n\nMyStruct2 temp2; // ERROR: use of managed variable in the destructor of\n                 //        object with static storage duration\n\n__device__ __managed__ const int const_var = 10;         // ERROR: const-qualified type\n\n__device__ __managed__ int&      reference = global_var; // ERROR: reference type\n\ntemplate <int* Addr>\nstruct MyStruct3 {};\n\nMyStruct3<&global_var> temp;     // ERROR: address of managed variable is not a constant expression\n\n__global__ void kernel(int* ptr) {\n    assert(ptr == &global_var);  // OK\n    global_var = 20;             // OK\n}\n\nint main() {\n    int* ptr = &global_var;      // OK\n    kernel<<<1, 1>>>(ptr);\n    cudaDeviceSynchronize();\n    global_var++;                // OK\n    decltype(global_var) var1;   // ERROR: managed variable used as unparenthesized argument to decltype\n\n    decltype((global_var)) var2; // OK\n}\n```"}
{"id": "nvidia_cuda_cuda-programming-guide_00036", "source_lib": "nvidia_cuda_cuda-programming-guide", "title": "Example: Using block_tile_memory for larger tiles (C++)", "source_url": "https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis", "content": "### Example: Using block_tile_memory for larger tiles (C++)\n\nSource: https://docs.nvidia.com/cuda/cuda-programming-guide/05-appendices/device-callable-apis\n\nThis example illustrates the use of `block_tile_memory` to create tiles of size 128 across all Compute Capabilities. The comment indicates that `block_tile_memory` can be omitted on Compute Capability 8.0 or higher. This is intended for use within a CUDA kernel.\n\n```cpp\n/// The following code will create tiles of size 128 on all Compute Capabilities.\n/// block_tile_memory can be omitted on Compute Capability 8.0 or higher.\n__global__ void kernel(...) {\n    // reserve shared memory for thread_block_tile usage,\n\n```"}
{"id": "nvidia_cuda_00000", "source_lib": "nvidia_cuda", "title": "Install CUDA Runfile and Set Up Environment (Generic Linux)", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install CUDA Runfile and Set Up Environment (Generic Linux)\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls CUDA using the Runfile installer, which may require disabling Nouveau drivers and rebooting. It sets up the development environment and guides sample execution. This method is suitable for systems without RPM package managers or for custom installations.\n\n```shell\n# Disable Nouveau drivers if necessary\n# echo 'blacklist nouveau' | sudo tee /etc/modprobe.d/blacklist-nouveau.conf\n# sudo dracut --force\n# sudo reboot\n# Reboot into runlevel 3 by adding '3 nomodeset' to kernel parameters\n# Run the installer silently:\n# sudo sh cuda_<version>_<os>_<arch>.run --silent --accept-eula\n# Create xorg.conf file for NVIDIA GPU if needed\n# sudo nvidia-xconfig\n# sudo reboot\nexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\ngit clone https://github.com/nvidia/cuda-samples.git\ncd cuda-samples/Samples/0_Introduction/vectorAdd\nmake\n./vectorAdd\n```"}
{"id": "nvidia_cuda_00001", "source_lib": "nvidia_cuda", "title": "Install CUDA Runfile and Set Up Environment (OpenSUSE)", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install CUDA Runfile and Set Up Environment (OpenSUSE)\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls CUDA using the Runfile installer, which requires disabling Nouveau drivers and rebooting into runlevel 3. It also sets up the development environment and guides sample execution. This method involves modifying kernel parameters and creating an xorg.conf file.\n\n```shell\necho 'blacklist nouveau' | sudo tee /etc/modprobe.d/blacklist-nouveau.conf\nsudo dracut --force\nsudo reboot\n# Add '3 nomodeset' to kernel parameters\n# Then run the installer silently:\n# sudo sh cuda_11.8.0_520.61.05-1_linux-x86_64.run --silent --accept-eula\n# Create xorg.conf file for NVIDIA GPU\nsudo nvidia-xconfig\nsudo reboot\nexport PATH=/usr/local/cuda-11.8/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\ngit clone https://github.com/nvidia/cuda-samples.git\ncd cuda-samples/Samples/5_Domain_Specific/nbody\nmake\n./nbody\n```"}
{"id": "nvidia_cuda_00002", "source_lib": "nvidia_cuda", "title": "Install CUDA Upgrade Package (Example Command)", "source_url": "https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index", "content": "### Install CUDA Upgrade Package (Example Command)\n\nSource: https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index\n\nProvides an example command to install the CUDA 11.8 upgrade package for Linux-aarch64-jetson devices from CUDA Repos.\n\n```bash\nsudo apt-get update\nsudo apt-get install cuda-toolkit-11-8\n```"}
{"id": "nvidia_cuda_00003", "source_lib": "nvidia_cuda", "title": "Example: Solving Preconditioned System with cusparse Dcsrilu02", "source_url": "https://docs.nvidia.com/cuda/cusparse/index", "content": "### Example: Solving Preconditioned System with cusparse Dcsrilu02\n\nSource: https://docs.nvidia.com/cuda/cusparse/index\n\nThis C++ example demonstrates how to solve a preconditioned system M*y = x, where M is the product of LU factors (L and U) obtained from an incomplete LU factorization of a sparse matrix A. It covers the necessary setup steps including creating matrix descriptors, info structures, querying buffer sizes, allocating memory, and setting up matrix properties for the factorization and solving phases.\n\n```cpp\n// Suppose that A is m x m sparse matrix represented by CSR format,\n// Assumption:\n// - handle is already created by cusparseCreate(),\n// - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory,\n// - d_x is right hand side vector on device memory,\n// - d_y is solution vector on device memory.\n// - d_z is intermediate result on device memory.\n\ncusparseMatDescr_t descr_M = 0; cusparseMatDescr_t descr_L = 0; cusparseMatDescr_t descr_U = 0;\ncsrilu02Info_t info_M = 0;\ncsrsv2Info_t info_L = 0; csrsv2Info_t info_U = 0;\nint pBufferSize_M; int pBufferSize_L; int pBufferSize_U; int pBufferSize;\nvoid* pBuffer = 0;\nint structural_zero; int numerical_zero;\nconst double alpha = 1.;\nconst cusparseSolvePolicy_t policy_M = CUSPARSE_SOLVE_POLICY_NO_LEVEL;\nconst cusparseSolvePolicy_t policy_L = CUSPARSE_SOLVE_POLICY_NO_LEVEL;\nconst cusparseSolvePolicy_t policy_U = CUSPARSE_SOLVE_POLICY_USE_LEVEL;\nconst cusparseOperation_t trans_L = CUSPARSE_OPERATION_NON_TRANSPOSE;\nconst cusparseOperation_t trans_U = CUSPARSE_OPERATION_NON_TRANSPOSE;\n\n// step 1: create a descriptor which contains\n// - matrix M is base-1\n// - matrix L is base-1\n// - matrix L is lower triangular\n// - matrix L has unit diagonal\n// - matrix U is base-1\n// - matrix U is upper triangular\n// - matrix U has non-unit diagonal\ncusparseCreateMatDescr(&descr_M);\ncusparseSetMatIndexBase(descr_M, CUSPARSE_INDEX_BASE_ONE);\ncusparseSetMatType(descr_M, CUSPARSE_MATRIX_TYPE_GENERAL);\ncusparseCreateMatDescr(&descr_L);\ncusparseSetMatIndexBase(descr_L, CUSPARSE_INDEX_BASE_ONE);\ncusparseSetMatType(descr_L, CUSPARSE_MATRIX_TYPE_GENERAL);\ncusparseSetMatFillMode(descr_L, CUSPARSE_FILL_MODE_LOWER);\ncusparseSetMatDiagType(descr_L, CUSPARSE_DIAG_TYPE_UNIT);\ncusparseCreateMatDescr(&descr_U);\ncusparseSetMatIndexBase(descr_U, CUSPARSE_INDEX_BASE_ONE);\ncusparseSetMatType(descr_U, CUSPARSE_MATRIX_TYPE_GENERAL);\ncusparseSetMatFillMode(descr_U, CUSPARSE_FILL_MODE_UPPER);\ncusparseSetMatDiagType(descr_U, CUSPARSE_DIAG_TYPE_NON_UNIT);\n\n// step 2: create a empty info structure\n// we need one info for csrilu02 and two info's for csrsv2\ncusparseCreateCsrilu02Info(&info_M);\ncusparseCreateCsrsv2Info(&info_L);\ncusparseCreateCsrsv2Info(&info_U);\n\n// step 3: query how much memory used in csrilu02 and csrsv2, and allocate the buffer\ncusparseDcsrilu02_bufferSize(handle, m, nnz, descr_M, d_csrVal, d_csrRowPtr, d_csrColInd, info_M, &pBufferSize_M);\ncusparseDcsrsv2_bufferSize(handle, trans_L, m, nnz, descr_L, d_csrVal, d_csrRowPtr, d_csrColInd, info_L, &pBufferSize_L);\ncusparseDcsrsv2_bufferSize(handle, trans_U, m, nnz, descr_U, d_csrVal, d_csrRowPtr, d_csrColInd, info_U, &pBufferSize_U);\npBufferSize = max(pBufferSize_M, max(pBufferSize_L, pBufferSize_U));\n\n// pBuffer returned by cudaMalloc is automatically aligned to 128 bytes.\ncudaMalloc((void**)&pBuffer, pBufferSize);\n```"}
{"id": "nvidia_cuda_00004", "source_lib": "nvidia_cuda", "title": "Install CUDA RPM and Set Up Environment (Generic Linux)", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install CUDA RPM and Set Up Environment (Generic Linux)\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls CUDA using the RPM package manager, configures user permissions, sets up development environment variables, and provides instructions for running samples. This is a general approach for RPM-based systems.\n\n```shell\nsudo zypper install-repo --no-gpg-check <path_to_repo_file>\nsudo zypper refresh\nsudo rpm --import <gpg_key_url>\nsudo zypper install cuda\nsudo usermod -aG video ${USER}\nsudo reboot\nexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\ngit clone https://github.com/nvidia/cuda-samples.git\ncd cuda-samples/Samples/0_Introduction/vectorAdd\nmake\n./vectorAdd\n```"}
{"id": "nvidia_cuda_00005", "source_lib": "nvidia_cuda", "title": "Install CUDA Runtime Package using Pip", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install CUDA Runtime Package using Pip\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls the core CUDA runtime package using pip. This command fetches and installs the necessary components for running CUDA applications on Windows when using the pip wheel method.\n\n```bash\npip install nvidia-cuda-runtime-cu12\n```"}
{"id": "nvidia_cuda_00006", "source_lib": "nvidia_cuda", "title": "Install CUDA RPM and Set Up Environment (OpenSUSE)", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install CUDA RPM and Set Up Environment (OpenSUSE)\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls CUDA using the RPM package manager, configures user permissions, sets up development environment variables, and provides instructions for running samples. Requires Zypper for package management and specific environment variables for CUDA tools.\n\n```shell\nsudo zypper install-repo --no-gpg-check https://developer.download.nvidia.com/compute/cuda/repos/opensuse-leap/15.4/x86_64/cuda-opensuse-leap154-local.repo\nsudo zypper refresh\nsudo rpm --import https://developer.download.nvidia.com/compute/cuda/repos/opensuse-leap/15.4/x86_64/7fa2af80.asc\nsudo zypper install cuda\nsudo usermod -aG video ${USER}\nsudo reboot\nexport PATH=/usr/local/cuda-11.8/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\ngit clone https://github.com/nvidia/cuda-samples.git\ncd cuda-samples/Samples/5_Domain_Specific/nbody\nmake\n./nbody\n```"}
{"id": "nvidia_cuda_00007", "source_lib": "nvidia_cuda", "title": "Install CUDA Toolkit using Conda", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install CUDA Toolkit using Conda\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls the complete CUDA Toolkit using the Conda package manager. This command is used for a comprehensive installation of all CUDA components, including the runtime and development tools.\n\n```bash\nconda install cuda -c nvidia\n```"}
{"id": "nvidia_cuda_00008", "source_lib": "nvidia_cuda", "title": "Install CUDA SDK on SUSE Linux Enterprise Server", "source_url": "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index", "content": "### Install CUDA SDK on SUSE Linux Enterprise Server\n\nSource: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index\n\nInstalls the CUDA Software Development Kit (SDK) on SUSE Linux Enterprise Server. This command refreshes the Zypper repository cache and then installs the CUDA SDK package.\n\n```bash\nsudo zypper refresh\nsudo zypper install cuda\n```"}
{"id": "nvidia_cuda_00009", "source_lib": "nvidia_cuda", "title": "Example Usage of bsrilt02() for Solving Sparse Systems", "source_url": "https://docs.nvidia.com/cuda/cusparse/index", "content": "### Example Usage of bsrilt02() for Solving Sparse Systems\n\nSource: https://docs.nvidia.com/cuda/cusparse/index\n\nIllustrates the process of solving a sparse linear system M*y = x using the bsrilt02() function, where M is derived from the LU factorization of a sparse matrix A in BSR format. This example includes setup for matrix descriptors, factorization info, and solve policies.\n\n```cuda\n// Suppose that A is m x m sparse matrix represented by BSR format,\n// The number of block rows/columns is mb, and\n// the number of nonzero blocks is nnzb.\n// Assumption:\n// - handle is already created by cusparseCreate(),\n// - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of A on device memory,\n// - d_x is right hand side vector on device memory.\n// - d_y is solution vector on device memory.\n// - d_z is intermediate result on device memory.\n// - d_x, d_y and d_z are of size m.\n\ncusparseMatDescr_t descr_M = 0; cusparseMatDescr_t descr_L = 0; cusparseMatDescr_t descr_U = 0;\nbsrilu02Info_t info_M = 0; bsrsv2Info_t info_L = 0; bsrsv2Info_t info_U = 0;\nint pBufferSize_M; int pBufferSize_L; int pBufferSize_U; int pBufferSize;\nvoid *pBuffer = 0;\nint structural_zero; int numerical_zero;\nconst double alpha = 1.0;\nconst cusparseSolvePolicy_t policy_M = CUSPARSE_SOLVE_POLICY_NO_LEVEL;\nconst cusparseSolvePolicy_t policy_L = CUSPARSE_SOLVE_POLICY_NO_LEVEL;\nconst cusparseSolvePolicy_t policy_U = CUSPARSE_SOLVE_POLICY_USE_LEVEL;\nconst cusparseOperation_t trans_L = CUSPARSE_OPERATION_NON_TRANSPOSE;\nconst cusparseOperation_t trans_U = CUSPARSE_OPERATION_NON_TRANSPOSE;\nconst cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN;\n\n// step 1: create a descriptor which contains\n// - matrix M is base-1\n// - matrix L is base-1\n// - matrix L is lower triangular\n// - matrix L has unit diagonal\n// - matrix U is base-1\n// - matrix U is upper triangular\n// - matrix U has non-unit diagonal\n\n```"}
{"id": "nvidia_cuda_00010", "source_lib": "nvidia_cuda", "title": "Configure Local CUDA Repository (Amazon Linux)", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Configure Local CUDA Repository (Amazon Linux)\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nSets up a local repository for installing CUDA on Amazon Linux. This involves downloading and installing the repository metadata file, enabling it for package management.\n\n```shell\nsudo rpm -i <path_to_cuda_repo_file.rpm>\n```"}
{"id": "nvidia_cuda_00011", "source_lib": "nvidia_cuda", "title": "Install CUDA using RPM Installer", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install CUDA using RPM Installer\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls the CUDA Toolkit on Red Hat-based systems (RHEL, CentOS) using the RPM package manager. This command installs the repository metadata, cleans the yum cache, and then installs the CUDA package.\n\n```bash\nsudo yum install cuda\n```"}
{"id": "nvidia_cuda_00012", "source_lib": "nvidia_cuda", "title": "nvprune cuBLAS Library Optimization Example", "source_url": "https://docs.nvidia.com/cuda/cublas/index", "content": "### nvprune cuBLAS Library Optimization Example\n\nSource: https://docs.nvidia.com/cuda/cublas/index\n\nThis command-line example demonstrates how to use `nvprune` to reduce the size of the cuBLAS static library by including only device code for specific target architectures. It shows the recommended way to specify multiple architectures and contrasts it with a less optimal approach.\n\n```bash\nnvprune --generate-code code=sm_70 --generate-code code=sm_75 libcublasLt_static.a -o libcublasLt_static_sm70_sm75.a\n```\n\n```bash\nnvprune -arch=sm_75 libcublasLt_static.a -o libcublasLt_static_sm75.a\n```"}
{"id": "nvidia_cuda_00013", "source_lib": "nvidia_cuda", "title": "Install Previous CUDA Release using Conda", "source_url": "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index", "content": "### Install Previous CUDA Release using Conda\n\nSource: https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index\n\nInstalls a specific previous version of CUDA using Conda by appending the CUDA release label to the install command. If components are split across versions, multiple labels can be included.\n\n```bash\nconda install cuda-cnvidia/label/cuda-11.3.0\nconda install cuda-cnvidia/label/cuda-11.3.0-cnvidia/label/cuda-11.3.1\n```"}
{"id": "nvidia_cuda_00014", "source_lib": "nvidia_cuda", "title": "Dynamic Parallelism 'Hello World' Example", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### Dynamic Parallelism 'Hello World' Example\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nA basic 'Hello World' program demonstrating dynamic parallelism, including kernel launches from within other kernels.\n\n```APIDOC\n## Dynamic Parallelism 'Hello World' Example\n\n### Description\nThis example showcases dynamic parallelism by launching child kernels and other kernels from within a parent kernel, demonstrating inter-kernel communication and synchronization.\n\n### Method\nKernel Launch\n\n### Endpoint\nN/A\n\n### Parameters\nN/A\n\n### Request Example\n```c++\n#include<stdio.h>\n\n__global__ void childKernel() {\n    printf(\"Hello \");\n}\n\n__global__ void tailKernel() {\n    printf(\"World!\\n\");\n}\n\n__global__ void parentKernel() {\n    // launch child\n    childKernel<<<1,1>>>();\n    if (cudaSuccess != cudaGetLastError()) {\n        return;\n    }\n\n    // launch tail into cudaStreamTailLaunch stream\n    // implicitly synchronizes: waits for child to complete\n    tailKernel<<<1,1,0,cudaStreamTailLaunch>>>();\n}\n\nint main(int argc, char* argv[]) {\n    // launch parent\n    parentKernel<<<1,1>>>();\n    if (cudaSuccess != cudaGetLastError()) {\n        return 1;\n    }\n\n    // wait for parent to complete\n    if (cudaSuccess != cudaDeviceSynchronize()) {\n        return 2;\n    }\n\n    return 0;\n}\n```\n\n### Response\n#### Success Response (200)\nProgram executes successfully, printing \"Hello World!\\n\" to the console.\n\n#### Response Example\n```\nHello World!\n\n```\n```"}
{"id": "nvidia_cuda_00015", "source_lib": "nvidia_cuda", "title": "Example: Using CUlaunchAttribute for Cooperative Launch", "source_url": "https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC", "content": "### Example: Using CUlaunchAttribute for Cooperative Launch\n\nSource: https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC\n\nDemonstrates how to set up a `CUlaunchConfig` with a cooperative launch attribute and invoke `cuLaunchKernelEx`.\n\n```c\nCUlaunchAttribute coopAttr = {.id = CU_LAUNCH_ATTRIBUTE_COOPERATIVE,\n                                .value = 1};\nCUlaunchConfig config = {... // set block and grid dimensions\n                              .attrs = &coopAttr,\n                              .numAttrs = 1};\n\ncuLaunchKernelEx(&config, kernel, NULL, NULL);\n```"}
{"id": "nvidia_cuda_00016", "source_lib": "nvidia_cuda", "title": "Example: Solving Preconditioned System with Incomplete Cholesky (C++)", "source_url": "https://docs.nvidia.com/cuda/cusparse/index", "content": "### Example: Solving Preconditioned System with Incomplete Cholesky (C++)\n\nSource: https://docs.nvidia.com/cuda/cusparse/index\n\nDemonstrates solving a preconditioned system M*y = x, where M is the product of an incomplete Cholesky factorization (M=LLH). This example utilizes cusparseMatDescr, csric02Info, and csrsv2Info structures, along with buffer allocation and solve policy configurations.\n\n```cuda\n// Suppose that A is m x m sparse matrix represented by CSR format,\n// Assumption:\n// - handle is already created by cusparseCreate(),\n// - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory,\n// - d_x is right hand side vector on device memory,\n// - d_y is solution vector on device memory.\n// - d_z is intermediate result on device memory.\n\ncusparseMatDescr_t descr_M = 0;\ncusparseMatDescr_t descr_L = 0;\ncsric02Info_t info_M = 0;\ncsrsv2Info_t info_L = 0;\ncsrsv2Info_t info_Lt = 0;\nint pBufferSize_M;\nint pBufferSize_L;\nint pBufferSize_Lt;\nint pBufferSize;\nvoid* pBuffer = 0;\nint structural_zero;\nint numerical_zero;\nconst double alpha = 1.0;\nconst cusparseSolvePolicy_t policy_M = CUSPARSE_SOLVE_POLICY_NO_LEVEL;\nconst cusparseSolvePolicy_t policy_L = CUSPARSE_SOLVE_POLICY_NO_LEVEL;\nconst cusparseSolvePolicy_t policy_Lt = CUSPARSE_SOLVE_POLICY_USE_LEVEL;\nconst cusparseOperation_t trans_L = CUSPARSE_OPERATION_NON_TRANSPOSE;\nconst cusparseOperation_t trans_Lt = CUSPARSE_OPERATION_TRANSPOSE;\n\n// step 1: create a descriptor which contains\n// - matrix M is base-1\n// - matrix L is base-1\n// - matrix L is lower triangular\n// - matrix L has non-unit diagonal\ncusparseCreateMatDescr(&descr_M);\ncusparseSetMatIndexBase(descr_M, CUSPARSE_INDEX_BASE_ONE);\ncusparseSetMatType(descr_M, CUSPARSE_MATRIX_TYPE_GENERAL);\ncusparseCreateMatDescr(&descr_L);\ncusparseSetMatIndexBase(descr_L, CUSPARSE_INDEX_BASE_ONE);\ncusparseSetMatType(descr_L, CUSPARSE_MATRIX_TYPE_GENERAL);\ncusparseSetMatFillMode(descr_L, CUSPARSE_FILL_MODE_LOWER);\ncusparseSetMatDiagType(descr_L, CUSPARSE_DIAG_TYPE_NON_UNIT);\n\n// step 2: create a empty info structure\n// we need one info for csric02 and two info's for csrsv2\ncusparseCreateCsric02Info(&info_M);\ncusparseCreateCsrsv2Info(&info_L);\ncusparseCreateCsrsv2Info(&info_Lt);\n\n// step 3: query how much memory used in csric02 and csrsv2, and allocate the buffer\n\n```"}
{"id": "nvidia_cuda_00017", "source_lib": "nvidia_cuda", "title": "Install Optional CUDA Packages using Pip", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install Optional CUDA Packages using Pip\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls additional CUDA-related Python packages using pip. This command allows for the selective installation of components like cupti, nvcc, cublas, and others, based on project requirements.\n\n```bash\npip install nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12\n```"}
{"id": "nvidia_cuda_00018", "source_lib": "nvidia_cuda", "title": "Install nvidia-pyindex Python Package using Pip", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Install nvidia-pyindex Python Package using Pip\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls the nvidia-pyindex package, which is necessary to fetch additional Python modules from the NVIDIA NGC PyPI repository. This is a prerequisite for installing CUDA runtime packages via pip on Windows.\n\n```bash\npip install nvidia-pyindex\n```"}
{"id": "nvidia_cuda_00019", "source_lib": "nvidia_cuda", "title": "Reboot System after Driver Installation", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Reboot System after Driver Installation\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nReboots the system to ensure that the newly installed NVIDIA drivers are loaded correctly. This is a critical step after installing CUDA components that include kernel modules.\n\n```bash\nsudo reboot\n```"}
{"id": "nvidia_cuda_00020", "source_lib": "nvidia_cuda", "title": "CUDA C++ cg::reduce Example with cg::plus", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### CUDA C++ cg::reduce Example with cg::plus\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nThis example demonstrates the use of `cg::reduce` with `cg::plus<int>` for an asynchronous reduction, followed by a synchronization step. It highlights the typical pattern for performing reductions within a cooperative group.\n\n```cuda_c++\ncg::reduce_update_async(tile,total_sum,thread_sum,cg::plus<int>());// synchronize the block, to ensure all async reductions are ready\nblock.sync();\n```"}
{"id": "nvidia_cuda_00021", "source_lib": "nvidia_cuda", "title": "CUDA C: Example of cudaMemAdviseSetReadMostly", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### CUDA C: Example of cudaMemAdviseSetReadMostly\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nDemonstrates allocating memory using `cudaMallocManaged`, setting the `cudaMemAdviseSetReadMostly` hint on the allocated region, and then performing operations. The example shows initializing data on the CPU, prefetching it to multiple devices, and running a kernel that only reads the data.\n\n```c\nvoid test_advise_managed(cudaStream_t stream) {\n    char* dataPtr;\n    size_t dataSize = 64 * TPB; // 16 KiB\n    // Allocate memory using cudaMallocManaged\n    // (malloc may be used on systems with full CUDA Unified memory support)\n    cudaMallocManaged(&dataPtr, dataSize);\n    // Set the advice on the memory region\n    cudaMemLocation loc = {.type = cudaMemLocationTypeDevice, .id = myGpuId};\n    cudaMemAdvise(dataPtr, dataSize, cudaMemAdviseSetReadMostly, loc);\n    int outerLoopIter = 0;\n    while (outerLoopIter < maxOuterLoopIter) {\n        // The data is written to in the outer loop on the CPU\n        init_data(dataPtr, dataSize);\n        // The data is made available to all GPUs by prefetching.\n        // Prefetching here causes read duplication of data instead\n        // of data migration\n        cudaMemLocation location;\n        location.type = cudaMemLocationTypeDevice;\n        for (int device = 0; device < maxDevices; device++) {\n            location.id = device;\n            cudaMemPrefetchAsync(dataPtr, dataSize, location, 0 /* flags */, stream);\n        }\n        // The kernel only reads this data in the inner loop\n        int innerLoopIter = 0;\n        while (innerLoopIter < maxInnerLoopIter) {\n            mykernel<<<32, TPB, 0, stream>>>((const char*)dataPtr, dataSize);\n            innerLoopIter++;\n        }\n        outerLoopIter++;\n    }\n    cudaFree(dataPtr);\n}\n```"}
{"id": "nvidia_cuda_00022", "source_lib": "nvidia_cuda", "title": "C cuBLAS Application: 0-based Indexing Example", "source_url": "https://docs.nvidia.com/cuda/cublas/index", "content": "### C cuBLAS Application: 0-based Indexing Example\n\nSource: https://docs.nvidia.com/cuda/cublas/index\n\nIllustrates an application in C utilizing the legacy cuBLAS API with 0-based indexing. This example mirrors the 1-based version but adjusts index calculations for 0-based array access, covering matrix initialization, device data transfer, modification, data retrieval, and output. It requires standard C libraries, math.h, and cublas.h.\n\n```c\n#include<stdio.h>#include<stdlib.h>#include<math.h>#include\"cublas.h\"#define M 6\n#define N 5\n#define IDX2C(i,j,ld) (((j)*(ld))+(i))\n\nstatic__inline__voidmodify(float*m,intldm,intn,intp,intq,floatalpha,floatbeta){cublasSscal(n-q,alpha,&m[IDX2C(p,q,ldm)],ldm);cublasSscal(ldm-p,beta,&m[IDX2C(p,q,ldm)],1);}intmain(void){inti,j;\ncublasStatusstat;\nfloat*devPtrA;\nfloat*a=0;\na=(float*)malloc(M*N*sizeof(*a));\nif(!a){\nprintf(\"host memory allocation failed\");\nreturnEXIT_FAILURE;\n}\n\nfor(j=0;j<N;j++){\nfor(i=0;i<M;i++){\na[IDX2C(i,j,M)]=(float)(i*M+j+1);\n}\n}\n\ncublasInit();\nstat=cublasAlloc(M*N,sizeof(*a),(void**)&devPtrA);\nif(stat!=CUBLAS_STATUS_SUCCESS){\nprintf(\"device memory allocation failed\");\ncublasShutdown();\nreturnEXIT_FAILURE;\n}\n\nstat=cublasSetMatrix(M,N,sizeof(*a),a,M,devPtrA,M);\nif(stat!=CUBLAS_STATUS_SUCCESS){\nprintf(\"data download failed\");\ncublasFree(devPtrA);\ncublasShutdown();\nreturnEXIT_FAILURE;\n}\n\nmodify(devPtrA,M,N,1,2,16.0f,12.0f);\n\nstat=cublasGetMatrix(M,N,sizeof(*a),devPtrA,M,a,M);\nif(stat!=CUBLAS_STATUS_SUCCESS){\nprintf(\"data upload failed\");\ncublasFree(devPtrA);\ncublasShutdown();\nreturnEXIT_FAILURE;\n}\n\ncublasFree(devPtrA);\ncublasShutdown();\n\nfor(j=0;j<N;j++){\nfor(i=0;i<M;i++){\nprintf(\"%7.0f\",a[IDX2C(i,j,M)]);\n}\nprintf(\"\\n\");\n}\n\nfree(a);\nreturnEXIT_SUCCESS;\n}\n\n```"}
{"id": "nvidia_cuda_00023", "source_lib": "nvidia_cuda", "title": "Prepare Amazon Linux 2023 for CUDA Installation", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Prepare Amazon Linux 2023 for CUDA Installation\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nInstalls kernel headers and development packages required for CUDA on Amazon Linux 2023. This step is crucial for compiling CUDA applications and ensuring compatibility with the system's kernel.\n\n```shell\nsudo yum install kernel-devel-$(uname -r)\nsudo yum install kernel-headers-$(uname -r)\n```"}
{"id": "nvidia_cuda_00024", "source_lib": "nvidia_cuda", "title": "C/C++: cuBLAS Application with 0-based Indexing", "source_url": "https://docs.nvidia.com/cuda/cublas/index", "content": "### C/C++: cuBLAS Application with 0-based Indexing\n\nSource: https://docs.nvidia.com/cuda/cublas/index\n\nThis C code example demonstrates an application using the cuBLAS library with 0-based indexing. It mirrors the functionality of the 1-based indexing example, including matrix initialization, GPU data transfer, modification via cuBLAS functions, and data retrieval. The primary difference lies in the indexing macros and loop bounds used.\n\n```c\n#include<stdio.h>#include<stdlib.h>#include<math.h>#include<cuda_runtime.h>#include\"cublas_v2.h\"#define M 6\n#define N 5\n#define IDX2C(i,j,ld) (((j)*(ld))+(i))\n\nstatic__inline__voidmodify(cublasHandle_thandle,float*m,intldm,intn,intp,intq,floatalpha,floatbeta){cublasSscal(handle,n-q,&alpha,&m[IDX2C(p,q,ldm)],ldm);cublasSscal(handle,ldm-p,&beta,&m[IDX2C(p,q,ldm)],1);}intmain(void){\n    cudaError_tcudaStat;\n    cublasStatus_tstat;\n    cublasHandle_thandle;\n    inti,j;\n    float*devPtrA;\n    float*a=0;\n\n    a=(float*)malloc(M*N*sizeof(*a));\n    if(!a){\n        printf(\"host memory allocation failed\");\n        returnEXIT_FAILURE;\n    }\n\n    for(j=0;j<N;j++){\n        for(i=0;i<M;i++){\n            a[IDX2C(i,j,M)]=(float)(i*N+j+1);\n        }\n    }\n\n    cudaStat=cudaMalloc((void**)&devPtrA,M*N*sizeof(*a));\n    if(cudaStat!=cudaSuccess){\n        printf(\"device memory allocation failed\");\n        free(a);\n        returnEXIT_FAILURE;\n    }\n\n    stat=cublasCreate(&handle);\n    if(stat!=CUBLAS_STATUS_SUCCESS){\n        printf(\"CUBLAS initialization failed\\n\");\n        free(a);\n        cudaFree(devPtrA);\n        returnEXIT_FAILURE;\n    }\n\n    stat=cublasSetMatrix(M,N,sizeof(*a),a,M,devPtrA,M);\n    if(stat!=CUBLAS_STATUS_SUCCESS){\n        printf(\"data download failed\");\n        free(a);\n        cudaFree(devPtrA);\n        cublasDestroy(handle);\n        returnEXIT_FAILURE;\n    }\n\n    modify(handle,devPtrA,M,N,1,2,16.0f,12.0f);\n\n    stat=cublasGetMatrix(M,N,sizeof(*a),devPtrA,M,a,M);\n    if(stat!=CUBLAS_STATUS_SUCCESS){\n        printf(\"data upload failed\");\n        free(a);\n        cudaFree(devPtrA);\n        cublasDestroy(handle);\n        returnEXIT_FAILURE;\n    }\n\n    cudaFree(devPtrA);\n    cublasDestroy(handle);\n\n    for(j=0;j<N;j++){\n        for(i=0;i<M;i++){\n            printf(\"%7.0f\",a[IDX2C(i,j,M)]);\n        }\n        printf(\"\\n\");\n    }\n\n    free(a);\n    returnEXIT_SUCCESS;\n}\n\n```"}
{"id": "nvidia_cuda_00025", "source_lib": "nvidia_cuda", "title": "Compile and Run CUDA Sample Programs", "source_url": "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index", "content": "### Compile and Run CUDA Sample Programs\n\nSource: https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index\n\nInstructions for compiling and running CUDA sample programs like `deviceQuery` and `bandwidthTest` to verify hardware and software configuration. The sample programs are available on GitHub.\n\n```bash\n# Clone the repository\ngit clone https://github.com/nvidia/cuda-samples.git\n\n# Navigate to the samples directory and follow build/run instructions on GitHub.\n# Example for deviceQuery (assuming VS solution files exist):\n# cd cuda-samples/Samples/deviceQuery\n# msbuild deviceQuery.sln /p:Configuration=Release\n# .Release\\deviceQuery.exe\n\n# Example for bandwidthTest:\n# cd cuda-samples/Samples/bandwidthTest\n# msbuild bandwidthTest.sln /p:Configuration=Release\n# .Release\\bandwidthTest.exe\n```"}
{"id": "nvidia_cuda_00026", "source_lib": "nvidia_cuda", "title": "Example: General BSR to CSR Conversion Procedure (cuSPARSE)", "source_url": "https://docs.nvidia.com/cuda/cusparse/index", "content": "### Example: General BSR to CSR Conversion Procedure (cuSPARSE)\n\nSource: https://docs.nvidia.com/cuda/cusparse/index\n\nThis example demonstrates the steps involved in converting a sparse matrix from general BSR format to CSR format using cuSPARSE. It includes memory allocation for the CSR output arrays and the function call for single-precision floating-point numbers.\n\n```cuda\n// Given general BSR format (bsrRowPtrA, bsrColIndA, bsrValA) and\n// blocks of BSR format are stored in column-major order.\ncusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN;\nint m = mb * rowBlockDim;\nint n = nb * colBlockDim;\nint nnzb = bsrRowPtrA[mb] - bsrRowPtrA[0]; // number of blocks\nint nnz = nnzb * rowBlockDim * colBlockDim; // number of elements\n\ncudaMalloc((void**)&csrRowPtrC, sizeof(int) * (m + 1));\ncudaMalloc((void**)&csrColIndC, sizeof(int) * nnz);\ncudaMalloc((void**)&csrValC, sizeof(float) * nnz);\n\ncusparseSgebsr2csr(handle, dir, mb, nb, descrA, bsrValA, bsrRowPtrA, bsrColIndA, rowBlockDim, colBlockDim, descrC, csrValC, csrRowPtrC, csrColIndC);\n```"}
{"id": "nvidia_cuda_00027", "source_lib": "nvidia_cuda", "title": "System Allocator: Direct Managed Memory Access Example (C++)", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### System Allocator: Direct Managed Memory Access Example (C++)\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nDemonstrates direct unified memory access from the host using `malloc` and `cudaMemAdvise` for system allocator. This example highlights how CPU accesses to GPU-resident memory behave differently based on the `cudaDevAttrDirectManagedMemAccessFromHost` attribute, affecting page faults and data migrations.\n\n```c++\n__global__ void write(int* ret, int a, int b) { ret[threadIdx.x] = a + b + threadIdx.x; }\n__global__ void append(int* ret, int a, int b) { ret[threadIdx.x] += a + b + threadIdx.x; }\nvoid test_malloc() {\n    int* ret = (int*)malloc(1000 * sizeof(int)); // for shared page table systems, the following hint is not necesary\n    cudaMemLocation location = {.type = cudaMemLocationTypeHost};\n    cudaMemAdvise(ret, 1000 * sizeof(int), cudaMemAdviseSetAccessedBy, location);\n    write<<<1, 1000>>>(ret, 10, 100); // pages populated in GPU memory\n    cudaDeviceSynchronize();\n    for (int i = 0; i < 1000; i++) printf(\"%d: A+B = %d\\n\", i, ret[i]);\n    // directManagedMemAccessFromHost=1: CPU accesses GPU memory directly without migrations\n    // directManagedMemAccessFromHost=0: CPU faults and triggers device-to-host migrations\n    append<<<1, 1000>>>(ret, 10, 100); // directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations\n    cudaDeviceSynchronize();\n    // directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations\n    free(ret);\n}\n```"}
{"id": "nvidia_cuda_00028", "source_lib": "nvidia_cuda", "title": "Get CUDA Driver Version", "source_url": "https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__VERSION", "content": "### Get CUDA Driver Version\n\nSource: https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__VERSION\n\nRetrieves the latest CUDA version supported by the installed driver. The version is returned as an integer representing major and minor versions.\n\n```APIDOC\n## GET /cuda/driver/version\n\n### Description\nRetrieves the latest CUDA version supported by the driver. The version is returned as an integer in the format (1000 * major + 10 * minor).\n\n### Method\nGET\n\n### Endpoint\n/cuda/driver/version\n\n### Parameters\n#### Query Parameters\n- **driverVersion** (int*) - Output - Pointer to an integer where the CUDA driver version will be returned.\n\n### Request Example\n```\nGET /cuda/driver/version?driverVersion=<output_pointer>\n```\n\n### Response\n#### Success Response (200)\n- **driverVersion** (int) - The CUDA driver version (e.g., 9020 for CUDA 9.2).\n\n#### Response Example\n```json\n{\n  \"driverVersion\": 9020\n}\n```\n\n#### Error Response\n- **CUDA_ERROR_INVALID_VALUE** - Returned if `driverVersion` is NULL.\n```"}
{"id": "nvidia_cuda_00029", "source_lib": "nvidia_cuda", "title": "Get Matrix Format (cuSolverRF)", "source_url": "https://docs.nvidia.com/cuda/cusolver/index", "content": "### Get Matrix Format (cuSolverRF)\n\nSource: https://docs.nvidia.com/cuda/cusolver/index\n\nRetrieves the matrix format and unit diagonal type configured for various cuSolverRF routines. This information is relevant for setup, value resetting, and factor extraction functions within the library.\n\n```c\ncusolverStatus_t cusolverRfGetMatrixFormat(\n    cusolverRfHandle_t handle,\n    cusolverRfMatrixFormat_t* format,\n    cusolverRfUnitDiagonal_t* diag\n);\n```"}
{"id": "nvidia_cuda_00030", "source_lib": "nvidia_cuda", "title": "Launch Setup APIs (CDP1)", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### Launch Setup APIs (CDP1)\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nDescribes the device runtime APIs `cudaGetParameterBuffer` and `cudaLaunchDevice` used for kernel launches directly from PTX or CUDA C++, detailing their purpose and differences from host runtime equivalents.\n\n```APIDOC\n## Launch Setup APIs (CDP1)\n\n### Description\nKernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying `cudaGetParameterBuffer()` and `cudaLaunchDevice()` APIs. It is permitted for a CUDA application to call these APIs itself, with the same requirements as for PTX. In both cases, the user is then responsible for correctly populating all necessary data structures in the correct format according to specification. Backwards compatibility is guaranteed in these data structures. As with host-side launch, the device-side operator `<<<>>>` maps to underlying kernel launch APIs. This is so that users targeting PTX will be able to enact a launch, and so that the compiler front-end can translate `<<<>>>` into these calls.\n\n### Method\nN/A (API Definitions)\n\n### Endpoint\nN/A\n\n### Parameters\nN/A\n\n### Request Example\n```c++\nextern __device__ cudaError_t cudaGetParameterBuffer(void **params);\nextern __device__ cudaError_t cudaLaunchDevice(void *kernel, void *params, dim3 gridDim, dim3 blockDim, unsigned int sharedMemSize = 0, cudaStream_t stream = 0);\n```\n\n### Response\nN/A\n\n```"}
{"id": "nvidia_cuda_00031", "source_lib": "nvidia_cuda", "title": "Get CUBLAS Logger Callback", "source_url": "https://docs.nvidia.com/cuda/cublas/index", "content": "### Get CUBLAS Logger Callback\n\nSource: https://docs.nvidia.com/cuda/cublas/index\n\nRetrieves the function pointer to a custom user-defined callback function previously installed using cublasSetLoggerCallback. If no custom callback is set, it returns zero. This is useful for debugging or intercepting log messages.\n\n```c\ncublasStatus_t cublasGetLoggerCallback(cublasLogCallback* userCallback)\n```"}
{"id": "nvidia_cuda_00032", "source_lib": "nvidia_cuda", "title": "CUDA C++ Intrinsic for Fast Single-Precision Division", "source_url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index", "content": "### CUDA C++ Intrinsic for Fast Single-Precision Division\n\nSource: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index\n\nThis example demonstrates the use of `__fdividef` intrinsic function in CUDA C++ for faster single-precision floating-point division compared to the standard division operator. This function is part of the CUDA C++ Programming Guide and can offer performance benefits.\n\n```cuda_c++\nfloat result = __fdividef(x, y);\n```"}
{"id": "nvidia_cuda_00033", "source_lib": "nvidia_cuda", "title": "Hello World Program with Dynamic Parallelism", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### Hello World Program with Dynamic Parallelism\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nA simple 'Hello World' CUDA program demonstrating dynamic parallelism. It features a parent kernel launching a child kernel and a tail kernel, showcasing inter-kernel communication and stream management using `cudaStreamTailLaunch`.\n\n```cuda\n#include<stdio.h>\n__global__voidchildKernel(){\n  printf(\"Hello \");\n}\n__global__voidtailKernel(){\n  printf(\"World!\\n\");\n}\n__global__voidparentKernel(){\n  // launch child\n  childKernel<<<1,1>>>();\n  if(cudaSuccess!=cudaGetLastError()){\n    return;\n  }\n  // launch tail into cudaStreamTailLaunch stream\n  // implicitly synchronizes: waits for child to complete\n  tailKernel<<<1,1,0,cudaStreamTailLaunch>>>();\n}\nintmain(intargc,char*argv[]){\n  // launch parent\n  parentKernel<<<1,1>>>();\n  if(cudaSuccess!=cudaGetLastError()){\n    return1;\n  }\n  // wait for parent to complete\n  if(cudaSuccess!=cudaDeviceSynchronize()){\n    return2;\n  }\n  return0;\n}\n```"}
{"id": "nvidia_cuda_00034", "source_lib": "nvidia_cuda", "title": "Fortran 77 cuBLAS Matrix Modification Example", "source_url": "https://docs.nvidia.com/cuda/cublas/index", "content": "### Fortran 77 cuBLAS Matrix Modification Example\n\nSource: https://docs.nvidia.com/cuda/cublas/index\n\nThis Fortran 77 code implements a matrix modification routine using the cuBLAS library. It handles device pointer types based on architecture (32-bit or 64-bit) and calls cuBLAS functions like cublas_sscal for scaling matrix elements. It requires cuBLAS to be installed and linked during compilation.\n\n```fortran\n! Example B.2. Same Application Using Non-thunking cuBLAS Calls\n!-------------------------------------------------------------\n#define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1))\nsubroutine modify(devPtrM,ldm,n,p,q,alpha,beta)\nimplicit none\ninteger sizeof_real\nparameter(sizeof_real=4)\ninteger ldm,n,p,q\n#if ARCH_64\ninteger*8devPtrM\n#else\ninteger*4devPtrM\n#endif\nreal*4alpha,beta\ncall cublas_sscal(n-p+1,alpha,1devPtrM+IDX2F(p,q,ldm)*sizeof_real,2ldm)\ncall cublas_sscal(ldm-p+1,beta,1devPtrM+IDX2F(p,q,ldm)*sizeof_real,21)\nreturn\n    end\n    program matrixmod\nimplicit none\ninteger M,N,sizeof_real\n#if ARCH_64\ninteger*8devPtrA\n#else\ninteger*4devPtrA\n#endif\nparameter(M=6,N=5,sizeof_real=4)\nreal*4a(M,N)\ninteger i,j,stat\nexternal cublas_init,cublas_set_matrix,cublas_get_matrix\nexternal cublas_shutdown,cublas_alloc\ninteger cublas_alloc,cublas_set_matrix,cublas_get_matrix\ndo j=1,N\ndo i=1,M\na(i,j)=(i-1)*M+j\nenddo\n    enddo\n    call cublas_init\nstat=cublas_alloc(M*N,sizeof_real,devPtrA)\nif(stat.NE.0)then\n        write(*,*)\"device memory allocation failed\"\ncall cublas_shutdown\nstop\n    endif\nstat=cublas_set_matrix(M,N,sizeof_real,a,M,devPtrA,M)\nif(stat.NE.0)then\n        call cublas_free(devPtrA)\nwrite(*,*)\"data download failed\"\ncall cublas_shutdown\nstop\n    endif\ncall modify(devPtrA,M,N,2,3,16.0,12.0)\nstat=cublas_get_matrix(M,N,sizeof_real,devPtrA,M,a,M)\nif(stat.NE.0)then\ncall cublas_free(devPtrA)\nwrite(*,*)\"data upload failed\"\ncall cublas_shutdown\nstop\n    endif\ncall cublas_free(devPtrA)\ncall cublas_shutdown\ndo j=1,N\n    doi=1,M\nwrite(*,\"(F7.0$)\") a(i,j)\nenddo\nwrite(*,*) \"\"\nenddo\n\nstop\nend\n```"}
{"id": "nvidia_cuda_00035", "source_lib": "nvidia_cuda", "title": "Prefetch Example with System Allocator (C)", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### Prefetch Example with System Allocator (C)\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nThis C code demonstrates prefetching data for a system-allocated memory region. It shows migrating data to a GPU for kernel execution and then back to the CPU.\n\n```c\nvoid test_prefetch_sam(cudaStream_t s) {\n    char* data = (char*)malloc(N);\n    init_data(data, N);\n    // execute on CPU\n    cudaMemLocation location = { .type = cudaMemLocationTypeDevice, .id = myGpuId };\n    cudaMemPrefetchAsync(data, N, location, s, 0 /* flags */); // prefetch to GPU\n    mykernel << <(N + TPB - 1) / TPB, TPB, 0, s >> > (data, N); // execute on GPU\n    location = { .type = cudaMemLocationTypeHost };\n    cudaMemPrefetchAsync(data, N, location, s, 0 /* flags */); // prefetch to CPU\n    cudaStreamSynchronize(s);\n    use_data(data, N);\n    free(data);\n}\n```"}
{"id": "nvidia_cuda_00036", "source_lib": "nvidia_cuda", "title": "Get CUDA Driver Version (C/C++)", "source_url": "https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART____VERSION", "content": "### Get CUDA Driver Version (C/C++)\n\nSource: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART____VERSION\n\nRetrieves the latest CUDA version supported by the installed NVIDIA driver. This function is part of the CUDA Runtime API and is intended for host-side execution. It returns a cudaError_t status and populates an integer pointer with the driver version. Potential return values include cudaSuccess and cudaErrorInvalidValue.\n\n```c\n__host__ \ncudaError_t cudaDriverGetVersion ( int*driverVersion )\n\n```"}
{"id": "nvidia_cuda_00037", "source_lib": "nvidia_cuda", "title": "Check CUDA Toolkit Version", "source_url": "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index", "content": "### Check CUDA Toolkit Version\n\nSource: https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index\n\nVerifies the installed CUDA Toolkit version by running the `nvcc -V` command in a Windows Command Prompt.\n\n```bash\nnvcc -V\n```"}
{"id": "nvidia_cuda_00038", "source_lib": "nvidia_cuda", "title": "CUDA Driver API Initialization and Kernel Launch (C)", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### CUDA Driver API Initialization and Kernel Launch (C)\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nThis C code snippet demonstrates the initialization of the CUDA Driver API using `cuInit()`, device retrieval, context creation, module loading from a PTX file, device memory allocation, data transfer from host to device, kernel function retrieval, and finally, the invocation of a CUDA kernel using `cuLaunchKernel()`. It assumes pre-existing host memory allocation and initialization.\n\n```c\nintmain(){\n  intN=...;\n  size_tsize=N*sizeof(float);\n  // Allocate input vectors h_A and h_B in host memory\n  float*h_A=(float*)malloc(size);\n  float*h_B=(float*)malloc(size);\n  // Initialize input vectors\n  ...\n  // Initialize\n  cuInit(0);\n  // Get number of devices supporting CUDA\n  intdeviceCount=0;\n  cuDeviceGetCount(&deviceCount);\n  if(deviceCount==0){\n    printf(\"There is no device supporting CUDA.\\n\");\n    exit(0);\n  }\n  // Get handle for device 0\n  CUdevicecuDevice;\n  cuDeviceGet(&cuDevice,0);\n  // Create context\n  CUcontextcuContext;\n  cuCtxCreate(&cuContext,NULL,0,cuDevice);\n  // Create module from binary file\n  CUmodulecuModule;\n  cuModuleLoad(&cuModule,\"VecAdd.ptx\");\n  // Allocate vectors in device memory\n  CUdeviceptrd_A;\n  cuMemAlloc(&d_A,size);\n  CUdeviceptrd_B;\n  cuMemAlloc(&d_B,size);\n  CUdeviceptrd_C;\n  cuMemAlloc(&d_C,size);\n  // Copy vectors from host memory to device memory\n  cuMemcpyHtoD(d_A,h_A,size);\n  cuMemcpyHtoD(d_B,h_B,size);\n  // Get function handle from module\n  CUfunctionvecAdd;\n  cuModuleGetFunction(&vecAdd,cuModule,\"VecAdd\");\n  // Invoke kernel\n  intthreadsPerBlock=256;\n  intblocksPerGrid=(N+threadsPerBlock-1)/threadsPerBlock;\n  void*args[]={&d_A,&d_B,&d_C,&N};\n  cuLaunchKernel(vecAdd,blocksPerGrid,1,1,threadsPerBlock,1,1,0,0,args,0);\n  ...\n}\n```"}
{"id": "nvidia_cuda_00039", "source_lib": "nvidia_cuda", "title": "CUDA C++ Dynamic Parallelism 'Hello World' Example", "source_url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index", "content": "### CUDA C++ Dynamic Parallelism 'Hello World' Example\n\nSource: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index\n\nA CUDA C++ program demonstrating dynamic parallelism by launching a child kernel from a parent kernel and synchronizing their execution. It includes basic error checking for kernel launches and synchronization.\n\n```cuda-c++\n#include<stdio.h>\n__global__ void childKernel() {\n    printf(\"Hello \");\n}\n\n__global__ void parentKernel() {\n    // launch child\n    childKernel<<<1, 1>>>();\n    if (cudaSuccess != cudaGetLastError()) {\n        return;\n    }\n    // wait for child to complete\n    if (cudaSuccess != cudaDeviceSynchronize()) {\n        return;\n    }\n    printf(\"World!\\n\");\n}\n\nint main(int argc, char* argv[]) {\n    // launch parent\n    parentKernel<<<1, 1>>>();\n    if (cudaSuccess != cudaGetLastError()) {\n        return 1;\n    }\n    // wait for parent to complete\n    if (cudaSuccess != cudaDeviceSynchronize()) {\n        return 2;\n    }\n    return 0;\n}\n```"}
{"id": "nvidia_cuda_00040", "source_lib": "nvidia_cuda", "title": "Set up CUDA Development Environment Variables", "source_url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index", "content": "### Set up CUDA Development Environment Variables\n\nSource: https://docs.nvidia.com/cuda/cuda-quick-start-guide/index\n\nConfigures the system's PATH and LD_LIBRARY_PATH environment variables to include the CUDA installation directories. This allows the system to find CUDA executables and libraries from any location.\n\n```bash\nexport PATH=/usr/local/cuda-12.0/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n```"}
{"id": "pytorch_00000", "source_lib": "pytorch", "title": "Untitled", "source_url": "", "content": "# PyTorch: Deep Learning Framework\n\nPyTorch is a Python package providing tensor computation with strong GPU acceleration and deep neural networks built on a tape-based automatic differentiation system. It combines the flexibility of Python with the performance of optimized C++ backends, enabling researchers and practitioners to build and train neural networks efficiently. PyTorch emphasizes a dynamic computational graph approach, allowing networks to change behavior on-the-fly without rebuilding static graphs, making it ideal for research and production deployment.\n\nThe framework is structured in layers: high-level Python APIs (torch.nn, torch.optim) for model building, a core autograd engine for automatic differentiation, and an efficient C++ backend (ATen) with 466+ native operations. PyTorch supports multiple accelerators including NVIDIA GPUs (CUDA), Apple Silicon (MPS), Intel GPUs (XPU), and CPUs with vectorized operations. Advanced features include TorchScript for model serialization, distributed training primitives for scaling across multiple machines, and torch.compile for graph optimization and JIT compilation.\n\n## Core Tensor Operations\n\n### Creating and Manipulating Tensors\n\nBasic tensor creation and arithmetic operations form the foundation of PyTorch computations.\n\n```python\nimport torch\n\n# Tensor creation\nx = torch.randn(3, 4)  # Random tensor with shape (3, 4)\ny = torch.zeros(3, 4)  # Zero tensor\nz = torch.ones(3, 4)   # Ones tensor\na = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\nb = torch.linspace(0, 1, 5)  # [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Tensor operations\nresult = x + y  # Element-wise addition\nmatmul_result = torch.matmul(x, y.t())  # Matrix multiplication\nbroadcasted = x * torch.tensor([1, 2, 3, 4])  # Broadcasting\n\n# Indexing and slicing\nfirst_row = x[0, :]  # First row\nsubset = x[1:3, 2:]  # Rows 1-2, columns 2-end\n\n# Shape operations\nreshaped = x.view(2, 6)  # Reshape to (2, 6)\ntransposed = x.t()  # Transpose\nflattened = x.flatten()  # Flatten to 1D\n\n# Device transfer (CPU to GPU)\nif torch.cuda.is_available():\n    x_gpu = x.cuda()  # Move to GPU\n    y_gpu = y.to('cuda:0')  # Alternative syntax\n    result_gpu = x_gpu + y_gpu\n    result_cpu = result_gpu.cpu()  # Move back to CPU\n\nprint(f\"Shape: {x.shape}, dtype: {x.dtype}, device: {x.device}\")\n```\n\n### Advanced Tensor Operations\n\nLinear algebra, reduction operations, and statistical functions for numerical computation.\n\n```python\nimport torch\n\n# Linear algebra operations\nA = torch.randn(4, 4)\nB = torch.randn(4, 4)\n\n# Matrix operations\ndet = torch.det(A)  # Determinant\ninv = torch.inverse(A)  # Matrix inverse\neigenvalues, eigenvectors = torch.eig(A, eigenvectors=True)\nU, S, V = torch.svd(A)  # Singular value decomposition\nQ, R = torch.qr(A)  # QR decomposition\n\n# Solving linear systems: Ax = b\nb = torch.randn(4, 1)\nx = torch.linalg.solve(A, b)\n\n# Reduction operations\nx = torch.randn(3, 4, 5)\nsum_all = x.sum()  # Sum all elements\nsum_dim = x.sum(dim=1)  # Sum along dimension 1\nmean_val = x.mean()\nmax_val, max_idx = x.max(dim=2)  # Max along dimension 2 with indices\nnorm = torch.norm(x, p=2)  # L2 norm\n\n# Statistical operations\nstd = x.std()  # Standard deviation\nvar = x.var()  # Variance\nmedian = torch.median(x)\nquantile = torch.quantile(x, q=0.75)\n\n# Comparison operations\nmask = x > 0  # Boolean mask\nclamped = torch.clamp(x, min=-1, max=1)  # Clamp values\nselected = torch.where(x > 0, x, torch.zeros_like(x))  # Conditional selection\n```\n\n## Automatic Differentiation (Autograd)\n\n### Basic Gradient Computation\n\nComputing gradients automatically using PyTorch's tape-based autograd system.\n\n```python\nimport torch\n\n# Enable gradient tracking\nx = torch.tensor([2.0, 3.0], requires_grad=True)\ny = torch.tensor([1.0, 4.0], requires_grad=True)\n\n# Forward pass: build computational graph\nz = x ** 2 + 3 * y\nloss = z.sum()\n\n# Backward pass: compute gradients\nloss.backward()\n\n# Access gradients\nprint(f\"\u2202loss/\u2202x = {x.grad}\")  # [4.0, 6.0]\nprint(f\"\u2202loss/\u2202y = {y.grad}\")  # [3.0, 3.0]\n\n# Manual gradient computation for comparison\n# loss = x[0]^2 + 3*y[0] + x[1]^2 + 3*y[1]\n# \u2202loss/\u2202x[0] = 2*x[0] = 4.0\n# \u2202loss/\u2202y[0] = 3.0\n\n# Zero gradients for next iteration\nx.grad.zero_()\ny.grad.zero_()\n\n# Compute gradients for specific tensors\nz2 = (x * y).sum()\ngrad_x, grad_y = torch.autograd.grad(z2, [x, y])\nprint(f\"Computed gradients: x={grad_x}, y={grad_y}\")\n```\n\n### Custom Autograd Functions\n\nDefining custom differentiable operations with forward and backward passes.\n\n```python\nimport torch\nfrom torch.autograd import Function\n\nclass CustomExp(Function):\n    \"\"\"Custom exponential function with backward pass.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"Forward pass: compute exponential.\"\"\"\n        result = input.exp()\n        ctx.save_for_backward(result)  # Save for backward\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"Backward pass: compute gradient.\n\n        d(exp(x))/dx = exp(x)\n        \"\"\"\n        result, = ctx.saved_tensors\n        return grad_output * result\n\n# Using custom function\ncustom_exp = CustomExp.apply\n\nx = torch.tensor([1.0, 2.0], requires_grad=True)\ny = custom_exp(x)\nloss = y.sum()\nloss.backward()\n\nprint(f\"Input: {x}\")\nprint(f\"Output: {y}\")  # [e^1, e^2]\nprint(f\"Gradient: {x.grad}\")  # [e^1, e^2]\n\n# Gradient checking for numerical verification\nfrom torch.autograd import gradcheck\n\ninput_test = torch.randn(3, dtype=torch.double, requires_grad=True)\ntest = gradcheck(custom_exp, input_test, eps=1e-6, atol=1e-4)\nprint(f\"Gradient check passed: {test}\")\n```\n\n### Context Managers for Gradient Control\n\nManaging gradient computation behavior with context managers.\n\n```python\nimport torch\n\nx = torch.randn(3, 4, requires_grad=True)\ny = torch.randn(3, 4, requires_grad=True)\n\n# Disable gradient computation (inference mode)\nwith torch.no_grad():\n    z = x + y  # No gradient tracking\n    prediction = torch.nn.functional.softmax(z, dim=1)\n    print(f\"z.requires_grad: {z.requires_grad}\")  # False\n\n# Enable gradient computation\nwith torch.enable_grad():\n    w = x * 2  # Gradient tracking enabled\n    print(f\"w.requires_grad: {w.requires_grad}\")  # True\n\n# Inference mode (more aggressive optimization)\nwith torch.inference_mode():\n    inference_result = x @ y.t()\n    # Cannot call .backward() on tensors created here\n\n# Temporarily disable gradients for specific operations\nx = torch.randn(10, requires_grad=True)\nwith torch.no_grad():\n    x_normalized = x / x.norm()  # No gradient\nx_normalized.requires_grad = True  # Re-enable if needed\n\n# Anomaly detection for debugging\nwith torch.autograd.detect_anomaly():\n    # Raises detailed error for NaN/Inf gradients\n    loss = (x ** 2).sum()\n    loss.backward()\n```\n\n## Neural Networks (torch.nn)\n\n### Building Neural Network Modules\n\nCreating custom neural network modules and layers.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CustomNet(nn.Module):\n    \"\"\"Custom neural network with multiple layers.\"\"\"\n\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(CustomNet, self).__init__()\n        # Define layers\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.dropout = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n\n    def forward(self, x):\n        \"\"\"Forward pass through the network.\"\"\"\n        # Layer 1 with batch norm and ReLU\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n\n        # Layer 2\n        x = self.fc2(x)\n        x = F.relu(x)\n\n        # Output layer\n        x = self.fc3(x)\n        return x\n\n# Initialize model\nmodel = CustomNet(input_size=784, hidden_size=256, num_classes=10)\n\n# Create sample input (batch_size=32, features=784)\nx = torch.randn(32, 784)\n\n# Forward pass\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # [32, 10]\n\n# Access parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\n\n# Inspect layer weights\nprint(f\"First layer weight shape: {model.fc1.weight.shape}\")\nprint(f\"First layer bias shape: {model.fc1.bias.shape}\")\n```\n\n### Convolutional Neural Networks\n\nBuilding convolutional networks for image processing tasks.\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass ConvNet(nn.Module):\n    \"\"\"Convolutional Neural Network for image classification.\"\"\"\n\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32,\n                               kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n\n        # Pooling layers\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Batch normalization\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n\n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        \"\"\"Forward pass: x shape [batch, 3, 32, 32]\"\"\"\n        # Conv block 1: 32x32 -> 16x16\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n        x = self.pool(x)\n\n        # Conv block 2: 16x16 -> 8x8\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = torch.relu(x)\n        x = self.pool(x)\n\n        # Conv block 3: 8x8 -> 4x4\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = torch.relu(x)\n        x = self.pool(x)\n\n        # Flatten: [batch, 128, 4, 4] -> [batch, 2048]\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\n# Create model and sample input\nmodel = ConvNet(num_classes=10)\nx = torch.randn(8, 3, 32, 32)  # Batch of 8 RGB 32x32 images\n\n# Forward pass\noutput = model(x)\nprint(f\"Input shape: {x.shape}\")   # [8, 3, 32, 32]\nprint(f\"Output shape: {output.shape}\")  # [8, 10]\n\n# Apply softmax for probabilities\nprobabilities = torch.softmax(output, dim=1)\npredicted_classes = torch.argmax(probabilities, dim=1)\nprint(f\"Predicted classes: {predicted_classes}\")\n```\n\n### Recurrent Neural Networks\n\nImplementing sequence models with LSTM and GRU layers.\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass LSTMClassifier(nn.Module):\n    \"\"\"LSTM-based sequence classifier.\"\"\"\n\n    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n                 num_layers, num_classes, dropout=0.5):\n        super(LSTMClassifier, self).__init__()\n\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM layers\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True\n        )\n\n        # Fully connected output layer\n        # *2 because bidirectional\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: [batch_size, seq_length] tensor of token indices\n        \"\"\"\n        # Embedding: [batch, seq_len] -> [batch, seq_len, emb_dim]\n        embedded = self.embedding(x)\n        embedded = self.dropout(embedded)\n\n        # LSTM: [batch, seq_len, emb_dim] -> [batch, seq_len, hidden*2]\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n\n        # Use final hidden states from both directions\n        # hidden: [num_layers*2, batch, hidden]\n        hidden_fwd = hidden[-2, :, :]  # Forward direction\n        hidden_bwd = hidden[-1, :, :]  # Backward direction\n        hidden_concat = torch.cat([hidden_fwd, hidden_bwd], dim=1)\n\n        # Dropout and classification\n        hidden_concat = self.dropout(hidden_concat)\n        output = self.fc(hidden_concat)\n\n        return output\n\n# Create model\nmodel = LSTMClassifier(\n    vocab_size=10000,\n    embedding_dim=128,\n    hidden_dim=256,\n    num_layers=2,\n    num_classes=5,\n    dropout=0.5\n)\n\n# Sample input: batch of sequences\nbatch_size = 16\nseq_length = 50\nx = torch.randint(0, 10000, (batch_size, seq_length))\n\n# Forward pass\noutput = model(x)\nprint(f\"Input shape: {x.shape}\")   # [16, 50]\nprint(f\"Output shape: {output.shape}\")  # [16, 5]\n\n# For sequence generation (many-to-many)\nclass Seq2SeqLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(Seq2SeqLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # x: [batch, seq_len, input_dim]\n        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden]\n        output = self.fc(lstm_out)  # [batch, seq_len, output_dim]\n        return output\n\nseq2seq = Seq2SeqLSTM(input_dim=10, hidden_dim=64, output_dim=10)\nx_seq = torch.randn(8, 20, 10)  # [batch, time, features]\noutput_seq = seq2seq(x_seq)\nprint(f\"Seq2Seq output shape: {output_seq.shape}\")  # [8, 20, 10]\n```\n\n### Transformer Models\n\nBuilding transformer architectures with multi-head attention.\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass TransformerClassifier(nn.Module):\n    \"\"\"Transformer-based sequence classifier.\"\"\"\n\n    def __init__(self, vocab_size, d_model, nhead, num_layers,\n                 num_classes, dim_feedforward=2048, dropout=0.1):\n        super(TransformerClassifier, self).__init__()\n\n        # Token embedding and positional encoding\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = nn.Parameter(torch.randn(1, 5000, d_model))\n\n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=num_layers\n        )\n\n        # Classification head\n        self.fc = nn.Linear(d_model, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        \"\"\"Forward pass.\n\n        Args:\n            x: [batch, seq_len] token indices\n            mask: [batch, seq_len] attention mask (optional)\n        \"\"\"\n        batch_size, seq_len = x.shape\n\n        # Embedding with positional encoding\n        x = self.embedding(x)  # [batch, seq_len, d_model]\n        x = x + self.pos_encoder[:, :seq_len, :]\n        x = self.dropout(x)\n\n        # Transformer encoding\n        if mask is not None:\n            # Convert padding mask to attention mask\n            mask = (mask == 0)  # True for padding tokens\n\n        encoded = self.transformer(x, src_key_padding_mask=mask)\n\n        # Pool over sequence (use first token, like BERT's [CLS])\n        pooled = encoded[:, 0, :]\n\n        # Classification\n        output = self.fc(pooled)\n        return output\n\n# Create model\nmodel = TransformerClassifier(\n    vocab_size=10000,\n    d_model=512,\n    nhead=8,\n    num_layers=6,\n    num_classes=10,\n    dim_feedforward=2048,\n    dropout=0.1\n)\n\n# Sample input with padding\nbatch_size = 16\nseq_len = 100\nx = torch.randint(0, 10000, (batch_size, seq_len))\n\n# Create padding mask (1 for valid tokens, 0 for padding)\nmask = torch.ones_like(x)\nmask[:, 50:] = 0  # Last 50 tokens are padding\n\n# Forward pass\noutput = model(x, mask=mask)\nprint(f\"Input shape: {x.shape}\")   # [16, 100]\nprint(f\"Output shape: {output.shape}\")  # [16, 10]\n\n# Multi-head attention example\nmha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)\nquery = torch.randn(16, 100, 512)\nkey = value = query  # Self-attention\nattn_output, attn_weights = mha(query, key, value)\nprint(f\"Attention output shape: {attn_output.shape}\")  # [16, 100, 512]\nprint(f\"Attention weights shape: {attn_weights.shape}\")  # [16, 100, 100]\n```\n\n## Loss Functions and Optimization\n\n### Loss Functions\n\nCommon loss functions for training neural networks.\n\n```python\nimport torch\nimport torch.nn as nn\n\n# Classification losses\nbatch_size = 32\nnum_classes = 10\n\n# Cross-entropy loss (combines log_softmax and NLLLoss)\npredictions = torch.randn(batch_size, num_classes)\ntargets = torch.randint(0, num_classes, (batch_size,))\nce_loss = nn.CrossEntropyLoss()\nloss = ce_loss(predictions, targets)\nprint(f\"Cross-entropy loss: {loss.item():.4f}\")\n\n# Binary cross-entropy loss\nbinary_predictions = torch.sigmoid(torch.randn(batch_size, 1))\nbinary_targets = torch.randint(0, 2, (batch_size, 1)).float()\nbce_loss = nn.BCELoss()\nloss = bce_loss(binary_predictions, binary_targets)\nprint(f\"Binary cross-entropy loss: {loss.item():.4f}\")\n\n# BCE with logits (more numerically stable)\nlogits = torch.randn(batch_size, 1)\nbce_logits_loss = nn.BCEWithLogitsLoss()\nloss = bce_logits_loss(logits, binary_targets)\nprint(f\"BCE with logits loss: {loss.item():.4f}\")\n\n# Regression losses\npredictions = torch.randn(batch_size, 1)\ntargets = torch.randn(batch_size, 1)\n\n# Mean squared error\nmse_loss = nn.MSELoss()\nloss = mse_loss(predictions, targets)\nprint(f\"MSE loss: {loss.item():.4f}\")\n\n# Mean absolute error (L1 loss)\nmae_loss = nn.L1Loss()\nloss = mae_loss(predictions, targets)\nprint(f\"MAE loss: {loss.item():.4f}\")\n\n# Smooth L1 loss (Huber loss)\nsmooth_l1_loss = nn.SmoothL1Loss()\nloss = smooth_l1_loss(predictions, targets)\nprint(f\"Smooth L1 loss: {loss.item():.4f}\")\n\n# Contrastive and metric learning losses\nembeddings = torch.randn(batch_size, 128)\nembeddings = nn.functional.normalize(embeddings, dim=1)\n\n# Cosine embedding loss\nembedding1 = embeddings[:16]\nembedding2 = embeddings[16:]\nlabels = torch.randint(-1, 2, (16,)).float() * 2 - 1  # -1 or 1\ncosine_loss = nn.CosineEmbeddingLoss()\nloss = cosine_loss(embedding1, embedding2, labels)\nprint(f\"Cosine embedding loss: {loss.item():.4f}\")\n\n# Triplet margin loss\nanchor = embeddings[:10]\npositive = embeddings[10:20]\nnegative = embeddings[20:30]\ntriplet_loss = nn.TripletMarginLoss(margin=1.0)\nloss = triplet_loss(anchor, positive, negative)\nprint(f\"Triplet margin loss: {loss.item():.4f}\")\n```\n\n### Optimizers\n\nTraining neural networks with various optimization algorithms.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Create a simple model\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n# Stochastic Gradient Descent (SGD)\noptimizer_sgd = optim.SGD(\n    model.parameters(),\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=1e-4,  # L2 regularization\n    nesterov=True\n)\n\n# Adam optimizer\noptimizer_adam = optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    eps=1e-8,\n    weight_decay=1e-4\n)\n\n# AdamW (Adam with decoupled weight decay)\noptimizer_adamw = optim.AdamW(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    weight_decay=0.01\n)\n\n# Training loop example\ncriterion = nn.CrossEntropyLoss()\noptimizer = optimizer_adam\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass\n        loss.backward()\n\n        # Optional: gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Update parameters\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n\n# Different learning rates for different layers\noptimizer_grouped = optim.Adam([\n    {'params': model[0].parameters(), 'lr': 1e-3},\n    {'params': model[2].parameters(), 'lr': 5e-4},\n    {'params': model[4].parameters(), 'lr': 1e-4}\n], lr=1e-3)  # Default lr for any params not specified\n\n# Learning rate scheduling\nfrom torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau\n\n# Step decay\nscheduler_step = StepLR(optimizer, step_size=30, gamma=0.1)\n\n# Cosine annealing\nscheduler_cosine = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\n# Reduce on plateau\nscheduler_plateau = ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.1,\n    patience=10,\n    verbose=True\n)\n\n# Training with scheduler\nfor epoch in range(num_epochs):\n    train_loss = 0.0\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    # Step scheduler\n    scheduler_step.step()\n    # Or for plateau scheduler\n    # scheduler_plateau.step(train_loss)\n\n    print(f'Epoch {epoch}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n```\n\n## Data Loading and Processing\n\n### Dataset and DataLoader\n\nCreating custom datasets and loading data efficiently.\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass CustomDataset(Dataset):\n    \"\"\"Custom dataset for loading data.\"\"\"\n\n    def __init__(self, data, labels, transform=None):\n        \"\"\"Initialize dataset.\n\n        Args:\n            data: numpy array or tensor of data samples\n            labels: numpy array or tensor of labels\n            transform: optional transform to apply to samples\n        \"\"\"\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return total number of samples.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Get a sample and its label.\n\n        Args:\n            idx: index of sample to retrieve\n\n        Returns:\n            Tuple of (sample, label)\n        \"\"\"\n        sample = self.data[idx]\n        label = self.labels[idx]\n\n        # Apply transform if provided\n        if self.transform:\n            sample = self.transform(sample)\n\n        # Convert to tensors if not already\n        if not isinstance(sample, torch.Tensor):\n            sample = torch.tensor(sample, dtype=torch.float32)\n        if not isinstance(label, torch.Tensor):\n            label = torch.tensor(label, dtype=torch.long)\n\n        return sample, label\n\n# Create sample data\nnum_samples = 1000\ndata = np.random.randn(num_samples, 28, 28)\nlabels = np.random.randint(0, 10, num_samples)\n\n# Create dataset\ndataset = CustomDataset(data, labels)\n\n# Create DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,  # Parallel data loading\n    pin_memory=True,  # Faster GPU transfer\n    drop_last=True  # Drop incomplete last batch\n)\n\n# Iterate through batches\nfor batch_idx, (batch_data, batch_labels) in enumerate(dataloader):\n    print(f\"Batch {batch_idx}: data shape {batch_data.shape}, labels shape {batch_labels.shape}\")\n    # batch_data: [32, 28, 28]\n    # batch_labels: [32]\n\n    if batch_idx == 2:  # Just show first 3 batches\n        break\n\n# Custom collate function for variable-length sequences\ndef collate_fn(batch):\n    \"\"\"Custom collate function for padding sequences.\n\n    Args:\n        batch: List of (data, label) tuples\n    \"\"\"\n    data_list, labels = zip(*batch)\n\n    # Pad sequences to same length\n    from torch.nn.utils.rnn import pad_sequence\n    data_padded = pad_sequence(data_list, batch_first=True, padding_value=0)\n    labels = torch.tensor(labels)\n\n    return data_padded, labels\n\n# Variable length sequences\nsequences = [torch.randn(np.random.randint(10, 50), 128) for _ in range(100)]\nseq_labels = torch.randint(0, 5, (100,))\nseq_dataset = [(seq, label) for seq, label in zip(sequences, seq_labels)]\n\nseq_dataloader = DataLoader(\n    seq_dataset,\n    batch_size=16,\n    collate_fn=collate_fn,\n    shuffle=True\n)\n\nfor batch_data, batch_labels in seq_dataloader:\n    print(f\"Padded batch shape: {batch_data.shape}\")\n    break  # [16, max_seq_len, 128]\n```\n\n### Data Transformations\n\nApplying transformations and augmentations to data.\n\n```python\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\n\nclass TransformDataset(Dataset):\n    \"\"\"Dataset with transformations.\"\"\"\n\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample, label\n\n# Compose multiple transformations\ntransform_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.RandomCrop(size=28, padding=4),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n])\n\n# Test transformations (no augmentation)\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n])\n\n# Custom transformation\nclass AddGaussianNoise:\n    \"\"\"Add Gaussian noise to tensor.\"\"\"\n\n    def __init__(self, mean=0., std=0.1):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        return tensor + noise\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(mean={self.mean}, std={self.std})'\n\n# Use custom transformation\ntransform_with_noise = transforms.Compose([\n    transforms.ToTensor(),\n    AddGaussianNoise(mean=0, std=0.05),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n# Apply transformations\ndata = np.random.randint(0, 255, (100, 28, 28, 3), dtype=np.uint8)\nlabels = np.random.randint(0, 10, 100)\n\ntrain_dataset = TransformDataset(data, labels, transform=transform_train)\ntest_dataset = TransformDataset(data, labels, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n```\n\n## Model Training and Evaluation\n\n### Complete Training Pipeline\n\nFull training loop with validation and checkpointing.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport time\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        running_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n\n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}/{len(train_loader)}, '\n                  f'Loss: {loss.item():.4f}, '\n                  f'Acc: {100.*correct/total:.2f}%')\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100. * correct / total\n    return epoch_loss, epoch_acc\n\ndef validate(model, val_loader, criterion, device):\n    \"\"\"Validate model.\"\"\"\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n\n            output = model(data)\n            loss = criterion(output, target)\n\n            val_loss += loss.item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n\n    val_loss /= len(val_loader)\n    val_acc = 100. * correct / total\n    return val_loss, val_acc\n\n# Complete training script\ndef train_model(model, train_loader, val_loader, num_epochs=10,\n                lr=0.001, device='cuda'):\n    \"\"\"Complete training pipeline.\"\"\"\n\n    # Setup\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=5, verbose=True\n    )\n\n    # Training history\n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': []\n    }\n\n    best_val_acc = 0.0\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n\n        # Train\n        train_loss, train_acc = train_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n\n        # Validate\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n        # Update scheduler\n        scheduler.step(val_loss)\n\n        # Save history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        epoch_time = time.time() - start_time\n\n        print(f'\\nEpoch {epoch+1}/{num_epochs} ({epoch_time:.2f}s)')\n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n\n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_acc': val_acc,\n            }, 'best_model.pth')\n            print(f'Saved best model with val_acc: {val_acc:.2f}%')\n\n    return history\n\n# Load model checkpoint\ndef load_checkpoint(model, optimizer, checkpoint_path):\n    \"\"\"Load model from checkpoint.\"\"\"\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    val_acc = checkpoint['val_acc']\n    print(f'Loaded checkpoint from epoch {epoch} with val_acc: {val_acc:.2f}%')\n    return model, optimizer, epoch\n\n# Example usage\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nmodel = CustomNet(input_size=784, hidden_size=256, num_classes=10)\nhistory = train_model(model, train_loader, val_loader,\n                     num_epochs=10, lr=0.001, device=device)\n```\n\n### Model Evaluation and Inference\n\nEvaluating trained models and making predictions.\n\n```python\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\ndef evaluate_model(model, test_loader, device='cuda'):\n    \"\"\"Comprehensive model evaluation.\"\"\"\n    model.eval()\n    all_preds = []\n    all_targets = []\n    all_probs = []\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data = data.to(device)\n            output = model(data)\n\n            # Get predictions and probabilities\n            probs = torch.softmax(output, dim=1)\n            _, predicted = output.max(1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(target.numpy())\n            all_probs.extend(probs.cpu().numpy())\n\n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n    all_probs = np.array(all_probs)\n\n    # Overall accuracy\n    accuracy = (all_preds == all_targets).mean()\n    print(f'Test Accuracy: {accuracy*100:.2f}%')\n\n    # Classification report\n    print('\\nClassification Report:')\n    print(classification_report(all_targets, all_preds))\n\n    # Confusion matrix\n    cm = confusion_matrix(all_targets, all_preds)\n    print('\\nConfusion Matrix:')\n    print(cm)\n\n    return {\n        'predictions': all_preds,\n        'targets': all_targets,\n        'probabilities': all_probs,\n        'accuracy': accuracy,\n        'confusion_matrix': cm\n    }\n\n# Single sample inference\ndef predict_single(model, sample, device='cuda'):\n    \"\"\"Predict on a single sample.\"\"\"\n    model.eval()\n\n    # Add batch dimension if needed\n    if sample.dim() == 1 or sample.dim() == 2:\n        sample = sample.unsqueeze(0)\n\n    sample = sample.to(device)\n\n    with torch.no_grad():\n        output = model(sample)\n        probs = torch.softmax(output, dim=1)\n        confidence, predicted = probs.max(1)\n\n    return predicted.item(), confidence.item()\n\n# Batch inference\ndef predict_batch(model, data_loader, device='cuda'):\n    \"\"\"Make predictions on entire dataset.\"\"\"\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for data, _ in data_loader:\n            data = data.to(device)\n            output = model(data)\n            _, predicted = output.max(1)\n            predictions.extend(predicted.cpu().numpy())\n\n    return np.array(predictions)\n\n# Model ensemble\ndef ensemble_predict(models, data_loader, device='cuda'):\n    \"\"\"Ensemble prediction from multiple models.\"\"\"\n    all_outputs = []\n\n    for model in models:\n        model.eval()\n        model_outputs = []\n\n        with torch.no_grad():\n            for data, _ in data_loader:\n                data = data.to(device)\n                output = model(data)\n                probs = torch.softmax(output, dim=1)\n                model_outputs.append(probs.cpu())\n\n        all_outputs.append(torch.cat(model_outputs, dim=0))\n\n    # Average predictions\n    ensemble_probs = torch.stack(all_outputs).mean(dim=0)\n    _, predictions = ensemble_probs.max(1)\n\n    return predictions.numpy(), ensemble_probs.numpy()\n\n# Example usage\nmodel = CustomNet(input_size=784, hidden_size=256, num_classes=10)\nmodel.load_state_dict(torch.load('best_model.pth')['model_state_dict'])\nmodel = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Evaluate\nresults = evaluate_model(model, test_loader)\n\n# Single prediction\nsample = torch.randn(784)\npred_class, confidence = predict_single(model, sample)\nprint(f'Predicted class: {pred_class}, Confidence: {confidence:.4f}')\n\n# Batch predictions\npredictions = predict_batch(model, test_loader)\nprint(f'Predicted {len(predictions)} samples')\n```\n\n## Distributed Training\n\n### Data Parallel Training\n\nTraining on multiple GPUs using DataParallel and DistributedDataParallel.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nfrom torch.utils.data import DataLoader, DistributedSampler\nimport os\n\n# Simple DataParallel (single machine, multiple GPUs)\ndef train_dataparallel(model, train_loader, num_epochs=10):\n    \"\"\"Train using DataParallel.\"\"\"\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        model = nn.DataParallel(model)\n\n    model = model.cuda()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n# DistributedDataParallel setup\ndef setup_ddp(rank, world_size):\n    \"\"\"Setup distributed training.\"\"\"\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # Initialize process group\n    dist.init_process_group(\n        backend='nccl',  # 'gloo' for CPU\n        rank=rank,\n        world_size=world_size\n    )\n\ndef cleanup_ddp():\n    \"\"\"Cleanup distributed training.\"\"\"\n    dist.destroy_process_group()\n\ndef train_ddp(rank, world_size, model, dataset):\n    \"\"\"Training function for DDP.\"\"\"\n    setup_ddp(rank, world_size)\n\n    # Move model to GPU\n    model = model.to(rank)\n    model = DistributedDataParallel(model, device_ids=[rank])\n\n    # Create distributed sampler\n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=True\n    )\n\n    # Create data loader\n    train_loader = DataLoader(\n        dataset,\n        batch_size=32,\n        sampler=sampler,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop\n    for epoch in range(10):\n        # Set epoch for sampler (for shuffling)\n        sampler.set_epoch(epoch)\n\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(rank), target.to(rank)\n\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            if rank == 0 and batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n\n    cleanup_ddp()\n\n# Launch distributed training\ndef launch_ddp_training():\n    \"\"\"Launch DDP training on multiple GPUs.\"\"\"\n    import torch.multiprocessing as mp\n\n    world_size = torch.cuda.device_count()\n    model = CustomNet(input_size=784, hidden_size=256, num_classes=10)\n\n    # Create dataset\n    dataset = CustomDataset(data, labels)\n\n    # Spawn processes\n    mp.spawn(\n        train_ddp,\n        args=(world_size, model, dataset),\n        nprocs=world_size,\n        join=True\n    )\n\n# Gradient accumulation for larger effective batch size\ndef train_with_accumulation(model, train_loader, accumulation_steps=4):\n    \"\"\"Training with gradient accumulation.\"\"\"\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer.zero_grad()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.cuda(), target.cuda()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Normalize loss for accumulation\n        loss = loss / accumulation_steps\n        loss.backward()\n\n        # Update weights after accumulation_steps\n        if (batch_idx + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            print(f'Batch {batch_idx}, Loss: {loss.item()*accumulation_steps:.4f}')\n\n# Example: Launch training\nif __name__ == '__main__':\n    # For single machine multi-GPU\n    model = CustomNet(input_size=784, hidden_size=256, num_classes=10)\n    # train_dataparallel(model, train_loader)\n\n    # For distributed training\n    # launch_ddp_training()\n```\n\n## Model Serialization and Deployment\n\n### Saving and Loading Models\n\nPersisting trained models and loading them for inference.\n\n```python\nimport torch\nimport torch.nn as nn\n\n# Method 1: Save/Load entire model (not recommended)\nmodel = CustomNet(input_size=784, hidden_size=256, num_classes=10)\n\n# Save entire model\ntorch.save(model, 'entire_model.pth')\n\n# Load entire model\nloaded_model = torch.load('entire_model.pth')\nloaded_model.eval()\n\n# Method 2: Save/Load state dict (recommended)\n# Save only model parameters\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n# Load parameters\nmodel = CustomNet(input_size=784, hidden_size=256, num_classes=10)\nmodel.load_state_dict(torch.load('model_weights.pth'))\nmodel.eval()\n\n# Method 3: Save complete checkpoint\ncheckpoint = {\n    'epoch': 10,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': 0.4,\n    'accuracy': 0.92,\n    'hyperparameters': {\n        'lr': 0.001,\n        'batch_size': 32,\n        'hidden_size': 256\n    }\n}\ntorch.save(checkpoint, 'checkpoint.pth')\n\n# Load checkpoint\ncheckpoint = torch.load('checkpoint.pth')\nmodel = CustomNet(\n    input_size=784,\n    hidden_size=checkpoint['hyperparameters']['hidden_size'],\n    num_classes=10\n)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer = torch.optim.Adam(model.parameters())\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# Resume training\nmodel.train()\nfor new_epoch in range(epoch, 100):\n    # Continue training...\n    pass\n\n# Save for inference on different devices\n# Save on GPU, load on CPU\ntorch.save(model.state_dict(), 'model.pth')\nmodel = CustomNet(input_size=784, hidden_size=256, num_classes=10)\nmodel.load_state_dict(torch.load('model.pth', map_location='cpu'))\n\n# Save on CPU, load on GPU\nmodel.load_state_dict(torch.load('model.pth', map_location='cuda:0'))\n\n# Cross-platform saving (handle different PyTorch versions)\ntorch.save(model.state_dict(), 'model.pth', _use_new_zipfile_serialization=True)\n```\n\n### TorchScript for Production\n\nConverting models to TorchScript for deployment without Python.\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass ScriptableNet(nn.Module):\n    \"\"\"Model designed for TorchScript conversion.\"\"\"\n\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(ScriptableNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = ScriptableNet(784, 256, 10)\nmodel.eval()\n\n# Method 1: Tracing (record operations)\nexample_input = torch.randn(1, 784)\ntraced_model = torch.jit.trace(model, example_input)\n\n# Save traced model\ntraced_model.save('traced_model.pt')\n\n# Load traced model\nloaded_traced = torch.jit.load('traced_model.pt')\noutput = loaded_traced(example_input)\nprint(f\"Output shape: {output.shape}\")\n\n# Method 2: Scripting (analyze code)\nscripted_model = torch.jit.script(model)\nscripted_model.save('scripted_model.pt')\n\n# Load and use scripted model\nloaded_scripted = torch.jit.load('scripted_model.pt')\noutput = loaded_scripted(example_input)\n\n# Optimize for inference\noptimized_model = torch.jit.optimize_for_inference(traced_model)\n\n# Freeze model (make constants read-only)\nfrozen_model = torch.jit.freeze(scripted_model)\n\n# View TorchScript IR\nprint(traced_model.graph)\nprint(scripted_model.code)\n\n# Using TorchScript in C++\n\"\"\"\n// C++ code example\n#include <torch/script.h>\n\nint main() {\n    torch::jit::script::Module model;\n    model = torch::jit::load(\"traced_model.pt\");\n\n    std::vector<torch::jit::IValue> inputs;\n    inputs.push_back(torch::randn({1, 784}));\n\n    at::Tensor output = model.forward(inputs).toTensor();\n    std::cout << output << std::endl;\n\n    return 0;\n}\n\"\"\"\n```\n\n## PyTorch Summary\n\nPyTorch provides a comprehensive deep learning ecosystem with tensor computation, automatic differentiation, neural network building blocks, and production deployment tools. Its dynamic computational graph approach enables flexible model architectures that can change during runtime, making it particularly suitable for research and rapid prototyping while maintaining production-grade performance through TorchScript compilation and distributed training capabilities.\n\nThe framework's integration patterns follow a consistent workflow: define model architecture using nn.Module classes, specify loss functions and optimizers, implement training loops with forward/backward passes, and deploy models using state dictionaries or TorchScript. Advanced users can extend PyTorch with custom autograd functions, CUDA kernels, and C++ extensions. The rich ecosystem includes torchvision for computer vision, torchaudio for speech processing, and torchtext for NLP tasks, all following similar API conventions for seamless integration into end-to-end machine learning pipelines."}
