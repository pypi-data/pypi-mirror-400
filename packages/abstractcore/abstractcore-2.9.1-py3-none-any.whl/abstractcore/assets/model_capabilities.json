{
  "models": {
    "gpt-4": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "source": "OpenAI official docs",
      "canonical_name": "gpt-4",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-4-turbo": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "notes": "GPT-4 Turbo with vision capabilities",
      "source": "OpenAI official docs 2025",
      "canonical_name": "gpt-4-turbo",
      "aliases": [
        "gpt-4-turbo-preview"
      ],
      "max_tokens": 128000
    },
    "gpt-4-turbo-with-vision": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "notes": "GPT-4 Turbo with vision capabilities",
      "source": "OpenAI official docs 2025",
      "canonical_name": "gpt-4-turbo-with-vision",
      "aliases": [
        "gpt-4-turbo-vision",
        "gpt-4-vision-preview"
      ],
      "max_tokens": 128000
    },
    "gpt-4o": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "video_support": true,
      "image_resolutions": [
        "variable"
      ],
      "image_tokenization_method": "tile_based",
      "base_image_tokens": 85,
      "tokens_per_tile": 170,
      "tile_size": "512x512",
      "max_image_dimension": 2048,
      "short_side_resize_target": 768,
      "detail_levels": [
        "low",
        "high",
        "auto"
      ],
      "low_detail_tokens": 85,
      "notes": "Multimodal omni model, 2x faster, half price, 5x higher rate limits (updated Nov 2024)",
      "source": "OpenAI official docs 2025",
      "canonical_name": "gpt-4o",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-4o-long-output": {
      "max_output_tokens": 64000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "16x output capacity variant",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-4o-long-output",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-4o-mini": {
      "max_output_tokens": 16000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "source": "OpenAI official docs",
      "canonical_name": "gpt-4o-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-3.5-turbo": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "source": "OpenAI official docs",
      "canonical_name": "gpt-3.5-turbo",
      "aliases": [],
      "max_tokens": 16385
    },
    "o1": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Reasoning model, no native tool support",
      "source": "OpenAI official docs",
      "canonical_name": "o1",
      "aliases": [],
      "max_tokens": 128000
    },
    "o1-mini": {
      "max_output_tokens": 65536,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "OpenAI official docs",
      "canonical_name": "o1-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "o3": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "notes": "May hallucinate tools with complex sets",
      "source": "OpenAI official docs",
      "canonical_name": "o3",
      "aliases": [],
      "max_tokens": 128000
    },
    "o3-mini": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "notes": "Known issues in Assistants API",
      "source": "OpenAI official docs",
      "canonical_name": "o3-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "claude-3.5-sonnet": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "image_tokenization_method": "pixel_area_based",
      "token_formula": "(width * height) / 750",
      "pixel_divisor": 750,
      "max_image_dimension": 1568,
      "token_cap": 1600,
      "min_dimension_warning": 200,
      "audio_support": false,
      "notes": "disable_parallel_tool_use option available",
      "source": "Anthropic official docs",
      "canonical_name": "claude-3.5-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3.7-sonnet": {
      "max_output_tokens": 128000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Extended output with beta header",
      "source": "Anthropic official docs",
      "canonical_name": "claude-3.7-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3.5-haiku": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "More likely to call unnecessary tools",
      "source": "Anthropic official docs",
      "canonical_name": "claude-3.5-haiku",
      "aliases": [
        "claude-3-5-haiku-20241022"
      ],
      "max_tokens": 200000
    },
    "claude-3-opus": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "max_tools": 1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "source": "Anthropic official docs",
      "canonical_name": "claude-3-opus",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3-sonnet": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "max_tools": 1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "source": "Anthropic official docs",
      "canonical_name": "claude-3-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3-haiku": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "max_tools": 1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "source": "Anthropic official docs",
      "canonical_name": "claude-3-haiku",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-haiku-4-5": {
      "max_output_tokens": 64000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude Haiku 4.5 series. Anthropic API enforces a 64K max output token cap (currently 64000).",
      "source": "Anthropic API error cap (max_tokens <= 64000)",
      "canonical_name": "claude-haiku-4-5",
      "aliases": [
        "claude-haiku-4-5-20251001",
        "anthropic/claude-haiku-4-5"
      ],
      "max_tokens": 200000
    },
    "claude-4-opus": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4 Opus with enhanced capabilities",
      "source": "Anthropic official docs",
      "canonical_name": "claude-4-opus",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-4.1-opus": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4.1 Opus with improved performance",
      "source": "Anthropic official docs",
      "canonical_name": "claude-4.1-opus",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-4-sonnet": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4 Sonnet with balanced performance",
      "source": "Anthropic official docs",
      "canonical_name": "claude-4-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-4.5-sonnet": {
      "max_output_tokens": 64000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4.5 Sonnet. Anthropic API enforces a 64K max output token cap (currently 64000).",
      "source": "Anthropic API error cap (max_tokens <= 64000)",
      "canonical_name": "claude-4.5-sonnet",
      "aliases": [
        "claude-sonnet-4-5",
        "claude-sonnet-4-5-20250929",
        "anthropic/claude-sonnet-4-5"
      ],
      "max_tokens": 200000
    },
    "claude-opus-4-5": {
      "max_output_tokens": 64000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude Opus 4.5. Anthropic API enforces a 64K max output token cap (currently 64000).",
      "source": "Anthropic API error cap (max_tokens <= 64000)",
      "canonical_name": "claude-opus-4-5",
      "aliases": [
        "claude-opus-4-5-20251101",
        "anthropic/claude-opus-4-5"
      ],
      "max_tokens": 200000
    },
    "llama-3.2-1b": {
      "max_output_tokens": 2048,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Text only, optimized for edge",
      "source": "Meta official docs",
      "canonical_name": "llama-3.2-1b",
      "aliases": [],
      "max_tokens": 8192
    },
    "llama-3.2-3b": {
      "max_output_tokens": 2048,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Text only, optimized for edge",
      "source": "Meta official docs",
      "canonical_name": "llama-3.2-3b",
      "aliases": [],
      "max_tokens": 8192
    },
    "llama-3.2-11b-vision": {
      "max_output_tokens": 2048,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "notes": "Vision model, may be limited by deployment platform",
      "source": "Meta official docs",
      "canonical_name": "llama-3.2-11b-vision",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.3-70b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Improved multilingual and tool use",
      "source": "Meta official docs",
      "canonical_name": "llama-3.3-70b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.1-8b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Fine-tuned on tool calling",
      "source": "Meta official docs",
      "canonical_name": "llama-3.1-8b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.1-70b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Fine-tuned on tool calling",
      "source": "Meta official docs",
      "canonical_name": "llama-3.1-70b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.1-405b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Largest Llama model",
      "source": "Meta official docs",
      "canonical_name": "llama-3.1-405b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-4": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "notes": "Multimodal with early fusion, 109B total params (MoE)",
      "source": "Meta announcement",
      "canonical_name": "llama-4",
      "aliases": [],
      "max_tokens": 10000000
    },
    "qwen2.5-0.5b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 0.5B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-0.5b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-1.5b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 1.5B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-1.5b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-3b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 3B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-3b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-7b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 7B model with MCP support",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-7b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen2.5-14b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 14B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-14b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen2.5-32b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 32B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-32b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen2.5-72b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 72B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-72b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-0.6b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 base model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-0.6b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3-1.7b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 1.7B model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-1.7b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3-4b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 4B model with extended context via YaRN scaling",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-4b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-32b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 32B model with advanced thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-32b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-30b-a3b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 MoE model with 4-bit precision, 30B total/3B active parameters",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-30b-a3b",
      "aliases": [],
      "max_tokens": 40960
    },
    "qwen3-30b-a3b-2507": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3-30B-A3B-Instruct-2507 with enhanced reasoning, coding, and mathematical skills. Supports up to 256K context, extendable to 1M tokens",
      "source": "Alibaba Qwen3 2507 release",
      "canonical_name": "qwen3-30b-a3b-2507",
      "aliases": [
        "qwen/qwen3-30b-a3b-2507"
      ],
      "max_tokens": 262144
    },
    "qwen3-coder-30b": {
      "max_output_tokens": 65536,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "architecture": "mixture_of_experts",
      "total_parameters": "30.5B",
      "active_parameters": "3.3B",
      "experts": 128,
      "experts_activated": 8,
      "notes": "Code-focused MoE model (30.5B total/3.3B active, 128 experts/8 activated). Native tool support via chatml-function-calling format. Supports up to 1M tokens with YaRN extension.",
      "source": "Qwen HuggingFace model card 2025",
      "canonical_name": "qwen3-coder-30b",
      "aliases": [
        "Qwen/Qwen3-Coder-30B-A3B-Instruct",
        "qwen3-coder-30b-a3b",
        "qwen3-coder-30b-a3b-instruct"
      ],
      "max_tokens": 262144
    },
    "qwen2-vl": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "source": "Alibaba official docs",
      "canonical_name": "qwen2-vl",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-vl": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "notes": "Qwen2.5-VL multimodal model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-vl",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-2": {
      "max_output_tokens": 2048,
      "tool_support": "none",
      "structured_output": "none",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-2",
      "aliases": [],
      "max_tokens": 2048
    },
    "phi-3-mini": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Poor JSON, use XML format",
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-mini",
      "aliases": [],
      "max_tokens": 4096
    },
    "phi-3-small": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-small",
      "aliases": [],
      "max_tokens": 8192
    },
    "phi-3-medium": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-medium",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-3.5-mini": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3.5-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-3.5-moe": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Mixture of Experts",
      "source": "Microsoft official docs",
      "canonical_name": "phi-3.5-moe",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-3-vision": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-vision",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-4": {
      "max_output_tokens": 16000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Extended to 16K during mid-training",
      "source": "Microsoft official docs",
      "canonical_name": "phi-4",
      "aliases": [],
      "max_tokens": 16000
    },
    "mistral-7b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-7b",
      "aliases": [],
      "max_tokens": 8192
    },
    "mixtral-8x7b": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "MoE architecture",
      "source": "Mistral AI docs",
      "canonical_name": "mixtral-8x7b",
      "aliases": [],
      "max_tokens": 32768
    },
    "mixtral-8x22b": {
      "max_output_tokens": 65536,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mixtral-8x22b",
      "aliases": [],
      "max_tokens": 65536
    },
    "mistral-small": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-small",
      "aliases": [],
      "max_tokens": 32768
    },
    "mistral-medium": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-medium",
      "aliases": [],
      "max_tokens": 32768
    },
    "mistral-large": {
      "max_output_tokens": 128000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-large",
      "aliases": [],
      "max_tokens": 128000
    },
    "codestral": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Code-specialized",
      "source": "Mistral AI docs",
      "canonical_name": "codestral",
      "aliases": [],
      "max_tokens": 32768
    },
    "magistral-small-2509": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "notes": "Mistral vision model optimized for multimodal tasks",
      "source": "Mistral AI 2025 release",
      "canonical_name": "magistral-small-2509",
      "aliases": [
        "mistralai/magistral-small-2509"
      ],
      "max_tokens": 128000
    },
    "Qwen/Qwen3-VL-8B-Instruct-FP8": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "notes": "Qwen3-VL 8B model with FP8 quantization for HuggingFace, optimized for efficient inference",
      "source": "Qwen team 2025 HuggingFace release",
      "canonical_name": "Qwen/Qwen3-VL-8B-Instruct-FP8",
      "aliases": [
        "qwen3-vl-8b-fp8",
        "qwen3-vl-8b-instruct-fp8"
      ],
      "max_tokens": 262144
    },
    "llama3.2-vision:11b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "560x560",
        "1120x560",
        "560x1120",
        "1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "image_patch_size": 14,
      "max_image_tokens": 6400,
      "image_tokenization_method": "resolution_tier_based",
      "supported_resolutions": [
        [
          560,
          560
        ],
        [
          1120,
          560
        ],
        [
          560,
          1120
        ],
        [
          1120,
          1120
        ]
      ],
      "base_tokens_per_resolution": {
        "560x560": 1600,
        "1120x560": 3200,
        "560x1120": 3200,
        "1120x1120": 6400
      },
      "notes": "Llama 3.2 Vision 11B model with multimodal capabilities for visual recognition and reasoning",
      "source": "Meta AI Llama 3.2 release",
      "canonical_name": "llama3.2-vision:11b",
      "aliases": [
        "llama3.2-vision-11b",
        "llama-3.2-vision:11b"
      ],
      "max_tokens": 131072
    },
    "llama3.2-vision:70b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "560x560",
        "1120x560",
        "560x1120",
        "1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "image_patch_size": 14,
      "max_image_tokens": 6400,
      "notes": "Llama 3.2 Vision 70B model with advanced multimodal capabilities for complex visual reasoning",
      "source": "Meta AI Llama 3.2 release",
      "canonical_name": "llama3.2-vision:70b",
      "aliases": [
        "llama3.2-vision-70b",
        "llama-3.2-vision:70b"
      ],
      "max_tokens": 131072
    },
    "llama3.2-vision:90b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "560x560",
        "1120x560",
        "560x1120",
        "1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "image_patch_size": 14,
      "max_image_tokens": 6400,
      "notes": "Llama 3.2 Vision 90B model with top-tier multimodal capabilities for advanced visual understanding",
      "source": "Meta AI Llama 3.2 release",
      "canonical_name": "llama3.2-vision:90b",
      "aliases": [
        "llama3.2-vision-90b",
        "llama-3.2-vision:90b"
      ],
      "max_tokens": 131072
    },
    "gemma-2b": {
      "max_output_tokens": 8192,
      "tool_support": "none",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma-2b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma-7b": {
      "max_output_tokens": 8192,
      "tool_support": "none",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma-7b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma2-9b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma2-9b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma2-27b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma2-27b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma3": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Native function calling support introduced in Gemma 3",
      "source": "Google docs",
      "canonical_name": "gemma3",
      "aliases": [],
      "max_tokens": 128000
    },
    "codegemma": {
      "max_output_tokens": 8192,
      "tool_support": "none",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Code-specialized",
      "source": "Google docs",
      "canonical_name": "codegemma",
      "aliases": [],
      "max_tokens": 8192
    },
    "paligemma": {
      "max_output_tokens": 1024,
      "tool_support": "none",
      "structured_output": "none",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "224x224",
        "448x448",
        "896x896"
      ],
      "audio_support": false,
      "notes": "Vision-language model",
      "source": "Google docs",
      "canonical_name": "paligemma",
      "aliases": [],
      "max_tokens": 8192
    },
    "glm-4": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "ChatGLM4 from Zhipu AI",
      "source": "Model documentation",
      "canonical_name": "glm-4",
      "aliases": [],
      "max_tokens": 128000
    },
    "glm-4-9b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "9B parameter version",
      "source": "Model documentation",
      "canonical_name": "glm-4-9b",
      "aliases": [],
      "max_tokens": 128000
    },
    "glm-4-9b-0414-4bit": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "MLX quantized version",
      "source": "Model documentation",
      "canonical_name": "glm-4-9b-0414-4bit",
      "aliases": [],
      "max_tokens": 128000
    },
    "deepseek-r1": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Reasoning model with native tool calling capability",
      "source": "MLX community",
      "canonical_name": "deepseek-r1",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Instruct model with good reasoning. Use prompted tool support when running via MLX.",
      "source": "MLX community",
      "canonical_name": "qwen3",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3-14b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 14B model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-14b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-next-80b-a3b": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3-Next hybrid MoE with 512 experts/10 activated, extensible to 1M tokens with YaRN. Uses <|tool_call|> format for tool calls",
      "source": "Alibaba Qwen3-Next technical report",
      "canonical_name": "qwen3-next-80b-a3b",
      "aliases": [
        "qwen/qwen3-next-80b"
      ],
      "max_tokens": 262144
    },
    "gpt-5": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 base model with multimodal capabilities",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-turbo": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 Turbo with faster inference",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-turbo",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-pro": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 Pro with extended output capabilities",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-pro",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-mini": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 Mini with cost-optimized performance",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-mini",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-vision": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 2048x2048"
      ],
      "audio_support": true,
      "notes": "GPT-5 Vision with enhanced multimodal capabilities",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-vision",
      "aliases": [],
      "max_tokens": 200000
    },
    "qwen3-8b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 8B model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-8b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-235b-a22b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 MoE model with 4-bit precision, 235B total/22B active parameters",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-235b-a22b",
      "aliases": [],
      "max_tokens": 40960
    },
    "qwen3-vl": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "video_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "notes": "Qwen3-VL multimodal model with vision and video support, 32x32 pixel patches",
      "source": "Alibaba Qwen3-VL technical report",
      "canonical_name": "qwen3-vl",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen2.5-vl-7b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "image_resolutions": [
        "56x56 to 3584x3584"
      ],
      "max_image_resolution": "3584x3584",
      "image_patch_size": 14,
      "max_image_tokens": 16384,
      "pixel_grouping": "28x28",
      "image_tokenization_method": "patch_based_adaptive",
      "adaptive_resolution": true,
      "min_resolution": 56,
      "max_resolution": 3584,
      "vision_encoder": "ViT-based",
      "notes": "Qwen2.5-VL 7B parameter vision model, 28x28 pixel patches, max 3584x3584 resolution",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-vl-7b",
      "aliases": [
        "qwen/qwen2.5-vl-7b",
        "unsloth/Qwen2.5-VL-7B-Instruct-GGUF"
      ],
      "max_tokens": 128000
    },    
    "gemma3-4b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "896x896"
      ],
      "max_image_resolution": "896x896",
      "vision_encoder": "SigLIP-400M",
      "image_tokens_per_image": 256,
      "adaptive_windowing": true,
      "image_tokenization_method": "fixed_resolution",
      "fixed_resolution": [
        896,
        896
      ],
      "preprocessing": "automatic_resize_and_crop",
      "notes": "Gemma3 4B parameter model with vision support, 896x896 fixed resolution with adaptive windowing",
      "source": "Google Gemma3 documentation 2025",
      "canonical_name": "gemma3-4b",
      "aliases": [
        "gemma3:4b"
      ],
      "max_tokens": 128000
    },
    "qwen2.5vl:7b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "image_resolutions": [
        "56x56 to 3584x3584"
      ],
      "max_image_resolution": "3584x3584",
      "image_patch_size": 14,
      "max_image_tokens": 16384,
      "pixel_grouping": "28x28",
      "notes": "Qwen2.5-VL 7B Ollama variant, 28x28 pixel patches, max 3584x3584 resolution",
      "source": "Ollama model library",
      "canonical_name": "qwen2.5vl:7b",
      "aliases": [
        "qwen2.5vl"
      ],
      "max_tokens": 128000
    },
    "gemma3:4b-it-qat": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "896x896"
      ],
      "max_image_resolution": "896x896",
      "vision_encoder": "SigLIP-400M",
      "image_tokens_per_image": 256,
      "adaptive_windowing": true,
      "notes": "Gemma3 4B instruct-tuned quantized model for Ollama, 896x896 fixed resolution",
      "source": "Ollama model library",
      "canonical_name": "gemma3:4b-it-qat",
      "aliases": [],
      "max_tokens": 128000
    },
    "gemma3n:e4b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "video_support": true,
      "image_resolutions": [
        "896x896"
      ],
      "max_image_resolution": "896x896",
      "vision_encoder": "SigLIP-400M",
      "image_tokens_per_image": 256,
      "adaptive_windowing": true,
      "memory_footprint": "3GB",
      "notes": "Gemma3n device-optimized multimodal model, 896x896 fixed resolution",
      "source": "Google Gemma3n documentation 2025",
      "canonical_name": "gemma3n:e4b",
      "aliases": [
        "gemma3n:e4b:latest",
        "gemma-3n-e4b",
        "google/gemma-3n-e4b",
        "gemma3n",
        "gemma3n:e2b:latest",
        "gemma3n:e2b"
      ],
      "max_tokens": 32768
    },
    "seed-oss": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "thinking_budget": true,
      "notes": "SEED-OSS 36B parameter model with 512K context and thinking budget control",
      "source": "ByteDance SEED-OSS documentation",
      "canonical_name": "seed-oss",
      "aliases": [],
      "max_tokens": 524288
    },
    "glm-4.5": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "GLM-4.5 MoE model with 355B total/32B active parameters",
      "source": "Zhipu AI GLM-4.5 announcement",
      "canonical_name": "glm-4.5",
      "aliases": [],
      "max_tokens": 128000
    },
    "glm-4.6": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "GLM-4.6 MoE model with enhanced capabilities",
      "source": "Zhipu AI GLM-4.6 announcement",
      "canonical_name": "glm-4.6",
      "aliases": [
        "zai-org/GLM-4.6",
        "zai-org/GLM-4.6-FP8",
        "glm-4.6-fp8"
      ],
      "max_tokens": 128000
    },
    "glm-4.5-air": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "GLM-4.5-Air lightweight model optimized for efficiency",
      "source": "Zhipu AI GLM-4.5-Air announcement",
      "canonical_name": "glm-4.5-air",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-4-109b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "notes": "LLaMA 4 109B total parameters (MoE), multimodal with early fusion",
      "source": "Meta LLaMA 4 announcement",
      "canonical_name": "llama-4-109b",
      "aliases": [],
      "max_tokens": 10000000
    },
    "granite3.2:2b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.2 2B text-only model with reasoning capabilities",
      "source": "IBM Granite 3.2 technical report",
      "canonical_name": "granite3.2:2b",
      "aliases": [
        "granite3.2-2b"
      ],
      "max_tokens": 32768
    },
    "granite3.2:8b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.2 8B text-only model with reasoning capabilities",
      "source": "IBM Granite 3.2 technical report",
      "canonical_name": "granite3.2:8b",
      "aliases": [
        "granite3.2-8b"
      ],
      "max_tokens": 32768
    },
    "granite3.2-vision:2b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "768x768"
      ],
      "max_image_resolution": "768x768",
      "vision_encoder": "SigLIP2-so400m-patch14-384",
      "image_patch_size": 14,
      "image_tokenization_method": "patch_based",
      "notes": "IBM Granite 3.2-Vision 2B model with SigLIP2 encoder, optimized for visual document understanding",
      "source": "IBM Granite 3.2 technical report arXiv:2502.09927",
      "canonical_name": "granite3.2-vision:2b",
      "aliases": [
        "granite3.2-vision:latest",
        "granite3.2-vision",
        "granite-vision",
        "ibm-granite-vision"
      ],
      "max_tokens": 32768
    },
    "gemini-2.5-flash": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "video_support": true,
      "image_resolutions": [
        "224x224",
        "448x448",
        "1024x1024"
      ],
      "max_image_resolution": "768x768",
      "image_tokenization_method": "gemini_vision_encoder",
      "thinking_support": true,
      "thinking_budget": true,
      "notes": "Optimized for speed and efficiency, suitable for high-volume, latency-sensitive tasks. Supports configurable thinking budgets",
      "source": "Google AI official docs 2025",
      "canonical_name": "gemini-2.5-flash",
      "aliases": [
        "gemini-2.5-flash-001"
      ],
      "max_tokens": 1000000
    },
    "gemini-2.5-pro": {
      "max_output_tokens": 65536,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "video_support": true,
      "image_resolutions": [
        "224x224",
        "448x448",
        "1024x1024"
      ],
      "max_image_resolution":  "768x768",
      "image_tokenization_method": "gemini_vision_encoder",
      "thinking_support": true,
      "thinking_budget": true,
      "notes": "Most advanced Gemini model for complex reasoning, coding, and mathematical problem-solving. Features Deep Think mode for enhanced reasoning",
      "source": "Google AI official docs 2025",
      "canonical_name": "gemini-2.5-pro",
      "aliases": [
        "gemini-2.5-pro-001"
      ],
      "max_tokens": 1048576
    },
    "granite3.3:2b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.3 2B text-only model with enhanced reasoning capabilities",
      "source": "IBM Granite 3.3 release announcement",
      "canonical_name": "granite3.3:2b",
      "aliases": [
        "granite3.3-2b"
      ],
      "max_tokens": 32768
    },
    "granite3.3:8b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.3 8B text-only model with enhanced reasoning capabilities",
      "source": "IBM Granite 3.3 release announcement",
      "canonical_name": "granite3.3:8b",
      "aliases": [
        "granite3.3-8b"
      ],
      "max_tokens": 32768
    },
    "embeddinggemma:300m": {
      "max_output_tokens": 0,
      "tool_support": "none",
      "structured_output": "none",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Text embedding model, not for generation or vision",
      "source": "Google Gemma documentation",
      "canonical_name": "embeddinggemma:300m",
      "aliases": [
        "google/embeddinggemma-300m"
      ],
      "max_tokens": 8192,
      "model_type": "embedding"
    },
    "blip-image-captioning-base": {
      "max_output_tokens": 512,
      "tool_support": "none",
      "structured_output": "none",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "224x224",
        "384x384"
      ],
      "max_image_resolution": "384x384",
      "vision_encoder": "ViT-B/16",
      "image_patch_size": 16,
      "image_tokenization_method": "patch_based",
      "base_image_tokens": 577,
      "notes": "Salesforce BLIP image captioning model, primarily for image-to-text tasks",
      "source": "Salesforce BLIP documentation",
      "canonical_name": "blip-image-captioning-base",
      "aliases": [
        "Salesforce/blip-image-captioning-base"
      ],
      "max_tokens": 2048
    },
    "glyph": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "base_model": "GLM-4.1V-9B-Base",
      "total_parameters": "10B",
      "tensor_type": "BF16",
      "image_tokenization_method": "visual_text_compression",
      "optimized_for_glyph": true,
      "text_image_processing": true,
      "architecture": "glm4v",
      "requires_processor": true,
      "message_format": "glm_special_tokens",
      "conversation_template": {
        "system_prefix": "<|system|>\n",
        "system_suffix": "\n",
        "user_prefix": "<|user|>\n", 
        "user_suffix": "\n",
        "assistant_prefix": "<|assistant|>\n",
        "assistant_suffix": "\n"
      },
      "model_class": "AutoModelForImageTextToText",
      "processor_class": "AutoProcessor",
      "trust_remote_code": true,
      "transformers_version_min": "4.57.1",
      "notes": "Glyph framework for scaling context windows via visual-text compression. Built on GLM-4.1V-9B-Base. Renders long text into images for VLM processing. Requires AutoModelForImageTextToText and AutoProcessor with trust_remote_code=True.",
      "source": "HuggingFace zai-org/Glyph model card",
      "canonical_name": "glyph",
      "aliases": [
        "zai-org/Glyph"
      ],
      "max_tokens": 131072,
      "license": "MIT",
      "arxiv": "2510.17800",
      "repository": "https://github.com/thu-coai/Glyph"
    },
    "glm-4.1v-9b-base": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "total_parameters": "9B",
      "base_model": "GLM-4-9B-0414",
      "image_tokenization_method": "glm_vision_encoder",
      "architecture": "glm4v",
      "requires_processor": true,
      "message_format": "glm_special_tokens",
      "model_class": "AutoModelForImageTextToText",
      "processor_class": "AutoProcessor",
      "trust_remote_code": true,
      "transformers_version_min": "4.57.1",
      "notes": "GLM-4.1V 9B base model, backbone for Glyph visual-text compression framework",
      "source": "HuggingFace zai-org/GLM-4.1V-9B-Base",
      "canonical_name": "glm-4.1v-9b-base",
      "aliases": [
        "zai-org/GLM-4.1V-9B-Base"
      ],
      "max_tokens": 131072
    },
    "glm-4.1v-9b-thinking": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "up to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "total_parameters": "10B",
      "base_model": "GLM-4-9B-0414",
      "image_tokenization_method": "glm_vision_encoder",
      "thinking_support": true,
      "reasoning_paradigm": "chain_of_thought",
      "adaptive_resolution": true,
      "aspect_ratio_support": "arbitrary",
      "architecture": "glm4v",
      "requires_processor": true,
      "message_format": "glm_special_tokens",
      "model_class": "AutoModelForImageTextToText",
      "processor_class": "AutoProcessor",
      "trust_remote_code": true,
      "transformers_version_min": "4.57.1",
      "notes": "GLM-4.1V-9B-Thinking with Chain-of-Thought reasoning, 64K context, arbitrary aspect ratios up to 4K resolution. First reasoning-focused VLM in the series, matches 72B models on 18 benchmark tasks.",
      "source": "HuggingFace zai-org/GLM-4.1V-9B-Thinking and GitHub zai-org/GLM-V",
      "canonical_name": "glm-4.1v-9b-thinking",
      "aliases": [
        "zai-org/GLM-4.1V-9B-Thinking",
        "glm-4.1v-thinking",
        "glm4.1v-9b-thinking"
      ],
      "max_tokens": 65536,
      "arxiv": "2507.01006"
    },
    "mistral-small-3.1-24b-instruct": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "up to 2048x2048"
      ],
      "max_image_resolution": "2048x2048",
      "image_tokenization_method": "mistral_vision_encoder",
      "notes": "Mistral Small 3.1 with 24B parameters, 128K context, multimodal understanding. Released March 2025.",
      "source": "Mistral AI documentation and HuggingFace",
      "canonical_name": "mistral-small-3.1-24b-instruct",
      "aliases": [
        "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
      ],
      "max_tokens": 131072,
      "total_parameters": "24B",
      "release_date": "2025-03-17"
    },
    "mistral-small-3.2-24b-instruct": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "up to 2048x2048"
      ],
      "max_image_resolution": "2048x2048",
      "image_tokenization_method": "mistral_vision_encoder",
      "tensor_type": "BF16",
      "gpu_memory_required": "55GB",
      "notes": "Mistral Small 3.2 with 24B parameters, 128K context. Improved instruction following, reduced repetition, enhanced function calling. Released June 2025.",
      "source": "HuggingFace mistralai/Mistral-Small-3.2-24B-Instruct-2506",
      "canonical_name": "mistral-small-3.2-24b-instruct",
      "aliases": [
        "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
      ],
      "max_tokens": 131072,
      "total_parameters": "24B",
      "release_date": "2025-06-01"
    },
    "llama-4-scout": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "video_support": false,
      "image_resolutions": [
        "up to 1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "architecture": "mixture_of_experts",
      "active_parameters": "17B",
      "total_parameters": "109B",
      "experts": 16,
      "image_tokenization_method": "resolution_tier_based",
      "notes": "LLaMA 4 Scout with MoE architecture, 17B active/109B total parameters, 10M context window. Multimodal with early fusion. Released April 2025.",
      "source": "Meta LLaMA 4 documentation and NVIDIA docs",
      "canonical_name": "llama-4-scout",
      "aliases": [
        "llama4-17b-scout-16e-instruct",
        "llama-4-17b-scout-16e-instruct"
      ],
      "max_tokens": 10000000,
      "release_date": "2025-04-05",
      "image_patch_size": 14,
      "max_image_tokens": 6400
    },
    "llama-4-maverick": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "video_support": false,
      "image_resolutions": [
        "up to 1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "architecture": "mixture_of_experts",
      "active_parameters": "17B",
      "total_parameters": "400B",
      "experts": 128,
      "image_tokenization_method": "resolution_tier_based",
      "notes": "LLaMA 4 Maverick with MoE architecture, 17B active/400B total parameters, 1M context window. Optimized for coding and reasoning. Released April 2025.",
      "source": "Meta LLaMA 4 documentation and Oracle docs",
      "canonical_name": "llama-4-maverick",
      "aliases": [
        "llama4-17b-maverick-128e-instruct"
      ],
      "max_tokens": 1000000,
      "release_date": "2025-04-05",
      "image_patch_size": 14,
      "max_image_tokens": 6400
    },
    "llama-4-behemoth": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "video_support": false,
      "image_resolutions": [
        "up to 1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "architecture": "mixture_of_experts",
      "active_parameters": "288B",
      "total_parameters": "2T",
      "experts": 16,
      "image_tokenization_method": "resolution_tier_based",
      "notes": "LLaMA 4 Behemoth teacher model with 288B active/2T total parameters. Designed for distilling performance into smaller models. Announced April 2025.",
      "source": "Meta LLaMA 4 announcement and PromptHub",
      "canonical_name": "llama-4-behemoth",
      "aliases": [
        "llama4-288b-behemoth-16e"
      ],
      "max_tokens": 1000000,
      "release_date": "2025-04-05",
      "status": "announced",
      "image_patch_size": 14,
      "max_image_tokens": 6400
    },
    "minimax-m2": {
      "max_output_tokens": 131072,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "architecture": "mixture_of_experts",
      "active_parameters": "10B",
      "total_parameters": "230B",
      "thinking_paradigm": "interleaved_thinking",
      "thinking_format": "<think>...</think>",
      "notes": "MiniMax M2 open-source MoE model (230B total/10B active) optimized for coding and agentic workflows. Industry-leading 204K token context window with 131K output capacity. Uses interleaved thinking with <think> tags for reasoning. Achieves strong performance on SWE-Bench and Terminal-Bench tasks. Ranked #5 on Artificial Analysis Intelligence Index. Efficient deployment at up to 8% cost of comparable models. Supports complete tool calling for agent workflows. Runs seamlessly on 8xH100 setup using vLLM.",
      "source": "MiniMax official docs (HuggingFace MiniMaxAI/MiniMax-M2, Microsoft Azure AI Foundry blog)",
      "canonical_name": "minimax-m2",
      "aliases": [
        "MiniMaxAI/MiniMax-M2",
        "mlx-community/minimax-m2",
        "mlx-community/MiniMax-M2",
        "unsloth/MiniMax-M2-GGUF",
        "minimax-m2-230b",
        "minimax-m2-10b-active",
        "minimax/minimax-m2"
      ],
      "max_tokens": 208896,
      "release_date": "2025-01",
      "license": "Apache-2.0",
      "inference_parameters": {
        "temperature": 1.0,
        "top_p": 0.95,
        "top_k": 40
      },
      "default_system_prompt": "You are a helpful assistant. Your name is MiniMax-M2 and is built by MiniMax."
    },
    "minimax-m2.1": {
      "max_output_tokens": 131072,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "architecture": "mixture_of_experts",
      "active_parameters": "10B",
      "total_parameters": "229B",
      "tensor_type": "FP8",
      "thinking_paradigm": "interleaved_thinking",
      "thinking_format": "<think>...</think>",
      "agentic_coding": true,
      "notes": "MiniMax M2.1 enhanced MoE model (229B total/10B active) optimized for advanced coding, agentic workflows, and full-stack development. 200K token context window with massive 128K output capacity. Significant improvements over M2 in multilingual software engineering (SWE-bench Multilingual: 72.5%), achieving performance close to Claude Opus 4.5. Excels at full-stack development with VIBE average of 88.6 (Web: 91.5, Android: 89.7). Uses interleaved thinking with <think> tags. Achieves 74.0% on SWE-bench Verified, 47.9% on Terminal-bench 2.0. Supports complete native tool calling for agent workflows.",
      "source": "MiniMax official docs (platform.minimax.io, HuggingFace MiniMaxAI/MiniMax-M2.1)",
      "canonical_name": "minimax-m2.1",
      "aliases": [
        "MiniMaxAI/MiniMax-M2.1",
        "minimaxai/minimax-m2.1",
        "minimax-m2.1-229b",
        "minimax-m2.1-10b-active",
        "minimax/minimax-m2.1"
      ],
      "max_tokens": 204800,
      "release_date": "2024-12",
      "license": "Modified-MIT",
      "arxiv": "2509.06501",
      "benchmarks": {
        "SWE-bench Verified": 74.0,
        "Multi-SWE-bench": 49.4,
        "SWE-bench Multilingual": 72.5,
        "Terminal-bench 2.0": 47.9,
        "SWE-bench Verified (Droid)": 71.3,
        "SWE-bench Verified (mini-swe-agent)": 67.0,
        "SWT-bench": 69.3,
        "SWE-Perf": 3.1,
        "SWE-Review": 8.9,
        "OctoCodingbench": 26.1,
        "VIBE Average": 88.6,
        "VIBE-Web": 91.5,
        "VIBE-Android": 89.7,
        "VIBE-Simulation": 87.1,
        "VIBE-iOS": 88.0,
        "VIBE-Backend": 86.7,
        "Toolathlon": 43.5,
        "BrowseComp": 47.4,
        "BrowseComp (context management)": 62.0,
        "AIME25": 83.0,
        "MMLU-Pro": 88.0,
        "GPQA-D": 83.0,
        "HLE w/o tools": 22.2,
        "LCB": 81.0,
        "SciCode": 41.0,
        "IFBench": 70.0,
        "AA-LCR": 62.0,
        "-Bench Telecom": 87.0
      },
      "inference_parameters": {
        "temperature": 1.0,
        "top_p": 0.95,
        "top_k": 40
      },
      "default_system_prompt": "You are a helpful assistant. Your name is MiniMax-M2.1 and is built by MiniMax."
    },
    "glm-4.6v": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "thinking_output_field": "reasoning_content",
      "image_resolutions": [
        "up to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "architecture": "mixture_of_experts",
      "total_parameters": "106B",
      "image_tokenization_method": "glm_vision_encoder",
      "adaptive_resolution": true,
      "aspect_ratio_support": "arbitrary",
      "native_function_calling": true,
      "interleaved_generation": true,
      "document_understanding": true,
      "frontend_replication": true,
      "tool_calling_format": "glm_xml",
      "tool_calling_parser": "glm46v",
      "output_wrappers": {
        "start": "<|begin_of_box|>",
        "end": "<|end_of_box|>"
      },
      "thinking_control": "/nothink",
      "thinking_tags": ["<think>", "</think>"],
      "notes": "GLM-4.6V foundation model (106B params) for cloud deployment. Native multimodal function calling with vision-driven tool use using XML format: <tool_call>function_name\\n<arg_key>key</arg_key>\\n<arg_value>value</arg_value>\\n</tool_call>. Supports interleaved image-text generation, 128K context, multimodal document understanding, and frontend replication from screenshots. Generates reasoning in 'reasoning_content' field or <think></think> tags. Achieves SoTA performance in visual understanding among similar parameter scales. Thinking can be disabled with '/nothink' suffix in user message. See: https://github.com/zai-org/GLM-V",
      "source": "HuggingFace zai-org/GLM-4.6V and GLM-V GitHub",
      "canonical_name": "glm-4.6v",
      "aliases": [
        "zai-org/GLM-4.6V",
        "zai-org/GLM-4.6V-FP8",
        "glm-4.6v-106b",
        "glm-4.6v-fp8"
      ],
      "max_tokens": 128000,
      "release_date": "2025-05-07",
      "arxiv": "2507.01006",
      "license": "MIT"
    },
    "glm-4.6v-flash": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "thinking_output_field": "reasoning_content",
      "image_resolutions": [
        "up to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "total_parameters": "9B",
      "image_tokenization_method": "glm_vision_encoder",
      "adaptive_resolution": true,
      "aspect_ratio_support": "arbitrary",
      "native_function_calling": true,
      "interleaved_generation": true,
      "document_understanding": true,
      "frontend_replication": true,
      "tool_calling_format": "glm_xml",
      "tool_calling_parser": "glm46v",
      "output_wrappers": {
        "start": "<|begin_of_box|>",
        "end": "<|end_of_box|>"
      },
      "thinking_control": "/nothink",
      "thinking_tags": ["<think>", "</think>"],
      "notes": "GLM-4.6V-Flash lightweight model (9B params) optimized for local deployment and low-latency applications. Maintains native multimodal function calling using XML format: <tool_call>function_name\\n<arg_key>key</arg_key>\\n<arg_value>value</arg_value>\\n</tool_call>. Generates reasoning in 'reasoning_content' field or <think></think> tags. Ideal for edge and resource-constrained environments while preserving core GLM-4.6V capabilities. Thinking can be disabled with '/nothink' suffix. See: https://github.com/zai-org/GLM-V",
      "source": "HuggingFace zai-org/GLM-4.6V-Flash and GLM-V GitHub",
      "canonical_name": "glm-4.6v-flash",
      "aliases": [
        "zai-org/GLM-4.6V-Flash",
        "zai-org/GLM-4.6V-Flash-FP8",
        "glm-4.6v-9b",
        "glm-4.6v-flash-fp8"
      ],
      "max_tokens": 128000,
      "release_date": "2025-05-07",
      "arxiv": "2507.01006",
      "license": "MIT"
    },
    "glm-4.7": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "architecture": "mixture_of_experts",
      "total_parameters": "358B",
      "thinking_paradigm": "multi_mode",
      "thinking_modes": ["interleaved_thinking", "preserved_thinking", "turn_level_thinking"],
      "native_function_calling": true,
      "agentic_coding": true,
      "terminal_tasks": true,
      "web_browsing": true,
      "tool_calling_parser": "glm47",
      "reasoning_parser": "glm45",
      "notes": "GLM-4.7 latest MoE model (358B params) with enhanced coding, reasoning, and agentic capabilities. Achieves 73.8% on SWE-bench Verified, 66.7% on SWE-bench Multilingual, and 41% on Terminal Bench 2.0. Supports advanced thinking modes: Interleaved (think before actions), Preserved (cross-turn consistency), and Turn-level. Excels at tool using (-Bench: 87.4%), web browsing (BrowseComp: 52%), and complex reasoning (HLE w/ Tools: 42.8%, AIME 2025: 95.7%). 128K context window with 32K output capacity. Optimized for modern coding environments including Claude Code, Kilo Code, Cline, Roo Code.",
      "source": "HuggingFace zai-org/GLM-4.7 and GLM technical blog",
      "canonical_name": "glm-4.7",
      "aliases": [
        "zai-org/GLM-4.7",
        "zai-org/GLM-4.7-FP8",
        "glm-4.7-fp8",
        "glm-4.7-358b"
      ],
      "max_tokens": 128000,
      "release_date": "2025-06",
      "arxiv": "2508.06471",
      "license": "MIT",
      "benchmarks": {
        "SWE-bench Verified": 73.8,
        "SWE-bench Multilingual": 66.7,
        "Terminal Bench Hard": 33.3,
        "Terminal Bench 2.0": 41.0,
        "HLE": 24.8,
        "HLE (w/ Tools)": 42.8,
        "AIME 2025": 95.7,
        "HMMT Feb. 2025": 97.1,
        "HMMT Nov. 2025": 93.5,
        "IMOAnswerBench": 82.0,
        "LiveCodeBench-v6": 84.9,
        "BrowseComp": 52.0,
        "BrowseComp (w/ Context Manage)": 67.5,
        "BrowseComp-Zh": 66.6,
        "-Bench": 87.4,
        "MMLU-Pro": 84.3,
        "GPQA-Diamond": 85.7
      },
      "inference_parameters": {
        "temperature": 1.0,
        "top_p": 0.95,
        "enable_thinking": true,
        "clear_thinking": false
      }
    },
    "devstral-small-2-24b": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "total_parameters": "24B",
      "architecture": "mistral3",
      "tensor_type": "FP8",
      "agentic_coding": true,
      "tool_calling_parser": "mistral",
      "notes": "Devstral Small 2 agentic LLM for software engineering (24B params, FP8). Excels at tool use, codebase exploration, multi-file edits. 256K context. Strong performance on SWE-bench Verified (68.0%), Terminal-Bench (22.5%), and SWE-bench Multilingual (55.7%). Improved generalization over predecessors. Uses Mistral 3 architecture with rope-scaling and Scalable-Softmax.",
      "source": "Mistral AI Devstral 2 docs and HuggingFace",
      "canonical_name": "devstral-small-2-24b",
      "aliases": [
        "mistralai/Devstral-Small-2-24B-Instruct-2512",
        "devstral-small-2",
        "devstral-small-2-24b-instruct"
      ],
      "max_tokens": 262144,
      "release_date": "2025-12",
      "license": "Apache-2.0"
    },
    "devstral-2-123b": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "total_parameters": "123B",
      "architecture": "ministral3",
      "tensor_type": "FP8",
      "agentic_coding": true,
      "tool_calling_parser": "mistral",
      "notes": "Devstral 2 flagship agentic LLM for software engineering (123B params, FP8). Excels at tool use, codebase exploration, multi-file edits. 256K context. Top-tier performance on SWE-bench Verified (72.2%), Terminal-Bench (32.6%), and SWE-bench Multilingual (61.3%). Improved generalization and better performance than predecessors.",
      "source": "Mistral AI Devstral 2 docs and HuggingFace",
      "canonical_name": "devstral-2-123b",
      "aliases": [
        "mistralai/Devstral-2-123B-Instruct-2512",
        "devstral-2",
        "devstral-2-123b-instruct"
      ],
      "max_tokens": 262144,
      "release_date": "2025-12",
      "license": "Modified-MIT"
    },
    "qwen3-235b-a22b-2507": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": false,
      "architecture": "mixture_of_experts",
      "total_parameters": "235B",
      "active_parameters": "22B",
      "experts": 128,
      "experts_activated": 8,
      "tensor_type": "BF16",
      "notes": "Qwen3-235B-A22B-Instruct-2507 non-thinking mode (235B total/22B active, 128 experts/8 activated). Significant improvements in instruction following, reasoning, math, science, coding, tool usage. Enhanced 256K long-context understanding, extendable to 1M tokens with DCA+MInference. Substantial gains in multilingual knowledge. Better alignment for subjective tasks.",
      "source": "Qwen HuggingFace and Qwen3 technical report arXiv:2505.09388",
      "canonical_name": "qwen3-235b-a22b-2507",
      "aliases": [
        "Qwen/Qwen3-235B-A22B-Instruct-2507",
        "qwen3-235b-a22b-instruct-2507",
        "qwen3-235b-2507"
      ],
      "max_tokens": 262144,
      "release_date": "2025-07",
      "arxiv": "2505.09388",
      "license": "Apache-2.0"
    },
    "qwen3-235b-a22b-2507-fp8": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": false,
      "architecture": "mixture_of_experts",
      "total_parameters": "235B",
      "active_parameters": "22B",
      "experts": 128,
      "experts_activated": 8,
      "tensor_type": "FP8",
      "quantization_method": "fine_grained_fp8_block128",
      "notes": "FP8-quantized version of Qwen3-235B-A22B-Instruct-2507. Fine-grained fp8 quantization with block size 128. Same capabilities as BF16 version but more efficient inference. Note: transformers has issues with fine-grained fp8 in distributed inference (may need CUDA_LAUNCH_BLOCKING=1).",
      "source": "Qwen HuggingFace and Qwen3 technical report arXiv:2505.09388",
      "canonical_name": "qwen3-235b-a22b-2507-fp8",
      "aliases": [
        "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8",
        "qwen3-235b-a22b-instruct-2507-fp8",
        "qwen3-235b-2507-fp8"
      ],
      "max_tokens": 262144,
      "release_date": "2025-07",
      "arxiv": "2505.09388",
      "license": "Apache-2.0"
    },
    "granite-4.0-h-tiny": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "architecture": "granitemoehybrid",
      "total_parameters": "7B",
      "active_parameters": "1B",
      "experts": 64,
      "experts_activated": 6,
      "expert_hidden_size": 512,
      "shared_expert_hidden_size": 1024,
      "attention_layers": 4,
      "mamba2_layers": 36,
      "mamba2_state_size": 128,
      "embedding_size": 1536,
      "tensor_type": "BF16",
      "notes": "Granite 4.0-H-Tiny hybrid MoE model (7B total/1B active, 64 experts/6 activated). Combines 4 attention layers with 36 Mamba2 layers. 128K context. Enhanced tool-calling and instruction following. Strong performance on coding, math, and alignment tasks. Optimized for enterprise applications with improved IF capabilities.",
      "source": "IBM Granite 4.0 HuggingFace and technical report",
      "canonical_name": "granite-4.0-h-tiny",
      "aliases": [
        "ibm-granite/granite-4.0-h-tiny",
        "granite-4.0-h-tiny-moe",
        "granite-h-tiny"
      ],
      "max_tokens": 131072,
      "release_date": "2025-10-02",
      "license": "Apache-2.0"
    },
    "gpt-oss-20b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "thinking_budget": true,
      "architecture": "mixture_of_experts",
      "total_parameters": "21B",
      "active_parameters": "3.6B",
      "tensor_type": "BF16+U8",
      "quantization_method": "MXFP4",
      "response_format": "harmony",
      "reasoning_levels": ["low", "medium", "high"],
      "agentic_capabilities": true,
      "function_calling": true,
      "web_browsing": true,
      "python_execution": true,
      "fine_tunable": true,
      "notes": "OpenAI GPT-OSS 20B open-weight model (21B total/3.6B active). Designed for lower latency, local, and specialized use cases. MXFP4 quantization enables running within 16GB memory. Configurable reasoning effort (low/medium/high). Full chain-of-thought access. Requires harmony response format. Apache 2.0 license for commercial use.",
      "source": "OpenAI GPT-OSS HuggingFace and arXiv:2508.10925",
      "canonical_name": "gpt-oss-20b",
      "aliases": [
        "openai/gpt-oss-20b",
        "gpt-oss:20b"
      ],
      "max_tokens": 128000,
      "release_date": "2025-08",
      "arxiv": "2508.10925",
      "license": "Apache-2.0"
    },
    "gpt-oss-120b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "thinking_budget": true,
      "architecture": "mixture_of_experts",
      "total_parameters": "117B",
      "active_parameters": "5.1B",
      "tensor_type": "BF16+U8",
      "quantization_method": "MXFP4",
      "response_format": "harmony",
      "reasoning_levels": ["low", "medium", "high"],
      "agentic_capabilities": true,
      "function_calling": true,
      "web_browsing": true,
      "python_execution": true,
      "fine_tunable": true,
      "gpu_memory_required": "80GB",
      "notes": "OpenAI GPT-OSS 120B open-weight model (117B total/5.1B active). Production-ready for general purpose, high reasoning use cases. MXFP4 quantization enables single 80GB GPU deployment (H100/MI300X). Configurable reasoning effort (low/medium/high). Full chain-of-thought access. Requires harmony response format. Apache 2.0 license for commercial use.",
      "source": "OpenAI GPT-OSS HuggingFace and arXiv:2508.10925",
      "canonical_name": "gpt-oss-120b",
      "aliases": [
        "openai/gpt-oss-120b",
        "gpt-oss:120b"
      ],
      "max_tokens": 128000,
      "release_date": "2025-08",
      "arxiv": "2508.10925",
      "license": "Apache-2.0"
    },
    "qwen3-vl-2b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": true,
      "image_resolutions": [
        "64x64 to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "image_tokenization_method": "patch_based_adaptive",
      "adaptive_resolution": true,
      "min_resolution": 64,
      "max_resolution": 4096,
      "vision_encoder": "ViT-based",
      "visual_agent": true,
      "visual_coding": true,
      "spatial_perception": true,
      "document_understanding": true,
      "ocr_languages": 32,
      "architecture_updates": ["Interleaved-MRoPE", "DeepStack", "Text-Timestamp-Alignment"],
      "notes": "Qwen3-VL 2B dense model with 256K context. Visual agent for GUI operation, visual coding (Draw.io/HTML/CSS/JS), advanced spatial perception with 2D/3D grounding. Enhanced OCR (32 languages), long video understanding with second-level indexing. Text understanding on par with pure LLMs.",
      "source": "Qwen HuggingFace and Qwen3 technical report arXiv:2505.09388",
      "canonical_name": "qwen3-vl-2b",
      "aliases": [
        "Qwen/Qwen3-VL-2B-Instruct",
        "qwen3-vl-2b-instruct"
      ],
      "max_tokens": 262144,
      "release_date": "2025-05",
      "arxiv": "2505.09388",
      "license": "Apache-2.0"
    },
    "qwen3-vl-4b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": true,
      "image_resolutions": [
        "64x64 to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "image_tokenization_method": "patch_based_adaptive",
      "adaptive_resolution": true,
      "min_resolution": 64,
      "max_resolution": 4096,
      "vision_encoder": "ViT-based",
      "visual_agent": true,
      "visual_coding": true,
      "spatial_perception": true,
      "document_understanding": true,
      "ocr_languages": 32,
      "total_parameters": "4.83B",
      "architecture_updates": ["Interleaved-MRoPE", "DeepStack", "Text-Timestamp-Alignment"],
      "notes": "Qwen3-VL 4B dense model (4.83B params) with 256K context, optimized for LMStudio. Visual agent for GUI operation, visual coding (Draw.io/HTML/CSS/JS), advanced spatial perception with 2D/3D grounding. Enhanced OCR (32 languages), long video understanding. FP8 checkpoints available.",
      "source": "Qwen HuggingFace and Qwen3 technical report arXiv:2505.09388",
      "canonical_name": "qwen3-vl-4b",
      "aliases": [
        "Qwen/Qwen3-VL-4B-Instruct",
        "qwen3-vl-4b-instruct",
        "qwen/qwen3-vl-4b"
      ],
      "max_tokens": 262144,
      "release_date": "2025-05",
      "arxiv": "2505.09388",
      "license": "Apache-2.0"
    },
    "qwen3-vl-8b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": true,
      "image_resolutions": [
        "64x64 to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "image_tokenization_method": "patch_based_adaptive",
      "adaptive_resolution": true,
      "min_resolution": 64,
      "max_resolution": 4096,
      "vision_encoder": "ViT-based",
      "visual_agent": true,
      "visual_coding": true,
      "spatial_perception": true,
      "document_understanding": true,
      "ocr_languages": 32,
      "total_parameters": "8.77B",
      "architecture_updates": ["Interleaved-MRoPE", "DeepStack", "Text-Timestamp-Alignment"],
      "notes": "Qwen3-VL 8B dense model (8.77B params) with 256K context, optimized for LMStudio. Most powerful vision-language model in Qwen series. Visual agent for GUI operation, visual coding, advanced spatial perception with 2D/3D grounding. Enhanced OCR (32 languages), long video understanding with second-level indexing. FP8 checkpoints available.",
      "source": "Qwen HuggingFace and Qwen3 technical report arXiv:2505.09388",
      "canonical_name": "qwen3-vl-8b",
      "aliases": [
        "Qwen/Qwen3-VL-8B-Instruct",
        "qwen3-vl-8b-instruct",
        "qwen/qwen3-vl-8b"
      ],
      "max_tokens": 262144,
      "release_date": "2025-05",
      "arxiv": "2505.09388",
      "license": "Apache-2.0"
    },
    "qwen3-vl-30b-a3b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": true,
      "image_resolutions": [
        "64x64 to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "image_tokenization_method": "patch_based_adaptive",
      "adaptive_resolution": true,
      "min_resolution": 64,
      "max_resolution": 4096,
      "vision_encoder": "ViT-based",
      "visual_agent": true,
      "visual_coding": true,
      "spatial_perception": true,
      "document_understanding": true,
      "ocr_languages": 32,
      "architecture": "mixture_of_experts",
      "total_parameters": "30.5B",
      "active_parameters": "3.3B",
      "architecture_updates": ["Interleaved-MRoPE", "DeepStack", "Text-Timestamp-Alignment"],
      "notes": "Qwen3-VL 30B MoE model (30.5B total/3.3B active), best performing vision model in the series. 128K context. Visual agent for GUI operation, visual coding (Draw.io/HTML/CSS/JS), advanced spatial perception with 2D/3D grounding. Enhanced OCR (32 languages), long video understanding with second-level indexing. Text understanding on par with pure LLMs.",
      "source": "Qwen HuggingFace and Qwen3 technical report arXiv:2505.09388",
      "canonical_name": "qwen3-vl-30b-a3b",
      "aliases": [
        "Qwen/Qwen3-VL-30B-A3B-Instruct",
        "qwen3-vl-30b-a3b-instruct",
        "qwen/qwen3-vl-30b"
      ],
      "max_tokens": 131072,
      "release_date": "2025-05",
      "arxiv": "2505.09388",
      "license": "Apache-2.0"
    },
    "qwen3-vl-235b-a22b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": true,
      "image_resolutions": [
        "64x64 to 4096x4096"
      ],
      "max_image_resolution": "4096x4096",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "image_tokenization_method": "patch_based_adaptive",
      "adaptive_resolution": true,
      "min_resolution": 64,
      "max_resolution": 4096,
      "vision_encoder": "ViT-based",
      "visual_agent": true,
      "visual_coding": true,
      "spatial_perception": true,
      "document_understanding": true,
      "ocr_languages": 32,
      "architecture": "mixture_of_experts",
      "total_parameters": "235B",
      "active_parameters": "22B",
      "experts": 128,
      "experts_activated": 8,
      "architecture_updates": ["Interleaved-MRoPE", "DeepStack", "Text-Timestamp-Alignment"],
      "notes": "Qwen3-VL 235B MoE model (235B total/22B active, 128 experts/8 activated), flagship vision model. 256K context expandable to 1M. Visual agent for GUI operation, visual coding (Draw.io/HTML/CSS/JS), advanced spatial perception with 2D/3D grounding. Enhanced OCR (32 languages), long video understanding with second-level indexing. Text understanding on par with pure LLMs. Superior visual perception and reasoning.",
      "source": "Qwen HuggingFace and Qwen3 technical report arXiv:2505.09388",
      "canonical_name": "qwen3-vl-235b-a22b",
      "aliases": [
        "Qwen/Qwen3-VL-235B-A22B-Instruct",
        "qwen3-vl-235b-a22b-instruct"
      ],
      "max_tokens": 262144,
      "release_date": "2025-05",
      "arxiv": "2505.09388",
      "license": "Apache-2.0"
    },
    "nemotron-3-nano-30b-a3b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "video_support": false,
      "thinking_support": true,
      "thinking_budget": false,
      "architecture": "nemotron_hybrid_moe",
      "total_parameters": "30B",
      "active_parameters": "3.5B",
      "experts": 128,
      "experts_activated": 6,
      "shared_experts": 1,
      "attention_layers": 6,
      "mamba2_layers": 23,
      "tensor_type": "BF16",
      "reasoning_paradigm": "unified_reasoning_response",
      "reasoning_configurable": true,
      "agentic_capabilities": true,
      "function_calling": true,
      "tool_calling_format": "json",
      "languages": ["English", "German", "Spanish", "French", "Italian", "Japanese"],
      "notes": "NVIDIA Nemotron-3-Nano hybrid MoE model (30B total/3.5B active, 128 experts/6 activated + 1 shared). Combines 23 Mamba-2 layers with 6 Attention layers. Unified model for reasoning and non-reasoning tasks with configurable reasoning mode. Generates reasoning trace before final response. 256K context extendable to 1M with YaRN. Strong performance on AIME25 (99.2% with tools), SWE-Bench (38.8%), MiniF2F (50.0% pass@1). Native tool calling via chatml-function-calling format. Commercial use ready.",
      "source": "NVIDIA Nemotron HuggingFace and technical report",
      "canonical_name": "nemotron-3-nano-30b-a3b",
      "aliases": [
        "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16",
        "NVIDIA-Nemotron-3-Nano-30B-A3B-BF16",
        "nvidia/nemotron-3-nano",
        "nemotron-3-nano",
        "nemotron-nano-30b"
      ],
      "max_tokens": 262144,
      "release_date": "2025-12",
      "license": "nvidia-open-model-license",
      "benchmarks": {
        "MMLU-Pro": 78.3,
        "AIME25 (no tools)": 89.1,
        "AIME25 (with tools)": 99.2,
        "GPQA (no tools)": 73.0,
        "GPQA (with tools)": 75.0,
        "LiveCodeBench v6": 68.3,
        "SciCode (subtask)": 33.3,
        "HLE (no tools)": 10.6,
        "HLE (with tools)": 15.5,
        "MiniF2F pass@1": 50.0,
        "MiniF2F pass@32": 79.9,
        "Terminal Bench (hard subset)": 8.5,
        "SWE-Bench (OpenHands)": 38.8,
        "TauBench V2 (Average)": 49.0,
        "BFCL v4": 53.8,
        "IFBench (prompt)": 71.5,
        "Scale AI Multi Challenge": 38.5,
        "Arena-Hard-V2 (Hard Prompt)": 72.1,
        "Arena-Hard-V2 (Average)": 67.7
      }
    }
  },
  "tool_support_levels": {
    "native": "Full native API support with structured tool calling",
    "prompted": "Works with careful prompt engineering",
    "none": "No tool support capabilities"
  },
  "structured_output_levels": {
    "native": "Native JSON mode or structured output support",
    "prompted": "Can output JSON with prompting",
    "none": "Poor structured output capability"
  },
  "capability_types": {
    "thinking_support": "Chain-of-thought reasoning capabilities",
    "thinking_budget": "Configurable reasoning length control",
    "video_support": "Video processing capabilities",
    "fim_support": "Fill-in-the-middle code completion"
  },
  "vlm_tokenization_research": {
    "openai_gpt4v_formula": {
      "step1": "Resize to fit 2048x2048 (preserve aspect ratio)",
      "step2": "Resize shortest side to 768px",
      "step3": "Calculate tiles: ceil(width/512) * ceil(height/512)",
      "step4": "Total tokens = 85 + (tiles * 170)",
      "low_detail": "Fixed 85 tokens regardless of size",
      "research_source": "OpenAI official documentation + Image Tokenization research"
    },
    "anthropic_claude_formula": {
      "formula": "min((width * height) / 750, 1600)",
      "pixel_divisor": 750,
      "token_cap": 1600,
      "resize_trigger": "max(width, height) > 1568",
      "warning_threshold": "min(width, height) < 200",
      "research_source": "Anthropic Claude documentation + research analysis"
    },
    "google_gemini_formula": {
      "small_image": "width <= 384 AND height <= 384 \u2192 258 tokens",
      "large_image": "ceil(width/768) * ceil(height/768) * 258 tokens",
      "small_threshold": 384,
      "tile_size": 768,
      "tokens_per_tile": 258,
      "research_source": "Google Gemini documentation + research analysis"
    },
    "qwen_vl_adaptive_formula": {
      "formula": "min(ceil(width/patch_size) * ceil(height/patch_size), max_tokens)",
      "adaptive_resize": "Resize to fit within [min_res, max_res] range",
      "patch_sizes": {
        "qwen2.5": 14,
        "qwen3": 16
      },
      "research_source": "Qwen-VL technical documentation + research"
    },
    "vision_transformer_baseline": {
      "standard_patch_size": 16,
      "formula": "tokens = (height * width) / (patch_size^2)",
      "typical_range": [
        196,
        2048
      ],
      "research_source": "Vision Transformer foundational paper"
    }
  },
  "generic_vision_model": {
    "max_output_tokens": 4096,
    "tool_support": "prompted",
    "structured_output": "prompted",
    "parallel_tools": false,
    "vision_support": true,
    "audio_support": false,
    "video_support": false,
    "image_resolutions": [
      "up to 1024x1024"
    ],
    "max_image_resolution": "1024x1024",
    "image_patch_size": 16,
    "max_image_tokens": 2048,
    "image_tokenization_method": "patch_based",
    "adaptive_resolution": false,
    "vision_encoder": "generic_vit",
    "notes": "Generic vision model fallback with conservative parameters that should work with most VLMs",
    "source": "AbstractCore generic fallback",
    "canonical_name": "generic_vision_model",
    "aliases": [],
    "max_tokens": 32768
  },
  "default_capabilities": {
    "max_output_tokens": 4096,
    "tool_support": "none",
    "structured_output": "none",
    "parallel_tools": false,
    "vision_support": false,
    "audio_support": false,
    "thinking_support": false,
    "thinking_budget": false,
    "video_support": false,
    "fim_support": false,
    "max_tokens": 16384
  }
}
