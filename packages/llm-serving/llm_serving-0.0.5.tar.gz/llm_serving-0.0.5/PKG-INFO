Metadata-Version: 2.4
Name: llm_serving
Version: 0.0.5
Summary: A package to perform LLM inference in Solenium/Unergy HQ
Home-page: https://gitlab.com/unergy-dev/ai/llm_serving_module
Author: Unergy/Solenium
Author-email: pablo@solenium.co
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Requires-Dist: httpx
Requires-Dist: langchain
Requires-Dist: langchain-core
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: summary

# llm_serving_module

Esta liberÃ­a contiene distintos mÃ³dulos para hacer inferencia de modelos de Ollama, HuggingFace y Google por medio de los servicios desplegados en el servidor local de Unergy/Solenium. El usuario debe tener configurada la conexiÃ³n por medio de VPN al servidor para hacer uso de estos.

Entre algunas de las funcionalidades estÃ¡n:

1. Servicio de inferencia a Google por medio de rotaciÃ³n automÃ¡tica: Con la funciÃ³n 'google_inference_request', puede hacer inferencia dando el prompt de sistema, de usuario y, opcionalmente, el formato en el que quiere recibir el resultado (un objeto que herede de BaseModel de Pydantic). 

Este servicio usa como backend un sistema de colas por medio de Redis y Celery; tambiÃ©n almacena la informaciÃ³n de tokens en una base de datos PostgreSQL.
