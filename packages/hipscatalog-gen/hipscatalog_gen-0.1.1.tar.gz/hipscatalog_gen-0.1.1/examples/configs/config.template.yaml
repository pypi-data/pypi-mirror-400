# =============================================================================
# HiPS Catalog Pipeline — config.yaml (Template)
# =============================================================================
# Configuration file for the modular package `hipscatalog_gen`
#
# The pipeline:
#   - Reads large catalogs (Parquet / CSV / TSV / HATS)
#   - Distributes data with Dask (for Parquet/CSV/TSV) or LSDB (for HATS)
#   - Validates and normalizes RA/DEC (RA in degrees, DEC in degrees)
#   - Computes densmaps (per-depth HEALPix counts) and a MOC
#   - Selects sources per depth (score- or magnitude-based)
#   - Writes HiPS-compliant tiles (Norder*/Dir*/Npix*.tsv + Allsky.tsv)
#   - Produces MOC, density maps, metadata, and a process.log
#
# HATS catalogs:
#   - Opened via lsdb.open_catalog()
#   - Columns restricted to RA/DEC, score expression dependencies, mag column (if mag_global)
#     plus any columns listed in columns.keep.
# =============================================================================


# -----------------------------------------------------------------------------
# Input catalog(s)
# -----------------------------------------------------------------------------
input:
  # List of input paths (files or directories). Supports wildcards.
#   - Parquet/CSV/TSV → file paths or glob patterns.
#   - HATS           → root catalog folder (where collection.properties lives).
  paths:
    - "/path/to/your/catalog/*.parquet"

  # Input format:
  #   "parquet" → Dask DataFrame via dd.read_parquet
  #   "csv"     → dd.read_csv (ASCII with delimiter auto-detected or ascii_format)
  #   "tsv"     → dd.read_csv with tab separator
  #   "hats"    → LSDB catalog opened via lsdb.open_catalog
  format: parquet

  # For CSV/TSV only:
  #   header: true  → first row has column names
  #   header: false → no header; columns are accessed by 1-based indices
  #                   in the "columns" section (ra/dec).
  # Ignored for Parquet and HATS.
  # header: true

  # Optional hint for ASCII formats ("CSV" or "TSV").
  # Usually not needed; the separator is inferred from the extension.
  # Only used when format is "csv" or "tsv".
  # ascii_format: CSV


# -----------------------------------------------------------------------------
# Column mapping
# -----------------------------------------------------------------------------
columns:
  # RA/DEC column names or 1-based indices (if input.header=false for CSV/TSV).
  # Units must be degrees:
  #   - RA accepted in [0, 360) or [-180, 180] (internally normalized to [0, 360)).
  #   - DEC must be in [-90, 90].
  # Values outside these ranges cause a hard error.
  ra: ra
  dec: dec

  # Optional explicit list of columns to keep in output tiles.
  #
  # Behavior is unified across all formats (Parquet, CSV, TSV, HATS):
  #
  #   - If omitted or null:
  #       → Keep all input columns. RA/DEC, score expression deps, and mag/flux
  #         (if selection_mode = "mag_global") are ordered first.
  #
  #   - If empty list []:
  #       → Keep only the essential subset (RA, DEC, score deps, mag/flux if mag_global).
  #
  #   - If provided (non-empty list):
  #       → Keep the essential subset plus all columns listed here (filtered by availability).
  #
  # This restriction minimizes I/O and memory footprint while preserving all
  # necessary columns for selection and output.
  keep:
    - ra
    - dec
    - mag_g
    - mag_r


# -----------------------------------------------------------------------------
# Algorithmic controls
# -----------------------------------------------------------------------------
algorithm:

  # ==============================
  # Common settings (all modes)
  # ==============================
  selection_mode: "mag_global"  # "mag_global" | "score_global" | "score_density_hybrid"
  level_limit: 11             # HiPS max depth (4..11)
  # moc_order: 11             # Optional MOC order (defaults to level_limit)
  selection_defaults:
    hist_nbins: 2048
    adaptive_range: "complete"
    order_desc: false
    keep_invalid_values: false     # map NaN/Inf to sentinel only when adaptive_range=complete
    tie_column: null               # optional tie-breaker; falls back to RA/DEC when null

  # ==============================
  # mag_global block
  # (precedence: mag_column OR flux_column)
  # ==============================
  mag_global:
    mag_column: "mag_r"             # Magnitude column (mutually exclusive with flux_column)
    flux_column: null               # Flux column to derive magnitudes (requires mag_offset)
    mag_offset: null                # Required if flux_column is set
    mag_min: null                   # Optional lower mag bound (auto if null)
    mag_max: null                   # Optional upper mag bound (auto if null)
    adaptive_range: "complete"      # "complete" | "hist_peak"
    hist_nbins: 2048                # Histogram bins for mag_global
    #k_1: null                      # Optional targets per active tile for depths 1..3 (alias for n_*)
    #k_2: null
    #k_3: null
    n_1: null                       # Optional targets for depths 1..3
    n_2: null
    n_3: null
    order_desc: false               # false → lower magnitudes/scores are better; true → higher are better (inherits algorithm.order_desc if omitted)
    keep_invalid_values: false      # if true (complete mode only), map NaN/Inf to a sentinel and include them in the last slice
    tie_column: null                # optional tie-breaker; falls back to RA/DEC when null

  # ==============================
  # score_global block
  # ==============================
  score_global:
    #score_column: null             # Column or expression to evaluate globally
    #score_min: null                # Optional lower bound (auto if null)
    #score_max: null                # Optional upper bound (auto if null)
    adaptive_range: "complete"      # "complete" | "hist_peak"
    hist_nbins: 2048                # Histogram bins for score_global
    #k_1: null                      # Optional targets per active tile for depths 1..3 (alias for n_*)
    #k_2: null
    #k_3: null
    #n_1: null                      # Optional targets for depths 1..3
    #n_2: null
    #n_3: null
    order_desc: false               # false → lower scores are better; true → higher scores are better (inherits algorithm.order_desc if omitted)
    keep_invalid_values: false      # if true (complete mode only), map NaN/Inf to sentinel and include in last slice
    tie_column: null                # optional tie-breaker; falls back to RA/DEC when null

  # ==============================
  # score_density_hybrid block
  # ==============================
  score_density_hybrid:
    #score_column: null             # Column or expression to evaluate globally
    #score_min: null                # Optional lower bound (auto if null)
    #score_max: null                # Optional upper bound (auto if null)
    adaptive_range: "complete"      # "complete" | "hist_peak"
    hist_nbins: 2048                # Histogram bins for score_density_hybrid
    #k_1: null                      # Optional targets per active tile for depths 1..3 (alias for n_*)
    #k_2: null
    #k_3: null
    density_bias_n1: 1.0            # Density bias for Norder 1 (0.0 uniform, 1.0 follows densmap)
    density_bias_n2: 1.0            # Density bias for Norder 2
    density_bias_n3: 1.0            # Density bias for Norder 3
    #n_1: null                      # Optional fixed totals for depths 1..3 (must be in order)
    #n_2: null
    #n_3: null
    order_desc: false               # false → lower scores are better; true → higher scores are better (inherits algorithm.order_desc if omitted)
    keep_invalid_values: false      # if true (complete mode only), map NaN/Inf to sentinel and include in last slice
    tie_column: null                # optional tie-breaker; falls back to RA/DEC when null

# -----------------------------------------------------------------------------
# Cluster settings (Dask or SLURM)
# -----------------------------------------------------------------------------
cluster:
  # Execution mode for the Dask cluster:
  #   "local" → LocalCluster on the current machine.
  #   "slurm" → SLURMCluster via dask-jobqueue (requires dask-jobqueue installed).
  mode: local

  # Dask worker configuration (for both local and slurm modes):
  #   - n_workers          → number of worker processes.
  #   - threads_per_worker → threads per worker.
  #   - memory_per_worker  → memory limit per worker, as a string ("8GB", "32GB", etc.).
  n_workers: 3
  threads_per_worker: 2
  memory_per_worker: "4GB"

  # Memory vs throughput policy (applies to all modes):
  #   low_memory_mode=true  → persist_ddfs=False, avoid_computes_wherever_possible=True
  #   low_memory_mode=false → persist_ddfs=True, avoid_computes_wherever_possible=False
  low_memory_mode: true

  # SLURM options (used only if mode: "slurm").
  # For mode: "local", this block is ignored.
  slurm:
    # SLURM queue/partition and accounting.
    queue: "cpu"
    account: "your-account"

    # Additional SLURM directives passed to dask-jobqueue.
    job_extra_directives:
      - "--partition=cpu"
      - "--time=02:00:00"
    # Optional extras, e.g.:
    # - "--constraint=..."
    # - "--qos=..."
    # - "--mem=0"   # use all node memory if allowed

    # Dask performance reports (diagnostics_mode):
    #   "per_step" → one HTML report per labeled step (default).
    #   "global"   → a single report for the whole pipeline (dask_global.html).
    #   "off"      → diagnostics disabled (no performance_report).
    diagnostics_mode: global


# -----------------------------------------------------------------------------
# Output settings
# -----------------------------------------------------------------------------
output:
  # Output root directory for the HiPS hierarchy.
  # The pipeline will create subdirectories:
  #   - Norder*/Dir*/Npix*.tsv  (tiles)
  #   - Norder*/Allsky.tsv      (summary per depth, when applicable)
  #   - densmap_o*.fits         (densmaps)
  #   - Moc.fits / Moc.json     (MOC)
  #   - properties / metadata.xml
  #   - process.log             (full textual log)
  out_dir: "/path/to/output/hips_catalog"

  # If false and out_dir already exists, the run fails.
  # If true, delete its contents before writing the new HiPS catalog.
  overwrite: false

  # Catalog / HiPS name:
  #   - Used in metadata (properties, metadata.xml).
  #   - Used for some internal titles and logs.
  cat_name: "My_HiPS_Catalog"

  # Default sky position for visualization (RA DEC in degrees, space-separated).
  # Used only in metadata for visualization tools (e.g. Aladin) and does not
  # affect the selection logic.
  target: "0 0"

  # Unique IVOID identifier for this HiPS (recommended for HiPS 1.4).
  # Example: "ivo://YOUR_INSTITUTE/YourCatalogName"
  # Not strictly required by the pipeline, but some HiPS tools expect it.
  creator_did: "ivo://PRIVATE_USER/My_HiPS_Catalog"

  # Short human-readable title for the HiPS (required by HiPS 1.4 metadata).
  # Only affects descriptive metadata; no impact on the selection.
  obs_title: "My HiPS Catalog for testing"
