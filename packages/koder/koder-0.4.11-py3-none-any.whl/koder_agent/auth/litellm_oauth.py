"""Custom LiteLLM provider for OAuth-based authentication.

Implements custom LLM handlers for OAuth providers (google, claude, chatgpt, antigravity)
that use Bearer token authentication instead of API keys.

Reference: https://docs.litellm.ai/docs/providers/custom_llm_server
"""

import asyncio
import json
import logging
import time
import uuid
from typing import Any, AsyncIterator, Dict, Iterator, List, Optional

import aiohttp
import litellm
from litellm import CustomLLM
from litellm.types.utils import GenericStreamingChunk, ModelResponse, Usage

from koder_agent.auth.client_integration import get_oauth_token
from koder_agent.auth.codex_prompts import CODEX_PROMPTS
from koder_agent.auth.constants import (
    ANTHROPIC_BETA_HEADERS,
)

logger = logging.getLogger(__name__)

# Gemini Code Assist endpoint for OAuth-based access
GEMINI_CODE_ASSIST_ENDPOINT = "https://cloudcode-pa.googleapis.com"

# Antigravity endpoint fallbacks (daily → autopush → prod)
ANTIGRAVITY_ENDPOINT_DAILY = "https://daily-cloudcode-pa.sandbox.googleapis.com"
ANTIGRAVITY_ENDPOINT_AUTOPUSH = "https://autopush-cloudcode-pa.sandbox.googleapis.com"
ANTIGRAVITY_ENDPOINT_PROD = "https://cloudcode-pa.googleapis.com"
ANTIGRAVITY_ENDPOINT_FALLBACKS = [
    ANTIGRAVITY_ENDPOINT_DAILY,
    ANTIGRAVITY_ENDPOINT_AUTOPUSH,
    ANTIGRAVITY_ENDPOINT_PROD,
]

# Default project ID for Antigravity (hardcoded fallback)
# Used when loadManagedProject doesn't return a project
ANTIGRAVITY_DEFAULT_PROJECT_ID = "rising-fact-p41fc"

# Code Assist headers (from reference implementation)
CODE_ASSIST_HEADERS = {
    "User-Agent": "cloud-code-gemini-vscode/2.0.0 GPN:cloud-code-gemini;",
    "X-Goog-Api-Client": "cloud-code-gemini-vscode/2.0.0",
    "Client-Metadata": json.dumps(
        {
            "clientName": "koder",
            "clientVersion": "1.0.0",
            "os": "unknown",
            "ideType": "vscode",
            "ideVersion": "1.96.0",
            "clientId": str(uuid.uuid4()),
        }
    ),
}

# Antigravity-specific headers (from reference implementation)
ANTIGRAVITY_HEADERS = {
    "User-Agent": "antigravity/1.11.5 windows/amd64",
    "X-Goog-Api-Client": "google-cloud-sdk vscode_cloudshelleditor/0.1",
    "Client-Metadata": '{"ideType":"IDE_UNSPECIFIED","platform":"PLATFORM_UNSPECIFIED","pluginType":"GEMINI"}',
}

# Claude thinking tier budgets (from reference: antigravity-auth model-resolver.ts)
# The tier suffix (-low/-medium/-high) is client-side config, not part of API model name
CLAUDE_THINKING_BUDGETS = {
    "low": 8192,
    "medium": 16384,
    "high": 32768,
}

# Claude thinking models require maxOutputTokens >= budget + response
CLAUDE_THINKING_MAX_OUTPUT_TOKENS = 65536

# OpenAI API endpoint (standard API key authentication)
OPENAI_API_BASE = "https://api.openai.com/v1"

# ChatGPT Codex Backend endpoint (OAuth authentication)
# ChatGPT Plus/Pro subscription uses a different backend from standard OpenAI API
CHATGPT_CODEX_BASE = "https://chatgpt.com/backend-api"

# JWT claim path for ChatGPT account ID
JWT_CLAIM_PATH = "https://api.openai.com/auth"

# ChatGPT Codex headers
CHATGPT_CODEX_HEADERS = {
    "OpenAI-Beta": "responses=experimental",
    "originator": "codex_cli_rs",
}

# Codex instructions bundle (generated by scripts/generate_codex_prompts.py)

# Default Codex instructions (simplified version)
# Reference: https://github.com/openai/codex/blob/main/codex-rs/core/gpt_5_codex_prompt.md
DEFAULT_CODEX_INSTRUCTIONS = """You are a coding assistant. You help users write, debug, and understand code.

Core principles:
1. Be concise but thorough in explanations
2. Write clean, readable code following best practices
3. Explain your reasoning when making decisions
4. Ask clarifying questions when requirements are ambiguous

When writing code:
- Follow the language's style conventions
- Include error handling where appropriate
- Add comments for complex logic
- Consider edge cases

When debugging:
- Analyze the problem systematically
- Explain the root cause
- Provide a clear fix
"""

# Anthropic API endpoint
ANTHROPIC_API_BASE = "https://api.anthropic.com/v1"

# Claude OAuth requires this exact system prompt prefix as the first content block
CLAUDE_CODE_SYSTEM_PREFIX = "You are Claude Code, Anthropic's official CLI for Claude."


class GoogleOAuthLLM(CustomLLM):
    """Custom LiteLLM handler for Google/Gemini OAuth access.

    Uses Gemini Code Assist endpoint with Bearer token authentication
    instead of API key authentication.
    """

    def __init__(self):
        """Initialize Google OAuth LLM handler."""
        super().__init__()
        self.provider_id = "google"
        self._managed_project_id: Optional[str] = None

    def _get_access_token(self) -> Optional[str]:
        """Get OAuth access token for Google."""
        tokens = get_oauth_token(self.provider_id)
        if tokens:
            return tokens.access_token
        return None

    async def _ensure_project_context(self, access_token: str) -> str:
        """Ensure we have a valid managed project ID.

        Calls loadCodeAssist API to get the user's managed project.
        If not onboarded, calls onboardUser to create one.

        Args:
            access_token: Valid OAuth access token

        Returns:
            Managed project ID
        """
        # Return cached project ID if available
        if self._managed_project_id:
            return self._managed_project_id

        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            **CODE_ASSIST_HEADERS,
        }

        # Try to load existing project
        async with aiohttp.ClientSession() as session:
            # First, try loadCodeAssist
            load_body = {
                "metadata": {
                    "ideType": "IDE_UNSPECIFIED",
                    "platform": "PLATFORM_UNSPECIFIED",
                    "pluginType": "GEMINI",
                }
            }

            async with session.post(
                f"{GEMINI_CODE_ASSIST_ENDPOINT}/v1internal:loadCodeAssist",
                json=load_body,
                headers=headers,
            ) as response:
                if response.ok:
                    data = await response.json()
                    if data.get("cloudaicompanionProject"):
                        self._managed_project_id = data["cloudaicompanionProject"]
                        return self._managed_project_id

            # If no project, try to onboard with FREE tier
            onboard_body = {
                "tierId": "FREE",
                "metadata": {
                    "ideType": "IDE_UNSPECIFIED",
                    "platform": "PLATFORM_UNSPECIFIED",
                    "pluginType": "GEMINI",
                },
            }

            for _ in range(5):  # Retry up to 5 times
                async with session.post(
                    f"{GEMINI_CODE_ASSIST_ENDPOINT}/v1internal:onboardUser",
                    json=onboard_body,
                    headers=headers,
                ) as response:
                    if response.ok:
                        data = await response.json()
                        if data.get("done"):
                            project_id = (
                                data.get("response", {})
                                .get("cloudaicompanionProject", {})
                                .get("id")
                            )
                            if project_id:
                                self._managed_project_id = project_id
                                return self._managed_project_id

                # Wait before retrying
                await asyncio.sleep(3)

        raise ValueError(
            "Failed to get managed project for Google OAuth. "
            "Please ensure your Google account has access to Gemini."
        )

    def _build_request_body(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        project_id: str,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """Build Gemini Code Assist request body.

        Wraps the request in the format expected by cloudcode-pa.googleapis.com.

        Args:
            model: Model name
            messages: OpenAI-style messages
            project_id: Managed project ID from loadCodeAssist
            **kwargs: Additional generation config
        """
        from koder_agent.auth.tool_utils import (
            convert_tool_calls_to_gemini_parts,
            convert_tool_message_to_gemini_part,
            convert_tools_to_gemini_format,
        )

        # Convert OpenAI-style messages to Gemini format
        contents = []
        system_instruction = None

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            tool_calls = msg.get("tool_calls")

            if role == "system":
                # Extract system instruction
                if isinstance(content, str):
                    system_instruction = {"parts": [{"text": content}]}
                continue

            # Handle tool result messages
            if role == "tool":
                # Tool results go as user role with functionResponse part
                tool_part = convert_tool_message_to_gemini_part(msg)
                contents.append({"role": "user", "parts": [tool_part]})
                continue

            # Map roles: user->user, assistant->model
            gemini_role = "model" if role == "assistant" else "user"
            parts: List[Dict[str, Any]] = []

            # Convert content
            if isinstance(content, str) and content:
                parts.append({"text": content})
            elif isinstance(content, list):
                # Handle multimodal content
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") == "text":
                            parts.append({"text": item.get("text", "")})
                        elif item.get("type") == "image_url":
                            # Handle image content
                            image_url = item.get("image_url", {})
                            if isinstance(image_url, dict):
                                url = image_url.get("url", "")
                                if url.startswith("data:"):
                                    # Base64 image
                                    parts.append({"inline_data": {"data": url}})

            # Handle tool calls from assistant messages
            if tool_calls and isinstance(tool_calls, list):
                tool_call_parts = convert_tool_calls_to_gemini_parts(tool_calls)
                parts.extend(tool_call_parts)

            if parts:
                contents.append({"role": gemini_role, "parts": parts})

        # Build request payload
        request_payload: Dict[str, Any] = {"contents": contents}

        if system_instruction:
            request_payload["systemInstruction"] = system_instruction

        # Add generation config
        generation_config: Dict[str, Any] = {}
        if "temperature" in kwargs:
            generation_config["temperature"] = kwargs["temperature"]
        if "max_tokens" in kwargs:
            generation_config["maxOutputTokens"] = kwargs["max_tokens"]
        if "top_p" in kwargs:
            generation_config["topP"] = kwargs["top_p"]

        if generation_config:
            request_payload["generationConfig"] = generation_config

        # Convert and add tools if provided
        tools = kwargs.get("tools")
        if tools:
            gemini_tools = convert_tools_to_gemini_format(tools)
            if gemini_tools:
                request_payload["tools"] = gemini_tools

        # Extract model name without provider prefix
        model_name = model
        if "/" in model:
            model_name = model.split("/")[-1]

        return {
            "project": project_id,
            "model": model_name,
            "request": request_payload,
        }

    def _parse_response(self, response_data: Dict[str, Any]) -> ModelResponse:
        """Parse Gemini response into LiteLLM ModelResponse format."""
        from koder_agent.auth.tool_utils import extract_tool_calls_from_gemini_response

        # Handle wrapped response format
        if "response" in response_data:
            response_data = response_data["response"]

        # Extract content and tool calls from candidates
        content = ""
        tool_calls: Optional[List[Dict[str, Any]]] = None

        if "candidates" in response_data:
            candidates = response_data["candidates"]
            if candidates and len(candidates) > 0:
                candidate = candidates[0]
                if "content" in candidate:
                    parts = candidate["content"].get("parts", [])
                    for part in parts:
                        if "text" in part:
                            content += part["text"]

        # Extract tool calls
        tool_calls = extract_tool_calls_from_gemini_response(response_data)

        # Extract usage
        usage_data = response_data.get("usageMetadata", {})
        usage = Usage(
            prompt_tokens=usage_data.get("promptTokenCount", 0),
            completion_tokens=usage_data.get("candidatesTokenCount", 0),
            total_tokens=usage_data.get("totalTokenCount", 0),
        )

        # Build message based on whether we have tool calls
        if tool_calls:
            message: Dict[str, Any] = {
                "role": "assistant",
                "content": content if content else None,
                "tool_calls": tool_calls,
            }
            finish_reason = "tool_calls"
        else:
            message = {
                "role": "assistant",
                "content": content,
            }
            finish_reason = "stop"

        return ModelResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            created=int(time.time()),
            model=response_data.get("model", "gemini"),
            choices=[
                {
                    "index": 0,
                    "message": message,
                    "finish_reason": finish_reason,
                }
            ],
            usage=usage,
        )

    def completion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Synchronous completion (not implemented - use async)."""
        raise NotImplementedError("Use acompletion for Google OAuth")

    async def acompletion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Async completion using Gemini Code Assist endpoint."""
        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for Google. "
                "Run 'koder auth login google' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        merged_kwargs = {**kwargs, **optional_params}

        # Get managed project ID
        project_id = await self._ensure_project_context(access_token)

        # Determine if streaming
        stream = merged_kwargs.get("stream", False)

        # Build request with managed project ID
        body = self._build_request_body(model, messages, project_id, **merged_kwargs)

        # Determine action
        action = "streamGenerateContent" if stream else "generateContent"
        url = f"{GEMINI_CODE_ASSIST_ENDPOINT}/v1internal:{action}"
        if stream:
            url += "?alt=sse"

        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            **CODE_ASSIST_HEADERS,
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                url,
                json=body,
                headers=headers,
            ) as response:
                if not response.ok:
                    error_text = await response.text()
                    raise ValueError(f"Google OAuth API error ({response.status}): {error_text}")

                if stream:
                    # Handle streaming response
                    return await self._handle_streaming_response(response, model)
                else:
                    response_data = await response.json()
                    return self._parse_response(response_data)

    async def _handle_streaming_response(
        self,
        response: aiohttp.ClientResponse,
        model: str,
    ) -> ModelResponse:
        """Handle SSE streaming response and return complete ModelResponse."""
        from koder_agent.auth.tool_utils import extract_tool_calls_from_gemini_response

        full_content = ""
        all_tool_calls: List[Dict[str, Any]] = []
        last_data: Optional[Dict[str, Any]] = None

        async for line in response.content:
            line_str = line.decode("utf-8").strip()
            if not line_str or not line_str.startswith("data:"):
                continue

            try:
                data = json.loads(line_str[5:].strip())
                last_data = data
                # Handle wrapped response
                if "response" in data:
                    data = data["response"]

                if "candidates" in data:
                    for candidate in data["candidates"]:
                        if "content" in candidate:
                            for part in candidate["content"].get("parts", []):
                                if "text" in part:
                                    full_content += part["text"]
            except json.JSONDecodeError:
                continue

        # Extract tool calls from final response
        if last_data:
            tool_calls = extract_tool_calls_from_gemini_response(last_data)
            if tool_calls:
                all_tool_calls = tool_calls

        # Build message based on whether we have tool calls
        if all_tool_calls:
            message: Dict[str, Any] = {
                "role": "assistant",
                "content": full_content if full_content else None,
                "tool_calls": all_tool_calls,
            }
            finish_reason = "tool_calls"
        else:
            message = {
                "role": "assistant",
                "content": full_content,
            }
            finish_reason = "stop"

        return ModelResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            created=int(time.time()),
            model=model,
            choices=[
                {
                    "index": 0,
                    "message": message,
                    "finish_reason": finish_reason,
                }
            ],
        )

    def streaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> Iterator[GenericStreamingChunk]:
        """Synchronous streaming (not implemented - use async)."""
        raise NotImplementedError("Use astreaming for Google OAuth")

    async def astreaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> AsyncIterator[GenericStreamingChunk]:
        """Async streaming using Gemini Code Assist endpoint."""
        from koder_agent.auth.tool_utils import extract_tool_calls_from_gemini_response

        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for Google. "
                "Run 'koder auth login google' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        merged_kwargs = {**kwargs, **optional_params}

        # Get managed project ID (required for API access)
        project_id = await self._ensure_project_context(access_token)

        # Build request with streaming enabled and managed project ID
        merged_kwargs["stream"] = True
        body = self._build_request_body(model, messages, project_id, **merged_kwargs)

        url = f"{GEMINI_CODE_ASSIST_ENDPOINT}/v1internal:streamGenerateContent?alt=sse"

        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
            **CODE_ASSIST_HEADERS,
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                url,
                json=body,
                headers=headers,
            ) as response:
                if not response.ok:
                    error_text = await response.text()
                    raise ValueError(f"Google OAuth API error ({response.status}): {error_text}")

                chunk_index = 0
                has_tool_calls = False
                last_data: Optional[Dict[str, Any]] = None

                async for line in response.content:
                    line_str = line.decode("utf-8").strip()
                    if not line_str or not line_str.startswith("data:"):
                        continue

                    try:
                        data = json.loads(line_str[5:].strip())
                        last_data = data
                        # Handle wrapped response
                        if "response" in data:
                            data = data["response"]

                        if "candidates" in data:
                            for candidate in data["candidates"]:
                                if "content" in candidate:
                                    for part in candidate["content"].get("parts", []):
                                        if "text" in part:
                                            yield GenericStreamingChunk(
                                                text=part["text"],
                                                is_finished=False,
                                                finish_reason=None,
                                                usage=None,
                                                index=chunk_index,
                                            )
                                            chunk_index += 1
                                        elif "functionCall" in part:
                                            # Yield tool call
                                            has_tool_calls = True
                                            func_call = part["functionCall"]
                                            name = func_call.get("name", "")
                                            args = func_call.get("args", {})
                                            call_id = func_call.get(
                                                "id", f"call_{uuid.uuid4().hex[:8]}"
                                            )
                                            yield GenericStreamingChunk(
                                                text="",
                                                is_finished=False,
                                                finish_reason=None,
                                                usage=None,
                                                index=chunk_index,
                                                tool_use={
                                                    "id": call_id,
                                                    "type": "function",
                                                    "function": {
                                                        "name": name,
                                                        "arguments": (
                                                            json.dumps(args)
                                                            if isinstance(args, dict)
                                                            else str(args)
                                                        ),
                                                    },
                                                },
                                            )
                                            chunk_index += 1
                    except json.JSONDecodeError:
                        continue

                # Check for tool calls in final data (in case not detected in stream)
                if last_data and not has_tool_calls:
                    tool_calls = extract_tool_calls_from_gemini_response(last_data)
                    if tool_calls:
                        has_tool_calls = True
                        # Yield any tool calls from final data
                        for tc in tool_calls:
                            yield GenericStreamingChunk(
                                text="",
                                is_finished=False,
                                finish_reason=None,
                                usage=None,
                                index=chunk_index,
                                tool_use={
                                    "id": tc.get("id", f"call_{uuid.uuid4().hex[:8]}"),
                                    "type": "function",
                                    "function": tc.get("function", {}),
                                },
                            )
                            chunk_index += 1

                # Final chunk with appropriate finish reason
                finish_reason = "tool_calls" if has_tool_calls else "stop"
                yield GenericStreamingChunk(
                    text="",
                    is_finished=True,
                    finish_reason=finish_reason,
                    usage=None,
                    index=chunk_index,
                )


class ClaudeOAuthLLM(CustomLLM):
    """Custom LiteLLM handler for Claude/Anthropic OAuth access.

    Uses Anthropic API with Bearer token authentication. Claude Pro/Max OAuth
    requires the system field as an array of content blocks with the Claude Code
    identification string as the first block.
    """

    def __init__(self):
        """Initialize Claude OAuth LLM handler."""
        super().__init__()
        self.provider_id = "claude"

    def _get_access_token(self) -> Optional[str]:
        """Get OAuth access token for Claude."""
        tokens = get_oauth_token(self.provider_id)
        if tokens:
            return tokens.access_token
        return None

    async def _build_request_body(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """Build Anthropic API request body."""
        from .tool_utils import clean_json_schema, ensure_tool_has_properties

        # Extract system messages and build anthropic messages
        system_parts = []
        anthropic_messages: List[Dict[str, Any]] = []
        pending_tool_results: List[Dict[str, Any]] = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                system_content = content if isinstance(content, str) else str(content)
                system_parts.append(system_content)
                continue

            # Handle tool result messages (role="tool")
            if role == "tool":
                tool_use_id = msg.get("tool_call_id", msg.get("tool_use_id", ""))
                result_content = msg.get("content", "")
                pending_tool_results.append(
                    {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": (
                            result_content
                            if isinstance(result_content, str)
                            else json.dumps(result_content)
                        ),
                    }
                )
                continue

            # Flush pending tool results as a user message before the next non-tool message
            if pending_tool_results:
                anthropic_messages.append(
                    {
                        "role": "user",
                        "content": pending_tool_results,
                    }
                )
                pending_tool_results = []

            # Handle assistant messages with tool_calls
            tool_calls = msg.get("tool_calls")
            if role == "assistant" and tool_calls:
                content_blocks: List[Dict[str, Any]] = []
                # Add text content if present
                if isinstance(content, str) and content:
                    content_blocks.append({"type": "text", "text": content})
                # Convert tool_calls to tool_use blocks
                for call in tool_calls:
                    func = call.get("function", {})
                    name = func.get("name", call.get("name", ""))
                    args_str = func.get("arguments", call.get("arguments", "{}"))
                    call_id = call.get("id", f"toolu_{uuid.uuid4().hex[:24]}")
                    # Parse arguments
                    if isinstance(args_str, str):
                        try:
                            args = json.loads(args_str)
                        except json.JSONDecodeError:
                            args = {}
                    else:
                        args = args_str or {}
                    content_blocks.append(
                        {
                            "type": "tool_use",
                            "id": call_id,
                            "name": name,
                            "input": args,
                        }
                    )
                anthropic_messages.append(
                    {
                        "role": "assistant",
                        "content": content_blocks,
                    }
                )
            else:
                anthropic_messages.append(
                    {
                        "role": role,
                        "content": content,
                    }
                )

        # Flush any remaining tool results
        if pending_tool_results:
            anthropic_messages.append(
                {
                    "role": "user",
                    "content": pending_tool_results,
                }
            )

        # Extract model name without provider prefix
        model_name = model
        if "/" in model:
            model_name = model.split("/")[-1]

        # Build system prompt as array of content blocks (required for Claude OAuth)
        system_blocks: List[Dict[str, str]] = [{"type": "text", "text": CLAUDE_CODE_SYSTEM_PREFIX}]

        for part in system_parts:
            if part.strip() == CLAUDE_CODE_SYSTEM_PREFIX.strip():
                continue
            if part.startswith(CLAUDE_CODE_SYSTEM_PREFIX):
                remainder = part[len(CLAUDE_CODE_SYSTEM_PREFIX) :].strip()
                if remainder:
                    system_blocks.append({"type": "text", "text": remainder})
            else:
                system_blocks.append({"type": "text", "text": part})

        body: Dict[str, Any] = {
            "model": model_name,
            "messages": anthropic_messages,
            "max_tokens": kwargs.get("max_tokens", 4096),
            "system": system_blocks,
        }

        if "temperature" in kwargs:
            body["temperature"] = kwargs["temperature"]
        if "top_p" in kwargs:
            body["top_p"] = kwargs["top_p"]

        # Convert and add tools
        tools = kwargs.get("tools")
        if tools:
            anthropic_tools = []
            for tool in tools:
                if not isinstance(tool, dict):
                    continue
                # Handle OpenAI function tool format
                if tool.get("type") == "function" and isinstance(tool.get("function"), dict):
                    func = tool["function"]
                    name = func.get("name", "")
                    description = func.get("description", "")
                    params = func.get("parameters", {"type": "object", "properties": {}})
                elif "name" in tool:
                    name = tool.get("name", "")
                    description = tool.get("description", "")
                    params = (
                        tool.get("parameters")
                        or tool.get("input_schema")
                        or {"type": "object", "properties": {}}
                    )
                else:
                    continue
                # Clean schema for Claude
                cleaned_params = clean_json_schema(params)
                cleaned_params = ensure_tool_has_properties(cleaned_params)
                anthropic_tools.append(
                    {
                        "name": name,
                        "description": description,
                        "input_schema": cleaned_params,
                    }
                )
            if anthropic_tools:
                body["tools"] = anthropic_tools

        return body

    def _parse_response(self, response_data: Dict[str, Any], model: str) -> ModelResponse:
        """Parse Anthropic response into LiteLLM ModelResponse format."""
        # Extract content and tool_use blocks
        content = ""
        tool_calls: List[Dict[str, Any]] = []
        content_blocks = response_data.get("content", [])

        for block in content_blocks:
            block_type = block.get("type", "")
            if block_type == "text":
                content += block.get("text", "")
            elif block_type == "tool_use":
                # Convert tool_use to OpenAI tool_calls format
                tool_calls.append(
                    {
                        "id": block.get("id", f"call_{uuid.uuid4().hex[:8]}"),
                        "type": "function",
                        "function": {
                            "name": block.get("name", ""),
                            "arguments": json.dumps(block.get("input", {})),
                        },
                    }
                )

        # Extract usage
        usage_data = response_data.get("usage", {})
        usage = Usage(
            prompt_tokens=usage_data.get("input_tokens", 0),
            completion_tokens=usage_data.get("output_tokens", 0),
            total_tokens=(usage_data.get("input_tokens", 0) + usage_data.get("output_tokens", 0)),
        )

        # Build message with optional tool_calls
        message: Dict[str, Any] = {
            "role": "assistant",
            "content": content if content else None,
        }
        if tool_calls:
            message["tool_calls"] = tool_calls

        # Map stop_reason to finish_reason
        stop_reason = response_data.get("stop_reason", "end_turn")
        if stop_reason == "tool_use" or tool_calls:
            finish_reason = "tool_calls"
        elif stop_reason == "end_turn":
            finish_reason = "stop"
        else:
            finish_reason = stop_reason

        return ModelResponse(
            id=response_data.get("id", f"msg_{uuid.uuid4().hex}"),
            created=int(time.time()),
            model=response_data.get("model", model),
            choices=[
                {
                    "index": 0,
                    "message": message,
                    "finish_reason": finish_reason,
                }
            ],
            usage=usage,
        )

    def completion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Synchronous completion (not implemented - use async)."""
        raise NotImplementedError("Use acompletion for Claude OAuth")

    async def acompletion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Async completion using Anthropic API with OAuth."""
        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for Claude. "
                "Run 'koder auth login claude' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        merged_kwargs = {**kwargs, **optional_params}

        body = await self._build_request_body(model, messages, **merged_kwargs)

        headers = {
            "authorization": f"Bearer {access_token}",
            "content-type": "application/json",
            "anthropic-version": "2023-06-01",
            "anthropic-beta": ",".join(ANTHROPIC_BETA_HEADERS),
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{ANTHROPIC_API_BASE}/messages",
                json=body,
                headers=headers,
            ) as response:
                if not response.ok:
                    error_text = await response.text()
                    raise ValueError(f"Claude OAuth API error ({response.status}): {error_text}")

                response_data = await response.json()
                return self._parse_response(response_data, model)

    def streaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> Iterator[GenericStreamingChunk]:
        """Synchronous streaming (not implemented - use async)."""
        raise NotImplementedError("Use astreaming for Claude OAuth")

    async def astreaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> AsyncIterator[GenericStreamingChunk]:
        """Async streaming using Anthropic API with OAuth."""
        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for Claude. "
                "Run 'koder auth login claude' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        merged_kwargs = {**kwargs, **optional_params}

        body = await self._build_request_body(model, messages, **merged_kwargs)
        body["stream"] = True

        headers = {
            "authorization": f"Bearer {access_token}",
            "content-type": "application/json",
            "accept": "text/event-stream",
            "anthropic-version": "2023-06-01",
            "anthropic-beta": ",".join(ANTHROPIC_BETA_HEADERS),
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{ANTHROPIC_API_BASE}/messages",
                json=body,
                headers=headers,
            ) as response:
                if not response.ok:
                    error_text = await response.text()
                    raise ValueError(f"Claude OAuth API error ({response.status}): {error_text}")

                chunk_index = 0
                has_tool_calls = False
                # Track current tool_use block being streamed
                current_tool_use: Optional[Dict[str, Any]] = None
                current_tool_json = ""

                async for line in response.content:
                    line_str = line.decode("utf-8").strip()
                    if not line_str or not line_str.startswith("data:"):
                        continue

                    if line_str == "data: [DONE]":
                        break

                    try:
                        data = json.loads(line_str[5:].strip())
                        event_type = data.get("type", "")

                        if event_type == "content_block_start":
                            # Check if this is a tool_use block
                            content_block = data.get("content_block", {})
                            if content_block.get("type") == "tool_use":
                                has_tool_calls = True
                                current_tool_use = {
                                    "id": content_block.get("id", f"call_{uuid.uuid4().hex[:8]}"),
                                    "name": content_block.get("name", ""),
                                }
                                current_tool_json = ""

                        elif event_type == "content_block_delta":
                            delta = data.get("delta", {})
                            delta_type = delta.get("type", "")

                            if delta_type == "text_delta":
                                yield GenericStreamingChunk(
                                    text=delta.get("text", ""),
                                    is_finished=False,
                                    finish_reason=None,
                                    usage=None,
                                    index=chunk_index,
                                )
                                chunk_index += 1
                            elif delta_type == "input_json_delta" and current_tool_use:
                                # Accumulate JSON for tool_use input
                                current_tool_json += delta.get("partial_json", "")

                        elif event_type == "content_block_stop":
                            # Finalize tool_use block if we were building one
                            if current_tool_use:
                                yield GenericStreamingChunk(
                                    text="",
                                    is_finished=False,
                                    finish_reason=None,
                                    usage=None,
                                    index=chunk_index,
                                    tool_use={
                                        "id": current_tool_use["id"],
                                        "type": "function",
                                        "function": {
                                            "name": current_tool_use["name"],
                                            "arguments": current_tool_json or "{}",
                                        },
                                    },
                                )
                                chunk_index += 1
                                current_tool_use = None
                                current_tool_json = ""

                        elif event_type == "message_stop":
                            break
                    except json.JSONDecodeError:
                        continue

                # Final chunk with appropriate finish_reason
                finish_reason = "tool_calls" if has_tool_calls else "stop"
                yield GenericStreamingChunk(
                    text="",
                    is_finished=True,
                    finish_reason=finish_reason,
                    usage=None,
                    index=chunk_index,
                )


class ChatGPTOAuthLLM(CustomLLM):
    """Custom LiteLLM handler for ChatGPT/OpenAI OAuth access.

    Uses ChatGPT Codex Backend API (chatgpt.com/backend-api) with Bearer token
    authentication and stateless mode (store=false). Uses the Codex "Responses"
    format with input array instead of messages array.
    """

    def __init__(self):
        """Initialize ChatGPT OAuth LLM handler."""
        super().__init__()
        self.provider_id = "chatgpt"
        self._account_id: Optional[str] = None

    def _get_access_token(self) -> Optional[str]:
        """Get OAuth access token for ChatGPT."""
        tokens = get_oauth_token(self.provider_id)
        if tokens:
            return tokens.access_token
        return None

    def _extract_account_id_from_jwt(self, token: str) -> Optional[str]:
        """Extract ChatGPT account ID from JWT access token.

        The account ID is stored at ["https://api.openai.com/auth"]["chatgpt_account_id"]
        in the JWT payload.

        Args:
            token: JWT access token

        Returns:
            Account ID string or None
        """
        # Return cached account ID
        if self._account_id:
            return self._account_id

        try:
            parts = token.split(".")
            if len(parts) != 3:
                return None

            # Decode payload (second part)
            payload = parts[1]
            # Add padding if needed
            padding = 4 - len(payload) % 4
            if padding != 4:
                payload += "=" * padding

            import base64

            decoded = base64.urlsafe_b64decode(payload)
            data = json.loads(decoded)

            # Extract account ID from JWT claim path
            auth_claim = data.get(JWT_CLAIM_PATH, {})
            account_id = auth_claim.get("chatgpt_account_id")
            if account_id:
                self._account_id = account_id
            return account_id
        except Exception:
            return None

    def _normalize_model(self, model: str) -> str:
        """Normalize model name to ChatGPT Codex-supported variants.

        Reference: /tmp/oauth-providers/chatgpt-auth/lib/request/helpers/model-map.ts
        """
        # Strip provider prefix
        model_id = model.split("/")[-1] if "/" in model else model

        # Model normalization based on reference implementation
        model_lower = model_id.lower()

        # GPT-5.2 Codex variants (newest, supports xhigh)
        if "gpt-5.2-codex" in model_lower or "gpt 5.2 codex" in model_lower:
            return "gpt-5.2-codex"

        # GPT-5.2 general purpose (supports none/low/medium/high/xhigh)
        if "gpt-5.2" in model_lower or "gpt 5.2" in model_lower:
            return "gpt-5.2"

        # GPT-5.1 Codex Max (supports xhigh)
        if "gpt-5.1-codex-max" in model_lower or "codex-max" in model_lower:
            return "gpt-5.1-codex-max"

        # GPT-5.1 Codex Mini
        if (
            "gpt-5.1-codex-mini" in model_lower
            or "codex-mini" in model_lower
            or "codex_mini" in model_lower
        ):
            return "gpt-5.1-codex-mini"

        # Legacy Codex Mini
        if "codex-mini-latest" in model_lower or "gpt-5-codex-mini" in model_lower:
            return "gpt-5.1-codex-mini"

        # GPT-5.1 Codex (standard)
        if "gpt-5.1-codex" in model_lower or "gpt 5.1 codex" in model_lower:
            return "gpt-5.1-codex"

        # GPT-5.1 general purpose
        if "gpt-5.1" in model_lower or "gpt 5.1" in model_lower:
            return "gpt-5.1"

        # Legacy GPT-5 Codex -> map to GPT-5.1 Codex
        if "gpt-5-codex" in model_lower or "gpt 5 codex" in model_lower:
            return "gpt-5.1-codex"

        # Generic codex -> default to GPT-5.1 Codex
        if "codex" in model_lower:
            return "gpt-5.1-codex"

        # Legacy GPT-5 -> map to GPT-5.1
        if "gpt-5" in model_lower or "gpt 5" in model_lower:
            return "gpt-5.1"

        # Return as-is for unknown models
        return model_id

    def _coerce_input_item(self, item: Any) -> Optional[Dict[str, Any]]:
        """Coerce an input item to a dict, if possible."""
        if item is None:
            return None
        if isinstance(item, dict):
            return dict(item)
        if hasattr(item, "model_dump"):
            try:
                return item.model_dump()
            except Exception:
                return None
        if hasattr(item, "dict"):
            try:
                return item.dict()
            except Exception:
                return None
        try:
            return dict(item)
        except Exception:
            return None

    def _sanitize_input_items(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filter unsupported items and strip IDs for stateless Codex mode."""
        sanitized: List[Dict[str, Any]] = []
        for item in items:
            if not isinstance(item, dict):
                continue
            if item.get("type") == "item_reference":
                continue
            cleaned = dict(item)
            cleaned.pop("id", None)
            sanitized.append(cleaned)

        # Handle orphaned function_call_output items (missing matching function_call)
        call_ids = {
            it.get("call_id")
            for it in sanitized
            if it.get("type") == "function_call" and it.get("call_id")
        }
        fixed: List[Dict[str, Any]] = []
        for item in sanitized:
            if item.get("type") == "function_call_output":
                call_id = item.get("call_id")
                if call_id and call_id not in call_ids:
                    tool_name = item.get("name") or "tool"
                    output = item.get("output")
                    if output is None:
                        output_text = ""
                    elif isinstance(output, str):
                        output_text = output
                    else:
                        try:
                            output_text = json.dumps(output)
                        except Exception:
                            output_text = str(output)
                    if len(output_text) > 16000:
                        output_text = output_text[:16000] + "\n...[truncated]"
                    fixed.append(
                        {
                            "type": "message",
                            "role": "assistant",
                            "content": [
                                {
                                    "type": "input_text",
                                    "text": (
                                        f"[Previous {tool_name} result; "
                                        f"call_id={call_id}]: {output_text}"
                                    ),
                                }
                            ],
                        }
                    )
                    continue
            fixed.append(item)

        return fixed

    def _get_model_family(self, normalized_model: str) -> str:
        """Determine Codex prompt family from normalized model name."""
        if "gpt-5.2-codex" in normalized_model or "gpt 5.2 codex" in normalized_model:
            return "gpt-5.2-codex"
        if "codex-max" in normalized_model:
            return "codex-max"
        if "codex" in normalized_model or normalized_model.startswith("codex-"):
            return "codex"
        if "gpt-5.2" in normalized_model:
            return "gpt-5.2"
        return "gpt-5.1"

    async def _get_codex_instructions(self, normalized_model: str) -> str:
        """Load bundled Codex system instructions."""
        model_family = self._get_model_family(normalized_model)
        instructions = CODEX_PROMPTS.get(model_family)
        if instructions:
            return instructions

        logger.error("Missing bundled instructions for %s; using default.", model_family)
        return DEFAULT_CODEX_INSTRUCTIONS

    def _convert_content_to_input(self, content: Any, role: str = "user") -> List[Dict[str, Any]]:
        """Convert message content into Codex input content array.

        Args:
            content: Message content (string, list, or other)
            role: Message role - determines content type (input_text vs output_text)
        """
        # Determine content type based on role
        # Assistant messages use output_text, all others use input_text
        content_type = "output_text" if role == "assistant" else "input_text"

        if content is None:
            return []
        if isinstance(content, str):
            return [{"type": content_type, "text": content}]
        if isinstance(content, list):
            content_array = []
            for item in content:
                if isinstance(item, dict):
                    item_type = item.get("type")
                    if item_type == "text":
                        content_array.append({"type": content_type, "text": item.get("text", "")})
                    elif item_type in ("input_text", "output_text"):
                        # Preserve existing type if it's already correct format
                        content_array.append(item)
                    else:
                        content_array.append(item)
                else:
                    content_array.append({"type": content_type, "text": str(item)})
            return content_array
        return [{"type": content_type, "text": str(content)}]

    def _convert_messages_to_input(
        self,
        messages: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Convert OpenAI-style messages to Codex input format.

        Codex API uses 'input' array with different structure:
        - type: "message"
        - role: "user" | "assistant" | "developer" (instead of "system")
        - content: array of content objects

        Args:
            messages: OpenAI-style messages

        Returns:
            Codex-style input array
        """
        input_items = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content")

            # Convert role names
            if role == "system":
                role = "developer"

            # Tool output messages -> function_call_output items
            if role == "tool":
                call_id = msg.get("tool_call_id") or msg.get("call_id") or msg.get("id")
                output = content
                if output is None:
                    output_text = ""
                elif isinstance(output, str):
                    output_text = output
                else:
                    try:
                        output_text = json.dumps(output)
                    except Exception:
                        output_text = str(output)
                item: Dict[str, Any] = {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": output_text,
                }
                tool_name = msg.get("name") or msg.get("tool_name")
                if tool_name:
                    item["name"] = tool_name
                input_items.append(item)
                continue

            content_array = self._convert_content_to_input(content, role)
            if content_array:
                input_items.append(
                    {
                        "type": "message",
                        "role": role,
                        "content": content_array,
                    }
                )

            # Convert tool calls to function_call items
            tool_calls = msg.get("tool_calls") or []
            if tool_calls:
                for call in tool_calls:
                    if not isinstance(call, dict):
                        continue
                    fn = call.get("function") or {}
                    input_items.append(
                        {
                            "type": "function_call",
                            "call_id": call.get("id") or call.get("call_id") or msg.get("id"),
                            "name": fn.get("name") or call.get("name"),
                            "arguments": fn.get("arguments") or call.get("arguments"),
                        }
                    )

            # Legacy function_call field
            function_call = msg.get("function_call")
            if isinstance(function_call, dict):
                input_items.append(
                    {
                        "type": "function_call",
                        "call_id": msg.get("id") or msg.get("call_id"),
                        "name": function_call.get("name"),
                        "arguments": function_call.get("arguments"),
                    }
                )

        return self._sanitize_input_items(input_items)

    async def _build_request_body(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """Build ChatGPT Codex API request body."""
        model_name = self._normalize_model(model)

        # Use Responses-style input if provided; otherwise convert messages
        raw_input = kwargs.get("input")
        if raw_input is not None:
            input_items: List[Dict[str, Any]] = []
            if isinstance(raw_input, list):
                for item in raw_input:
                    coerced = self._coerce_input_item(item)
                    if coerced is not None:
                        input_items.append(coerced)
            else:
                coerced = self._coerce_input_item(raw_input)
                if coerced is not None:
                    input_items.append(coerced)
        else:
            input_items = self._convert_messages_to_input(messages)

        # Filter unsupported items and strip IDs
        input_items = self._sanitize_input_items(input_items)

        body: Dict[str, Any] = {
            "model": model_name,
            "input": input_items,
            "store": False,  # Required for ChatGPT backend (stateless mode)
            "stream": True,  # Always stream for Codex API
        }

        # Use Codex CLI instructions (required by ChatGPT backend)
        body["instructions"] = await self._get_codex_instructions(model_name)

        # Add include for encrypted reasoning content (required for stateless mode)
        include = kwargs.get("include")
        if include is None:
            include = ["reasoning.encrypted_content"]
        else:
            include = list(include)
            if "reasoning.encrypted_content" not in include:
                include.append("reasoning.encrypted_content")
        body["include"] = include

        # Add text verbosity (default: medium, matches Codex CLI)
        text = kwargs.get("text")
        if text is None:
            text = {}
        elif hasattr(text, "model_dump"):
            try:
                text = text.model_dump()
            except Exception:
                text = {}
        elif not isinstance(text, dict):
            text = {}
        verbosity = kwargs.get("verbosity") or text.get("verbosity") or "medium"
        text["verbosity"] = verbosity
        body["text"] = text

        # Add reasoning config based on model family
        # Reference: getReasoningConfig() in request-transformer.ts
        reasoning_payload = kwargs.get("reasoning")
        reasoning: Dict[str, Any] = {}
        if reasoning_payload is not None:
            if isinstance(reasoning_payload, dict):
                reasoning = dict(reasoning_payload)
            elif hasattr(reasoning_payload, "model_dump"):
                try:
                    reasoning = reasoning_payload.model_dump()
                except Exception:
                    reasoning = {}
            elif hasattr(reasoning_payload, "dict"):
                try:
                    reasoning = reasoning_payload.dict()
                except Exception:
                    reasoning = {}

        reasoning_effort = reasoning.get("effort") or kwargs.get("reasoning_effort", None)

        normalized_name = model_name.lower()
        is_gpt52_codex = "gpt-5.2-codex" in normalized_name or "gpt 5.2 codex" in normalized_name
        is_gpt52_general = (
            "gpt-5.2" in normalized_name or "gpt 5.2" in normalized_name
        ) and not is_gpt52_codex
        is_codex_max = "codex-max" in normalized_name or "codex max" in normalized_name
        is_codex_mini = (
            "codex-mini" in normalized_name
            or "codex mini" in normalized_name
            or "codex_mini" in normalized_name
            or "codex-mini-latest" in normalized_name
        )
        is_codex = "codex" in normalized_name and not is_codex_mini
        is_lightweight = not is_codex_mini and (
            "nano" in normalized_name or "mini" in normalized_name
        )
        is_gpt51_general = (
            ("gpt-5.1" in normalized_name or "gpt 5.1" in normalized_name)
            and not is_codex
            and not is_codex_max
            and not is_codex_mini
        )

        supports_xhigh = is_gpt52_general or is_gpt52_codex or is_codex_max
        supports_none = is_gpt52_general or is_gpt51_general

        if reasoning_effort is None:
            if is_codex_mini:
                reasoning_effort = "medium"
            elif supports_xhigh:
                reasoning_effort = "high"
            elif is_lightweight:
                reasoning_effort = "minimal"
            else:
                reasoning_effort = "medium"

        if is_codex_mini:
            if reasoning_effort in ["minimal", "low", "none"]:
                reasoning_effort = "medium"
            elif reasoning_effort == "xhigh":
                reasoning_effort = "high"
            elif reasoning_effort not in ["medium", "high"]:
                reasoning_effort = "medium"

        if not supports_xhigh and reasoning_effort == "xhigh":
            reasoning_effort = "high"

        if not supports_none and reasoning_effort == "none":
            reasoning_effort = "low"

        if is_codex and reasoning_effort == "minimal":
            reasoning_effort = "low"

        reasoning["effort"] = reasoning_effort
        reasoning.setdefault("summary", "auto")
        body["reasoning"] = reasoning

        # Convert and add tools if present (Chat Completions → Responses format)
        tools = kwargs.get("tools")
        if tools:
            from koder_agent.auth.tool_utils import convert_tools_to_codex_format

            codex_tools = convert_tools_to_codex_format(tools)
            if codex_tools:
                body["tools"] = codex_tools

        # Pass through supported Responses-style fields when present (excluding tools)
        passthrough_fields = [
            "tool_choice",
            "parallel_tool_calls",
            "metadata",
            "truncation",
            "previous_response_id",
            "prompt",
            "temperature",
            "top_p",
            "prompt_cache_key",
        ]
        for field in passthrough_fields:
            value = kwargs.get(field)
            if value is not None:
                body[field] = value

        # Remove unsupported parameters
        body.pop("max_output_tokens", None)
        body.pop("max_completion_tokens", None)

        return body

    async def _iter_sse_lines(self, content: aiohttp.StreamReader) -> AsyncIterator[str]:
        """Yield SSE lines from an aiohttp stream, handling chunk boundaries."""
        buffer = ""
        async for chunk in content:
            buffer += chunk.decode("utf-8", errors="ignore")
            while "\n" in buffer:
                line, buffer = buffer.split("\n", 1)
                yield line.rstrip("\r")
        if buffer:
            yield buffer.rstrip("\r")

    def _parse_codex_response(
        self,
        response_lines: List[str],
        model: str,
    ) -> ModelResponse:
        """Parse Codex SSE response into LiteLLM ModelResponse format.

        Codex API always returns SSE format, even for non-streaming requests.
        We need to aggregate all chunks and extract the final content and tool calls.
        """
        from koder_agent.auth.tool_utils import extract_tool_calls_from_codex_response

        full_content = ""
        response_id = f"chatcmpl-{uuid.uuid4().hex}"
        usage_data = {}
        tool_calls: List[Dict[str, Any]] = []
        final_response_obj: Optional[Dict[str, Any]] = None

        for line in response_lines:
            if not line.startswith("data:"):
                continue

            data_str = line[5:].strip()
            if data_str == "[DONE]":
                break

            try:
                data = json.loads(data_str)

                # Extract response ID
                if "id" in data:
                    response_id = data["id"]

                # Extract content from output_text events
                event_type = data.get("type", "")
                if event_type == "response.output_text.delta":
                    delta = data.get("delta", "")
                    if delta:
                        full_content += delta
                elif event_type in ("response.completed", "response.done"):
                    # Extract usage and final response object
                    response_obj = data.get("response", {})
                    final_response_obj = response_obj
                    usage_data = response_obj.get("usage", {})
                    # Extract output content and tool calls
                    output = response_obj.get("output", [])
                    for item in output:
                        if item.get("type") == "message":
                            for content_block in item.get("content", []):
                                if content_block.get("type") == "output_text":
                                    full_content = content_block.get("text", full_content)
            except json.JSONDecodeError:
                continue

        # Extract tool calls from final response
        if final_response_obj:
            extracted_calls = extract_tool_calls_from_codex_response(final_response_obj)
            if extracted_calls:
                tool_calls = extracted_calls

        usage = Usage(
            prompt_tokens=usage_data.get("input_tokens", 0),
            completion_tokens=usage_data.get("output_tokens", 0),
            total_tokens=(usage_data.get("input_tokens", 0) + usage_data.get("output_tokens", 0)),
        )

        # Build message based on whether we have tool calls
        if tool_calls:
            message: Dict[str, Any] = {
                "role": "assistant",
                "content": full_content if full_content else None,
                "tool_calls": tool_calls,
            }
            finish_reason = "tool_calls"
        else:
            message = {
                "role": "assistant",
                "content": full_content,
            }
            finish_reason = "stop"

        return ModelResponse(
            id=response_id,
            created=int(time.time()),
            model=model,
            choices=[
                {
                    "index": 0,
                    "message": message,
                    "finish_reason": finish_reason,
                }
            ],
            usage=usage,
        )

    def completion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Synchronous completion (not implemented - use async)."""
        raise NotImplementedError("Use acompletion for ChatGPT OAuth")

    async def acompletion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Async completion using ChatGPT Codex Backend API with OAuth."""
        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for ChatGPT. "
                "Run 'koder auth login chatgpt' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        merged_kwargs = {**kwargs, **optional_params}

        # Extract account ID from JWT
        account_id = self._extract_account_id_from_jwt(access_token)
        if not account_id:
            raise ValueError(
                "Failed to extract ChatGPT account ID from token. "
                "Please re-authenticate with 'koder auth login chatgpt'."
            )

        body = await self._build_request_body(model, messages, **merged_kwargs)

        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
            "chatgpt-account-id": account_id,
            **CHATGPT_CODEX_HEADERS,
        }
        prompt_cache_key = body.get("prompt_cache_key") or merged_kwargs.get("prompt_cache_key")
        if prompt_cache_key:
            headers["conversation_id"] = prompt_cache_key
            headers["session_id"] = prompt_cache_key

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{CHATGPT_CODEX_BASE}/codex/responses",
                json=body,
                headers=headers,
            ) as response:
                if not response.ok:
                    error_text = await response.text()
                    raise ValueError(f"ChatGPT OAuth API error ({response.status}): {error_text}")

                # Collect all SSE lines
                response_lines = []
                async for line in self._iter_sse_lines(response.content):
                    line_str = line.strip()
                    if line_str:
                        response_lines.append(line_str)

                return self._parse_codex_response(response_lines, model)

    def streaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> Iterator[GenericStreamingChunk]:
        """Synchronous streaming (not implemented - use async)."""
        raise NotImplementedError("Use astreaming for ChatGPT OAuth")

    async def astreaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> AsyncIterator[GenericStreamingChunk]:
        """Async streaming using ChatGPT Codex Backend API with OAuth."""
        from koder_agent.auth.tool_utils import extract_tool_calls_from_codex_response

        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for ChatGPT. "
                "Run 'koder auth login chatgpt' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        merged_kwargs = {**kwargs, **optional_params}

        # Extract account ID from JWT
        account_id = self._extract_account_id_from_jwt(access_token)
        if not account_id:
            raise ValueError(
                "Failed to extract ChatGPT account ID from token. "
                "Please re-authenticate with 'koder auth login chatgpt'."
            )

        body = await self._build_request_body(model, messages, **merged_kwargs)

        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
            "chatgpt-account-id": account_id,
            **CHATGPT_CODEX_HEADERS,
        }
        prompt_cache_key = body.get("prompt_cache_key") or merged_kwargs.get("prompt_cache_key")
        if prompt_cache_key:
            headers["conversation_id"] = prompt_cache_key
            headers["session_id"] = prompt_cache_key

        url = f"{CHATGPT_CODEX_BASE}/codex/responses"

        async with aiohttp.ClientSession() as session:
            async with session.post(
                url,
                json=body,
                headers=headers,
            ) as response:
                if not response.ok:
                    error_text = await response.text()
                    raise ValueError(f"ChatGPT OAuth API error ({response.status}): {error_text}")

                chunk_index = 0
                has_tool_calls = False
                final_response_obj: Optional[Dict[str, Any]] = None

                async for line in self._iter_sse_lines(response.content):
                    line_str = line.strip()
                    if not line_str or not line_str.startswith("data:"):
                        continue

                    data_str = line_str[5:].strip()
                    if data_str == "[DONE]":
                        break

                    try:
                        data = json.loads(data_str)
                        event_type = data.get("type", "")

                        # Handle output_text delta events
                        if event_type == "response.output_text.delta":
                            delta = data.get("delta", "")
                            if delta:
                                yield GenericStreamingChunk(
                                    text=delta,
                                    is_finished=False,
                                    finish_reason=None,
                                    usage=None,
                                    index=chunk_index,
                                )
                                chunk_index += 1
                        # Handle function call events
                        elif event_type == "response.function_call_arguments.delta":
                            # Tool call argument streaming - track but don't yield text yet
                            has_tool_calls = True
                        elif event_type in ("response.completed", "response.done"):
                            final_response_obj = data.get("response", {})
                            # Extract and yield tool calls from final response
                            if final_response_obj:
                                extracted = extract_tool_calls_from_codex_response(
                                    final_response_obj
                                )
                                if extracted:
                                    has_tool_calls = True
                                    # Yield each tool call as a separate chunk
                                    for tool_call in extracted:
                                        yield GenericStreamingChunk(
                                            text="",
                                            is_finished=False,
                                            finish_reason=None,
                                            usage=None,
                                            index=chunk_index,
                                            tool_use={
                                                "id": tool_call.get(
                                                    "id", f"call_{uuid.uuid4().hex[:8]}"
                                                ),
                                                "type": "function",
                                                "function": tool_call.get("function", {}),
                                            },
                                        )
                                        chunk_index += 1
                            break
                    except json.JSONDecodeError:
                        continue

                # Final chunk with appropriate finish reason
                finish_reason = "tool_calls" if has_tool_calls else "stop"
                yield GenericStreamingChunk(
                    text="",
                    is_finished=True,
                    finish_reason=finish_reason,
                    usage=None,
                    index=chunk_index,
                )


class AntigravityOAuthLLM(CustomLLM):
    """Custom LiteLLM handler for Antigravity OAuth access.

    Uses Antigravity (Gemini Code Assist) endpoint to access both
    Gemini 3 and Claude models through Google OAuth.

    Supports endpoint fallback: daily → autopush → prod
    Uses managed project from loadCodeAssist or falls back to default.
    """

    def __init__(self):
        """Initialize Antigravity OAuth LLM handler."""
        super().__init__()
        self.provider_id = "antigravity"
        self._managed_project_id: Optional[str] = None

    def _get_access_token(self) -> Optional[str]:
        """Get OAuth access token for Antigravity."""
        tokens = get_oauth_token(self.provider_id)
        if tokens:
            return tokens.access_token
        return None

    def _resolve_model_with_tier(self, model: str) -> tuple[str, Optional[str], Optional[int]]:
        """Resolve model name and extract thinking tier configuration.

        Claude models use tier suffixes (-low/-medium/-high) for thinking budget
        configuration, but the API model name doesn't include these suffixes.

        Example:
        - "claude-sonnet-4-5-thinking-high" → ("claude-sonnet-4-5-thinking", "high", 32768)
        - "claude-sonnet-4-5" → ("claude-sonnet-4-5", None, None)
        - "gemini-3-pro-high" → ("gemini-3-pro-high", None, None)  # Gemini keeps tier

        Args:
            model: Model name (possibly with tier suffix)

        Returns:
            Tuple of (actual_model_name, tier, thinking_budget)
        """
        import re

        # Strip provider prefix
        model_name = model
        if "/" in model:
            model_name = model.split("/")[-1]

        # Remove antigravity- prefix if present
        if model_name.startswith("antigravity-"):
            model_name = model_name[12:]

        # Check for Claude thinking models with tier suffix
        tier_match = re.match(
            r"^(claude-(?:sonnet|opus)-\d+-\d+(?:-thinking)?)-" r"(low|medium|high)$",
            model_name,
            re.IGNORECASE,
        )

        if tier_match:
            base_model = tier_match.group(1)
            tier = tier_match.group(2).lower()
            thinking_budget = CLAUDE_THINKING_BUDGETS.get(tier)
            return (base_model, tier, thinking_budget)

        # No tier suffix - return as-is
        return (model_name, None, None)

    def _is_claude_thinking_model(self, model: str) -> bool:
        """Check if model is a Claude thinking model."""
        lower = model.lower()
        return "claude" in lower and "thinking" in lower

    async def _load_managed_project(
        self, access_token: str, fallback_project_id: Optional[str] = None
    ) -> Optional[str]:
        """Load managed project from Code Assist API.

        Tries prod endpoint first (best for project resolution),
        then falls back to other endpoints.

        Reference: antigravity-auth project.ts loadManagedProject()

        Args:
            access_token: Valid OAuth access token
            fallback_project_id: Optional project ID to include in metadata

        Returns:
            Managed project ID or None
        """
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            "User-Agent": "google-api-nodejs-client/9.15.1",
            "X-Goog-Api-Client": "google-cloud-sdk vscode_cloudshelleditor/0.1",
            "Client-Metadata": ANTIGRAVITY_HEADERS["Client-Metadata"],
        }

        # Build metadata with optional duetProject
        # Reference: buildMetadata() in project.ts
        metadata: Dict[str, str] = {
            "ideType": "IDE_UNSPECIFIED",
            "platform": "PLATFORM_UNSPECIFIED",
            "pluginType": "GEMINI",
        }
        if fallback_project_id:
            metadata["duetProject"] = fallback_project_id

        load_body = {"metadata": metadata}

        # Try prod first for project resolution (best supported)
        load_endpoints = [
            ANTIGRAVITY_ENDPOINT_PROD,
            ANTIGRAVITY_ENDPOINT_DAILY,
            ANTIGRAVITY_ENDPOINT_AUTOPUSH,
        ]

        async with aiohttp.ClientSession() as session:
            for endpoint in load_endpoints:
                try:
                    async with session.post(
                        f"{endpoint}/v1internal:loadCodeAssist",
                        json=load_body,
                        headers=headers,
                    ) as response:
                        if response.ok:
                            data = await response.json()
                            # Extract project ID from response
                            # Reference: extractManagedProjectId() in project.ts
                            project = data.get("cloudaicompanionProject")
                            if isinstance(project, str):
                                return project
                            if isinstance(project, dict) and project.get("id"):
                                return project["id"]
                except Exception:
                    continue

        return None

    async def _ensure_project_context(self, access_token: str) -> str:
        """Ensure we have a valid project ID for Antigravity.

        Reference: antigravity-auth project.ts ensureProjectContext()

        Args:
            access_token: Valid OAuth access token

        Returns:
            Project ID (managed or fallback)
        """
        # Return cached project ID if available
        if self._managed_project_id:
            return self._managed_project_id

        # Try to load managed project with fallback in metadata
        # Reference: loadManagedProject(accessToken, parts.projectId ?? fallbackProjectId)
        project_id = await self._load_managed_project(access_token, ANTIGRAVITY_DEFAULT_PROJECT_ID)
        if project_id:
            self._managed_project_id = project_id
            return project_id

        # Fall back to default project ID
        logger.debug("Using default Antigravity project ID: %s", ANTIGRAVITY_DEFAULT_PROJECT_ID)
        return ANTIGRAVITY_DEFAULT_PROJECT_ID

    def _build_request_body(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        project_id: str,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """Build Antigravity request body.

        Args:
            model: Model name
            messages: OpenAI-style messages
            project_id: Project ID from _ensure_project_context
            **kwargs: Additional generation config
        """
        from .tool_utils import (
            apply_tool_pairing_fixes,
            convert_tool_calls_to_gemini_parts,
            convert_tool_message_to_gemini_part,
            convert_tools_to_claude_format,
            convert_tools_to_gemini_format,
        )

        # Resolve model name and thinking tier
        # Claude thinking models: strip tier suffix, extract budget
        # e.g., "antigravity-claude-opus-4-5-thinking-high" → "claude-opus-4-5-thinking" + budget=32768
        model_name, tier, thinking_budget = self._resolve_model_with_tier(model)
        is_claude = "claude" in model_name.lower()
        is_claude_thinking = self._is_claude_thinking_model(model_name)

        # Convert OpenAI-style messages to Gemini format
        contents: List[Dict[str, Any]] = []
        system_instruction = None
        pending_tool_responses: List[Dict[str, Any]] = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                if isinstance(content, str):
                    system_instruction = {"parts": [{"text": content}]}
                continue

            # Handle tool result messages (role="tool")
            if role == "tool":
                # Collect tool responses to group with the previous model response
                tool_part = convert_tool_message_to_gemini_part(msg)
                pending_tool_responses.append(tool_part)
                continue

            # Flush pending tool responses as a user message before the next message
            if pending_tool_responses:
                contents.append({"role": "user", "parts": pending_tool_responses})
                pending_tool_responses = []

            gemini_role = "model" if role == "assistant" else "user"

            # Check if assistant message has tool_calls
            tool_calls = msg.get("tool_calls")
            if role == "assistant" and tool_calls:
                # Convert tool_calls to functionCall parts
                parts = convert_tool_calls_to_gemini_parts(tool_calls)
                # Add text content if present
                if isinstance(content, str) and content:
                    parts.insert(0, {"text": content})
                contents.append({"role": gemini_role, "parts": parts})
            elif isinstance(content, str):
                contents.append({"role": gemini_role, "parts": [{"text": content}]})
            elif isinstance(content, list):
                parts = []
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") == "text":
                            parts.append({"text": item.get("text", "")})
                if parts:
                    contents.append({"role": gemini_role, "parts": parts})

        # Flush any remaining tool responses
        if pending_tool_responses:
            contents.append({"role": "user", "parts": pending_tool_responses})

        request_payload: Dict[str, Any] = {"contents": contents}

        if system_instruction:
            request_payload["systemInstruction"] = system_instruction

        # Convert and add tools
        tools = kwargs.get("tools")
        if tools:
            if is_claude:
                # Claude models need cleaned schemas and VALIDATED mode
                converted_tools = convert_tools_to_claude_format(tools)
            else:
                # Gemini models use simpler format
                converted_tools = convert_tools_to_gemini_format(tools)
            if converted_tools:
                request_payload["tools"] = converted_tools

        # Add generation config
        generation_config: Dict[str, Any] = {}
        if "temperature" in kwargs:
            generation_config["temperature"] = kwargs["temperature"]
        if "max_tokens" in kwargs:
            generation_config["maxOutputTokens"] = kwargs["max_tokens"]
        if "top_p" in kwargs:
            generation_config["topP"] = kwargs["top_p"]

        # Add thinking configuration for Claude thinking models
        # Reference: antigravity-auth/src/plugin/request.ts (lines 766-773)
        # Claude uses snake_case: include_thoughts, thinking_budget
        if is_claude_thinking:
            thinking_config: Dict[str, Any] = {
                "include_thoughts": True,
            }
            if thinking_budget:
                thinking_config["thinking_budget"] = thinking_budget

            generation_config["thinkingConfig"] = thinking_config

            # Ensure maxOutputTokens is large enough for thinking + response
            current_max = generation_config.get("maxOutputTokens", 0)
            if not current_max or current_max <= (thinking_budget or 0):
                generation_config["maxOutputTokens"] = CLAUDE_THINKING_MAX_OUTPUT_TOKENS

        if generation_config:
            request_payload["generationConfig"] = generation_config

        # Add toolConfig for Claude models (required even without tools)
        # Reference: antigravity-auth/src/plugin/request.ts (lines 726-739)
        if is_claude:
            request_payload["toolConfig"] = {"functionCallingConfig": {"mode": "VALIDATED"}}

        # Apply tool pairing fixes for Claude models (ID management)
        if is_claude:
            request_payload = apply_tool_pairing_fixes(request_payload, is_claude=True)

        return {
            "project": project_id,
            "model": model_name,
            "request": request_payload,
            "userAgent": "antigravity",
            "requestId": f"agent-{uuid.uuid4()}",
        }

    def _parse_response(self, response_data: Dict[str, Any]) -> ModelResponse:
        """Parse Antigravity response into LiteLLM ModelResponse format."""
        from .tool_utils import extract_tool_calls_from_gemini_response

        # Handle wrapped response format
        if "response" in response_data:
            response_data = response_data["response"]

        content = ""
        if "candidates" in response_data:
            candidates = response_data["candidates"]
            if candidates and len(candidates) > 0:
                candidate = candidates[0]
                if "content" in candidate:
                    parts = candidate["content"].get("parts", [])
                    for part in parts:
                        if "text" in part:
                            content += part["text"]

        # Extract tool calls from response
        tool_calls = extract_tool_calls_from_gemini_response(response_data)

        usage_data = response_data.get("usageMetadata", {})
        usage = Usage(
            prompt_tokens=usage_data.get("promptTokenCount", 0),
            completion_tokens=usage_data.get("candidatesTokenCount", 0),
            total_tokens=usage_data.get("totalTokenCount", 0),
        )

        # Build message with optional tool_calls
        message: Dict[str, Any] = {
            "role": "assistant",
            "content": content if content else None,
        }
        if tool_calls:
            message["tool_calls"] = tool_calls

        # Set finish_reason based on response content
        finish_reason = "tool_calls" if tool_calls else "stop"

        return ModelResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            created=int(time.time()),
            model=response_data.get("model", "antigravity"),
            choices=[
                {
                    "index": 0,
                    "message": message,
                    "finish_reason": finish_reason,
                }
            ],
            usage=usage,
        )

    def completion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Synchronous completion (not implemented - use async)."""
        raise NotImplementedError("Use acompletion for Antigravity OAuth")

    async def acompletion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> ModelResponse:
        """Async completion using Antigravity endpoint with fallback."""
        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for Antigravity. "
                "Run 'koder auth login antigravity' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        # Merge optional_params into kwargs for our internal use
        merged_kwargs = {**kwargs, **optional_params}

        # Get project ID (managed or fallback)
        project_id = await self._ensure_project_context(access_token)

        # Check if this is a Claude thinking model (before resolution)
        is_claude_thinking = self._is_claude_thinking_model(model)

        stream = merged_kwargs.get("stream", False)
        body = self._build_request_body(model, messages, project_id, **merged_kwargs)

        action = "streamGenerateContent" if stream else "generateContent"

        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            **ANTIGRAVITY_HEADERS,
        }

        # Add interleaved thinking header for Claude thinking models
        # This enables real-time streaming of thinking tokens
        if is_claude_thinking:
            headers["anthropic-beta"] = "interleaved-thinking-2025-05-14"

        last_error = None
        async with aiohttp.ClientSession() as session:
            # Try endpoints in fallback order
            for endpoint in ANTIGRAVITY_ENDPOINT_FALLBACKS:
                url = f"{endpoint}/v1internal:{action}"
                if stream:
                    url += "?alt=sse"

                try:
                    async with session.post(
                        url,
                        json=body,
                        headers=headers,
                    ) as response:
                        if response.ok:
                            if stream:
                                return await self._handle_streaming_response(response, model)
                            else:
                                response_data = await response.json()
                                return self._parse_response(response_data)
                        else:
                            last_error = await response.text()
                            # Continue to next endpoint on error
                            continue
                except Exception as e:
                    last_error = str(e)
                    continue

        raise ValueError(f"Antigravity OAuth API error (all endpoints failed): {last_error}")

    async def _handle_streaming_response(
        self,
        response: aiohttp.ClientResponse,
        model: str,
    ) -> ModelResponse:
        """Handle SSE streaming response and return complete ModelResponse."""
        from .tool_utils import extract_tool_calls_from_gemini_response

        full_content = ""
        all_tool_calls: List[Dict[str, Any]] = []

        async for line in response.content:
            line_str = line.decode("utf-8").strip()
            if not line_str or not line_str.startswith("data:"):
                continue

            try:
                data = json.loads(line_str[5:].strip())
                if "response" in data:
                    data = data["response"]

                if "candidates" in data:
                    for candidate in data["candidates"]:
                        if "content" in candidate:
                            for part in candidate["content"].get("parts", []):
                                if "text" in part:
                                    full_content += part["text"]

                # Extract tool calls from this chunk
                chunk_tool_calls = extract_tool_calls_from_gemini_response(data)
                if chunk_tool_calls:
                    all_tool_calls.extend(chunk_tool_calls)
            except json.JSONDecodeError:
                continue

        # Build message with optional tool_calls
        message: Dict[str, Any] = {
            "role": "assistant",
            "content": full_content if full_content else None,
        }
        if all_tool_calls:
            message["tool_calls"] = all_tool_calls

        finish_reason = "tool_calls" if all_tool_calls else "stop"

        return ModelResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            created=int(time.time()),
            model=model,
            choices=[
                {
                    "index": 0,
                    "message": message,
                    "finish_reason": finish_reason,
                }
            ],
        )

    def streaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> Iterator[GenericStreamingChunk]:
        """Synchronous streaming (not implemented - use async)."""
        raise NotImplementedError("Use astreaming for Antigravity OAuth")

    async def astreaming(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> AsyncIterator[GenericStreamingChunk]:
        """Async streaming using Antigravity endpoint with fallback."""
        access_token = self._get_access_token()
        if not access_token:
            raise ValueError(
                "No OAuth token available for Antigravity. "
                "Run 'koder auth login antigravity' to authenticate."
            )

        # Extract optional_params - LiteLLM passes tools and other params inside this dict
        optional_params = kwargs.pop("optional_params", {})
        merged_kwargs = {**kwargs, **optional_params}

        # Get project ID (managed or fallback)
        project_id = await self._ensure_project_context(access_token)

        # Check if this is a Claude thinking model (before resolution)
        is_claude_thinking = self._is_claude_thinking_model(model)

        merged_kwargs["stream"] = True
        body = self._build_request_body(model, messages, project_id, **merged_kwargs)

        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
            **ANTIGRAVITY_HEADERS,
        }

        # Add interleaved thinking header for Claude thinking models
        # This enables real-time streaming of thinking tokens
        if is_claude_thinking:
            headers["anthropic-beta"] = "interleaved-thinking-2025-05-14"

        last_error = None
        async with aiohttp.ClientSession() as session:
            # Try endpoints in fallback order
            for endpoint in ANTIGRAVITY_ENDPOINT_FALLBACKS:
                url = f"{endpoint}/v1internal:streamGenerateContent?alt=sse"

                try:
                    async with session.post(
                        url,
                        json=body,
                        headers=headers,
                    ) as response:
                        if not response.ok:
                            last_error = await response.text()
                            continue

                        chunk_index = 0
                        has_tool_calls = False
                        async for line in response.content:
                            line_str = line.decode("utf-8").strip()
                            if not line_str or not line_str.startswith("data:"):
                                continue

                            try:
                                data = json.loads(line_str[5:].strip())
                                if "response" in data:
                                    data = data["response"]

                                if "candidates" in data:
                                    for candidate in data["candidates"]:
                                        if "content" in candidate:
                                            for part in candidate["content"].get("parts", []):
                                                if "text" in part:
                                                    yield GenericStreamingChunk(
                                                        text=part["text"],
                                                        is_finished=False,
                                                        finish_reason=None,
                                                        usage=None,
                                                        index=chunk_index,
                                                    )
                                                    chunk_index += 1
                                                elif "functionCall" in part:
                                                    # Mark that we have tool calls
                                                    has_tool_calls = True
                                                    func_call = part["functionCall"]
                                                    name = func_call.get("name", "")
                                                    args = func_call.get("args", {})
                                                    call_id = func_call.get(
                                                        "id", f"call_{uuid.uuid4().hex[:8]}"
                                                    )
                                                    # Build tool_use dict
                                                    tool_use_data: Dict[str, Any] = {
                                                        "id": call_id,
                                                        "type": "function",
                                                        "function": {
                                                            "name": name,
                                                            "arguments": (
                                                                json.dumps(args)
                                                                if isinstance(args, dict)
                                                                else str(args)
                                                            ),
                                                        },
                                                    }
                                                    # Preserve thoughtSignature for Gemini 3 models
                                                    thought_sig = part.get("thoughtSignature")
                                                    if thought_sig:
                                                        tool_use_data["thought_signature"] = (
                                                            thought_sig
                                                        )
                                                        # Cache for retrieval during follow-up calls
                                                        # (LiteLLM streaming may lose the thought_signature)
                                                        from .tool_utils import (
                                                            cache_thought_signature,
                                                        )

                                                        cache_thought_signature(
                                                            call_id, name, thought_sig
                                                        )
                                                    # Yield tool call as a special chunk
                                                    # Also pass thought_signature in provider_specific_fields for LiteLLM
                                                    provider_fields = (
                                                        {"thought_signature": thought_sig}
                                                        if thought_sig
                                                        else None
                                                    )
                                                    yield GenericStreamingChunk(
                                                        text="",
                                                        is_finished=False,
                                                        finish_reason=None,
                                                        usage=None,
                                                        index=chunk_index,
                                                        tool_use=tool_use_data,
                                                        provider_specific_fields=provider_fields,
                                                    )
                                                    chunk_index += 1
                            except json.JSONDecodeError:
                                continue

                        finish_reason = "tool_calls" if has_tool_calls else "stop"
                        yield GenericStreamingChunk(
                            text="",
                            is_finished=True,
                            finish_reason=finish_reason,
                            usage=None,
                            index=chunk_index,
                        )
                        return  # Success, exit the loop
                except Exception as e:
                    last_error = str(e)
                    continue

        raise ValueError(
            f"Antigravity OAuth API streaming error (all endpoints failed): {last_error}"
        )


# Singleton instances
_google_oauth_llm = GoogleOAuthLLM()
_claude_oauth_llm = ClaudeOAuthLLM()
_chatgpt_oauth_llm = ChatGPTOAuthLLM()
_antigravity_oauth_llm = AntigravityOAuthLLM()


def register_oauth_providers() -> None:
    """Register all OAuth providers with LiteLLM.

    Call this function at startup to enable OAuth-based model access
    with prefixes like 'google/', 'claude/', 'chatgpt/', 'antigravity/'.

    Registration strategy:
    1. Add to provider_list - allows get_llm_provider() to recognize the prefix
    2. Add to _custom_providers - enables routing to custom handlers
    3. Remove ChatGPT models from open_ai_chat_completion_models - prevents
       the model name check from routing to OpenAI before custom handler
    """
    oauth_providers = ["google", "claude", "chatgpt", "antigravity"]

    # Register custom handlers
    litellm.custom_provider_map = [
        {"provider": "google", "custom_handler": _google_oauth_llm},
        {"provider": "claude", "custom_handler": _claude_oauth_llm},
        {"provider": "chatgpt", "custom_handler": _chatgpt_oauth_llm},
        {"provider": "antigravity", "custom_handler": _antigravity_oauth_llm},
    ]

    # Add to provider_list for get_llm_provider() to recognize the prefix
    for provider in oauth_providers:
        if provider not in litellm.provider_list:
            litellm.provider_list.append(provider)

    # Add to _custom_providers for custom handler routing in acompletion()
    existing_custom = list(litellm._custom_providers) if litellm._custom_providers else []
    for provider in oauth_providers:
        if provider not in existing_custom:
            existing_custom.append(provider)
    litellm._custom_providers = existing_custom

    # Remove ChatGPT/Codex models from open_ai_chat_completion_models
    # This prevents the model name check from catching these models
    # and routing to OpenAI before our custom handler is checked
    chatgpt_models = [
        m for m in litellm.open_ai_chat_completion_models if "gpt-5" in m or "codex" in m.lower()
    ]
    for model in chatgpt_models:
        if model in litellm.open_ai_chat_completion_models:
            litellm.open_ai_chat_completion_models.remove(model)

    logger.info(
        f"Registered OAuth providers with LiteLLM: {', '.join(oauth_providers)}. "
        f"Removed {len(chatgpt_models)} ChatGPT models from OpenAI routing."
    )


def get_oauth_model_prefix(provider: str) -> Optional[str]:
    """Get the LiteLLM model prefix for an OAuth provider.

    Args:
        provider: OAuth provider name (google, claude, chatgpt, antigravity)

    Returns:
        LiteLLM model prefix for OAuth access (same as provider name)
    """
    # Use original provider names directly - no transformation needed
    oauth_providers = {"google", "claude", "chatgpt", "antigravity"}
    provider_lower = provider.lower()
    return provider_lower if provider_lower in oauth_providers else None


def is_oauth_provider(provider: str) -> bool:
    """Check if a provider name is an OAuth provider.

    Args:
        provider: Provider name to check

    Returns:
        True if this is an OAuth provider
    """
    oauth_providers = {"google", "claude", "chatgpt", "antigravity"}
    return provider.lower() in oauth_providers
