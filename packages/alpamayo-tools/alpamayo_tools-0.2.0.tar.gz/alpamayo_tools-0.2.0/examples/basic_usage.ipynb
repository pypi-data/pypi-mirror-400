{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# alpamayo-tools: Basic Usage\n",
        "\n",
        "This notebook demonstrates the basic usage of alpamayo-tools for working with NVIDIA's Alpamayo/PhysicalAI-AV ecosystem.\n",
        "\n",
        "> **Note:** If you're running this from the repo with `uv sync --extra all --extra dev`, all dependencies are already installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation (for standalone use)\n",
        "\n",
        "Skip this if you cloned the repo and ran `uv sync`.\n",
        "\n",
        "```bash\n",
        "# Core package (dataloader only)\n",
        "pip install alpamayo-tools\n",
        "\n",
        "# With embeddings support\n",
        "pip install alpamayo-tools[embeddings]\n",
        "\n",
        "# With inference support (also need alpamayo_r1 from GitHub)\n",
        "pip install alpamayo-tools[inference]\n",
        "pip install git+https://github.com/NVlabs/alpamayo.git\n",
        "\n",
        "# Everything\n",
        "pip install alpamayo-tools[all]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Using the PyTorch DataLoader\n",
        "\n",
        "The `PhysicalAIDataset` provides a clean interface for loading PhysicalAI-AV data into PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total clips available: 227985\n",
            "Using clips: ['25cd4769-5dcf-4b53-a351-bf2c5deb6124', '2edf278f-d5e3-4b83-b5df-923a04335725']\n"
          ]
        }
      ],
      "source": [
        "# First, let's fetch some available clip IDs from the dataset\n",
        "import physical_ai_av\n",
        "\n",
        "avdi = physical_ai_av.PhysicalAIAVDatasetInterface()\n",
        "\n",
        "# Get all available clip IDs\n",
        "all_clip_ids = avdi.clip_index.index.tolist()\n",
        "print(f\"Total clips available: {len(all_clip_ids)}\")\n",
        "\n",
        "# Use first 2 for demo\n",
        "sample_clip_ids = all_clip_ids[:2]\n",
        "print(f\"Using clips: {sample_clip_ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 2 clips\n"
          ]
        }
      ],
      "source": [
        "from alpamayo_tools import PhysicalAIDataset, DatasetConfig\n",
        "\n",
        "# Configure the dataset\n",
        "config = DatasetConfig(\n",
        "    clip_ids=sample_clip_ids,\n",
        "    cameras=(\"camera_front_wide_120fov\", \"camera_front_tele_30fov\"),\n",
        "    num_frames=4,\n",
        "    num_history_steps=16,  # 1.6s history @ 10Hz\n",
        "    num_future_steps=64,   # 6.4s future @ 10Hz\n",
        "    stream=True,  # Stream from HuggingFace\n",
        ")\n",
        "\n",
        "# Create dataset\n",
        "dataset = PhysicalAIDataset(config)\n",
        "print(f\"Dataset size: {len(dataset)} clips\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample keys: ['clip_id', 't0_us', 'frames', 'camera_indices', 'ego_history_xyz', 'ego_history_rot', 'ego_future_xyz', 'ego_future_rot', 'frame_timestamps']\n",
            "Frames shape: torch.Size([2, 4, 3, 1080, 1920])\n",
            "History positions shape: torch.Size([16, 3])\n",
            "Future positions shape: torch.Size([64, 3])\n"
          ]
        }
      ],
      "source": [
        "# Get a single sample\n",
        "sample = dataset[0]\n",
        "\n",
        "print(\"Sample keys:\", list(sample.keys()))\n",
        "print(f\"Frames shape: {sample['frames'].shape}\")  # (N_cameras, num_frames, 3, H, W)\n",
        "print(f\"History positions shape: {sample['ego_history_xyz'].shape}\")  # (16, 3)\n",
        "print(f\"Future positions shape: {sample['ego_future_xyz'].shape}\")  # (64, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch frames shape: torch.Size([2, 2, 4, 3, 1080, 1920])\n",
            "Batch history shape: torch.Size([2, 16, 3])\n"
          ]
        }
      ],
      "source": [
        "# Use with DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "from alpamayo_tools import collate_fn\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Set > 0 for parallel loading\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "for batch in loader:\n",
        "    print(f\"Batch frames shape: {batch['frames'].shape}\")\n",
        "    print(f\"Batch history shape: {batch['ego_history_xyz'].shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading from a file\n",
        "\n",
        "You can also load clip IDs from a parquet or text file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From parquet file\n",
        "# config = DatasetConfig(\n",
        "#     clip_ids_file=\"path/to/clip_ids.parquet\",\n",
        "#     cameras=(\"camera_front_wide_120fov\",),\n",
        "# )\n",
        "\n",
        "# From text file (one clip ID per line)\n",
        "# config = DatasetConfig(\n",
        "#     clip_ids_file=\"path/to/clip_ids.txt\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CoC Embeddings\n",
        "\n",
        "The `CoCEmbedder` provides a simple interface for embedding Chain-of-Cognition reasoning text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimension: 384\n",
            "Device: mps:0\n"
          ]
        }
      ],
      "source": [
        "from alpamayo_tools import CoCEmbedder\n",
        "\n",
        "# Initialize embedder (uses all-MiniLM-L6-v2 by default)\n",
        "embedder = CoCEmbedder()\n",
        "\n",
        "print(f\"Embedding dimension: {embedder.embedding_dim}\")\n",
        "print(f\"Device: {embedder.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (3, 384)\n"
          ]
        }
      ],
      "source": [
        "# Embed reasoning texts\n",
        "texts = [\n",
        "    \"The vehicle ahead is braking. Reduce speed to maintain safe following distance.\",\n",
        "    \"Clear road ahead with no obstacles. Continue at current speed.\",\n",
        "    \"Pedestrian crossing detected on the right. Slow down and prepare to stop.\",\n",
        "]\n",
        "\n",
        "embeddings = embedder.embed(texts)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")  # (3, 384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity matrix:\n",
            "[[1.        0.6757734 0.5420312]\n",
            " [0.6757734 1.0000001 0.5415089]\n",
            " [0.5420312 0.5415089 1.0000001]]\n"
          ]
        }
      ],
      "source": [
        "# Compute similarity between texts\n",
        "import numpy as np\n",
        "\n",
        "# Cosine similarity (embeddings are normalized by default)\n",
        "similarity_matrix = embeddings @ embeddings.T\n",
        "print(\"Similarity matrix:\")\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Alpamayo Inference (Requires GPU)\n",
        "\n",
        "The `AlpamayoPredictor` provides a simple interface for running Alpamayo-R1 inference.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU with 24GB+ VRAM\n",
        "- `alpamayo_r1` package installed from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping: CUDA not available\n"
          ]
        }
      ],
      "source": [
        "# Note: This requires alpamayo_r1 and a GPU\n",
        "# pip install git+https://github.com/NVlabs/alpamayo.git\n",
        "\n",
        "from alpamayo_tools.inference import AlpamayoPredictor\n",
        "import torch\n",
        "\n",
        "# Load the model (skip this cell if you don't have a GPU)\n",
        "if torch.cuda.is_available():\n",
        "    predictor = AlpamayoPredictor.from_pretrained(\n",
        "        model_id=\"nvidia/Alpamayo-R1-10B\",\n",
        "        dtype=torch.bfloat16,\n",
        "    )\n",
        "    print(\"Model loaded!\")\n",
        "else:\n",
        "    print(\"Skipping: CUDA not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference on a clip (skip if no GPU)\n",
        "if torch.cuda.is_available():\n",
        "    result = predictor.predict_from_clip(\n",
        "        clip_id=sample_clip_ids[0],\n",
        "        t0_us=5_100_000,  # 5.1 seconds into clip\n",
        "        num_samples=1,\n",
        "    )\n",
        "\n",
        "    print(f\"Trajectory shape: {result.trajectory_xyz.shape}\")  # (64, 3)\n",
        "    print(f\"\\nReasoning:\\n{result.reasoning_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Or use with our dataset\n",
        "if torch.cuda.is_available():\n",
        "    sample = dataset[0]\n",
        "    result = predictor.predict_from_dataset_sample(sample)\n",
        "\n",
        "    print(f\"Predicted trajectory: {result.trajectory_xyz.shape}\")\n",
        "    print(f\"Ground truth trajectory: {sample['ego_future_xyz'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CLI: Generate Teacher Labels\n",
        "\n",
        "The package includes a CLI tool for generating teacher labels at scale:\n",
        "\n",
        "```bash\n",
        "# Basic usage\n",
        "alpamayo-generate-labels \\\n",
        "    --clip-ids-file train_clips.parquet \\\n",
        "    --output-dir ./labels\n",
        "\n",
        "# With sharding for multi-GPU\n",
        "# GPU 0:\n",
        "CUDA_VISIBLE_DEVICES=0 alpamayo-generate-labels \\\n",
        "    --clip-ids-file train_clips.parquet \\\n",
        "    --output-dir ./labels \\\n",
        "    --shard 0/2\n",
        "\n",
        "# GPU 1:\n",
        "CUDA_VISIBLE_DEVICES=1 alpamayo-generate-labels \\\n",
        "    --clip-ids-file train_clips.parquet \\\n",
        "    --output-dir ./labels \\\n",
        "    --shard 1/2\n",
        "\n",
        "# Resume from checkpoint\n",
        "alpamayo-generate-labels \\\n",
        "    --clip-ids-file train_clips.parquet \\\n",
        "    --output-dir ./labels \\\n",
        "    --resume\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Working with Output Labels\n",
        "\n",
        "The generated labels are saved as compressed NPZ files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load generated labels (requires running the CLI first)\n",
        "# import numpy as np\n",
        "#\n",
        "# data = np.load(\"./labels/teacher_labels.npz\", allow_pickle=True)\n",
        "#\n",
        "# print(\"Available arrays:\", list(data.keys()))\n",
        "# print(f\"Number of samples: {len(data['clip_ids'])}\")\n",
        "# print(f\"Trajectory shape: {data['trajectory_xyz'].shape}\")  # (N, 64, 3)\n",
        "# print(f\"Rotation shape: {data['trajectory_rot'].shape}\")  # (N, 64, 3, 3)\n",
        "# print(f\"CoC embedding shape: {data['coc_embeddings'].shape}\")  # (N, 384)\n",
        "#\n",
        "# # Access individual samples\n",
        "# idx = 0\n",
        "# print(f\"Clip ID: {data['clip_ids'][idx]}\")\n",
        "# print(f\"Reasoning: {data['coc_texts'][idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Always close the dataset to release video reader resources\n",
        "dataset.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
