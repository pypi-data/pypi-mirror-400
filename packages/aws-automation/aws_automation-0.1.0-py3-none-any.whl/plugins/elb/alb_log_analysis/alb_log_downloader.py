import bisect
import concurrent.futures
import gc
import gzip
import os
import shutil
from collections import namedtuple
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import pytz  # type: ignore[import-untyped]
from botocore.exceptions import ClientError
from rich.console import Console
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
)

# ì½˜ì†” ë° ë¡œê±° (aa_cli.aa.ui ë˜ëŠ” ë¡œì»¬ ìƒì„±)
try:
    from cli.ui import console, logger
except ImportError:
    import logging

    console = Console()
    logger = logging.getLogger(__name__)

import contextlib

from core.tools.cache import get_cache_dir


def _create_report_directory(prefix: str, session_name: str = "") -> str:
    """ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„± (ë ˆê±°ì‹œ í˜¸í™˜)"""
    from datetime import datetime

    timestamp = datetime.now().strftime("%Y%m%d")
    base_dir = os.path.join("output", session_name or "default", prefix, timestamp)
    os.makedirs(base_dir, exist_ok=True)
    return base_dir


class LogDownloadError(Exception):
    """ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜"""

    pass


# ê²½ëŸ‰í™”ëœ S3 ê°ì²´ ì •ë³´ (ìºì‹œ ì œê±°)
S3LogFile = namedtuple("S3LogFile", ["key", "last_modified", "size", "timestamp"])


class ALBLogDownloader:
    def __init__(
        self,
        s3_client: Any,
        s3_uri: str,
        start_datetime: Any,
        end_datetime: Any | None = None,
        timezone: str = "Asia/Seoul",
        max_workers: int = 5,
        chunk_size: int = 8 * 1024 * 1024,  # 8MB
        session_name: str = "default",
        batch_size: int = 50,
        smart_filtering: bool = True,  # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ í™œì„±í™”
    ):
        """
        ALB ë¡œê·¸ ë‹¤ìš´ë¡œë”ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            s3_client: S3 í´ë¼ì´ì–¸íŠ¸ (readonly ì—­í• ë¡œ ì„¤ì •ëœ í´ë¼ì´ì–¸íŠ¸)
            s3_uri: S3 URI (ì˜ˆ: s3://bucket-name/prefix/AWSLogs/account-number/elasticloadbalancing/region)
            start_datetime: ì‹œì‘ ì‹œê°„ (datetime ê°ì²´ ë˜ëŠ” ë¬¸ìì—´)
            end_datetime: ì¢…ë£Œ ì‹œê°„ (datetime ê°ì²´ ë˜ëŠ” ë¬¸ìì—´, ê¸°ë³¸ê°’: None)
            timezone: íƒ€ì„ì¡´ (ê¸°ë³¸ê°’: Asia/Seoul)
            max_workers: ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìµœëŒ€ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 5)
            chunk_size: ë‹¤ìš´ë¡œë“œ ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 8MB)
            session_name: ì„¸ì…˜ ì´ë¦„ (ê¸°ë³¸ê°’: default)
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ (ê¸°ë³¸ê°’: 50)
            smart_filtering: ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ í™œì„±í™” ì—¬ë¶€
        """
        # ğŸš€ auth ì„¸ì…˜ ìœ ì§€ - ì›ë³¸ í´ë¼ì´ì–¸íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        self.s3_client = s3_client  # authì—ì„œ ë°›ì€ ì¸ì¦ëœ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        self.session_name = session_name
        self.batch_size = batch_size
        self.smart_filtering = smart_filtering

        # S3 URI íŒŒì‹±
        if not s3_uri.startswith("s3://"):
            raise ValueError("S3 URIëŠ” 's3://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")

        self.s3_uri = s3_uri  # Store for later use

        # s3:// ì œê±°
        path = s3_uri[5:]

        # ë²„í‚· ì´ë¦„ê³¼ ì ‘ë‘ì‚¬ ë¶„ë¦¬
        parts = path.split("/", 1)
        self.bucket_name = parts[0]
        self.prefix = parts[1] if len(parts) > 1 else ""

        # ì ‘ë‘ì‚¬ê°€ ìˆëŠ” ê²½ìš° ëì— '/' ì¶”ê°€
        if self.prefix and not self.prefix.endswith("/"):
            self.prefix += "/"

        # datetime ê°ì²´ ë˜ëŠ” ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        if isinstance(start_datetime, str):
            try:
                self.start_datetime = datetime.strptime(start_datetime, "%Y-%m-%d %H:%M")
            except ValueError as e:
                raise ValueError(f"ì˜ëª»ëœ ì‹œì‘ ì‹œê°„ í˜•ì‹: {start_datetime}") from e
        else:
            self.start_datetime = start_datetime

        if end_datetime is None:
            self.end_datetime = datetime.now()
        elif isinstance(end_datetime, str):
            try:
                self.end_datetime = datetime.strptime(end_datetime, "%Y-%m-%d %H:%M")
            except ValueError as e:
                raise ValueError(f"ì˜ëª»ëœ ì¢…ë£Œ ì‹œê°„ í˜•ì‹: {end_datetime}") from e
        else:
            self.end_datetime = end_datetime

        # íƒ€ì„ì¡´ ì„¤ì •
        try:
            self.timezone = pytz.timezone(timezone)
            # ì‹œê°„ì— íƒ€ì„ì¡´ ì •ë³´ ì¶”ê°€
            self.start_datetime = self.timezone.localize(self.start_datetime)
            self.end_datetime = self.timezone.localize(self.end_datetime)

            # UTCë¡œ ë³€í™˜ (S3 íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” UTC ê¸°ì¤€)
            self.start_datetime_utc = self.start_datetime.astimezone(pytz.UTC)
            self.end_datetime_utc = self.end_datetime.astimezone(pytz.UTC)
        except pytz.exceptions.UnknownTimeZoneError:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´ '{timezone}'ì…ë‹ˆë‹¤. UTCë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.timezone = pytz.UTC
            self.start_datetime = self.timezone.localize(self.start_datetime)
            self.end_datetime = self.timezone.localize(self.end_datetime)
            self.start_datetime_utc = self.start_datetime
            self.end_datetime_utc = self.end_datetime

        self.console = console

        # ALB ë¡œê·¸ ì „ìš© ë””ë ‰í† ë¦¬ (temp/alb í•˜ìœ„)
        alb_data_dir = get_cache_dir("alb")
        self.temp_dir = os.path.join(alb_data_dir, "gz")  # gz íŒŒì¼ ì €ì¥
        self.decompressed_dir = os.path.join(alb_data_dir, "log")  # ì••ì¶• í•´ì œëœ ë¡œê·¸ ì €ì¥

        # ìš”ì²­ ë²”ìœ„ ë¯¸ìŠ¤ë§¤ì¹˜ ì‹œ ì‚¬ìš©ì ì•ˆë‚´ë¥¼ ìœ„í•´ S3ì—ì„œ í™•ì¸ëœ ì‹¤ì œ ë¡œê·¸ ë²”ìœ„(KST)ë¥¼ ì €ì¥
        self.available_range_local: tuple[datetime, datetime] | None = None

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.temp_dir, exist_ok=True)
        os.makedirs(self.decompressed_dir, exist_ok=True)

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = _create_report_directory("alb_log", self.session_name)
        self.report_filename = self._generate_report_filename()

        self.console.print(f"â° ë¶„ì„ ê¸°ê°„: {self.start_datetime} ~ {self.end_datetime} ({timezone})")

    def _generate_report_filename(self) -> str:
        """ë³´ê³ ì„œ íŒŒì¼ ì´ë¦„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        from secrets import token_hex

        # AWS ALB ë¡œê·¸ íŒŒì¼ ë„¤ì´ë° ê·œì¹™ì— ë§ëŠ” íŒŒì¼ëª… ìƒì„± ì‹œë„
        try:
            aws_account_id = "unknown"
            region = "unknown"

            # S3 URIì—ì„œ ì •ë³´ ì¶”ì¶œ
            if "/AWSLogs/" in self.s3_uri:
                path = self.s3_uri.replace("s3://", "")
                aws_logs_path = path.split("/AWSLogs/")[1]
                path_parts = aws_logs_path.split("/")

                if len(path_parts) >= 3:
                    aws_account_id = path_parts[0]
                    region = path_parts[2]

            # Load Balancer ID ì¶”ì¶œ
            s3_parts = self.s3_uri.replace("s3://", "").split("/")
            bucket_name = s3_parts[0] if len(s3_parts) > 0 else ""
            prefix_parts = s3_parts[1:-2] if len(s3_parts) > 2 else []  # AWSLogs ì „ê¹Œì§€ì˜ prefix

            load_balancer_id = ""
            if prefix_parts:
                load_balancer_id = "-".join(prefix_parts).replace("/", "-")
            elif bucket_name:
                load_balancer_id = bucket_name.replace("-", "-")

            if not load_balancer_id:
                load_balancer_id = "elb"

            # ê°„ê²°í•˜ê³  ê³ ìœ í•œ ì´ë¦„ìœ¼ë¡œ ì¶•ì†Œ
            suffix = token_hex(4)  # 8 hex chars
            return f"{aws_account_id}_{region}_alb_{suffix}.xlsx"

        except Exception:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ íŒŒì¼ëª… ë°˜í™˜
            suffix = token_hex(4)
            return f"alb_{suffix}.xlsx"

    def _smart_date_range_optimization(self) -> list[str]:
        """ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ë²”ìœ„ ìµœì í™” - ì‹œê°„ ë²”ìœ„ì— ë”°ë¼ ì ‘ë‘ì‚¬ ì„¸ë¶„í™”"""
        prefixes = []

        # ì‹œê°„ ë²”ìœ„ ê³„ì‚° (UTC ê¸°ì¤€)
        time_diff = self.end_datetime_utc - self.start_datetime_utc
        total_hours = time_diff.total_seconds() / 3600

        logger.debug(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ì‹œê°„: {total_hours:.1f}ì‹œê°„")

        # ğŸ”§ ìˆ˜ì •: ì›ë˜ ì‹œê°„ëŒ€(ë¡œì»¬) ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        # ALB ë¡œê·¸ íŒŒì¼ì€ S3ì— UTC ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ë˜ì§€ë§Œ, íŒŒì¼ ê²½ë¡œì˜ ë‚ ì§œ ë¶€ë¶„ì€
        # ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì‹œê°„ëŒ€ë¥¼ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤
        local_start_date = self.start_datetime.date()
        local_end_date = self.end_datetime.date()

        # ì•ˆì „ì„ ìœ„í•´ ì‹œì‘ë‚ ì§œì—ì„œ 1ì¼ ì•, ì¢…ë£Œë‚ ì§œì—ì„œ 1ì¼ ë’¤ê¹Œì§€ í™•ì¥
        extended_start_date = local_start_date - timedelta(days=1)
        extended_end_date = local_end_date + timedelta(days=1)

        logger.debug(f"ğŸ“… ë¡œì»¬ ë‚ ì§œ ë²”ìœ„: {local_start_date} ~ {local_end_date}")
        logger.debug(f"ğŸ“… í™•ì¥ëœ ë‚ ì§œ ë²”ìœ„: {extended_start_date} ~ {extended_end_date}")

        if total_hours <= 24:  # 24ì‹œê°„ ì´í•˜
            # ì¼ ë‹¨ìœ„ë¡œë§Œ ì¡°íšŒ (ALB ë¡œê·¸ëŠ” í•˜ë£¨ ë‹¨ìœ„ë¡œ íŒŒí‹°ì…”ë‹)
            current_date = extended_start_date

            while current_date <= extended_end_date:
                date_path = current_date.strftime("%Y/%m/%d")
                date_prefix = f"{self.prefix}{date_path}"
                prefixes.append(date_prefix)
                current_date += timedelta(days=1)

        elif total_hours <= 168:  # 1ì£¼ì¼ ì´í•˜
            # ì¼ ë‹¨ìœ„ ì¡°íšŒ
            current_date = extended_start_date

            while current_date <= extended_end_date:
                date_path = current_date.strftime("%Y/%m/%d")
                date_prefix = f"{self.prefix}{date_path}"
                prefixes.append(date_prefix)
                current_date += timedelta(days=1)

        else:  # 1ì£¼ì¼ ì´ˆê³¼
            # ê¸°ì¡´ ë°©ì‹: ì¼ ë‹¨ìœ„ ì ‘ë‘ì‚¬
            current_date = extended_start_date

            while current_date <= extended_end_date:
                date_path = current_date.strftime("%Y/%m/%d")
                date_prefix = f"{self.prefix}{date_path}"
                prefixes.append(date_prefix)
                current_date += timedelta(days=1)

        logger.debug(f"âœ“ ë‚ ì§œë³„ ì ‘ë‘ì‚¬ ìƒì„± ì™„ë£Œ: {len(prefixes)}ê°œ")
        logger.debug(f"ìƒì„±ëœ ì ‘ë‘ì‚¬ ëª©ë¡: {prefixes}")
        return prefixes

    def _extract_timestamp_from_key(self, key: str) -> datetime | None:
        """S3 ê°ì²´ í‚¤ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ALB ë¡œê·¸ íŒŒì¼ëª… í˜•ì‹ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            parts = key.split("/")
            if len(parts) >= 7:
                # íŒŒì¼ëª…ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
                filename = parts[-1]
                filename_parts = filename.split("_")

                if len(filename_parts) >= 5:
                    # ì‹¤ì œ ALB íŒŒì¼ëª… êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                    # íŒŒì¼ëª…: account_elasticloadbalancing_region_loadbalancer_timestamp_ip_randomstring.log.gz
                    # íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ë’¤ì—ì„œ 3ë²ˆì§¸ ìœ„ì¹˜ (IP ì£¼ì†Œ ì•)
                    timestamp_str = filename_parts[-3]  # timestamp ë¶€ë¶„ ì¶”ì¶œ (ìˆ˜ì •ë¨)

                    # ë””ë²„ê¹…ì„ ìœ„í•œ ì²« ë²ˆì§¸ íŒŒì¼ ì •ë³´ ì¶œë ¥
                    if not hasattr(self, "_debug_printed"):
                        logger.debug(f"ALB ë¡œê·¸ íŒŒì¼ëª… ë¶„ì„: {filename} â†’ {timestamp_str}")
                        self._debug_printed = True

                    # ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì‹œë„
                    try:
                        # 1. YYYYMMDDTHHMMSSZ í˜•ì‹ (Z í¬í•¨)
                        if "T" in timestamp_str and timestamp_str.endswith("Z"):
                            return datetime.strptime(timestamp_str, "%Y%m%dT%H%M%SZ").replace(tzinfo=pytz.UTC)
                    except ValueError:
                        pass

                    try:
                        # 2. YYYYMMDDTHHMMSS í˜•ì‹ (Z ì—†ìŒ)
                        if "T" in timestamp_str and len(timestamp_str) >= 15:
                            return datetime.strptime(timestamp_str, "%Y%m%dT%H%M%S").replace(tzinfo=pytz.UTC)
                    except ValueError:
                        pass

                    try:
                        # 3. ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ í˜•ì‹ (YYYYMMDDHHMMS)
                        if timestamp_str.isdigit() and len(timestamp_str) >= 14:
                            return datetime.strptime(timestamp_str[:14], "%Y%m%d%H%M%S").replace(tzinfo=pytz.UTC)
                    except ValueError:
                        pass

                    # ëª¨ë“  í˜•ì‹ ì‹¤íŒ¨ì‹œ ê²½ê³  ì¶œë ¥
                    logger.debug(f"íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì‹¤íŒ¨: {timestamp_str} (íŒŒì¼: {filename})")

                # ê²½ë¡œì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                try:
                    year = int(parts[-4])
                    month = int(parts[-3])
                    day = int(parts[-2])
                    # ê²½ë¡œì—ì„œ ì¶”ì¶œí•œ ë‚ ì§œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš© (ì‹œê°„ì€ ìì •ìœ¼ë¡œ ì„¤ì •)
                    return datetime(year, month, day, tzinfo=pytz.UTC)
                except (ValueError, IndexError):
                    pass

            return None
        except Exception as e:
            logger.debug(f"íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì‹¤íŒ¨ ({key}): {str(e)}")
            return None

    def _binary_search_time_filter(self, log_files: list[S3LogFile]) -> list[S3LogFile]:
        """ë°”ì´ë„ˆë¦¬ ì„œì¹˜ë¥¼ ì‚¬ìš©í•œ ì‹œê°„ ë²”ìœ„ í•„í„°ë§"""
        if not self.smart_filtering or not log_files:
            # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ë¹„í™œì„±í™”ì‹œ ê¸°ì¡´ ë°©ì‹
            return [
                f for f in log_files if f.timestamp and self.start_datetime_utc <= f.timestamp <= self.end_datetime_utc
            ]

        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” íŒŒì¼ë“¤ë§Œ í•„í„°ë§
        files_with_timestamp = [f for f in log_files if f.timestamp]
        files_without_timestamp = [f for f in log_files if not f.timestamp]

        logger.debug(
            f"ë°”ì´ë„ˆë¦¬ ì„œì¹˜ í•„í„°ë§: {len(log_files)} â†’ {len(files_with_timestamp)} íŒŒì¼ (íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì„±ê³µ)"
        )
        logger.debug(f"íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì‹¤íŒ¨: {len(files_without_timestamp)} íŒŒì¼")
        logger.debug(f"ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„: {self.start_datetime_utc} ~ {self.end_datetime_utc}")

        if not files_with_timestamp:
            logger.warning("íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return log_files  # ì›ë³¸ ë°˜í™˜

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œì— ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ë„ í¬í•¨ (ì•ˆì „ë§)
        if files_without_timestamp:
            logger.debug(f"íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì‹¤íŒ¨í•œ íŒŒì¼ {len(files_without_timestamp)}ê°œë„ í¬í•¨í•©ë‹ˆë‹¤.")

        # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •ë ¬
        files_with_timestamp.sort(key=lambda x: x.timestamp)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„ ì •ë³´ ì¶œë ¥ ë° ì‚¬ìš©ì íƒ€ì„ì¡´ ë²”ìœ„ ì €ì¥
        if files_with_timestamp:
            earliest_timestamp = files_with_timestamp[0].timestamp
            latest_timestamp = files_with_timestamp[-1].timestamp
            logger.debug(f"íŒŒì¼ íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„: {earliest_timestamp} ~ {latest_timestamp}")
            try:
                earliest_local = earliest_timestamp.astimezone(self.timezone)
                latest_local = latest_timestamp.astimezone(self.timezone)
                self.available_range_local = (earliest_local, latest_local)
            except Exception:
                self.available_range_local = None

        # ğŸ¯ ALB 5ë¶„ ë‹¨ìœ„ ì ì¬ íŠ¹ì„±ì— ë§ì¶˜ 10ë¶„ í™•ì¥ (ì—¬ìœ ìˆê²Œ)
        # ALBëŠ” 5ë¶„ êµ¬ê°„ ë¡œê·¸ë¥¼ êµ¬ê°„ ë ì‹œê°„ì— ì €ì¥ (ì˜ˆ: 08:00~08:05 â†’ T0805Z íŒŒì¼)
        from datetime import timedelta

        extended_start_datetime_utc = self.start_datetime_utc - timedelta(minutes=10)
        extended_end_datetime_utc = self.end_datetime_utc + timedelta(minutes=10)

        logger.debug(f"ìš”ì²­ëœ ì‹œê°„ ë²”ìœ„: {self.start_datetime_utc} ~ {self.end_datetime_utc} (UTC)")
        logger.debug(f"ALB íŠ¹ì„± ê³ ë ¤ í™•ì¥: {extended_start_datetime_utc} ~ {extended_end_datetime_utc} (Â±10ë¶„)")

        # ë°”ì´ë„ˆë¦¬ ì„œì¹˜ë¡œ ì‹œì‘/ë ì¸ë±ìŠ¤ ì°¾ê¸° (10ë¶„ í™•ì¥ëœ ë²”ìœ„ ì‚¬ìš©)
        start_idx = bisect.bisect_left(files_with_timestamp, extended_start_datetime_utc, key=lambda x: x.timestamp)

        end_idx = bisect.bisect_right(files_with_timestamp, extended_end_datetime_utc, key=lambda x: x.timestamp)

        logger.debug(f"ë°”ì´ë„ˆë¦¬ ì„œì¹˜ ê²°ê³¼: {start_idx} ~ {end_idx} (ì„ íƒ: {end_idx - start_idx}ê°œ)")

        filtered_files = files_with_timestamp[start_idx:end_idx]

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ë„ ì¶”ê°€ (ì•ˆì „ë§)
        filtered_files.extend(files_without_timestamp)

        logger.debug(f"ì‹œê°„ ë²”ìœ„ í•„í„°ë§ ì™„ë£Œ: {len(filtered_files)}ê°œ íŒŒì¼ ì„ íƒ")

        # ìµœì¢… í•„í„°ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ëª…í™•í•œ ë©”ì‹œì§€ ì œê³µ
        if not filtered_files:
            logger.error("âŒ ìš”ì²­ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ALB ë¡œê·¸ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            logger.error(f"   ìš”ì²­ ë²”ìœ„({self.timezone.zone}): {self.start_datetime} ~ {self.end_datetime}")
            if files_with_timestamp:
                earliest_local = files_with_timestamp[0].timestamp.astimezone(self.timezone)
                latest_local = files_with_timestamp[-1].timestamp.astimezone(self.timezone)
                self.available_range_local = (earliest_local, latest_local)
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‹œê° ê¸°ì¤€ ê¶Œì¥ êµ¬ê°„ ì œì•ˆ (ìµœê·¼ 10ë¶„)
                try:
                    suggest_start = latest_local - timedelta(minutes=10)
                    suggest_end = latest_local
                    logger.error(f"   S3 ì‹¤ì œ ë¡œê·¸ ë²”ìœ„: {earliest_local} ~ {latest_local}")
                    logger.error(f"   ê¶Œì¥: ìµœê·¼ ìœ íš¨ ì‹œê° ê·¼ì²˜ë¡œ ì¬ì‹œë„ (ì˜ˆ: {suggest_start} ~ {suggest_end})")
                except Exception:
                    logger.error(f"   S3 ì‹¤ì œ ë¡œê·¸ ë²”ìœ„: {earliest_local} ~ {latest_local}")
            logger.error(
                "   ì°¸ê³ : ALBëŠ” 5ë¶„ ë‹¨ìœ„ íŒŒì¼ì„ ìƒì„±í•˜ë©°, íŠ¸ë˜í”½ 0 ë˜ëŠ” ì „ì†¡ ì§€ì—° ì‹œ í•´ë‹¹ êµ¬ê°„ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )

        return filtered_files

    def _adaptive_batch_size(self, total_files: int, avg_file_size: float) -> int:
        """íŒŒì¼ ìˆ˜ì™€ í¬ê¸°ì— ë”°ë¥¸ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        base_batch_size = self.batch_size

        # íŒŒì¼ ìˆ˜ê°€ ì ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        if total_files < 20:
            return min(5, total_files)

        # íŒŒì¼ í¬ê¸°ê°€ í° ê²½ìš° ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        if avg_file_size > 50 * 1024 * 1024:  # 50MB ì´ìƒ
            return max(10, base_batch_size // 2)
        elif avg_file_size > 20 * 1024 * 1024:  # 20MB ì´ìƒ
            return max(20, int(base_batch_size / 1.5))  # float ê²°ê³¼ë¥¼ intë¡œ ë³€í™˜
        elif avg_file_size < 5 * 1024 * 1024:  # 5MB ë¯¸ë§Œ
            return min(100, base_batch_size * 2)

        return base_batch_size

    def _list_objects_for_prefix(self, prefix: str, progress: Progress, task_id: Any) -> list[S3LogFile]:
        """íŠ¹ì • ì ‘ë‘ì‚¬ì— ëŒ€í•œ S3 ê°ì²´ ëª©ë¡ì„ ìµœì í™”í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        result = []
        paginator = self.s3_client.get_paginator("list_objects_v2")

        try:
            for page in paginator.paginate(Bucket=self.bucket_name, Prefix=prefix):
                if "Contents" not in page:
                    continue

                for obj in page["Contents"]:
                    key = obj["Key"]
                    if key.endswith(".log.gz"):
                        # S3LogFile namedtuple ìƒì„±
                        timestamp = self._extract_timestamp_from_key(key)
                        log_file = S3LogFile(
                            key=key,
                            last_modified=obj["LastModified"].replace(tzinfo=pytz.UTC),
                            size=obj["Size"],
                            timestamp=timestamp,
                        )
                        result.append(log_file)
                        progress.update(task_id, advance=1)

            logger.debug(f"âœ“ ì ‘ë‘ì‚¬ '{prefix}'ì—ì„œ {len(result)}ê°œì˜ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return result

        except Exception as e:
            logger.error(f"âŒ ê°ì²´ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ({prefix}): {str(e)}")
            return []

    def _list_objects_for_prefix_simple(self, prefix: str) -> list[S3LogFile]:
        """íŠ¹ì • ì ‘ë‘ì‚¬ì— ëŒ€í•œ S3 ê°ì²´ ëª©ë¡ì„ ê°„ë‹¨íˆ ë°˜í™˜í•©ë‹ˆë‹¤ (Progress ì—†ì´)."""
        result = []
        paginator = self.s3_client.get_paginator("list_objects_v2")

        try:
            for page in paginator.paginate(Bucket=self.bucket_name, Prefix=prefix):
                if "Contents" not in page:
                    continue

                for obj in page["Contents"]:
                    key = obj["Key"]
                    if key.endswith(".log.gz"):
                        # S3LogFile namedtuple ìƒì„±
                        timestamp = self._extract_timestamp_from_key(key)
                        log_file = S3LogFile(
                            key=key,
                            last_modified=obj["LastModified"].replace(tzinfo=pytz.UTC),
                            size=obj["Size"],
                            timestamp=timestamp,
                        )
                        result.append(log_file)

            logger.debug(f"âœ“ ì ‘ë‘ì‚¬ '{prefix}'ì—ì„œ {len(result)}ê°œì˜ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return result

        except Exception as e:
            logger.error(f"âŒ ê°ì²´ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ({prefix}): {str(e)}")
            return []

    def _verify_s3_access(self) -> bool:
        """S3 ë²„í‚· ì ‘ê·¼ ê¶Œí•œì„ ì‚¬ì „ ê²€ì¦í•©ë‹ˆë‹¤.

        Returns:
            True: ì ‘ê·¼ ê°€ëŠ¥

        Raises:
            LogDownloadError: ì ‘ê·¼ ê¶Œí•œì´ ì—†ëŠ” ê²½ìš°
        """
        try:
            # HeadBucketìœ¼ë¡œ ë²„í‚· ì¡´ì¬ ë° ì ‘ê·¼ ê¶Œí•œ í™•ì¸
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            logger.debug(f"âœ“ S3 ë²„í‚· ì ‘ê·¼ í™•ì¸ ì™„ë£Œ: {self.bucket_name}")
            return True
        except ClientError as e:
            error_code = e.response.get("Error", {}).get("Code", "Unknown")

            if error_code == "403":
                raise LogDownloadError(
                    f"âŒ S3 ë²„í‚· '{self.bucket_name}'ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.\n\n"
                    f"   ê°€ëŠ¥í•œ ì›ì¸:\n"
                    f"   1. í˜„ì¬ ì—­í• /ì‚¬ìš©ìì—ê²Œ "
                    f"s3:GetObject, s3:ListBucket ê¶Œí•œì´ ì—†ìŒ\n"
                    f"   2. S3 ë²„í‚· ì •ì±…ì´ í˜„ì¬ ê³„ì •/ì—­í• ì˜ ì ‘ê·¼ì„ ê±°ë¶€\n"
                    f"   3. ë²„í‚·ì´ ë‹¤ë¥¸ AWS ê³„ì •ì— ìˆê³  "
                    f"í¬ë¡œìŠ¤ ê³„ì • ì ‘ê·¼ ì„¤ì •ì´ ì•ˆ ë¨\n\n"
                    f"   í•´ê²° ë°©ë²•:\n"
                    f"   - S3 ë²„í‚·ì´ ìˆëŠ” ê³„ì •ì—ì„œ ì§ì ‘ ì ‘ê·¼í•˜ê±°ë‚˜\n"
                    f"   - S3 ë²„í‚· ì •ì±…ì— í˜„ì¬ ì—­í• ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œ ì¶”ê°€ í•„ìš”"
                ) from e
            elif error_code == "404":
                raise LogDownloadError(
                    f"âŒ S3 ë²„í‚· '{self.bucket_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n   ë²„í‚· ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                ) from e
            else:
                raise LogDownloadError(
                    f"âŒ S3 ë²„í‚· '{self.bucket_name}' ì ‘ê·¼ ì˜¤ë¥˜: {error_code}\n   ìƒì„¸: {str(e)}"
                ) from e
        except Exception as e:
            raise LogDownloadError(f"âŒ S3 ë²„í‚· '{self.bucket_name}' ì ‘ê·¼ í™•ì¸ ì‹¤íŒ¨: {str(e)}") from e

    def download_logs(self) -> list[str]:
        """ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ S3ì—ì„œ ë¡œê·¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # ğŸ” S3 ë²„í‚· ì ‘ê·¼ ê¶Œí•œ ì‚¬ì „ ê²€ì¦
            self._verify_s3_access()

            # ğŸ—‚ï¸ ì„ì‹œ ë””ë ‰í„°ë¦¬ ì •ë¦¬ (ê¸°ì¡´ íŒŒì¼ ëª¨ë‘ ì‚­ì œ)
            logger.debug("âœ“ ë””ë ‰í„°ë¦¬ ì •ë¦¬ ì‹œì‘ (gz + log)")
            self._clean_directory(self.temp_dir)  # gz íŒŒì¼ ë””ë ‰í„°ë¦¬
            self._clean_directory(self.decompressed_dir)  # log íŒŒì¼ ë””ë ‰í„°ë¦¬
            logger.debug("âœ“ ë””ë ‰í„°ë¦¬ ì •ë¦¬ ì™„ë£Œ (gz + log)")

            # ë‚ ì§œ ë²”ìœ„ ìƒì„±
            date_prefixes = self._smart_date_range_optimization()

            if not date_prefixes:
                self.console.print("[yellow]âš ï¸ ìƒì„±ëœ ë‚ ì§œë³„ ì ‘ë‘ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
                return []

            # S3ì—ì„œ ë¡œê·¸ íŒŒì¼ ê²€ìƒ‰ ë° ì‹œê°„ í•„í„°ë§
            all_log_files = self._get_log_files_from_s3(date_prefixes)

            # ì „ì²´ ë°œê²¬ íŒŒì¼ ìˆ˜ ë¡œê·¸
            if all_log_files:
                self.console.print(
                    f"[cyan]âœ“ S3ì—ì„œ ì´ {len(all_log_files)}ê°œ íŒŒì¼ ë°œê²¬, ì‹œê°„ ë²”ìœ„ë¡œ í•„í„°ë§ ì¤‘...[/cyan]"
                )

            filtered_files = self._binary_search_time_filter(all_log_files)

            if not filtered_files:
                # ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ (ê°€ëŠ¥í•˜ë©´ ì‹¤ì œ ê°€ëŠ¥í•œ ë²”ìœ„ì™€ ê¶Œì¥ êµ¬ê°„ í¬í•¨)
                base_msg = f"[yellow]ì‹œê°„ ë²”ìœ„ ({self.start_datetime} ~ {self.end_datetime}) ë‚´ì— ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.[/yellow]"
                self.console.print(base_msg)
                if self.available_range_local:
                    earliest_local, latest_local = self.available_range_local
                    try:
                        suggest_start = latest_local - timedelta(minutes=10)
                        suggest_end = latest_local
                        self.console.print(
                            f"[yellow]- ì‹¤ì œ ë¡œê·¸ ë²”ìœ„({self.timezone.zone}): {earliest_local} ~ {latest_local}[/yellow]"
                        )
                        self.console.print(
                            f"[yellow]- ê¶Œì¥ ì¬ì‹œë„: {suggest_start} ~ {suggest_end} ë˜ëŠ” ë²”ìœ„ë¥¼ ë„“í˜€ ì¬ì‹œë„[/yellow]"
                        )
                    except Exception:
                        self.console.print(
                            f"[yellow]- ì‹¤ì œ ë¡œê·¸ ë²”ìœ„({self.timezone.zone}): {earliest_local} ~ {latest_local}[/yellow]"
                        )
                return []

            total_files = len(filtered_files)
            total_size = sum(f.size for f in filtered_files)
            avg_file_size = total_size / total_files if total_files > 0 else 0

            # ì ì‘í˜• ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            adaptive_batch_size = self._adaptive_batch_size(total_files, avg_file_size)

            self.console.print(
                f"[green]âœ“ í•„í„°ë§ ì™„ë£Œ: {total_files}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘ "
                f"(ì´ í¬ê¸°: {total_size / 1024 / 1024:.1f}MB)[/green]"
            )

            # íŒŒì¼ í¬ê¸°ë³„ ì •ë ¬ (í° íŒŒì¼ ë¨¼ì € - ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)
            filtered_files.sort(key=lambda x: x.size, reverse=True)

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‹¤ìš´ë¡œë“œ
            downloaded_files = []

            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TimeElapsedColumn(),
                console=self.console,
            ) as progress:
                download_task = progress.add_task(
                    f"[cyan]ë¡œê·¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ (0/{total_files})...",
                    total=total_files,
                )

                # ì ì‘í˜• ë°°ì¹˜ë¡œ ë‹¤ìš´ë¡œë“œ
                for i in range(0, total_files, adaptive_batch_size):
                    batch = filtered_files[i : i + adaptive_batch_size]
                    batch_size_mb = sum(f.size for f in batch) / 1024 / 1024

                    logger.debug(f"ë°°ì¹˜ {i // adaptive_batch_size + 1}: {len(batch)}ê°œ íŒŒì¼, {batch_size_mb:.1f}MB")

                    with concurrent.futures.ThreadPoolExecutor(
                        max_workers=min(self.max_workers, len(batch))
                    ) as executor:
                        future_to_file = {
                            executor.submit(
                                self._download_single_file,
                                f.key,
                                progress,
                                download_task,
                            ): f
                            for f in batch
                        }

                        for future in concurrent.futures.as_completed(future_to_file):
                            log_file = future_to_file[future]
                            try:
                                result = future.result()
                                if result:
                                    downloaded_files.append(result)
                                progress.update(download_task, advance=1)
                            except Exception as e:
                                logger.error(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({log_file.key}): {str(e)}")
                                progress.update(download_task, advance=1)

                    # ë°°ì¹˜ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()

            if not downloaded_files:
                raise LogDownloadError("âŒ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            self.console.print(f"[green]âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(downloaded_files)}ê°œ íŒŒì¼[/green]")
            return downloaded_files

        except Exception as e:
            logger.error(f"âŒ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise LogDownloadError(f"ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}") from e

    def _download_single_file(self, key: str, progress: Progress, task_id: Any) -> str | None:
        """ë‹¨ì¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # ğŸ”§ ìˆ˜ì •: ëª¨ë“  íŒŒì¼ì„ í•œ í´ë”ì— ì €ì¥ (ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ë¶„ë¦¬ ì œê±°)
            filename = os.path.basename(key)
            local_path = os.path.join(self.temp_dir, filename)

            # Path traversal ë°©ì§€: ê²½ë¡œê°€ temp_dir ë‚´ì— ìˆëŠ”ì§€ ê²€ì¦
            resolved_path = Path(local_path).resolve()
            temp_dir_resolved = Path(self.temp_dir).resolve()
            if not str(resolved_path).startswith(str(temp_dir_resolved)):
                logger.warning(f"Path traversal ì‹œë„ ê°ì§€: {key}")
                return None

            # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
            if os.path.exists(local_path):
                file_size = os.path.getsize(local_path)
                logger.debug(f"íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬: {filename} ({file_size / 1024 / 1024:.2f}MB)")
                return local_path

            try:
                # ğŸš€ Connection pool ìµœì í™”ëœ TransferConfig ì‚¬ìš© (auth ì„¸ì…˜ ìœ ì§€)
                from boto3.s3.transfer import TransferConfig

                # TransferConfig ì„¤ì • - Connection pool ê²½ê³  í•´ê²°
                transfer_config = TransferConfig(
                    multipart_threshold=self.chunk_size,
                    max_concurrency=10,  # auth session pool(10ê°œ)
                    multipart_chunksize=self.chunk_size,
                    use_threads=True,
                    max_io_queue=1000,  # I/O í í¬ê¸° ì¦ê°€
                    io_chunksize=262144,  # I/O ì²­í¬ í¬ê¸° ìµœì í™” (256KB)
                    num_download_attempts=3,  # ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ íšŸìˆ˜
                )

                with open(local_path, "wb") as f:
                    # authì—ì„œ ë°›ì€ ì¸ì¦ëœ S3 í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (ì„¸ì…˜ ìœ ì§€)
                    self.s3_client.download_fileobj(
                        Bucket=self.bucket_name,
                        Key=key,
                        Fileobj=f,
                        Config=transfer_config,  # ìµœì í™”ëœ TransferConfig ì‚¬ìš© â­
                    )

                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(local_path)
                progress.update(
                    task_id,
                    description="[cyan]ë‹¤ìš´ë¡œë“œ ì§„í–‰ì¤‘...",
                )

            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({key}): {str(e)}")
                if os.path.exists(local_path):
                    os.remove(local_path)
                return None

            return local_path

        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({key}): {str(e)}")
            return None

    def decompress_logs(self, gz_directory: str | None = None) -> str:
        """ì••ì¶•ëœ ë¡œê·¸ íŒŒì¼ì„ í•´ì œí•©ë‹ˆë‹¤."""
        if gz_directory is None:
            gz_directory = self.temp_dir

        try:
            logger.debug(f"ğŸ“‚ ì••ì¶• í•´ì œ ì‹œì‘ - gz ë””ë ‰í† ë¦¬: {gz_directory}")

            # gz íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²€ì¦
            if not os.path.exists(gz_directory):
                logger.error(f"âŒ gz ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {gz_directory}")
                raise FileNotFoundError(f"gz ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {gz_directory}")

            # gz íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            gz_files = []
            for root, _, files in os.walk(gz_directory):
                for file in files:
                    if file.endswith(".gz"):
                        gz_files.append(os.path.join(root, file))

            if not gz_files:
                logger.error("âŒ ì••ì¶•ëœ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                raise FileNotFoundError("ì••ì¶•ëœ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            logger.debug(f"ğŸ“¦ ë°œê²¬ëœ gz íŒŒì¼: {len(gz_files)}ê°œ")

            # í•´ì œëœ ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„± (ì¤‘ë³µ ì •ë¦¬ ë°©ì§€)
            self._clean_directory(self.decompressed_dir)
            logger.debug(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±: {self.decompressed_dir}")

            # gz íŒŒì¼ í•´ì œ
            decompressed_files = []
            logger.debug(f"ğŸ”§ ì••ì¶• í•´ì œ ì‹œì‘: {len(gz_files)}ê°œ íŒŒì¼")

            for gz_file_path in gz_files:
                # ğŸ”§ ìˆ˜ì •: ëª¨ë“  ë¡œê·¸ íŒŒì¼ì„ í•œ í´ë”ì— ì €ì¥ (ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ë¶„ë¦¬ ì œê±°)
                # ì••ì¶• í•´ì œí•  íŒŒì¼ ê²½ë¡œ
                log_file_path = os.path.join(
                    self.decompressed_dir, os.path.basename(gz_file_path)[:-3]
                )  # .gz í™•ì¥ì ì œê±°

                try:
                    # ì••ì¶• í•´ì œ ì§„í–‰
                    with (
                        gzip.open(gz_file_path, "rb") as gz_file,
                        open(log_file_path, "wb") as log_file,
                    ):
                        shutil.copyfileobj(gz_file, log_file)

                    # ê°œë³„ íŒŒì¼ ë¡œê·¸ë¥¼ DEBUGë¡œ ë³€ê²½ (í„°ë¯¸ë„ ì¶œë ¥ ì •ë¦¬)
                    logger.debug(
                        f"âœ“ ì••ì¶• í•´ì œ ì™„ë£Œ: {os.path.basename(gz_file_path)} -> {os.path.basename(log_file_path)}"
                    )

                    decompressed_files.append(log_file_path)

                except Exception as e:
                    logger.error(f"âŒ ì••ì¶• í•´ì œ ì‹¤íŒ¨ {os.path.basename(gz_file_path)}: {str(e)}")
                    continue

            # í•´ì œëœ ë¡œê·¸ íŒŒì¼ ê²€ì¦
            log_files = []
            for root, _, files in os.walk(self.decompressed_dir):
                for file in files:
                    if file.endswith(".log"):
                        log_files.append(os.path.join(root, file))

            if not log_files:
                logger.error("âŒ ì••ì¶• í•´ì œëœ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                raise FileNotFoundError("ì••ì¶• í•´ì œëœ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            # ì••ì¶• í•´ì œ ì™„ë£Œ í›„ ìš”ì•½ ì¶œë ¥ â­
            total_files = len(decompressed_files)
            if total_files > 0:
                logger.debug(f"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ: {total_files}ê°œ íŒŒì¼")
            else:
                logger.warning("âš ï¸ ì••ì¶• í•´ì œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"âŒ ì••ì¶• í•´ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return ""

        logger.debug(
            f"âœ… ì´ {len(decompressed_files) if 'decompressed_files' in locals() else 0}ê°œì˜ ë¡œê·¸ íŒŒì¼ì´ ì••ì¶• í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        )
        logger.debug(f"ğŸ“ ì••ì¶• í•´ì œ ë””ë ‰í† ë¦¬: {self.decompressed_dir}")
        return self.decompressed_dir

    def _clean_directory(self, directory: str) -> None:
        """ğŸ“ ë””ë ‰í† ë¦¬ ë‚´ë¶€ íŒŒì¼ë§Œ ì •ë¦¬ (ë””ë ‰í† ë¦¬ ìì²´ëŠ” ìœ ì§€)"""
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±ë§Œ
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.debug(f"âœ“ ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")
                return

            # ğŸ“ ë””ë ‰í† ë¦¬ ë‚´ë¶€ íŒŒì¼ê³¼ í•˜ìœ„ ë””ë ‰í† ë¦¬ë§Œ ì‚­ì œ (ë””ë ‰í† ë¦¬ ìì²´ëŠ” ìœ ì§€)
            for root, dirs, files in os.walk(directory, topdown=False):
                # íŒŒì¼ ì‚­ì œ
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        os.unlink(file_path)
                    except Exception as e:
                        logger.debug(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {file_path}, ì˜¤ë¥˜: {e}")

                # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì‚­ì œ (root ë””ë ‰í† ë¦¬ëŠ” ìœ ì§€)
                for dir_name in dirs:
                    dir_path = os.path.join(root, dir_name)
                    try:
                        os.rmdir(dir_path)
                    except Exception as e:
                        logger.debug(f"í•˜ìœ„ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {dir_path}, ì˜¤ë¥˜: {e}")

            logger.debug(f"âœ“ ë””ë ‰í† ë¦¬ ë‚´ë¶€ ì •ë¦¬ ì™„ë£Œ: {directory}")

        except Exception as e:
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {directory}, ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨í•´ë„ ë””ë ‰í„°ë¦¬ ìƒì„± ì‹œë„
            with contextlib.suppress(Exception):
                os.makedirs(directory, exist_ok=True)

    def _get_log_files_from_s3(self, date_prefixes: list[str]) -> list[S3LogFile]:
        """S3ì—ì„œ ë¡œê·¸ íŒŒì¼ ëª©ë¡ì„ ê°„ë‹¨í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        self.console.print("[blue]ğŸ“‹ S3ì—ì„œ ë¡œê·¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘...[/blue]")

        all_log_files = []

        # ë‚ ì§œë³„ ì ‘ë‘ì‚¬ ë³‘ë ¬ ì²˜ë¦¬ (Progress bar ì œê±°)
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(self.max_workers, len(date_prefixes))) as executor:
            future_to_prefix = {
                executor.submit(self._list_objects_for_prefix_simple, prefix): prefix for prefix in date_prefixes
            }

            for future in concurrent.futures.as_completed(future_to_prefix):
                prefix = future_to_prefix[future]
                try:
                    files = future.result()
                    all_log_files.extend(files)
                except Exception as e:
                    logger.error(f"âŒ ë¡œê·¸ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨ ({prefix}): {str(e)}")

        return all_log_files
