from typing import Dict, Optional, Any, List, TYPE_CHECKING
from pathlib import Path
import logging

from sagemaker.workflow.steps import ProcessingStep, Step
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.sklearn import SKLearnProcessor

from ...core.base.builder_base import StepBuilderBase
from ...core.deps.registry_manager import RegistryManager
from ...core.deps.dependency_resolver import UnifiedDependencyResolver

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    from ..configs.config_payload_step import PayloadConfig

# Import the payload specification
try:
    from ..specs.payload_spec import PAYLOAD_SPEC

    SPEC_AVAILABLE = True
except ImportError:
    PAYLOAD_SPEC = None
    SPEC_AVAILABLE = False

logger = logging.getLogger(__name__)


class PayloadStepBuilder(StepBuilderBase):
    """
    Builder for a MIMS Payload Generation ProcessingStep.

    This implementation uses the specification-driven approach where dependencies, outputs,
    and script contract are defined in the payload specification.
    """

    def __init__(
        self,
        config: "PayloadConfig",
        sagemaker_session=None,
        role: Optional[str] = None,
        registry_manager: Optional["RegistryManager"] = None,
        dependency_resolver: Optional["UnifiedDependencyResolver"] = None,
    ):
        """
        Initializes the builder with a specific configuration for the MIMS payload step.

        Args:
            config: A PayloadConfig instance containing all necessary settings.
            sagemaker_session: The SageMaker session object to manage interactions with AWS.
            role: The IAM role ARN to be used by the SageMaker Processing Job.
            registry_manager: Optional registry manager for dependency injection
            dependency_resolver: Optional dependency resolver for dependency injection
        """
        # Import at runtime to avoid circular imports
        from ..configs.config_payload_step import PayloadConfig

        if not isinstance(config, PayloadConfig):
            raise ValueError("PayloadStepBuilder requires a PayloadConfig instance.")

        # Use the payload specification if available
        spec = PAYLOAD_SPEC if SPEC_AVAILABLE else None

        super().__init__(
            config=config,
            spec=spec,
            sagemaker_session=sagemaker_session,
            role=role,
            registry_manager=registry_manager,
            dependency_resolver=dependency_resolver,
        )
        self.config: PayloadConfig = config

    def validate_configuration(self) -> None:
        """
        Validates the provided configuration to ensure all required fields for this
        specific step are present and valid before attempting to build the step.

        Raises:
            ValueError: If any required configuration is missing or invalid.
        """
        self.log_info("Validating PayloadConfig...")

        # Make sure bucket is set
        if not hasattr(self.config, "bucket") or not self.config.bucket:
            raise ValueError("PayloadConfig missing required attribute: bucket")

        # Note: sample_payload_s3_key validation removed since it's not used by the script

        # Validate other required attributes
        required_attrs = [
            "pipeline_name",
            "source_model_inference_content_types",
            "processing_instance_count",
            "processing_volume_size",
        ]

        for attr in required_attrs:
            if not hasattr(self.config, attr) or getattr(self.config, attr) in [
                None,
                "",
            ]:
                raise ValueError(f"PayloadConfig missing required attribute: {attr}")

        self.log_info("PayloadConfig validation succeeded.")

    def _create_processor(self) -> SKLearnProcessor:
        """
        Creates and configures the SKLearnProcessor for the SageMaker Processing Job.
        This defines the execution environment for the script, including the instance
        type, framework version, and environment variables.

        Returns:
            An instance of sagemaker.sklearn.SKLearnProcessor.
        """
        # Use processing_instance_type_large when use_large_processing_instance is True
        # Otherwise use processing_instance_type_small
        instance_type = (
            self.config.processing_instance_type_large
            if self.config.use_large_processing_instance
            else self.config.processing_instance_type_small
        )

        # Get framework version
        framework_version = getattr(
            self.config, "processing_framework_version", "1.0-1"
        )

        return SKLearnProcessor(
            framework_version=framework_version,
            role=self.role,
            instance_type=instance_type,
            instance_count=self.config.processing_instance_count,
            volume_size_in_gb=self.config.processing_volume_size,
            base_job_name=self._generate_job_name(),  # Use standardized method with auto-detection
            sagemaker_session=self.session,
            env=self._get_environment_variables(),
        )

    def _get_environment_variables(self) -> Dict[str, str]:
        """
        Constructs a dictionary of environment variables to be passed to the processing job.
        Uses the script contract from the payload specification to determine which
        environment variables to include, following the Single Source of Truth principle.

        Returns:
            A dictionary of environment variables.
        """
        # Get base environment variables from contract
        env_vars = super()._get_environment_variables()

        # Add payload-specific environment variables that may need special handling

        # For content types, use the config if available
        if hasattr(self.config, "source_model_inference_content_types"):
            env_vars["CONTENT_TYPES"] = ",".join(
                self.config.source_model_inference_content_types
            )

        # For numeric default, use config if available
        if hasattr(self.config, "default_numeric_value"):
            env_vars["DEFAULT_NUMERIC_VALUE"] = str(self.config.default_numeric_value)

        # For text default, use config if available
        if hasattr(self.config, "default_text_value"):
            env_vars["DEFAULT_TEXT_VALUE"] = str(self.config.default_text_value)

        # NEW: Unified FIELD_DEFAULTS as JSON string
        if hasattr(self.config, "field_defaults") and self.config.field_defaults:
            import json

            env_vars["FIELD_DEFAULTS"] = json.dumps(self.config.field_defaults)
            self.log_info(
                "Added FIELD_DEFAULTS for %d fields", len(self.config.field_defaults)
            )

        self.log_info("Payload environment variables configured")
        return env_vars

    def _get_inputs(self, inputs: Dict[str, Any]) -> List[ProcessingInput]:
        """
        Get inputs for the step using specification and contract.
        Adds support for custom_payload_path from config.

        This method creates ProcessingInput objects for each dependency defined in the specification.

        Args:
            inputs: Input data sources keyed by logical name

        Returns:
            List of ProcessingInput objects

        Raises:
            ValueError: If no specification or contract is available
        """
        # NEW: Check if user provided custom payload path in config
        # If so, add it to inputs dict before processing (supports S3 or local paths)
        if (
            hasattr(self.config, "custom_payload_path")
            and self.config.custom_payload_path
        ):
            # Add custom_payload_input to inputs dict
            inputs["custom_payload_input"] = self.config.custom_payload_path
            self.log_info(
                "Using custom payload from config: %s", self.config.custom_payload_path
            )

        if not self.spec:
            raise ValueError("Step specification is required")

        if not self.contract:
            raise ValueError("Script contract is required for input mapping")

        processing_inputs = []

        # Process each dependency in the specification
        for _, dependency_spec in self.spec.dependencies.items():
            logical_name = dependency_spec.logical_name

            # Skip if optional and not provided
            if not dependency_spec.required and logical_name not in inputs:
                continue

            # Make sure required inputs are present
            if dependency_spec.required and logical_name not in inputs:
                raise ValueError(f"Required input '{logical_name}' not provided")

            # Get container path from contract
            container_path = None
            if logical_name in self.contract.expected_input_paths:
                container_path = self.contract.expected_input_paths[logical_name]
            else:
                raise ValueError(f"No container path found for input: {logical_name}")

            # Use the input value directly - property references are handled by PipelineAssembler
            processing_inputs.append(
                ProcessingInput(
                    input_name=logical_name,
                    source=inputs[logical_name],
                    destination=container_path,
                )
            )

        return processing_inputs

    def _get_outputs(self, outputs: Dict[str, Any]) -> List[ProcessingOutput]:
        """
        Get outputs for the step using specification and contract.

        This method creates ProcessingOutput objects for each output defined in the specification.

        Args:
            outputs: Output destinations keyed by logical name

        Returns:
            List of ProcessingOutput objects

        Raises:
            ValueError: If no specification or contract is available
        """
        if not self.spec:
            raise ValueError("Step specification is required")

        if not self.contract:
            raise ValueError("Script contract is required for output mapping")

        processing_outputs = []

        # Process each output in the specification
        for _, output_spec in self.spec.outputs.items():
            logical_name = output_spec.logical_name

            # Get container path from contract
            container_path = None
            if logical_name in self.contract.expected_output_paths:
                container_path = self.contract.expected_output_paths[logical_name]
            else:
                raise ValueError(f"No container path found for output: {logical_name}")

            # Try to find destination in outputs
            destination = None

            # Look in outputs by logical name
            if logical_name in outputs:
                destination = outputs[logical_name]
            else:
                # Generate destination using base output path and Join for parameter compatibility
                from sagemaker.workflow.functions import Join

                base_output_path = self._get_base_output_path()
                destination = Join(
                    on="/", values=[base_output_path, "payload", logical_name]
                )
                self.log_info(
                    "Using generated destination for '%s': %s",
                    logical_name,
                    destination,
                )

            processing_outputs.append(
                ProcessingOutput(
                    output_name=logical_name,
                    source=container_path,
                    destination=destination,
                )
            )

        return processing_outputs

    def _get_job_arguments(self) -> Optional[List[str]]:
        """
        Returns None as job arguments since the payload script now uses
        standard paths defined directly in the script.

        Returns:
            None since no arguments are needed (unless overridden by config)
        """
        # If there are custom script arguments in the config, use those
        if (
            hasattr(self.config, "processing_script_arguments")
            and self.config.processing_script_arguments
        ):
            self.log_info(
                "Using custom script arguments from config: %s",
                self.config.processing_script_arguments,
            )
            return self.config.processing_script_arguments

        # Otherwise, no arguments are needed
        self.log_info("No command-line arguments needed for payload script")
        return None

    def create_step(self, **kwargs) -> ProcessingStep:
        """
        Creates the final, fully configured SageMaker ProcessingStep for the pipeline
        using the specification-driven approach.

        Args:
            **kwargs: Keyword arguments for configuring the step, including:
                - inputs: Input data sources keyed by logical name
                - outputs: Output destinations keyed by logical name
                - dependencies: Optional list of steps that this step depends on
                - enable_caching: A boolean indicating whether to cache the results of this step

        Returns:
            A configured sagemaker.workflow.steps.ProcessingStep instance.
        """
        self.log_info("Creating MIMS Payload ProcessingStep...")

        # Extract parameters
        inputs_raw = kwargs.get("inputs", {})
        outputs = kwargs.get("outputs", {})
        dependencies = kwargs.get("dependencies", [])
        enable_caching = kwargs.get("enable_caching", True)

        # Handle inputs
        inputs = {}

        # If dependencies are provided, extract inputs from them
        if dependencies:
            try:
                extracted_inputs = self.extract_inputs_from_dependencies(dependencies)
                inputs.update(extracted_inputs)
            except Exception as e:
                self.log_warning("Failed to extract inputs from dependencies: %s", e)

        # Add explicitly provided inputs (overriding any extracted ones)
        inputs.update(inputs_raw)

        # Create processor and get inputs/outputs
        processor = self._create_processor()
        proc_inputs = self._get_inputs(inputs)
        proc_outputs = self._get_outputs(outputs)
        job_args = self._get_job_arguments()

        # Get step name using standardized method with auto-detection
        step_name = self._get_step_name()

        # Get script path using modernized method with comprehensive fallbacks
        script_path = self.config.get_script_path()
        self.log_info("Using script path: %s", script_path)

        # Create step
        step = ProcessingStep(
            name=step_name,
            processor=processor,
            inputs=proc_inputs,
            outputs=proc_outputs,
            code=script_path,
            job_arguments=job_args,
            depends_on=dependencies,
            cache_config=self._get_cache_config(enable_caching),
        )

        # Attach specification to the step for future reference
        if hasattr(self, "spec") and self.spec:
            setattr(step, "_spec", self.spec)

        self.log_info("Created ProcessingStep with name: %s", step.name)
        return step
