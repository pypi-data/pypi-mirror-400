/*
 * codegen.c - C code generator
 * 
 * Generates single-file C99 inference code with embedded weights.
 */

#include "codegen.h"
#include <stdlib.h>
#include <string.h>
#include <stdarg.h>
#include <ctype.h>
#include <time.h>

// NOTE: Keep in sync with pyproject.toml
#define NNCC_VERSION "2.0.8"

// Configuration

void codegen_config_init(CodegenConfig* config) {
    memset(config, 0, sizeof(CodegenConfig));
    config->model_name = "model";
    config->simd_level = SIMD_NONE;
    config->use_libc_math = true;
    config->inline_weights = true;
    config->emit_comments = true;
    config->indent_spaces = 4;
}

// Output Helpers
static void cg_write(CodegenContext* ctx, const char* fmt, ...);

static void cg_printf(CodegenContext* ctx, const char* fmt, ...) {
    for (int i = 0; i < ctx->indent * ctx->config->indent_spaces; i++) {
        cg_write(ctx, " ");
    }
    va_list args;
    va_start(args, fmt);
    char buf[16384];  // Large buffer for weight arrays
    vsnprintf(buf, sizeof(buf), fmt, args);
    va_end(args);
    cg_write(ctx, "%s", buf);
}

static void cg_write(CodegenContext* ctx, const char* fmt, ...) {
    va_list args;
    
    if (ctx->out) {
        va_start(args, fmt);
        vfprintf(ctx->out, fmt, args);
        va_end(args);
    } else {
        // Measure size
        va_start(args, fmt);
        size_t len = vsnprintf(NULL, 0, fmt, args);
        va_end(args);
        
        if (ctx->buffer) {
             if (ctx->buffer_len + len + 1 > ctx->buffer_cap) {
                 size_t new_cap = ctx->buffer_cap * 2;
                 if (new_cap < ctx->buffer_len + len + 1) new_cap = ctx->buffer_len + len + 1 + 1024;
                 char* new_buf = (char*)realloc(ctx->buffer, new_cap);
                 if (new_buf) {
                     ctx->buffer = new_buf;
                     ctx->buffer_cap = new_cap;
                 } else {
                     ctx->error = true;
                     return;
                 }
             }
             va_start(args, fmt);
             vsnprintf(ctx->buffer + ctx->buffer_len, len + 1, fmt, args);
             va_end(args);
        }
        
        ctx->buffer_len += len;
    }
}

static void cg_indent(CodegenContext* ctx) { ctx->indent++; }
static void cg_dedent(CodegenContext* ctx) { if (ctx->indent > 0) ctx->indent--; }

static void cg_make_ident(const char* name, char* out, size_t size) {
    size_t j = 0;
    for (size_t i = 0; name[i] && j < size - 1; i++) {
        char c = name[i];
        if (isalnum((unsigned char)c)) out[j++] = c;
        else if (c == '.' || c == '/' || c == '-' || c == ' ') out[j++] = '_';
    }
    out[j] = '\0';
}

// Prototypes
static void emit_header(CodegenContext* ctx);
static void emit_footer(CodegenContext* ctx);
static void emit_weights(CodegenContext* ctx);
static void emit_scratch(CodegenContext* ctx);
static void emit_kernels(CodegenContext* ctx);
static void emit_inference(CodegenContext* ctx);

// Header Generation

static void emit_header(CodegenContext* ctx) {
    const char* name = ctx->config->model_name;
    char upper[128];
    size_t len = strlen(name);
    for (size_t i = 0; i < len && i < sizeof(upper) - 1; i++) {
        upper[i] = toupper((unsigned char)name[i]);
    }
    upper[len < sizeof(upper) - 1 ? len : sizeof(upper) - 1] = '\0';
    
    time_t now = time(NULL);
    struct tm* tm_now = localtime(&now);
    char timestamp[64];
    strftime(timestamp, sizeof(timestamp), "%Y-%m-%d %H:%M:%S", tm_now);
    
    cg_write(ctx, "/*\n");
    cg_write(ctx, " * %s.c - Neural network inference code\n", name);
    cg_write(ctx, " * Generated by nncc v%s on %s\n", NNCC_VERSION, timestamp);
    cg_write(ctx, " *\n");
    cg_write(ctx, " * Usage:\n");
    cg_write(ctx, " *   #define NNCC_IMPLEMENTATION\n");
    cg_write(ctx, " *   #include \"%s.c\"\n", name);
    cg_write(ctx, " *   %s_inference(input, output);\n", name);
    cg_write(ctx, " */\n\n");
    
    cg_write(ctx, "#ifndef %s_H\n", upper);
    cg_write(ctx, "#define %s_H\n\n", upper);
    
    cg_write(ctx, "#include <stdint.h>\n");
    cg_write(ctx, "#include <stddef.h>\n");
    if (ctx->config->use_libc_math) {
        cg_write(ctx, "#include <math.h>\n");
    }
    cg_write(ctx, "\n");
    
    /* API declaration */
    if (ctx->graph->num_inputs > 0 && ctx->graph->num_outputs > 0) {
        Tensor* in = ctx->graph->inputs[0];
        Tensor* out = ctx->graph->outputs[0];
        cg_write(ctx, "/* Input: [");
        for (int i = 0; i < in->ndim; i++) {
            if (i > 0) cg_write(ctx, ", ");
            cg_write(ctx, "%d", in->dims[i]);
        }
        cg_write(ctx, "] = %zu elements */\n", tensor_numel(in));
        
        cg_write(ctx, "/* Output: [");
        for (int i = 0; i < out->ndim; i++) {
            if (i > 0) cg_write(ctx, ", ");
            cg_write(ctx, "%d", out->dims[i]);
        }
        cg_write(ctx, "] = %zu elements */\n\n", tensor_numel(out));
    }
    
    cg_write(ctx, "void %s_inference(const float* input, float* output);\n\n", name);
    cg_write(ctx, "#endif /* %s_H */\n\n", upper);
    
    cg_write(ctx, "#if defined(%s_IMPLEMENTATION) || defined(NNCC_IMPLEMENTATION) || defined(MODEL_IMPLEMENTATION)\n\n", upper);
}

static void emit_footer(CodegenContext* ctx) {
    char upper[128];
    size_t len = strlen(ctx->config->model_name);
    for (size_t i = 0; i < len && i < sizeof(upper) - 1; i++) {
        upper[i] = toupper((unsigned char)ctx->config->model_name[i]);
    }
    upper[len < sizeof(upper) - 1 ? len : sizeof(upper) - 1] = '\0';
    
    cg_write(ctx, "#endif /* IMPLEMENTATION */\n");
}

// Weights Emission

static void emit_weights(CodegenContext* ctx) {
    const char* name = ctx->config->model_name;
    
    if (ctx->config->emit_comments) {
        cg_write(ctx, "/* Weights */\n\n");
    }
    
    for (int i = 0; i < ctx->graph->num_tensors; i++) {
        Tensor* t = ctx->graph->tensors[i];
        if (!t->is_weight || !t->data) continue;
        
        char ident[128];
        cg_make_ident(t->name, ident, sizeof(ident));
        
        size_t numel = tensor_numel(t);
        cg_write(ctx, "static const float %s_%s[%zu] = {\n", name, ident, numel);
        
        float* data = (float*)t->data;
        cg_indent(ctx);
        for (size_t j = 0; j < numel; j++) {
            if (j % 8 == 0 && j > 0) cg_write(ctx, "\n");
            if (j % 8 == 0) cg_printf(ctx, "");
            cg_write(ctx, "%.8ef", data[j]);
            if (j < numel - 1) cg_write(ctx, ", ");
        }
        cg_dedent(ctx);
        cg_write(ctx, "\n};\n\n");
    }
}

// Scratch Buffer



// Computes the last operation index that uses each tensor
static int* compute_tensor_liveness(const Graph* graph) {
    int* last_use = (int*)malloc(graph->num_tensors * sizeof(int));
    if (!last_use) return NULL;
    
    for (int i = 0; i < graph->num_tensors; i++) last_use[i] = -1;
    
    // Inputs and Weights are live forever (effectively) but we don't manage their memory here,
    // so we only care about intermediate activation tensors.
     
    for (int i = 0; i < graph->num_ops; i++) {
        Op* op = graph->ops[i];
        for (int j = 0; j < op->num_inputs; j++) {
            Tensor* t = op->inputs[j];
            // Update last use of input tensor
            if (last_use[t->id] < i) last_use[t->id] = i;
        }
        // Outputs are created at this op, so they are live at least here
        for (int j = 0; j < op->num_outputs; j++) {
            Tensor* t = op->outputs[j];
            if (last_use[t->id] < i) last_use[t->id] = i;
        }
    }
    return last_use;
}

typedef struct Block {
    int offset;
    int size;
    struct Block* next;
} Block;

// Greedy memory allocator that reuses memory of dead tensors
static int* allocate_scratch_greedy(const Graph* graph, int* out_total_size) {
    int* offsets = (int*)calloc(graph->num_tensors, sizeof(int));
    int* last_use = compute_tensor_liveness(graph);
    
    if (!offsets || !last_use) {
        free(offsets);
        free(last_use);
        return NULL;
    }
    
    // Track allocated blocks: TensorID -> {offset, size} (using arrays for simple map)
    // Implementation uses a sorted list of free blocks to manage memory holes.
    
    Block* free_list = NULL; // Sorted by offset? Size? Address order usually good for coalescing.
    int watermark = 0;
    
    // We process tensors in order of creation (which is Topological order of ops basically)
    // Actually we need to process Ops, and for each Op, allocate its Output tensors,
    // and deallocate its Input tensors (if they die).
     
    // Initialize offsets to -1
    for (int i = 0; i < graph->num_tensors; i++) offsets[i] = -1;
    
    for (int i = 0; i < graph->num_ops; i++) {
        Op* op = graph->ops[i];
        
        // 1. Allocate Outputs
        for (int j = 0; j < op->num_outputs; j++) {
            Tensor* t = op->outputs[j];
            if (t->is_weight || t->is_input || t->is_output) continue; // managed externally
            if (offsets[t->id] != -1) continue; // Already allocated? (Shouldn't happen in SSA-like)
            
            // In-place check: if Op supports in-place and input dies here, reuse input directly
            bool allocated = false;
            bool is_inplace_op = (op->type == OP_RELU || op->type == OP_RELU6 || 
                                  op->type == OP_SIGMOID || op->type == OP_TANH ||
                                  op->type == OP_DROPOUT);
            
            if (is_inplace_op && op->num_inputs > 0 && j == 0) {
                 Tensor* in = op->inputs[0];
                 // If input is intermediate, computed, and this is its last use
                 if (!in->is_weight && !in->is_input && !in->is_output && last_use[in->id] == i) {
                     offsets[t->id] = offsets[in->id];
                     // Extend life of this block to t's life (which is effectively just renaming the block ownership).
                     // Set last_use[in->id] to -1 to indicate it shouldn't be freed yet.
                     last_use[in->id] = -1; 
                     allocated = true;
                 }
            }
            
            if (!allocated) {
                int size = (int)tensor_numel(t);
                // Try to find best fit in free list
                Block* prev = NULL;
                Block* curr = free_list;
                Block* best = NULL;
                Block* best_prev = NULL;
                int min_waste = 2147483647;
                
                while (curr) {
                    if (curr->size >= size) {
                        int waste = curr->size - size;
                        if (waste < min_waste) {
                            min_waste = waste;
                            best = curr;
                            best_prev = prev;
                        }
                    }
                    prev = curr;
                    curr = curr->next;
                }
                
                if (best) {
                    // Use this block
                    offsets[t->id] = best->offset;
                    // If excessive waste, split? Simple greedy: assume blocks coalesce on free.
                    if (best->size > size) {
                        // shrink the free block
                        best->offset += size;
                        best->size -= size;
                    } else {
                        // remove exactly matching block
                        if (best_prev) best_prev->next = best->next;
                        else free_list = best->next;
                        free(best);
                    }
                } else {
                    // No fit, expand heap
                    offsets[t->id] = watermark;
                    watermark += size;
                }
            }
        }
        
        // 2. Deallocate Inputs (if they die)
        for (int j = 0; j < op->num_inputs; j++) {
            Tensor* t = op->inputs[j];
            if (t->is_weight || t->is_input || t->is_output) continue;
            
            if (last_use[t->id] == i) {
                // Tensor dies here, free its block
                // Don't free if it was never allocated (size 0?)
                if (offsets[t->id] == -1) continue; // Should not happen for valid tensors
                
                int off = offsets[t->id];
                int sz = (int)tensor_numel(t);
                
                /* Add to free list (sorted by offset to enable coalescing) */
                Block* new_block = (Block*)malloc(sizeof(Block));
                new_block->offset = off;
                new_block->size = sz;
                
                Block* prev = NULL;
                Block* curr = free_list;
                while (curr && curr->offset < off) {
                    prev = curr;
                    curr = curr->next;
                }
                
                /* Insert between prev and curr */
                new_block->next = curr;
                if (prev) prev->next = new_block;
                else free_list = new_block;
                
                /* Coalesce with next */
                if (new_block->next && (new_block->offset + new_block->size == new_block->next->offset)) {
                    Block* next = new_block->next;
                    new_block->size += next->size;
                    new_block->next = next->next;
                    free(next);
                }
                
                /* Coalesce with prev */
                if (prev && (prev->offset + prev->size == new_block->offset)) {
                    prev->size += new_block->size;
                    prev->next = new_block->next;
                    free(new_block);
                }
            }
        }
    }
    
    /* Cleanup free list */
    while (free_list) {
        Block* next = free_list->next;
        free(free_list);
        free_list = next;
    }
    
    free(last_use);
    *out_total_size = watermark;
    return offsets;
}

static void emit_scratch(CodegenContext* ctx) {
    // Calculated during inference emission now, or we can pre-calc.
    // Since we can't easily pass state between functions in this rigid structure without changing headers widely,
    // we will calculate it here just to get the size, and recalculate (deterministically) later.
    // Or better, we store it in context if we could. But context is stack alloc in generate().
    // Let's just recalc. It's fast.
    int total_size = 0;
    int* offsets = allocate_scratch_greedy(ctx->graph, &total_size);
    if (offsets) {
        if (total_size > 0) {
            cg_write(ctx, "static float %s_scratch[%d];\n\n", ctx->config->model_name, total_size);
        }
        free(offsets);
    }
}

// Kernel Functions

static void emit_kernels(CodegenContext* ctx) {
    bool needs_matmul = false, needs_relu = false, needs_relu6 = false;
    bool needs_sigmoid = false, needs_tanh = false, needs_softmax = false;
    bool needs_conv2d = false, needs_maxpool = false, needs_avgpool = false;
    bool needs_global_avgpool = false, needs_add = false, needs_mul = false;
    bool needs_batchnorm = false, needs_flatten = false;
    
    for (int i = 0; i < ctx->graph->num_ops; i++) {
        switch (ctx->graph->ops[i]->type) {
            case OP_MATMUL: case OP_GEMM: needs_matmul = true; break;
            case OP_RELU: needs_relu = true; break;
            case OP_RELU6: needs_relu6 = true; break;
            case OP_SIGMOID: needs_sigmoid = true; break;
            case OP_TANH: needs_tanh = true; break;
            case OP_SOFTMAX: needs_softmax = true; break;
            case OP_CONV2D: needs_conv2d = true; break;
            case OP_MAXPOOL2D: needs_maxpool = true; break;
            case OP_AVGPOOL2D: needs_avgpool = true; break;
            case OP_GLOBAL_AVGPOOL: needs_global_avgpool = true; break;
            case OP_ADD: needs_add = true; break;
            case OP_MUL: needs_mul = true; break;
            case OP_BATCHNORM: needs_batchnorm = true; break;
            case OP_FLATTEN: needs_flatten = true; break;
            default: break;
        }
    }
    
    if (ctx->config->emit_comments) {
        cg_write(ctx, "/* Kernels */\n\n");
    }
    
    if (needs_matmul) {
        cg_write(ctx, "static void matmul(const float* restrict a, const float* restrict b, int K, int N, float* restrict out) {\n");
        cg_write(ctx, "    for (int j = 0; j < N; j++) {\n");
        cg_write(ctx, "        float sum = 0.0f;\n");
        cg_write(ctx, "        for (int k = 0; k < K; k++) {\n");
        cg_write(ctx, "            sum += a[k] * b[j * K + k];\n");
        cg_write(ctx, "        }\n");
        cg_write(ctx, "        out[j] = sum;\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
        
        cg_write(ctx, "static void matmul_bias(const float* restrict a, const float* restrict b, const float* restrict bias, int K, int N, float* restrict out) {\n");
        cg_write(ctx, "    for (int j = 0; j < N; j++) {\n");
        cg_write(ctx, "        float sum = bias[j];\n");
        cg_write(ctx, "        for (int k = 0; k < K; k++) {\n");
        cg_write(ctx, "            sum += a[k] * b[j * K + k];\n");
        cg_write(ctx, "        }\n");
        cg_write(ctx, "        out[j] = sum;\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_relu) {
        cg_write(ctx, "static void relu_inplace(float* x, size_t n) {\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) {\n");
        cg_write(ctx, "        if (x[i] < 0.0f) x[i] = 0.0f;\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_relu6) {
        cg_write(ctx, "static void relu6_inplace(float* x, size_t n) {\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) {\n");
        cg_write(ctx, "        if (x[i] < 0.0f) x[i] = 0.0f;\n");
        cg_write(ctx, "        else if (x[i] > 6.0f) x[i] = 6.0f;\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_sigmoid) {
        cg_write(ctx, "static void sigmoid_inplace(float* x, size_t n) {\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) {\n");
        cg_write(ctx, "        x[i] = 1.0f / (1.0f + expf(-x[i]));\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_tanh) {
        cg_write(ctx, "static void tanh_inplace(float* x, size_t n) {\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) {\n");
        cg_write(ctx, "        x[i] = tanhf(x[i]);\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_softmax) {
        cg_write(ctx, "static void softmax(const float* restrict in, size_t n, float* restrict out) {\n");
        cg_write(ctx, "    float max_val = in[0];\n");
        cg_write(ctx, "    for (size_t i = 1; i < n; i++) {\n");
        cg_write(ctx, "        if (in[i] > max_val) max_val = in[i];\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "    float sum = 0.0f;\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) {\n");
        cg_write(ctx, "        out[i] = expf(in[i] - max_val);\n");
        cg_write(ctx, "        sum += out[i];\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) {\n");
        cg_write(ctx, "        out[i] /= sum;\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_conv2d) {
        // Added restrict and optimized loop structure hint
        cg_write(ctx, "static void conv2d(const float* restrict in, int H, int W, int C_in,\n");
        cg_write(ctx, "                   const float* restrict weight, const float* restrict bias, int C_out,\n");
        cg_write(ctx, "                   int KH, int KW, int SH, int SW, int PH, int PW,\n");
        cg_write(ctx, "                   float* restrict out, int H_out, int W_out) {\n");
        cg_write(ctx, "    for (int co = 0; co < C_out; co++) {\n");
        cg_write(ctx, "        float bias_val = bias ? bias[co] : 0.0f;\n");
        cg_write(ctx, "        for (int oh = 0; oh < H_out; oh++) {\n");
        cg_write(ctx, "            for (int ow = 0; ow < W_out; ow++) {\n");
        cg_write(ctx, "                float sum = bias_val;\n");
        cg_write(ctx, "                for (int ci = 0; ci < C_in; ci++) {\n");
        cg_write(ctx, "                    for (int kh = 0; kh < KH; kh++) {\n");
        cg_write(ctx, "                        for (int kw = 0; kw < KW; kw++) {\n");
        cg_write(ctx, "                            int ih = oh * SH - PH + kh;\n");
        cg_write(ctx, "                            int iw = ow * SW - PW + kw;\n");
        cg_write(ctx, "                            if (ih >= 0 && ih < H && iw >= 0 && iw < W) {\n");
        cg_write(ctx, "                                int in_idx = ci * H * W + ih * W + iw;\n");
        cg_write(ctx, "                                int w_idx = co * C_in * KH * KW + ci * KH * KW + kh * KW + kw;\n");
        cg_write(ctx, "                                sum += in[in_idx] * weight[w_idx];\n");
        cg_write(ctx, "                            }\n");
        cg_write(ctx, "                        }\n");
        cg_write(ctx, "                    }\n");
        cg_write(ctx, "                }\n");
        cg_write(ctx, "                out[co * H_out * W_out + oh * W_out + ow] = sum;\n");
        cg_write(ctx, "            }\n");
        cg_write(ctx, "        }\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_maxpool) {
        cg_write(ctx, "static void maxpool2d(const float* restrict in, int C, int H, int W,\n");
        cg_write(ctx, "                      int KH, int KW, int SH, int SW,\n");
        cg_write(ctx, "                      float* restrict out, int H_out, int W_out) {\n");
        cg_write(ctx, "    for (int c = 0; c < C; c++) {\n");
        cg_write(ctx, "        for (int oh = 0; oh < H_out; oh++) {\n");
        cg_write(ctx, "            for (int ow = 0; ow < W_out; ow++) {\n");
        cg_write(ctx, "                float max_val = -1e30f;\n");
        cg_write(ctx, "                for (int kh = 0; kh < KH; kh++) {\n");
        cg_write(ctx, "                    for (int kw = 0; kw < KW; kw++) {\n");
        cg_write(ctx, "                        int ih = oh * SH + kh;\n");
        cg_write(ctx, "                        int iw = ow * SW + kw;\n");
        cg_write(ctx, "                        if (ih < H && iw < W) {\n");
        cg_write(ctx, "                            float v = in[c * H * W + ih * W + iw];\n");
        cg_write(ctx, "                            if (v > max_val) max_val = v;\n");
        cg_write(ctx, "                        }\n");
        cg_write(ctx, "                    }\n");
        cg_write(ctx, "                }\n");
        cg_write(ctx, "                out[c * H_out * W_out + oh * W_out + ow] = max_val;\n");
        cg_write(ctx, "            }\n");
        cg_write(ctx, "        }\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_avgpool) {
        cg_write(ctx, "static void avgpool2d(const float* restrict in, int C, int H, int W,\n");
        cg_write(ctx, "                      int KH, int KW, int SH, int SW,\n");
        cg_write(ctx, "                      float* restrict out, int H_out, int W_out) {\n");
        cg_write(ctx, "    for (int c = 0; c < C; c++) {\n");
        cg_write(ctx, "        for (int oh = 0; oh < H_out; oh++) {\n");
        cg_write(ctx, "            for (int ow = 0; ow < W_out; ow++) {\n");
        cg_write(ctx, "                float sum = 0.0f;\n");
        cg_write(ctx, "                int count = 0;\n");
        cg_write(ctx, "                for (int kh = 0; kh < KH; kh++) {\n");
        cg_write(ctx, "                    for (int kw = 0; kw < KW; kw++) {\n");
        cg_write(ctx, "                        int ih = oh * SH + kh;\n");
        cg_write(ctx, "                        int iw = ow * SW + kw;\n");
        cg_write(ctx, "                        if (ih < H && iw < W) {\n");
        cg_write(ctx, "                            sum += in[c * H * W + ih * W + iw];\n");
        cg_write(ctx, "                            count++;\n");
        cg_write(ctx, "                        }\n");
        cg_write(ctx, "                    }\n");
        cg_write(ctx, "                }\n");
        cg_write(ctx, "                out[c * H_out * W_out + oh * W_out + ow] = sum / count;\n");
        cg_write(ctx, "            }\n");
        cg_write(ctx, "        }\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_global_avgpool) {
        cg_write(ctx, "static void global_avgpool(const float* restrict in, int C, int H, int W, float* restrict out) {\n");
        cg_write(ctx, "    int spatial = H * W;\n");
        cg_write(ctx, "    for (int c = 0; c < C; c++) {\n");
        cg_write(ctx, "        float sum = 0.0f;\n");
        cg_write(ctx, "        for (int i = 0; i < spatial; i++) {\n");
        cg_write(ctx, "            sum += in[c * spatial + i];\n");
        cg_write(ctx, "        }\n");
        cg_write(ctx, "        out[c] = sum / spatial;\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_add) {
        cg_write(ctx, "static void add_inplace(float* restrict a, const float* restrict b, size_t n) {\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) a[i] += b[i];\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_mul) {
        cg_write(ctx, "static void mul_inplace(float* restrict a, const float* restrict b, size_t n) {\n");
        cg_write(ctx, "    for (size_t i = 0; i < n; i++) a[i] *= b[i];\n");
        cg_write(ctx, "}\n\n");
    }
    
    if (needs_batchnorm) {
        cg_write(ctx, "static void batchnorm(float* restrict x, int C, int spatial,\n");
        cg_write(ctx, "                      const float* restrict gamma, const float* restrict beta,\n");
        cg_write(ctx, "                      const float* restrict mean, const float* restrict var, float eps) {\n");
        cg_write(ctx, "    for (int c = 0; c < C; c++) {\n");
        cg_write(ctx, "        float scale = gamma[c] / sqrtf(var[c] + eps);\n");
        cg_write(ctx, "        float shift = beta[c] - mean[c] * scale;\n");
        cg_write(ctx, "        for (int i = 0; i < spatial; i++) {\n");
        cg_write(ctx, "            x[c * spatial + i] = x[c * spatial + i] * scale + shift;\n");
        cg_write(ctx, "        }\n");
        cg_write(ctx, "    }\n");
        cg_write(ctx, "}\n\n");
    }
    
    (void)needs_flatten; // Flatten is just pointer reinterpretation
}

// Core Generation Routine (shared)
static NnccError codegen_emit_all(CodegenContext* ctx) {
    emit_header(ctx);
    emit_weights(ctx);
    emit_scratch(ctx);
    emit_kernels(ctx);
    emit_inference(ctx);
    emit_footer(ctx);
    return NNCC_OK;
}

// Inference Function Generation

static void emit_inference(CodegenContext* ctx) {
    const char* name = ctx->config->model_name;
    
    cg_write(ctx, "void %s_inference(const float* input, float* output) {\n", name);
    cg_indent(ctx);
    
    // Compute scratch offsets using greedy allocator
    int total_scratch = 0;
    int* offsets = allocate_scratch_greedy(ctx->graph, &total_scratch);
    
    // We expect total_scratch to match the one in emit_scratch, allowing for deterministic builds
    
    if (!offsets) {
        /* Fallback if allocation failed: use naive sum */
        offsets = (int*)calloc(ctx->graph->num_tensors, sizeof(int));
    }
    
    // Ensure weights/inputs/outputs have -1 offset (they don't use scratch)
    for (int i = 0; i < ctx->graph->num_tensors; i++) {
        Tensor* t = ctx->graph->tensors[i];
         if (t->is_weight || t->is_input || t->is_output) {
            offsets[i] = -1;
        }
    }
    
    // No need for manual update since allocator handles logic

    
    // Generate code for each op
    for (int i = 0; i < ctx->graph->num_ops; i++) {
        Op* op = ctx->graph->ops[i];
        
        if (ctx->config->emit_comments) {
            cg_printf(ctx, "/* %s: %s */\n", op->name, op_type_name(op->type));
        }
        
        // Helper to get pointer string
        #define GET_PTR(t, buf) do { \
            if ((t)->is_input) snprintf(buf, sizeof(buf), "input"); \
            else if ((t)->is_output) snprintf(buf, sizeof(buf), "output"); \
            else snprintf(buf, sizeof(buf), "%s_scratch + %d", name, offsets[(t)->id]); \
        } while(0)
        
        #define GET_WEIGHT(t, buf) do { \
            char ident[128]; \
            cg_make_ident((t)->name, ident, sizeof(ident)); \
            snprintf(buf, sizeof(buf), "%s_%s", name, ident); \
        } while(0)
        
        char in_ptr[256], out_ptr[256], w_ptr[256], b_ptr[256];
        
        switch (op->type) {
            case OP_FLATTEN: {
                /* Flatten copies input to scratch if needed */
                if (op->num_inputs > 0 && op->num_outputs > 0) {
                    Tensor* in = op->inputs[0];
                    Tensor* out = op->outputs[0];
                    size_t n = tensor_numel(in);
                    if (n == 0) {
                        /* Fall back to output size */
                        n = tensor_numel(out);
                    }
                    if (n > 0) {
                        GET_PTR(in, in_ptr);
                        GET_PTR(out, out_ptr);
                        /* Use proper memcpy for copy */
                        cg_printf(ctx, "{ float* _dst = %s; const float* _src = %s; for (size_t _i = 0; _i < %zu; _i++) _dst[_i] = _src[_i]; }\n", out_ptr, in_ptr, n);
                    }
                }
                break;
            }
            
            case OP_GEMM:
            case OP_MATMUL: {
                Tensor* in = op->inputs[0];
                Tensor* weight = op->inputs[1];
                Tensor* bias = (op->num_inputs > 2) ? op->inputs[2] : NULL;
                Tensor* out = op->outputs[0];
                
                // PyTorch weights are stored as [out_features, in_features] = [N, K]
                // Our matmul does: for j in N: out[j] = sum_k a[k] * b[k*N + j]
                // This requires weight to be stored as [K, N] (row-major)
                // But PyTorch stores as [N, K], so we need to transpose
                // Actually, safetensors has [K, N] after transpose in export
                // Let's derive K and N from weight shape assuming [out_features, in_features]
                int K = 0, N = 0;
                if (weight->ndim >= 2) {
                    // PyTorch Linear: weight is [out_features, in_features] = [N, K]
                    N = weight->dims[0];  // out_features
                    K = weight->dims[1];  // in_features
                } else if (weight->ndim == 1) {
                    N = weight->dims[0];
                    K = (int)(tensor_numel(weight) / N);
                }
                
                // Override with input numel if available and K is 0
                if (K == 0) {
                    K = (int)tensor_numel(in);
                }
                
                GET_PTR(in, in_ptr);
                GET_PTR(out, out_ptr);
                GET_WEIGHT(weight, w_ptr);
                
                if (bias) {
                    GET_WEIGHT(bias, b_ptr);
                    cg_printf(ctx, "matmul_bias(%s, %s, %s, %d, %d, %s);\n",
                              in_ptr, w_ptr, b_ptr, K, N, out_ptr);
                } else {
                    cg_printf(ctx, "matmul(%s, %s, %d, %d, %s);\n",
                              in_ptr, w_ptr, K, N, out_ptr);
                }
                break;
            }
            
            case OP_RELU: {
                // In-place activation - apply to the input tensor (which was output of previous op)
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                size_t n = tensor_numel(out);
                if (n == 0) n = tensor_numel(in);
                GET_PTR(in, in_ptr);  // Use INPUT since in-place
                cg_printf(ctx, "relu_inplace(%s, %zu);\n", in_ptr, n);
                break;
            }
            
            case OP_RELU6: {
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                size_t n = tensor_numel(out);
                if (n == 0) n = tensor_numel(in);
                GET_PTR(in, in_ptr);  /* Use INPUT since in-place */
                cg_printf(ctx, "relu6_inplace(%s, %zu);\n", in_ptr, n);
                break;
            }
            
            case OP_SIGMOID: {
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                size_t n = tensor_numel(out);
                if (n == 0) n = tensor_numel(in);
                GET_PTR(in, in_ptr);  /* Use INPUT since in-place */
                cg_printf(ctx, "sigmoid_inplace(%s, %zu);\n", in_ptr, n);
                break;
            }
            
            case OP_TANH: {
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                size_t n = tensor_numel(out);
                if (n == 0) n = tensor_numel(in);
                GET_PTR(in, in_ptr);  /* Use INPUT since in-place */
                cg_printf(ctx, "tanh_inplace(%s, %zu);\n", in_ptr, n);
                break;
            }
            
            case OP_SOFTMAX: {
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                size_t n = tensor_numel(out);
                if (n == 0) n = tensor_numel(in);
                GET_PTR(in, in_ptr);
                GET_PTR(out, out_ptr);
                cg_printf(ctx, "softmax(%s, %zu, %s);\n", in_ptr, n, out_ptr);
                break;
            }
            
            case OP_CONV2D: {
                Tensor* in = op->inputs[0];
                Tensor* weight = op->inputs[1];
                Tensor* bias = (op->num_inputs > 2) ? op->inputs[2] : NULL;
                Tensor* out = op->outputs[0];
                
                int C_in = in->dims[1], H = in->dims[2], W = in->dims[3];
                int C_out = out->dims[1], H_out = out->dims[2], W_out = out->dims[3];
                ConvAttrs* a = &op->attrs.conv;
                
                GET_PTR(in, in_ptr);
                GET_PTR(out, out_ptr);
                GET_WEIGHT(weight, w_ptr);
                
                if (bias) {
                    GET_WEIGHT(bias, b_ptr);
                    cg_printf(ctx, "conv2d(%s, %d, %d, %d, %s, %s, %d, %d, %d, %d, %d, %d, %d, %s, %d, %d);\n",
                              in_ptr, H, W, C_in, w_ptr, b_ptr, C_out,
                              a->kernel_h, a->kernel_w, a->stride_h, a->stride_w, a->pad_h, a->pad_w,
                              out_ptr, H_out, W_out);
                } else {
                    cg_printf(ctx, "conv2d(%s, %d, %d, %d, %s, 0, %d, %d, %d, %d, %d, %d, %d, %s, %d, %d);\n",
                              in_ptr, H, W, C_in, w_ptr, C_out,
                              a->kernel_h, a->kernel_w, a->stride_h, a->stride_w, a->pad_h, a->pad_w,
                              out_ptr, H_out, W_out);
                }
                break;
            }
            
            case OP_MAXPOOL2D: {
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                int C = in->dims[1], H = in->dims[2], W = in->dims[3];
                int H_out = out->dims[2], W_out = out->dims[3];
                PoolAttrs* a = &op->attrs.pool;
                
                GET_PTR(in, in_ptr);
                GET_PTR(out, out_ptr);
                cg_printf(ctx, "maxpool2d(%s, %d, %d, %d, %d, %d, %d, %d, %s, %d, %d);\n",
                          in_ptr, C, H, W, a->kernel_h, a->kernel_w, a->stride_h, a->stride_w,
                          out_ptr, H_out, W_out);
                break;
            }
            
            case OP_AVGPOOL2D: {
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                int C = in->dims[1], H = in->dims[2], W = in->dims[3];
                int H_out = out->dims[2], W_out = out->dims[3];
                PoolAttrs* a = &op->attrs.pool;
                
                GET_PTR(in, in_ptr);
                GET_PTR(out, out_ptr);
                cg_printf(ctx, "avgpool2d(%s, %d, %d, %d, %d, %d, %d, %d, %s, %d, %d);\n",
                          in_ptr, C, H, W, a->kernel_h, a->kernel_w, a->stride_h, a->stride_w,
                          out_ptr, H_out, W_out);
                break;
            }
            
            case OP_GLOBAL_AVGPOOL: {
                Tensor* in = op->inputs[0];
                Tensor* out = op->outputs[0];
                int C = in->dims[1], H = in->dims[2], W = in->dims[3];
                
                GET_PTR(in, in_ptr);
                GET_PTR(out, out_ptr);
                cg_printf(ctx, "global_avgpool(%s, %d, %d, %d, %s);\n",
                          in_ptr, C, H, W, out_ptr);
                break;
            }
            
            case OP_ADD: {
                Tensor* a = op->inputs[0];
                Tensor* b = op->inputs[1];
                size_t n = tensor_numel(a);
                GET_PTR(a, in_ptr);
                GET_PTR(b, out_ptr);
                cg_printf(ctx, "add_inplace(%s, %s, %zu);\n", in_ptr, out_ptr, n);
                break;
            }
            
            case OP_MUL: {
                Tensor* a = op->inputs[0];
                Tensor* b = op->inputs[1];
                size_t n = tensor_numel(a);
                GET_PTR(a, in_ptr);
                GET_PTR(b, out_ptr);
                cg_printf(ctx, "mul_inplace(%s, %s, %zu);\n", in_ptr, out_ptr, n);
                break;
            }
            
            case OP_RESHAPE:
            case OP_DROPOUT:
            case OP_IDENTITY:
                /* No-op at inference time */
                break;
                
            default:
                cg_printf(ctx, "/* TODO: %s */\n", op_type_name(op->type));
                break;
        }
        
        cg_write(ctx, "\n");
        
        #undef GET_PTR
        #undef GET_WEIGHT
    }
    
    free(offsets);
    cg_dedent(ctx);
    cg_write(ctx, "}\n\n");
}

// Main Generation Function

NnccError codegen_generate(const Graph* graph, const CodegenConfig* config) {
    FILE* f = fopen(config->output_path, "w");
    if (!f) return NNCC_ERROR_IO;
    
    CodegenContext ctx = {0};
    ctx.out = f;
    ctx.graph = graph;
    ctx.config = config;
    ctx.indent = 0;
    
    NnccError err = codegen_emit_all(&ctx);
    
    fclose(f);
    return err;
}

NnccError codegen_generate_to_memory(const Graph* graph, const CodegenConfig* config, char** out_buffer, size_t* out_len) {
    if ((!out_buffer && !out_len) || !config || !graph) return NNCC_ERROR_INTERNAL;
    
    CodegenContext ctx = {0};
    ctx.graph = graph;
    ctx.config = config;
    ctx.indent = 0;
    
    // First pass: measure size
    ctx.out = NULL;
    ctx.buffer = NULL;
    ctx.buffer_len = 0;
    ctx.buffer_cap = 0;
    
    codegen_emit_all(&ctx);
    size_t required_len = ctx.buffer_len;
    
    if (out_len) *out_len = required_len;
    
    if (out_buffer) {
        // Second pass: write
        ctx.indent = 0;
        ctx.buffer_len = 0;
        ctx.buffer_cap = required_len + 1;
        ctx.buffer = (char*)malloc(ctx.buffer_cap);
        if (!ctx.buffer) return NNCC_ERROR_OUT_OF_MEMORY;
        
        codegen_emit_all(&ctx);
        
        if (ctx.error) {
            free(ctx.buffer);
            return NNCC_ERROR_OUT_OF_MEMORY;
        }
        
        ctx.buffer[required_len] = '\0';
        *out_buffer = ctx.buffer;
    }
    
    return NNCC_OK;
}
