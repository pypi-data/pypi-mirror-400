# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/llm.ipynb.

# %% auto 0
__all__ = ['RemoteToolLLM', 'send_llm_request']

# %% ../nbs/llm.ipynb 3
import json
import requests
import litellm

from .app_config import MODEL, KERNEL_URL, NOTEBOOK_SYS_PROMPT, UE_TOOL_SYS_PROMPT 
from .llm_tools import TOOLS, TOOL_SCHEMAS

litellm.drop_params = True


# %% ../nbs/llm.ipynb 4
class RemoteToolLLM:
    """LLM chat client that executes tools in Unreal Engine via url."""
    def __init__(self, model='gpt-4.1',use_ue_tools=False):
        """
        LLM chat that executes tools in Unreal Engine.
        
        Args:
            model: LiteLLM model string
        """
        self.model = model
        
        # Fetch available tools from Unreal

        #self.tools = self._fetch_tools()
        #self.local_tools = self._fetch_local_tools()
        #self.all_tools = self.tools + self.local_tools
        if use_ue_tools:
            self.all_tools = self._fetch_tools()
            self.sys_prompt = UE_TOOL_SYS_PROMPT
            print(f"Connected to Unreal. Available tools: {[t['function']['name'] for t in self.all_tools]}")

        else:
            self.all_tools = self._fetch_local_tools()
            self.sys_prompt = NOTEBOOK_SYS_PROMPT
            print(f"Available Notebook tools: {[t['function']['name'] for t in self.all_tools]}")


        

    
    def _fetch_tools(self):
        """Fetch tool schemas from Unreal endpoint.
        
        Returns:
            List of tool schema dicts.
        """
        response = requests.get(f'{KERNEL_URL}/tools')
        return response.json()

    def _fetch_local_tools(self):
        """Get locally registered tool schemas.
        
        Returns:
            List of tool schema dicts.
        """
        global TOOL_SCHEMAS
        return TOOL_SCHEMAS
    
    def _execute_tool(self, func_name, args):
        """Execute a tool locally or in Unreal.
        
        Args:
            func_name: Name of tool to execute.
            args: Dict of arguments.
            
        Returns:
            Tool result as string.
        """
        if func_name in TOOLS:
            try:
                result = TOOLS[func_name](**args)
                return str(result)
            except Exception as e:
                return f"Local tool error: {str(e)}"

        response = requests.post(
            f'{KERNEL_URL}/execute_tool',
            json={'function': func_name, 'arguments': args}
        )
        data = response.json()
        # Check if there's an error
        if 'error' in data:
            return f"Error: {data['error']}"
        
        # Check if result exists
        if 'result' not in data:
            return f"Unexpected response: {data}"
        
        return data['result']
    
    def chat(self, prompt, history=None, max_steps=50, stream=True):
        """Send message and handle tool calls automatically.
        
        Args:
            prompt: User message.
            history: Prior conversation messages.
            system_prompt: System instruction.
            max_steps: Max tool call iterations.
            stream: Enable streaming output.
            
        Yields:
            Text chunks for display.
        """
        messages = []
        # Add system prompt if this is the first message
        if self.sys_prompt:
            messages.append({"role": "system", "content": self.sys_prompt})
        if history:
            messages += history
        
        # Add user message
        messages.append({"role": "user", "content": prompt})
        
        # Tool call loop
        for step in range(max_steps):
            # Call LLM
            response = litellm.completion(
                model=self.model,
                messages=messages,
                tools=self.all_tools,
                stream=stream
            )
                
            # Accumulate response
            assistant_content = ""
            tool_calls = []
            
            if stream:
                for chunk in response:
                    delta = chunk.choices[0].delta
                    
                    # Stream text content
                    if hasattr(delta, 'content') and delta.content:
                        assistant_content += delta.content
                        yield delta.content  
                    
                    # Accumulate tool calls
                    if hasattr(delta, 'tool_calls') and delta.tool_calls:
                        tc = delta.tool_calls[0]
                        idx = tc.index
                        
                        # Extend tool_calls list if needed
                        while len(tool_calls) <= idx:
                            tool_calls.append({
                                'id': None,
                                'function': {'name': '', 'arguments': ''},
                                'type': 'function'
                            })
                        
                        if tc.id:
                            tool_calls[idx]['id'] = tc.id
                        if hasattr(tc.function, 'name') and tc.function.name:
                            tool_calls[idx]['function']['name'] += tc.function.name
                        if hasattr(tc.function, 'arguments') and tc.function.arguments:
                            tool_calls[idx]['function']['arguments'] += tc.function.arguments
            else:
                # Non-streaming
                message = response.choices[0].message
                assistant_content = message.content or ""
                yield assistant_content  # 
                
                if message.tool_calls:
                    tool_calls = [
                        {
                            'id': tc.id,
                            'function': {
                                'name': tc.function.name,
                                'arguments': tc.function.arguments
                            },
                            'type': 'function'
                        }
                        for tc in message.tool_calls
                    ]
            
            # Add assistant message to history
            assistant_msg = {
                "role": "assistant",
                "content": assistant_content or None
            }
            if tool_calls:
                assistant_msg["tool_calls"] = tool_calls

            messages.append(assistant_msg)
            
            # If no tool calls, we're done
            if not tool_calls:
                break
            
            # Execute tools in Unreal
            for tc in tool_calls:
                func_name = tc['function']['name']
                try:
                    args = json.loads(tc['function']['arguments'])
                except:
                    args = {}
                
                yield f"\n\nðŸ”§ Calling {func_name}({json.dumps(args)})...\n\n"  
                
                # Call Unreal
                result = self._execute_tool(func_name, args)
                #yield f"\n\n Result{result}...\n\n"  #

                # Add tool result to messages
                messages.append({
                    "role": "tool",
                    "tool_call_id": tc['id'],
                    "content": result
                })
    

    
    def refresh_tools(self):
        """Reload tools from Unreal (call after registering new tools)."""
        self.tools = self._fetch_tools()


def send_llm_request(prompt, history=None, use_ue_tools=False):
    """Stream LLM response with tool execution.
    
    Args:
        prompt: User message.
        history: Prior conversation messages.
        
    Yields:
        Text chunks from LLM response.
    """
    chat = RemoteToolLLM( model=MODEL,
                          use_ue_tools=use_ue_tools)
    
    for chunk in chat.chat(prompt, history=history):
        yield chunk
