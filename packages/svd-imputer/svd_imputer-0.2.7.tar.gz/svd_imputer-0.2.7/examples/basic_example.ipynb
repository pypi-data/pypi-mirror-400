{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb9eec8",
   "metadata": {},
   "source": [
    "# svd_imputer how-to: quick start\n",
    "\n",
    "This short tutorial shows how to impute multivariate time series with missing values using the `svd_imputer` package. We'll:\n",
    "- Generate synthetic data for 10 sites with seasonality, trend, and interannual variability.\n",
    "- Introduce complex missing value patterns (random, continuous blocks).\n",
    "- Try 3 configurations of the `Imputer`:\n",
    "  1) Automatic rank via `variance_threshold`\n",
    "  2) Fixed rank\n",
    "  3) Automatic rank estimation\n",
    "- Estimate uncertainty using Monte Carlo iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2610c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make repo root importable\n",
    "repo_root = os.path.abspath(os.path.join('..'))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "from svd_imputer import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a6d57",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "We generate synthetic data for 10 sites. Each site has a shared base component, plus site-specific seasonality (annual and interannual), trend, and noise. We then introduce missing values: random missingness for some sites, and large continuous gaps for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: create synthetic data with ~15% NaNs per column\n",
    "def create_synthetic_data(n=180, n_sites=10, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = pd.date_range('2020-01-01', periods=n, freq='D')\n",
    "    t = np.arange(n)\n",
    "    \n",
    "    data_list = []\n",
    "    columns = []\n",
    "    \n",
    "    # Shared component\n",
    "    base = rng.normal(0, 0.1, size=n)\n",
    "    \n",
    "    for i in range(n_sites):\n",
    "        # Randomize parameters slightly for each site\n",
    "        phase = 0 #rng.uniform(0, 2*np.pi)\n",
    "        amp = rng.uniform(1.5, 2.5)\n",
    "        trend = 0.01 #rng.uniform(0.0, 0.03)\n",
    "        offset = rng.uniform(5, 15)\n",
    "        \n",
    "        # Interannual parameters\n",
    "        phase_2 = rng.uniform(0, 2*np.pi)\n",
    "        amp_2 = rng.uniform(1.0, 2.0)\n",
    "        \n",
    "        # Signal: Offset + Trend + Seasonality + Interannual\n",
    "        signal = (offset + \n",
    "                  amp*np.sin(2*np.pi*t/30 + phase) + \n",
    "                  amp_2*np.sin(2*np.pi*t/90 + phase_2) + \n",
    "                  trend*t)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = rng.normal(0, 0.05, size=n)\n",
    "        \n",
    "        # Combine\n",
    "        site_data = signal + noise + base\n",
    "        data_list.append(site_data)\n",
    "        columns.append(f'Site_{i+1}')\n",
    "        \n",
    "    data = np.stack(data_list, axis=1)\n",
    "    df = pd.DataFrame(data, index=idx, columns=columns)\n",
    "    df_truth = df.copy()\n",
    "    \n",
    "    # Introduce missing values\n",
    "    # Random missing for first 30% of sites\n",
    "    m = int(0.30 * n)\n",
    "    #for col in columns[:3]:\n",
    "    #    miss_idx = rng.choice(n, size=m, replace=False)\n",
    "    #    df.loc[idx[miss_idx], col] = np.nan\n",
    "\n",
    "    # Continuous missing for next 30% of sites\n",
    "    if len(columns) > 3:\n",
    "        for col in columns[3:6]:\n",
    "            df.iloc[-m:, df.columns.get_loc(col)] = np.nan\n",
    "        \n",
    "    # Random chunks for others\n",
    "    if len(columns) > 6:\n",
    "        for col in columns[:]:\n",
    "             miss_idx = rng.choice(n-m-1, size=m//2, replace=False)\n",
    "             df.loc[idx[miss_idx], col] = np.nan\n",
    "\n",
    "    return df,df_truth\n",
    "\n",
    "# Helper: plot imputation results\n",
    "\n",
    "def plot_imputation(df_original,df_truth, df_imputed=None, title='', columns=None,bounds=None):\n",
    "    cols = columns or df_original.columns.tolist()\n",
    "    fig, axes = plt.subplots(len(cols), 1, figsize=(7, 2*len(cols)), sharex=True)\n",
    "    if len(cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, col in zip(axes, cols):\n",
    "        missing_mask = df_original[col].isna()\n",
    "        # Observed scatter\n",
    "        ax.scatter(df_truth.index, df_truth.loc[:,col],\n",
    "                   s=18, color=\"#FC0404\", alpha=0.9, label='Truth', zorder=3)\n",
    "        ax.scatter(df_original.index[~missing_mask], df_original.loc[~missing_mask, col],\n",
    "                   s=18, color=\"#000000\", alpha=0.9, label='Observed', zorder=3)\n",
    "\n",
    "        if df_imputed is not None:\n",
    "            # Imputed line\n",
    "            ax.plot(df_imputed.index, df_imputed[col], color='#2b6cb0', lw=2, label='Imputed')\n",
    "\n",
    "            # Optional shading for missing regions\n",
    "            if bounds is not None:\n",
    "                lb,ub = bounds\n",
    "                colidx = df_imputed.columns.get_loc(col)\n",
    "                rowidx = np.where(missing_mask)[0]\n",
    "                if len(rowidx) > 0:\n",
    "                    ax.fill_between(df_imputed.index[missing_mask],\n",
    "                                    lb[rowidx,colidx],\n",
    "                                    ub[rowidx,colidx],\n",
    "                                    color='#2b6cb0', alpha=0.8, label='CI')\n",
    "        ax.set_ylabel(col)\n",
    "        ax.grid(True, alpha=0.25)\n",
    "    axes[0].set_title(title)\n",
    "    axes[-1].set_xlabel('Date')\n",
    "    # De-duplicate legend handles\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    axes[0].legend(by_label.values(), by_label.keys(), ncol=2, loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2237365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data and preview\n",
    "df,df_truth = create_synthetic_data(n=180*3, seed=42)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Missing per column:\\n\", df.isna().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e158fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imputation(df, df_truth,df_imputed=None, title='', columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292a9b0",
   "metadata": {},
   "source": [
    "## Example 1: Automatic rank (variance_threshold=0.90)\n",
    "We initialize the imputer with `variance_threshold=0.90`, meaning we want to retain enough singular values to explain 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8764b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(df,\n",
    "                  variance_threshold=.90,\n",
    "                  verbose=True)      # validate_dataframe() + preprocessing ONCE\n",
    "imputer.fit()                        # Pure computation, cached SVD components\n",
    "results = imputer.transform()        # Uses cached data + SVD components  \n",
    "\n",
    "plot_imputation(df,df_truth, results, title='Rank from variance threshold', columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7465f5f",
   "metadata": {},
   "source": [
    "## Example 2: Fixed rank (rank=4)\n",
    "Here we force the rank to be 4. This is useful if you have prior knowledge about the dimensionality of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(df,\n",
    "                  rank=4,\n",
    "                  verbose=True)      # validate_dataframe() + preprocessing ONCE\n",
    "imputer.fit()                        # Pure computation, cached SVD components\n",
    "results = imputer.transform()        # Uses cached data + SVD components  \n",
    "\n",
    "plot_imputation(df,df_truth, results, title='Fixed rank = 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52018ea9",
   "metadata": {},
   "source": [
    "## Example 3: Automatic rank estimation\n",
    "We can also let the imputer estimate the optimal rank automatically using `rank='auto'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(df,\n",
    "                  rank='auto',\n",
    "                  tol=1e-3,\n",
    "                  verbose=True)      # validate_dataframe() + preprocessing ONCE\n",
    "imputer.fit()                        # Pure computation, cached SVD components\n",
    "results = imputer.transform()        # Uses cached data + SVD components  \n",
    "\n",
    "plot_imputation(df,df_truth, results, title='rank auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fbb8d",
   "metadata": {},
   "source": [
    "## Estimate Uncertainty\n",
    "We can estimate the uncertainty of the imputation by performing multiple imputations (Monte Carlo). This gives us a distribution of imputed values for each missing point, from which we can calculate confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f67cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Imputation with Uncertainty (Rubin's Rules)\n",
    "df_imputed, df_uncertainty = imputer.fit_transform(\n",
    "    return_uncertainty=True, \n",
    "    n_imputations=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function for uncertainty\n",
    "def plot_unc(df,df_truth, df_imputed, df_uncertainty):\n",
    "    n_cols = df.shape[1]\n",
    "    fig,axs = plt.subplots(n_cols,1, figsize=(7, 2*n_cols),sharex=True)\n",
    "\n",
    "    if n_cols == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for e,ax in enumerate(axs):\n",
    "        col = df.columns[e]\n",
    "        # Original data points\n",
    "        ax.plot(df.index,df[col],'.',label='Original',alpha=0.5,zorder=100)\n",
    "        ax.plot(df_truth.index,df_truth[col],c='r',label='Truth',alpha=0.5,zorder=100)\n",
    "        # Imputed data line\n",
    "        ax.plot(df_imputed.index,df_imputed[col],'--',label='Imputed')\n",
    "        # Uncertainty intervals\n",
    "        lb = df_imputed[col] - 1.96 * np.sqrt(df_uncertainty[col])\n",
    "        ub = df_imputed[col] + 1.96 * np.sqrt(df_uncertainty[col])\n",
    "        ax.fill_between(df_imputed.index, lb, ub, color='0.5', alpha=0.3, label='95% CI', zorder=0) \n",
    "        ax.set_title(col)\n",
    "        ax.legend(loc='upper left',fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    return\n",
    "\n",
    "plot_unc(df,df_truth, df_imputed, df_uncertainty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
