"""
Evaluation Analytics - í‰ê°€ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
"""

import statistics
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from .results import BatchEvaluationResult


@dataclass
class MetricTrend:
    """ë©”íŠ¸ë¦­ ì¶”ì´"""

    metric_name: str
    timestamps: List[str]
    scores: List[float]
    trend: str  # "improving", "declining", "stable"
    average_score: float
    min_score: float
    max_score: float
    std_dev: float


@dataclass
class CorrelationAnalysis:
    """ìƒê´€ê´€ê³„ ë¶„ì„"""

    metric_a: str
    metric_b: str
    correlation: float
    significance: str  # "strong", "moderate", "weak", "none"


@dataclass
class EvaluationAnalytics:
    """í‰ê°€ ë¶„ì„ ê²°ê³¼"""

    metric_trends: List[MetricTrend]
    correlations: List[CorrelationAnalysis]
    summary_stats: Dict[str, Any]
    insights: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)


class EvaluationAnalyticsEngine:
    """
    í‰ê°€ ë¶„ì„ ì—”ì§„

    í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ íŠ¸ë Œë“œ, ìƒê´€ê´€ê³„, ì¸ì‚¬ì´íŠ¸ ì œê³µ
    """

    def __init__(self):
        self._history: List[Dict[str, Any]] = []

    def add_evaluation_result(
        self,
        result: BatchEvaluationResult,
        timestamp: Optional[datetime] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """
        í‰ê°€ ê²°ê³¼ ì¶”ê°€

        Args:
            result: ë°°ì¹˜ í‰ê°€ ê²°ê³¼
            timestamp: í‰ê°€ ì‹œê°„ (ì„ íƒì )
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒì )
        """
        self._history.append(
            {
                "timestamp": timestamp or datetime.now(),
                "result": result,
                "metadata": metadata or {},
            }
        )

    def analyze_trends(
        self,
        metric_name: Optional[str] = None,
        window_days: int = 30,
    ) -> List[MetricTrend]:
        """
        ë©”íŠ¸ë¦­ ì¶”ì´ ë¶„ì„

        Args:
            metric_name: ë©”íŠ¸ë¦­ ì´ë¦„ (ì„ íƒì , ì—†ìœ¼ë©´ ëª¨ë“  ë©”íŠ¸ë¦­)
            window_days: ë¶„ì„ ê¸°ê°„ (ì¼)

        Returns:
            List[MetricTrend]: ë©”íŠ¸ë¦­ë³„ ì¶”ì´
        """
        cutoff_date = datetime.now() - timedelta(days=window_days)
        recent_history = [h for h in self._history if h["timestamp"] >= cutoff_date]

        if not recent_history:
            return []

        # ë©”íŠ¸ë¦­ë³„ë¡œ ê·¸ë£¹í™”
        metrics_data: Dict[str, List[Dict[str, Any]]] = {}

        for record in recent_history:
            result = record["result"]
            for eval_result in result.results:
                metric = eval_result.metric_name
                if metric_name and metric != metric_name:
                    continue

                if metric not in metrics_data:
                    metrics_data[metric] = []

                metrics_data[metric].append(
                    {
                        "timestamp": record["timestamp"],
                        "score": eval_result.score,
                    }
                )

        # ì¶”ì´ ê³„ì‚°
        trends = []
        for metric, data in metrics_data.items():
            # ì‹œê°„ìˆœ ì •ë ¬
            data.sort(key=lambda x: x["timestamp"])

            timestamps = [d["timestamp"].isoformat() for d in data]
            scores = [d["score"] for d in data]

            if len(scores) < 2:
                continue

            # í†µê³„ ê³„ì‚°
            avg_score = statistics.mean(scores)
            min_score = min(scores)
            max_score = max(scores)
            std_dev = statistics.stdev(scores) if len(scores) > 1 else 0.0

            # ì¶”ì´ ê³„ì‚° (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)
            if len(scores) > 1:
                # ê°„ë‹¨í•œ ì¶”ì´: ìµœê·¼ ì ìˆ˜ì™€ ì´ˆê¸° ì ìˆ˜ ë¹„êµ
                mid_point = len(scores) // 2
                recent_avg = statistics.mean(scores[:mid_point]) if mid_point > 0 else avg_score
                early_avg = (
                    statistics.mean(scores[mid_point:]) if mid_point < len(scores) else avg_score
                )

                if recent_avg > early_avg * 1.05:
                    trend = "improving"
                elif recent_avg < early_avg * 0.95:
                    trend = "declining"
                else:
                    trend = "stable"
            else:
                trend = "stable"

            trends.append(
                MetricTrend(
                    metric_name=metric,
                    timestamps=timestamps,
                    scores=scores,
                    trend=trend,
                    average_score=avg_score,
                    min_score=min_score,
                    max_score=max_score,
                    std_dev=std_dev,
                )
            )

        return trends

    def analyze_correlations(
        self,
        window_days: int = 30,
        min_samples: int = 10,
    ) -> List[CorrelationAnalysis]:
        """
        ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„

        Args:
            window_days: ë¶„ì„ ê¸°ê°„ (ì¼)
            min_samples: ìµœì†Œ ìƒ˜í”Œ ìˆ˜

        Returns:
            List[CorrelationAnalysis]: ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼
        """
        cutoff_date = datetime.now() - timedelta(days=window_days)
        recent_history = [h for h in self._history if h["timestamp"] >= cutoff_date]

        if len(recent_history) < min_samples:
            return []

        # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ìˆ˜ì§‘
        metrics_scores: Dict[str, List[float]] = {}

        for record in recent_history:
            result = record["result"]
            for eval_result in result.results:
                metric = eval_result.metric_name
                if metric not in metrics_scores:
                    metrics_scores[metric] = []
                metrics_scores[metric].append(eval_result.score)

        # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
        valid_metrics = {
            m: scores for m, scores in metrics_scores.items() if len(scores) >= min_samples
        }

        if len(valid_metrics) < 2:
            return []

        # ëª¨ë“  ë©”íŠ¸ë¦­ ìŒì— ëŒ€í•´ ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = []
        metric_names = list(valid_metrics.keys())

        for i in range(len(metric_names)):
            for j in range(i + 1, len(metric_names)):
                metric_a = metric_names[i]
                metric_b = metric_names[j]

                scores_a = valid_metrics[metric_a]
                scores_b = valid_metrics[metric_b]

                # ê¸¸ì´ ë§ì¶”ê¸° (ìµœì†Œ ê¸¸ì´ë¡œ)
                min_len = min(len(scores_a), len(scores_b))
                scores_a = scores_a[:min_len]
                scores_b = scores_b[:min_len]

                if len(scores_a) < min_samples:
                    continue

                # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                correlation = self._calculate_correlation(scores_a, scores_b)

                # ìœ ì˜ì„± íŒë‹¨
                abs_corr = abs(correlation)
                if abs_corr >= 0.7:
                    significance = "strong"
                elif abs_corr >= 0.4:
                    significance = "moderate"
                elif abs_corr >= 0.2:
                    significance = "weak"
                else:
                    significance = "none"

                correlations.append(
                    CorrelationAnalysis(
                        metric_a=metric_a,
                        metric_b=metric_b,
                        correlation=correlation,
                        significance=significance,
                    )
                )

        return correlations

    def _calculate_correlation(self, x: List[float], y: List[float]) -> float:
        """í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        if len(x) != len(y):
            raise ValueError("Lists must have same length")

        n = len(x)
        if n < 2:
            return 0.0

        # í‰ê·  ê³„ì‚°
        mean_x = statistics.mean(x)
        mean_y = statistics.mean(y)

        # ë¶„ì‚° ë° ê³µë¶„ì‚° ê³„ì‚°
        sum_xy = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n))
        sum_x2 = sum((x[i] - mean_x) ** 2 for i in range(n))
        sum_y2 = sum((y[i] - mean_y) ** 2 for i in range(n))

        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        denominator = (sum_x2 * sum_y2) ** 0.5
        if denominator == 0:
            return 0.0

        correlation = sum_xy / denominator
        return correlation

    def generate_analytics(
        self,
        window_days: int = 30,
        include_insights: bool = True,
    ) -> EvaluationAnalytics:
        """
        ì¢…í•© ë¶„ì„ ìƒì„±

        Args:
            window_days: ë¶„ì„ ê¸°ê°„ (ì¼)
            include_insights: ì¸ì‚¬ì´íŠ¸ í¬í•¨ ì—¬ë¶€

        Returns:
            EvaluationAnalytics: ë¶„ì„ ê²°ê³¼
        """
        # ì¶”ì´ ë¶„ì„
        trends = self.analyze_trends(window_days=window_days)

        # ìƒê´€ê´€ê³„ ë¶„ì„
        correlations = self.analyze_correlations(window_days=window_days)

        # ìš”ì•½ í†µê³„
        summary_stats = self._calculate_summary_stats(window_days=window_days)

        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []
        if include_insights:
            insights = self._generate_insights(trends, correlations, summary_stats)

        return EvaluationAnalytics(
            metric_trends=trends,
            correlations=correlations,
            summary_stats=summary_stats,
            insights=insights,
            metadata={
                "window_days": window_days,
                "total_evaluations": len(self._history),
                "generated_at": datetime.now().isoformat(),
            },
        )

    def _calculate_summary_stats(self, window_days: int = 30) -> Dict[str, Any]:
        """ìš”ì•½ í†µê³„ ê³„ì‚°"""
        cutoff_date = datetime.now() - timedelta(days=window_days)
        recent_history = [h for h in self._history if h["timestamp"] >= cutoff_date]

        if not recent_history:
            return {
                "total_evaluations": 0,
                "average_scores": {},
                "metric_counts": {},
            }

        # ë©”íŠ¸ë¦­ë³„ í†µê³„
        metric_stats: Dict[str, List[float]] = {}

        for record in recent_history:
            result = record["result"]
            for eval_result in result.results:
                metric = eval_result.metric_name
                if metric not in metric_stats:
                    metric_stats[metric] = []
                metric_stats[metric].append(eval_result.score)

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        average_scores = {
            metric: statistics.mean(scores) for metric, scores in metric_stats.items()
        }

        # ë©”íŠ¸ë¦­ë³„ ê°œìˆ˜
        metric_counts = {metric: len(scores) for metric, scores in metric_stats.items()}

        return {
            "total_evaluations": len(recent_history),
            "average_scores": average_scores,
            "metric_counts": metric_counts,
            "metrics": list(metric_stats.keys()),
        }

    def _generate_insights(
        self,
        trends: List[MetricTrend],
        correlations: List[CorrelationAnalysis],
        summary_stats: Dict[str, Any],
    ) -> List[str]:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        # ì¶”ì´ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        improving_metrics = [t for t in trends if t.trend == "improving"]
        declining_metrics = [t for t in trends if t.trend == "declining"]

        if improving_metrics:
            insights.append(
                f"âœ… {len(improving_metrics)}ê°œ ë©”íŠ¸ë¦­ì´ ê°œì„  ì¤‘: "
                f"{', '.join(m.metric_name for m in improving_metrics)}"
            )

        if declining_metrics:
            insights.append(
                f"âš ï¸ {len(declining_metrics)}ê°œ ë©”íŠ¸ë¦­ì´ í•˜ë½ ì¤‘: "
                f"{', '.join(m.metric_name for m in declining_metrics)}"
            )

        # ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        strong_correlations = [c for c in correlations if c.significance == "strong"]
        if strong_correlations:
            insights.append(
                f"ğŸ”— {len(strong_correlations)}ê°œì˜ ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬: "
                f"{', '.join(f'{c.metric_a}-{c.metric_b}' for c in strong_correlations[:3])}"
            )

        # ìš”ì•½ í†µê³„ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        if summary_stats.get("average_scores"):
            avg_scores = summary_stats["average_scores"]
            best_metric = max(avg_scores.items(), key=lambda x: x[1])
            worst_metric = min(avg_scores.items(), key=lambda x: x[1])

            insights.append(f"ğŸ“Š ìµœê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­: {best_metric[0]} ({best_metric[1]:.3f})")
            insights.append(f"ğŸ“‰ ê°œì„  í•„ìš” ë©”íŠ¸ë¦­: {worst_metric[0]} ({worst_metric[1]:.3f})")

        return insights

    def clear_history(self, days: Optional[int] = None):
        """
        ê¸°ë¡ ì‚­ì œ

        Args:
            days: ìœ ì§€í•  ê¸°ê°„ (ì¼), Noneì´ë©´ ëª¨ë‘ ì‚­ì œ
        """
        if days is None:
            self._history.clear()
        else:
            cutoff_date = datetime.now() - timedelta(days=days)
            self._history = [h for h in self._history if h["timestamp"] >= cutoff_date]
