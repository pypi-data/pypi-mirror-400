import logging
import os
import re
import shutil

import fitz
import numpy as np
import requests
from scipy.spatial.distance import cosine

from jcclang.api import lifecycle
from jcclang.api.data_prepare import input_prepare, output_prepare
from jcclang.core.const import DataType

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# è¾“å…¥è¾“å‡ºè·¯å¾„
# input_file_path = "./input_files"        # è¾“å…¥ç›®å½•
# output_dir = "saveChunk"                 # æ–‡æœ¬å—å­˜æ”¾ç›®å½•
# output_file_path = "./output_dataset/output.json"  # æœ€ç»ˆæ•°æ®é›†è¾“å‡ºè·¯å¾„

input_file_path = input_prepare(DataType.DATASET, '')
output_dir = "saveChunk"  # æ–‡æœ¬å—å­˜æ”¾ç›®å½•
output_file_path = output_prepare(DataType.DATASET, 'output.json')  # æœ€ç»ˆæ•°æ®é›†è¾“å‡ºè·¯å¾„

# æ–‡æœ¬åˆ†å—å‚æ•°
chunk_max_length = 500  # æ¯ä¸ªæ–‡æœ¬å—æœ€å¤§å­—ç¬¦æ•°
start_chunk_threshold = 1000  # è¶…è¿‡è¿™ä¸ªé•¿åº¦å¼€å§‹åˆ†å—
similarity_threshold = 0.7  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼

# æ•°æ®é›†ç”Ÿæˆå‚æ•°
entries_per_file = 2

# Ollama æœ¬åœ° API
ollama_url = "http://127.0.0.1:11434/api/generate"
ollama_model = "qwen3:4b"


# ------------------ å·¥å…·å‡½æ•° ------------------
def clean_dir(directory):
    """æ¸…ç©ºå¹¶åˆ›å»ºæ–‡ä»¶å¤¹"""
    if os.path.exists(directory):
        shutil.rmtree(directory)
        logger.info(f"åˆ é™¤æ–‡ä»¶å¤¹ {directory}")
    os.makedirs(directory, exist_ok=True)
    logger.info(f"åˆ›å»ºæ–‡ä»¶å¤¹ {directory}")


def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


def pdf_to_text(pdf_path, txt_path):
    """PDF è½¬ TXT"""
    pdf_document = fitz.open(pdf_path)
    with open(txt_path, "w", encoding="utf-8") as text_file:
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text_file.write(page.get_text())
    pdf_document.close()


def get_file_type(file_path):
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    if ext == ".txt":
        return "txt"
    elif ext == ".pdf":
        return "pdf"
    else:
        return "unknown"


def save_chunks_to_files(chunks, output_dir):
    """ä¿å­˜æ–‡æœ¬å—åˆ°æ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    for i, chunk in enumerate(chunks):
        chunk_file = os.path.join(output_dir, f"chunk_{i + 1}.txt")
        with open(chunk_file, "w", encoding="utf-8") as f:
            f.write(chunk)
        logger.info(f"ä¿å­˜æ–‡æœ¬å— {i + 1} -> {chunk_file}")


def get_sentence_embedding(sentence):
    """è·å–å¥å­å‘é‡è¡¨ç¤ºï¼Œæš‚ç”¨ç®€å•å­—ç¬¦å‘é‡ï¼Œå¯æ›¿æ¢ä¸ºæ›´ç²¾ç»†æ–¹æ³•"""
    vec = np.array([ord(c) for c in sentence])
    if len(vec) == 0:
        vec = np.zeros(1)
    return vec / (np.linalg.norm(vec) + 1e-8)


def split_text_by_semantic(text, chunk_max_length, similarity_threshold=0.7):
    """åŸºäºç®€å•è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—"""
    sentences = re.split(r"(?<=[ã€‚ï¼ï¼Ÿï¼›\n])", text)
    chunks = []
    if not sentences:
        return [text]
    current_chunk = sentences[0]
    current_embedding = get_sentence_embedding(current_chunk)
    for s in sentences[1:]:
        s = s.strip()
        if not s:
            continue
        emb = get_sentence_embedding(s)
        sim = 1 - cosine(current_embedding, emb)
        if sim > similarity_threshold and len(current_chunk + s) <= chunk_max_length:
            current_chunk += s
            current_embedding = (current_embedding + emb) / 2
        else:
            chunks.append(current_chunk)
            current_chunk = s
            current_embedding = emb
    if current_chunk:
        chunks.append(current_chunk)
    return chunks


def generate_single_entry_ollama(text):
    """è°ƒç”¨æœ¬åœ° Ollama HTTP API ç”Ÿæˆå•æ¡ JSON æŒ‡ä»¤"""
    payload = {
        "model": ollama_model,
        "prompt": f"""
            åŸºäºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆ1æ¡é«˜è´¨é‡JSONæŒ‡ä»¤æ¡ç›®ï¼š
            {text}
            
            ç”Ÿæˆæ ¼å¼ï¼š
            {{
            "instruction": "...",
            "input": "...",
            "output": "..."
            }}
        """,
        "stream": False  # ğŸ‘ˆ å…³é”®ï¼šç¦ç”¨æµå¼ï¼Œç›´æ¥ä¸€æ¬¡æ€§è¿”å›å®Œæ•´ JSON
    }
    try:
        response = requests.post(ollama_url, json=payload, timeout=120)
        if response.status_code != 200:
            logger.error(f"Ollama è¿”å›é”™è¯¯ {response.status_code}: {response.text}")
            return None

        data = response.json()
        # Ollama çš„éæµå¼è¿”å›é‡Œ "response" å­—æ®µå°±æ˜¯å®Œæ•´çš„ç”Ÿæˆæ–‡æœ¬
        return data.get("response", "").strip()

    except Exception as e:
        logger.error(f"è°ƒç”¨ Ollama ç”Ÿæˆæ¡ç›®å¤±è´¥: {e}")
        return None


def generate_dataset(folder_path, output_file_path, entries_per_file=2):
    """ç”Ÿæˆæ•°æ®é›†"""
    result_list = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):
            file_path = os.path.join(folder_path, filename)
            text = read_text_file(file_path)
            logger.info(f"å¤„ç† {filename}")
            for j in range(entries_per_file):
                logger.info(f"  ç”Ÿæˆç¬¬ {j + 1}/{entries_per_file} æ¡")
                entry = generate_single_entry_ollama(text)
                if entry:
                    result_list.append(entry)
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
    with open(output_file_path, "w", encoding="utf-8") as f:
        f.write("{\n" + ",\n".join(result_list) + "\n}")
    logger.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ° {output_file_path}")


@lifecycle
def run():
    clean_dir(output_dir)

    for root, dirs, files in os.walk(input_file_path):
        for file in files:
            input_file = os.path.join(root, file)
            ftype = get_file_type(input_file)
            if ftype == "pdf":
                txt_file = input_file + ".txt"
                pdf_to_text(input_file, txt_file)
                input_file = txt_file
            elif ftype == "unknown":
                logger.warning(f"è·³è¿‡ä¸æ”¯æŒæ–‡ä»¶ç±»å‹: {input_file}")
                continue
            text = read_text_file(input_file)
            chunks = [text]
            if len(text) > start_chunk_threshold:
                chunks = split_text_by_semantic(text, chunk_max_length, similarity_threshold)
            save_chunks_to_files(chunks, output_dir)

    logger.info("å¼€å§‹ç”Ÿæˆæ•°æ®é›†...")
    generate_dataset(output_dir, output_file_path, entries_per_file)


if __name__ == "__main__":
    run()
