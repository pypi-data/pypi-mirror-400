"""
Phase 3: CheckpointManager - Working Memoryì™€ LTM ê°„ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

ì´ ëª¨ë“ˆì€ Working Memory ìŠ¬ë¡¯ê³¼ LTM ë¸”ë¡ ê°„ì˜ ì§€ëŠ¥ì  ì²´í¬í¬ì¸íŠ¸ ì—°ê²°ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
ì²´í¬í¬ì¸íŠ¸ë¥¼ í†µí•´ ì „ì²´ LTM ê²€ìƒ‰ ëŒ€ì‹  ê´€ë ¨ì„± ë†’ì€ ì§€ì—­ë§Œ ê²€ìƒ‰í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
"""

import time
import hashlib
import json
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional
import numpy as np


class CheckpointManager:
    """Working Memoryì™€ LTM ê°„ì˜ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""
    
    def __init__(self, db_manager, block_manager):
        self.db_manager = db_manager
        self.block_manager = block_manager
        self.checkpoint_cache = {}  # ë©”ëª¨ë¦¬ ë‚´ ìºì‹œ
        self._cache_lock = threading.RLock()  # ë™ì‹œì„± ì•ˆì „ì„±ì„ ìœ„í•œ ì¬ê·€ ë½
        self.max_checkpoints_per_slot = 10  # ìŠ¬ë¡¯ë‹¹ ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸ (8ê°œ ë¸”ë¡ ì²˜ë¦¬ ë³´ì¥)
        self.max_cache_size = 1000  # ìºì‹œ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œì–´)
        self.min_relevance_threshold = 0.3  # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ê°’
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.stats = {
            "checkpoints_created": 0,
            "checkpoints_accessed": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
    def create_checkpoint(self, working_memory_slot, related_blocks: List[Dict]) -> Dict[str, Any]:
        """Working Memory ìŠ¬ë¡¯ì— LTM ì²´í¬í¬ì¸íŠ¸ ìƒì„±"""
        start_time = time.perf_counter()
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ ê³„ì‚°
            context_hash = self._compute_context_hash(working_memory_slot.context)
            
            # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° êµ¬ì„±
            checkpoint_data = {
                "slot_id": working_memory_slot.slot_id,
                "context_hash": context_hash,
                "context_preview": working_memory_slot.context[:100],  # ë””ë²„ê¹…ìš©
                "ltm_blocks": [],
                "created_at": datetime.now().isoformat(),
                "last_accessed": datetime.now().isoformat(),
                "access_count": 0,
                "relevance_scores": []
            }
            
            # ê´€ë ¨ ë¸”ë¡ë“¤ ì²˜ë¦¬
            for i, block in enumerate(related_blocks[:self.max_checkpoints_per_slot]):
                if not isinstance(block, dict) or "block_index" not in block:
                    continue
                    
                # ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚°
                distance = self._calculate_semantic_distance(
                    working_memory_slot.embedding, 
                    block.get("embedding", [])
                )
                
                relevance_score = block.get("similarity_score", 0.5)
                
                block_data = {
                    "block_index": block["block_index"],
                    "relevance_score": relevance_score,
                    "semantic_distance": distance,
                    "keywords": block.get("keywords", []),
                    "content_preview": block.get("context", "")[:50],
                    "created_at": datetime.now().isoformat()
                }
                
                checkpoint_data["ltm_blocks"].append(block_data)
                checkpoint_data["relevance_scores"].append(relevance_score)
            
            # í‰ê·  ê´€ë ¨ì„± ê³„ì‚°
            if checkpoint_data["relevance_scores"]:
                checkpoint_data["avg_relevance"] = np.mean(checkpoint_data["relevance_scores"])
            else:
                checkpoint_data["avg_relevance"] = 0.0
            
            # ë©”ëª¨ë¦¬ ë‚´ ìºì‹œì— ì €ì¥ (ìŠ¤ë ˆë“œ ì•ˆì „)
            with self._cache_lock:
                # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
                if len(self.checkpoint_cache) >= self.max_cache_size:
                    self._cleanup_cache_by_size()
                
                self.checkpoint_cache[working_memory_slot.slot_id] = checkpoint_data
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["checkpoints_created"] += 1
            
            # ì„±ëŠ¥ ë¡œê¹…
            creation_time = (time.perf_counter() - start_time) * 1000
            print(f"    âœ… ì²´í¬í¬ì¸íŠ¸ ìƒì„±: ìŠ¬ë¡¯ {working_memory_slot.slot_id}, "
                  f"{len(checkpoint_data['ltm_blocks'])}ê°œ ë¸”ë¡, "
                  f"í‰ê·  ê´€ë ¨ì„±: {checkpoint_data['avg_relevance']:.3f}, "
                  f"ì‹œê°„: {creation_time:.2f}ms")
            
            return checkpoint_data
            
        except Exception as e:
            print(f"    [ERROR] ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def update_checkpoint_access(self, slot_id: str) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._cache_lock:
            if slot_id in self.checkpoint_cache:
                self.checkpoint_cache[slot_id]["last_accessed"] = datetime.now().isoformat()
                self.checkpoint_cache[slot_id]["access_count"] += 1
                self.stats["checkpoints_accessed"] += 1
                self.stats["cache_hits"] += 1
                return True
            else:
                self.stats["cache_misses"] += 1
                return False
    
    def get_checkpoint_radius(self, slot_id: str, radius: int = 15) -> List[int]:
        """ì²´í¬í¬ì¸íŠ¸ ì£¼ë³€ ë¸”ë¡ ì¸ë±ìŠ¤ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._cache_lock:
            if slot_id not in self.checkpoint_cache:
                return []
        
            checkpoint = self.checkpoint_cache[slot_id]
        all_indices = []
        
        try:
            for block_data in checkpoint["ltm_blocks"]:
                center_index = block_data["block_index"]
                
                # block_indexë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ)
                try:
                    center_index = int(center_index)
                except (ValueError, TypeError):
                    print(f"    âš ï¸ ì˜ëª»ëœ block_index í˜•ì‹: {center_index}")
                    continue
                
                # ì¤‘ì‹¬ ë¸”ë¡ ê¸°ì¤€ Â±radius ë²”ìœ„ì˜ ë¸”ë¡ë“¤
                start_index = max(0, center_index - radius)
                end_index = center_index + radius + 1
                
                # ë²”ìœ„ ë‚´ ëª¨ë“  ì¸ë±ìŠ¤ ì¶”ê°€
                all_indices.extend(range(start_index, end_index))
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_indices = sorted(list(set(all_indices)))
            
            return unique_indices
            
        except Exception as e:
            print(f"    âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë°˜ê²½ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_checkpoint_info(self, slot_id: str) -> Optional[Dict[str, Any]]:
        """ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¡°íšŒ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._cache_lock:
            return self.checkpoint_cache.get(slot_id)
    
    def get_all_checkpoints(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._cache_lock:
            return self.checkpoint_cache.copy()
    
    def cleanup_old_checkpoints(self, max_age_hours: int = 24) -> int:
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        current_time = datetime.now()
        removed_count = 0
        
        slots_to_remove = []
        
        for slot_id, checkpoint in self.checkpoint_cache.items():
            try:
                created_time = datetime.fromisoformat(checkpoint["created_at"])
                age_hours = (current_time - created_time).total_seconds() / 3600
                
                if age_hours > max_age_hours:
                    slots_to_remove.append(slot_id)
                    
            except Exception:
                # íŒŒì‹± ì‹¤íŒ¨í•œ ì²´í¬í¬ì¸íŠ¸ë„ ì œê±°
                slots_to_remove.append(slot_id)
        
        for slot_id in slots_to_remove:
            del self.checkpoint_cache[slot_id]
            removed_count += 1
        
        if removed_count > 0:
            print(f"    ğŸ§¹ ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ {removed_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
        
        return removed_count
    
    def _cleanup_cache_by_size(self) -> int:
        """ìºì‹œ í¬ê¸° ì œí•œì„ ìœ„í•œ ì •ë¦¬ (LRU ê¸°ë°˜)"""
        if len(self.checkpoint_cache) < self.max_cache_size:
            return 0
        
        removed_count = 0
        target_size = int(self.max_cache_size * 0.8)  # 80%ê¹Œì§€ ì¤„ì„
        
        # ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (LRU)
        sorted_slots = sorted(
            self.checkpoint_cache.items(),
            key=lambda x: x[1].get("last_accessed", ""),
            reverse=False  # ì˜¤ë˜ëœ ê²ƒë¶€í„°
        )
        
        # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°
        for slot_id, _ in sorted_slots:
            if len(self.checkpoint_cache) <= target_size:
                break
            del self.checkpoint_cache[slot_id]
            removed_count += 1
        
        if removed_count > 0:
            print(f"    ğŸ§¹ ìºì‹œ í¬ê¸° ì œí•œìœ¼ë¡œ {removed_count}ê°œ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬")
        
        return removed_count
    
    def get_stats(self) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í†µê³„ ë°˜í™˜"""
        cache_hit_rate = 0.0
        total_accesses = self.stats["cache_hits"] + self.stats["cache_misses"]
        
        if total_accesses > 0:
            cache_hit_rate = self.stats["cache_hits"] / total_accesses
        
        return {
            "checkpoints_active": len(self.checkpoint_cache),
            "checkpoints_created": self.stats["checkpoints_created"],
            "checkpoints_accessed": self.stats["checkpoints_accessed"],
            "cache_hit_rate": round(cache_hit_rate, 3),
            "total_ltm_blocks": sum(
                len(cp["ltm_blocks"]) for cp in self.checkpoint_cache.values()
            )
        }
    
    def _compute_context_hash(self, context: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ ê³„ì‚°"""
        return hashlib.md5(context.encode('utf-8')).hexdigest()[:16]
    
    def _calculate_semantic_distance(self, embedding1: List[float], embedding2: List[float]) -> float:
        """ë‘ ì„ë² ë”© ê°„ì˜ ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚°"""
        try:
            # ì…ë ¥ ê²€ì¦ ê°•í™”
            if not embedding1 or not embedding2:
                return 1.0  # ìµœëŒ€ ê±°ë¦¬
            
            # ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if not isinstance(embedding1, (list, tuple, np.ndarray)):
                print(f"    âš ï¸ embedding1ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(embedding1)}")
                return 1.0
            
            if not isinstance(embedding2, (list, tuple, np.ndarray)):
                print(f"    âš ï¸ embedding2ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(embedding2)}")
                return 1.0
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            vec1 = np.array(embedding1, dtype=float)
            vec2 = np.array(embedding2, dtype=float)
            
            # ë²¡í„° í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ìµœëŒ€ ê±°ë¦¬ ë°˜í™˜
            if len(vec1) != len(vec2):
                return 1.0
            
            # ë²¡í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìµœëŒ€ ê±°ë¦¬ ë°˜í™˜
            if vec1.size == 0 or vec2.size == 0:
                return 1.0
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 1.0
            
            cosine_similarity = dot_product / (norm1 * norm2)
            
            # ê±°ë¦¬ëŠ” 1 - ìœ ì‚¬ë„
            distance = 1.0 - cosine_similarity
            
            return max(0.0, min(1.0, distance))  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
            
        except Exception as e:
            print(f"    âš ï¸ ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 1.0  # ì˜¤ë¥˜ ì‹œ ìµœëŒ€ ê±°ë¦¬ ë°˜í™˜