
# Autogenerated by mlir-tblgen; don't manually edit.

from ._ods_common import _cext as _ods_cext
from ._ods_common import (
    equally_sized_accessor as _ods_equally_sized_accessor,
    get_default_loc_context as _ods_get_default_loc_context,
    get_op_results_or_values as _get_op_results_or_values,
    segmented_accessor as _ods_segmented_accessor,
)
_ods_ir = _ods_cext.ir
_ods_cext.globals.register_traceback_file_exclusion(__file__)

import builtins
from typing import Sequence as _Sequence, Union as _Union, Optional as _Optional


@_ods_cext.register_dialect
class _Dialect(_ods_ir.Dialect):
  DIALECT_NAMESPACE = "iree_tensor_ext"

@_ods_cext.register_operation(_Dialect)
class BitCastOp(_ods_ir.OpView):
  r"""
  Bitcasts a tensor |source| to the shape implied by this operations result
  type interleaved with |result_dims|, potentially with a different element
  type. For example,
  
  ```
  result_dims = {%0, %1}
  result_type = tensor<1x?x2x?x3 x!eltype>
  ```
  
  produces a tensor of shape [1, %0, 2, %1, 3] and element type `!eltype`.
  Note that the source and result tensors must serialized to the same size.
  
  Different from the flow.bitcast op, the op is allowed to be cloned into
  dispatch regions and supports transformations required by executable
  translation such as bufferization.
  """

  OPERATION_NAME = "iree_tensor_ext.bitcast"

  _ODS_OPERAND_SEGMENTS = [1,-1,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, result, source, source_dims, result_dims, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(_get_op_results_or_values(source_dims))
    operands.append(_get_op_results_or_values(result_dims))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value[_ods_ir.RankedTensorType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def source_dims(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def result_dims(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def result(self) -> _ods_ir.OpResult[_ods_ir.RankedTensorType]:
    return self.operation.results[0]

def bitcast(result, source, source_dims, result_dims, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return BitCastOp(result=result, source=source, source_dims=source_dims, result_dims=result_dims, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class CastToRaggedShapeOp(_ods_ir.OpView):
  r"""
  The op takes a shaped type and reinterprets its contents as a ragged shaped type. The
  dimensions specified by `ragged_dim` in the source is interpreted as two
  dimensions at `ragged_dim` and `ragged_dim + 1` in the result shaped type. The
  result shaped type has a rank of one more than that of the `source`. The result
  type also has the encoding set to an attribute of type `RaggedShapeAttr`
  with the specified `ragged_dim` used to specify the `raggedRow` in the
  encoding attribute. (See description of `RaggedShapeAttr`). The number of
  ragged rows is specified using `num_ragged_rows` argument. The size of each
  row is specified using `column_lengths` argument, which is a single rank
  shaped type of size `num_rows + 1` and where size of the `i`-th row of the result
  is computed using
  
  ```
  column_lengths[i+1] - column_lengths[i]
  ```
  
  In other words, `column_lengths` is same as the `ROW_INDEX` in CSR
  representation (https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)).
  An optional argument `max_ragged_column_length` allows specifying the dynamic
  upper bound for the number of columns (for static upper bounds the shape of the
  dimension in the result shaped type encodes this value)
  
  For example, if the `n` and `n+1` dimensions of the result represent this ragged structure
  
  ```
  OOOO
  OOOOOO
  OOO
  OOOOO
  ```
  
  then,
  - `ragged_dim` is `n`
  - `num_ragged_rows` is 4
  - `column_lengths` is [0, 4, 10, 13, 18]
  - result has an encoding attribute of `#iree_tensor_ext.ragged_shape<n>`.
  
  Contraints are
  1. `ragged_dim` < rank(`source`)
  2. rank(`columnLengths`) == 1.
  3. dim(`columnLengths`, 0) == `num_rows` + 1.
  4. dim(`result`, `raggedRowDim` + 1) >=
      (`columnLengths`[k+1] - `columnLengths`[k])
      for all 0 <= k < dim(`result`, `raggedRowDim`)
  """

  OPERATION_NAME = "iree_tensor_ext.cast_to_ragged_shape"

  _ODS_OPERAND_SEGMENTS = [1,1,0,0,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, result, source, raggedDim, columnLengths, sourceDynamicDims, *, numRaggedRows=None, avgRaggedColumnLength=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(columnLengths)
    operands.append(numRaggedRows)
    operands.append(avgRaggedColumnLength)
    operands.append(_get_op_results_or_values(sourceDynamicDims))
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["raggedDim"] = (raggedDim if (
    isinstance(raggedDim, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('IndexAttr')) else
      _ods_ir.AttrBuilder.get('IndexAttr')(raggedDim, context=_ods_context))
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def columnLengths(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def numRaggedRows(self) -> _Optional[_ods_ir.Value[_ods_ir.IndexType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def avgRaggedColumnLength(self) -> _Optional[_ods_ir.Value[_ods_ir.IndexType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def sourceDynamicDims(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 4)
    return operand_range

  @builtins.property
  def raggedDim(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["raggedDim"]

  @raggedDim.setter
  def raggedDim(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["raggedDim"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def cast_to_ragged_shape(result, source, ragged_dim, column_lengths, source_dynamic_dims, *, num_ragged_rows=None, avg_ragged_column_length=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return CastToRaggedShapeOp(result=result, source=source, raggedDim=ragged_dim, columnLengths=column_lengths, sourceDynamicDims=source_dynamic_dims, numRaggedRows=num_ragged_rows, avgRaggedColumnLength=avg_ragged_column_length, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ComputeBarrierEndOp(_ods_ir.OpView):
  r"""
  This barrier prevents certain transformations (like reshape propagation)
  from moving operations up across this boundary.
  The operand is returned as the result with identical type.
  """

  OPERATION_NAME = "iree_tensor_ext.compute_barrier.end"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, value, value_dims, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.extend(_get_op_results_or_values(value_dims))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value[_ods_ir.RankedTensorType]:
    return self.operation.operands[0]

  @builtins.property
  def value_dims(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 2 + 1
    return self.operation.operands[1:1 + _ods_variadic_group_length]

  @builtins.property
  def result(self) -> _ods_ir.OpResult[_ods_ir.RankedTensorType]:
    return self.operation.results[0]

def compute_barrier_end(result, value, value_dims, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return ComputeBarrierEndOp(result=result, value=value, value_dims=value_dims, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ComputeBarrierStartOp(_ods_ir.OpView):
  r"""
  This barrier prevents certain transformations (like reshape propagation)
  from moving operations down across this boundary.
  The operand is returned as the result with identical type.
  """

  OPERATION_NAME = "iree_tensor_ext.compute_barrier.start"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, value, value_dims, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.extend(_get_op_results_or_values(value_dims))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value[_ods_ir.RankedTensorType]:
    return self.operation.operands[0]

  @builtins.property
  def value_dims(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 2 + 1
    return self.operation.operands[1:1 + _ods_variadic_group_length]

  @builtins.property
  def result(self) -> _ods_ir.OpResult[_ods_ir.RankedTensorType]:
    return self.operation.results[0]

def compute_barrier_start(result, value, value_dims, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return ComputeBarrierStartOp(result=result, value=value, value_dims=value_dims, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class DispatchTensorLoadOp(_ods_ir.OpView):
  r"""
  Loads an input tensor or subtensor from an input placeholder. As each
  workgroup executes concurrently all workgroups will receive identical loaded
  results of regions that may overlap.
  """

  OPERATION_NAME = "iree_tensor_ext.dispatch.tensor.load"

  _ODS_OPERAND_SEGMENTS = [1,-1,-1,-1,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, result, source, source_dims, offsets, sizes, strides, static_offsets, static_sizes, static_strides, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(_get_op_results_or_values(source_dims))
    operands.append(_get_op_results_or_values(offsets))
    operands.append(_get_op_results_or_values(sizes))
    operands.append(_get_op_results_or_values(strides))
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["static_offsets"] = (static_offsets if (
    isinstance(static_offsets, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(static_offsets, context=_ods_context))
    attributes["static_sizes"] = (static_sizes if (
    isinstance(static_sizes, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(static_sizes, context=_ods_context))
    attributes["static_strides"] = (static_strides if (
    isinstance(static_strides, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(static_strides, context=_ods_context))
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def source_dims(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def offsets(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sizes(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def strides(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 4)
    return operand_range

  @builtins.property
  def static_offsets(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["static_offsets"]

  @static_offsets.setter
  def static_offsets(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["static_offsets"] = value

  @builtins.property
  def static_sizes(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["static_sizes"]

  @static_sizes.setter
  def static_sizes(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["static_sizes"] = value

  @builtins.property
  def static_strides(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["static_strides"]

  @static_strides.setter
  def static_strides(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["static_strides"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult[_ods_ir.RankedTensorType]:
    return self.operation.results[0]

def dispatch_tensor_load(result, source, source_dims, offsets, sizes, strides, static_offsets, static_sizes, static_strides, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return DispatchTensorLoadOp(result=result, source=source, source_dims=source_dims, offsets=offsets, sizes=sizes, strides=strides, static_offsets=static_offsets, static_sizes=static_sizes, static_strides=static_strides, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class DispatchTensorStoreOp(_ods_ir.OpView):
  r"""
  Stores a tensor or subtensor into an output tensor placeholder. As each
  workgroup executes concurrently behavior is undefined if more than one
  workgroup stores into overlapping regions of the full output tensor.
  """

  OPERATION_NAME = "iree_tensor_ext.dispatch.tensor.store"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,-1,-1,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, target, target_dims, offsets, sizes, strides, static_offsets, static_sizes, static_strides, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(target)
    operands.append(_get_op_results_or_values(target_dims))
    operands.append(_get_op_results_or_values(offsets))
    operands.append(_get_op_results_or_values(sizes))
    operands.append(_get_op_results_or_values(strides))
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["static_offsets"] = (static_offsets if (
    isinstance(static_offsets, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(static_offsets, context=_ods_context))
    attributes["static_sizes"] = (static_sizes if (
    isinstance(static_sizes, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(static_sizes, context=_ods_context))
    attributes["static_strides"] = (static_strides if (
    isinstance(static_strides, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(static_strides, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value[_ods_ir.RankedTensorType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def target(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def target_dims(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def offsets(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def sizes(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 4)
    return operand_range

  @builtins.property
  def strides(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 5)
    return operand_range

  @builtins.property
  def static_offsets(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["static_offsets"]

  @static_offsets.setter
  def static_offsets(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["static_offsets"] = value

  @builtins.property
  def static_sizes(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["static_sizes"]

  @static_sizes.setter
  def static_sizes(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["static_sizes"] = value

  @builtins.property
  def static_strides(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["static_strides"]

  @static_strides.setter
  def static_strides(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["static_strides"] = value

def dispatch_tensor_store(value, target, target_dims, offsets, sizes, strides, static_offsets, static_sizes, static_strides, *, loc=None, ip=None) -> DispatchTensorStoreOp:
  return DispatchTensorStoreOp(value=value, target=target, target_dims=target_dims, offsets=offsets, sizes=sizes, strides=strides, static_offsets=static_offsets, static_sizes=static_sizes, static_strides=static_strides, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class DispatchWorkgroupCountFromDagRootOp(_ods_ir.OpView):
  r"""
  When using tile + distribution of the root of the DAG (Directed Acyclic
  Graph) of ops within the dispatch to split the work amongst workgroups. The
  workload captured is the size of the iteration space of the root of the DAG.
  This op represents the computation that given the workload returns the
  number of workgroups to use. The backends are responsible for lowering this
  op into actual computation (typically based on the tile sizes used to tile
  and distribute the root of the DAG).
  """

  OPERATION_NAME = "iree_tensor_ext.dispatch.workgroup_count_from_dag_root"

  _ODS_REGIONS = (0, True)

  def __init__(self, operands_, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(operands_))
    _ods_context = _ods_get_default_loc_context(loc)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def operands_(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

  @builtins.property
  def x(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[0]

  @builtins.property
  def y(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[1]

  @builtins.property
  def z(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[2]

def dispatch_workgroup_count_from_dag_root(operands_, *, results=None, loc=None, ip=None) -> _ods_ir.OpResultList:
  return DispatchWorkgroupCountFromDagRootOp(operands_=operands_, results=results, loc=loc, ip=ip).results

@_ods_cext.register_operation(_Dialect)
class DispatchWorkgroupCountFromSliceOp(_ods_ir.OpView):
  r"""
  The default computation of the number of workgroups (or workgroup count)
  assumes that the dispatch + captured values is enough to compute the
  workgroup count. It does so by using a program slice of the values
  within the dispatch that represent the number of workgroups when available
  within the dispatch.
  Currently the arguments of index types captured by the
  `flow.dispatch.workgroups` is treated as the workload for the operation.
  It is a requirement that the slice of the program that computes the
  number of workgroups will need to have its leaves be these captured values.
  
  TODO: This could be generalized in future to allow the slices to encompass
  arbitrary computation. The computation of the workgroup count can then be
  done on the device itself, if this is data dependent. In such cases the
  workload could be more than just values of index types.
  """

  OPERATION_NAME = "iree_tensor_ext.dispatch.workgroup_count_from_slice"

  _ODS_REGIONS = (0, True)

  def __init__(self, operands_, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(operands_))
    _ods_context = _ods_get_default_loc_context(loc)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def operands_(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

  @builtins.property
  def x(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[0]

  @builtins.property
  def y(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[1]

  @builtins.property
  def z(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[2]

def dispatch_workgroup_count_from_slice(operands_, *, results=None, loc=None, ip=None) -> _ods_ir.OpResultList:
  return DispatchWorkgroupCountFromSliceOp(operands_=operands_, results=results, loc=loc, ip=ip).results

@_ods_cext.register_operation(_Dialect)
class DispatchWorkgroupCountSplitReductionModifierOp(_ods_ir.OpView):
  r"""
  A split reduction dispatch contains an `scf.forall` within which the
  partial reduction computation is done. The `scf.forall` represents parallel
  partial reduction computations. `workgroup_x`, `workgroup_y`, and
  `workgroup_z` represent the workgroups needed to execute the inner compute
  ops. This is combined with `workload` to determine the total number of
  workgroups required to run the computation after accounting for the
  `scf.forall`.
  """

  OPERATION_NAME = "iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier"

  _ODS_REGIONS = (0, True)

  def __init__(self, workgroup_x, workgroup_y, workgroup_z, workload, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(workgroup_x)
    operands.append(workgroup_y)
    operands.append(workgroup_z)
    operands.extend(_get_op_results_or_values(workload))
    _ods_context = _ods_get_default_loc_context(loc)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def workgroup_x(self) -> _ods_ir.Value[_ods_ir.IndexType]:
    return self.operation.operands[0]

  @builtins.property
  def workgroup_y(self) -> _ods_ir.Value[_ods_ir.IndexType]:
    return self.operation.operands[1]

  @builtins.property
  def workgroup_z(self) -> _ods_ir.Value[_ods_ir.IndexType]:
    return self.operation.operands[2]

  @builtins.property
  def workload(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 4 + 1
    return self.operation.operands[3:3 + _ods_variadic_group_length]

  @builtins.property
  def result_x(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[0]

  @builtins.property
  def result_y(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[1]

  @builtins.property
  def result_z(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[2]

def dispatch_workgroup_count_split_reduction_modifier(workgroup_x, workgroup_y, workgroup_z, workload, *, results=None, loc=None, ip=None) -> _ods_ir.OpResultList:
  return DispatchWorkgroupCountSplitReductionModifierOp(workgroup_x=workgroup_x, workgroup_y=workgroup_y, workgroup_z=workgroup_z, workload=workload, results=results, loc=loc, ip=ip).results

@_ods_cext.register_operation(_Dialect)
class DispatchWorkloadOrdinalOp(_ods_ir.OpView):
  r"""
  The arguments that represent the captured/returned values of the
  `flow.dispatch.workgroups`, i.e. the signature of the body of the op is not
  preserved during IREEs compilation. Since the workloads are derived from
  the operands captured by the operation, this op denotes the values captured
  as workloads. This can be used in the backends to map back to the workload
  values while materializing the workgroup count computation.
  
  TODO: Find a better way to represent this information, either by somehow
  propagating the signature of the created dispatch workgroup op through
  the compilation stack until the codegen backends, or as a separate
  list/attribute that can be plumbed through without using explicit ops.
  """

  OPERATION_NAME = "iree_tensor_ext.dispatch.workload.ordinal"

  _ODS_REGIONS = (0, True)

  def __init__(self, operand, ordinal, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(operand)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["ordinal"] = (ordinal if (
    isinstance(ordinal, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('IndexAttr')) else
      _ods_ir.AttrBuilder.get('IndexAttr')(ordinal, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def operand(self) -> _ods_ir.Value[_ods_ir.IndexType]:
    return self.operation.operands[0]

  @builtins.property
  def ordinal(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["ordinal"]

  @ordinal.setter
  def ordinal(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["ordinal"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult[_ods_ir.IndexType]:
    return self.operation.results[0]

def dispatch_workload_ordinal(operand, ordinal, *, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return DispatchWorkloadOrdinalOp(operand=operand, ordinal=ordinal, results=results, loc=loc, ip=ip).result
