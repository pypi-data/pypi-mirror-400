# -*- coding: utf-8 -*-
"""
arthub_api.utils
~~~~~~~~~~~~~~

This module provides utilities that are used within API
"""
import logging
import os
import shutil
import time
import random
import string
import requests
import threading
from . import models
from platformdirs import user_cache_dir
from .config import (
    api_config_oa,
    api_config_qq,
    api_config_public,
    api_config_oa_test,
    api_config_qq_test
)
from base64 import b64encode, b64decode
from Crypto.PublicKey import RSA
from Crypto.Cipher import PKCS1_OAEP
from Crypto.Hash import SHA256

from .__version__ import __title__

logger = logging.getLogger(__title__)


def _path_preprocess(path):
    path = path.strip()
    path = path.rstrip("\\/")
    return path


def create_empty_file(path):
    try:
        open(path, "w").close()
        return True
    except Exception:
        return False


def mkdir(path):
    path = _path_preprocess(path)
    if os.path.isdir(path):
        return True
    if os.path.isfile(path):
        return False
    try:
        os.makedirs(path)
    except Exception as e:
        return False
    return True


def remove(path):
    path = _path_preprocess(path)
    if not os.path.exists(path):
        return True
    try:
        if os.path.isfile(path):
            os.remove(path)
        elif os.path.isdir(path):
            shutil.rmtree(path)
    except Exception as e:
        return False
    return True


def current_milli_time():
    return (lambda: int(round(time.time() * 1000)))()


def get_random_string(length):
    return ''.join(random.sample(string.ascii_letters + string.digits, length))


class UploadFilePartReader(object):
    def __init__(self, file_, offset, length, callback=None):
        self._file = file_
        self._file.seek(offset)
        self._total_size = length
        self._completed_size = 0
        self._finished = False
        self._callback = callback

    def read(self, size=-1):
        if size == -1:
            self._finished = True
            return ""
        uncompleted_size = self._total_size - self._completed_size
        size_to_read = min(uncompleted_size, size)
        if size_to_read == 0:
            self._finished = True
            return ""
        
        # Read data, retry if read is incomplete
        # Compatible with Python 2 and 3: use file.read() return type directly
        # Python 2: file.read() returns str
        # Python 3: file.read() returns bytes
        content = self._file.read(size_to_read)
        actual_read = len(content) if content else 0
        remaining = size_to_read - actual_read
        max_read_attempts = 10  # Maximum 10 attempts to avoid infinite loop
        read_attempts = 0
        
        # If first read is incomplete, continue reading
        while remaining > 0 and read_attempts < max_read_attempts:
            chunk = self._file.read(remaining)
            if not chunk:
                # File read ended, but there's still data to read
                break
            # Compatible with Python 2 and 3: content and chunk have same type (both str or both bytes)
            content = content + chunk
            chunk_size = len(chunk)
            actual_read += chunk_size
            remaining -= chunk_size
            read_attempts += 1
        
        actual_size = len(content) if content else 0
        
        # Validate read data length
        if actual_size < size_to_read:
            # Check if this is normal file end case
            # If completed data + actual read data >= total size, it's file end
            remaining_expected = self._total_size - self._completed_size
            if actual_size == remaining_expected:
                # This is normal file end case, no warning needed
                logger.debug(
                    "[UploadFilePartReader] Read %d bytes at file end (requested %d, remaining %d). "
                    "This is normal for the last chunk.",
                    actual_size, size_to_read, remaining_expected
                )
            else:
                # Read incomplete, log warning
                logger.warning(
                    "[UploadFilePartReader] File read incomplete: requested %d bytes, got %d bytes. "
                    "Completed: %d/%d bytes. Remaining expected: %d. This may cause file corruption.",
                    size_to_read, actual_size, self._completed_size, self._total_size, remaining_expected
                )
                # If read data is significantly less than requested (less than 90%), log error
                if actual_size < size_to_read * 0.9:
                    logger.error(
                        "[UploadFilePartReader] Critical: File read severely incomplete. "
                        "Requested %d bytes, got %d bytes (%.1f%%). Upload may fail or cause corruption.",
                        size_to_read, actual_size, (actual_size / size_to_read * 100) if size_to_read > 0 else 0
                    )
        
        self._completed_size += actual_size
        if self._callback:
            self._callback(actual_size)
        return content


def upload_part_of_file(url, file_path, offset, length, callback=None, timeout=30):
    try:
        if not os.path.isfile(file_path):
            return models.Result(False, error_message="file \"%s\" not exist" % file_path)
        
        # Validate file size to ensure readable range
        try:
            file_size = os.path.getsize(file_path)
            if offset >= file_size:
                return models.Result(False, error_message="file offset %d exceeds file size %d" % (offset, file_size))
            if offset + length > file_size:
                logger.warning(
                    "[UploadFilePart] Requested range %d-%d exceeds file size %d. "
                    "Adjusting length from %d to %d.",
                    offset, offset + length, file_size, length, file_size - offset
                )
                length = file_size - offset
        except Exception as e:
            logger.warning("[UploadFilePart] Failed to get file size: %s, continuing anyway", str(e))
            file_size = None
        
        with open(file_path, 'rb') as file_:
            reader = UploadFilePartReader(file_, offset, length, callback)
            res = requests.put(url, data=reader, headers={
                "Accept": "application/json",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Content-Type": "application/octet-stream",
                "Content-Length": str(length)
            }, timeout=timeout)
            
            # Validate actual read data length after upload
            # Only log warning if read is incomplete, but don't fail the upload
            # as the read retry logic in UploadFilePartReader should have handled it
            if reader._completed_size != length:
                # Check if this is normal file end case
                if file_size is not None:
                    expected_read = file_size - offset
                    if reader._completed_size == expected_read:
                        # This is normal file end case
                        logger.debug(
                            "[UploadFilePart] Read %d bytes (expected %d, file end). This is normal for the last chunk.",
                            reader._completed_size, length
                        )
                    else:
                        # Read incomplete - log warning but don't fail (retry logic should handle)
                        logger.warning(
                            "[UploadFilePart] Read incomplete: expected %d bytes, got %d bytes. "
                            "File size: %d, offset: %d. Upload may have issues.",
                            length, reader._completed_size, file_size, offset
                        )
                else:
                    # File size unknown, just log warning
                    logger.warning(
                        "[UploadFilePart] Read incomplete: expected %d bytes, got %d bytes. "
                        "File size unknown, offset: %d.",
                        length, reader._completed_size, offset
                    )
            
            if not res.ok:
                return models.Result(False, error_message="status code: %d, response: %s" % (res.status_code, res.text))
            return models.Result(True, data=res)
    except requests.exceptions.Timeout as e:
        error_message = "upload request timeout after %d seconds: %s" % (timeout, str(e))
        logger.error("[UploadFilePart] %s" % error_message)
        return models.Result(False, error_message=error_message, can_retry=True)
    except requests.exceptions.RequestException as e:
        error_message = "upload request failed: %s" % str(e)
        logger.error("[UploadFilePart] %s" % error_message)
        return models.Result(False, error_message=error_message, can_retry=True)
    except Exception as e:
        error_message = "unexpected error during upload: %s" % str(e)
        logger.error("[UploadFilePart] %s" % error_message)
        return models.Result(False, error_message=error_message)


class UploadFileReader(object):
    def __init__(self, file_, callback):
        self._file = file_
        self._file.seek(0)
        self._total_size = os.path.getsize(file_.name)
        self._completed_size = 0
        self._finished = False
        self._callback = callback

    def read(self, size=-1):
        if size == -1:
            self._finished = True
            return ""
        
        # Read data, retry if read is incomplete
        # Compatible with Python 2 and 3: use file.read() return type directly
        content = self._file.read(size)
        actual_read = len(content) if content else 0
        remaining = size - actual_read
        max_read_attempts = 10  # Maximum 10 attempts to avoid infinite loop
        read_attempts = 0
        
        # If first read is incomplete, continue reading
        while remaining > 0 and read_attempts < max_read_attempts:
            chunk = self._file.read(remaining)
            if not chunk:
                # File read ended
                break
            # Compatible with Python 2 and 3: content and chunk have same type (both str or both bytes)
            content = content + chunk
            chunk_size = len(chunk)
            actual_read += chunk_size
            remaining -= chunk_size
            read_attempts += 1
        
        # Validate read data length
        actual_size = len(content) if content else 0
        if actual_size < size and self._completed_size + actual_size < self._total_size:
            # Read incomplete, and file still has remaining data, log warning
            logger.warning(
                "[UploadFileReader] File read incomplete: requested %d bytes, got %d bytes. "
                "Completed: %d/%d bytes. This may cause file corruption.",
                size, actual_size, self._completed_size, self._total_size
            )
        
        self._completed_size += actual_size
        if self._callback:
            self._callback(actual_size)
        return content


def upload_file(url, file_path, callback=None, timeout=60):
    try:
        if not os.path.isfile(file_path):
            return models.Result(False, error_message="file \"%s\" not exist" % file_path)
        with open(file_path, 'rb') as file_:
            res = requests.put(url, data=UploadFileReader(file_, callback), headers={
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Content-Type": "application/octet-stream",
                "Content-Length": str(os.path.getsize(file_path))
            }, timeout=timeout)
            if not res.ok:
                return models.Result(False, error_message="status code: %d" % res.status_code)
            return models.Result(True, data=res)
    except Exception as e:
        error_message = "send request \"%s\" exception \"%s\"" % (url, str(e))
        logger.error("[UploadFile] %s" % error_message)
        return models.Result(False, error_message=error_message)


def download_file(url, file_path, progress_cb=None, total_size=None, timeout=60,
                  progress_min_bytes=256 * 1024, progress_min_interval_ms=250, chunk_size=256 * 1024,
                  progress_percent_step=0.01):
    try:
        if os.path.exists(file_path):
            remove(file_path)

        if not create_empty_file(file_path):
            return models.Result(False, error_message="create \"%s\" failed" % file_path)

        # download file
        download_dir_path = os.path.dirname(file_path)
        if not mkdir(download_dir_path):
            return models.Result(False, error_message="create directory \"%s\" failed" % download_dir_path)

        res_download = requests.get(url, stream=True, timeout=timeout)

        if not res_download:
            return models.Result(False, error_message="request \"%s\" failed" % url)
        bytes_completed = 0
        stop_event = threading.Event()

        def _reporter():
            try:
                # fixed-interval reporting regardless of delta
                while not stop_event.wait(progress_min_interval_ms / 1000.0):
                    if progress_cb:
                        try:
                            progress_cb(bytes_completed, total_size if total_size is not None else 0)
                        except Exception:
                            pass
            except Exception:
                pass

        reporter_thread = None
        try:
            if progress_cb:
                reporter_thread = threading.Thread(target=_reporter)
                # Python 2/3 compatible: use setDaemon for Python 2, daemon property for Python 3
                try:
                    reporter_thread.daemon = True
                except AttributeError:
                    # Python 2.7 compatibility
                    reporter_thread.setDaemon(True)
                reporter_thread.start()

            with open(file_path, "ab") as f:
                for chunk in res_download.iter_content(chunk_size=chunk_size):
                    if not chunk:
                        continue
                    f.write(chunk)
                    f.flush()
                    bytes_completed += len(chunk)

            # ensure a final progress callback at completion
            if progress_cb:
                try:
                    progress_cb(bytes_completed, total_size if total_size is not None else 0)
                except Exception:
                    pass
        finally:
            try:
                stop_event.set()
                if reporter_thread is not None:
                    reporter_thread.join(timeout=1.0)
            except Exception:
                pass

        return models.Result(True)

    except Exception as e:
        return models.Result(False, error_message=e)


def get_download_file_size(url):
    try:
        # Send a GET request with Range header to get the file size
        headers = {'Range': 'bytes=0-0'}
        response = requests.get(url, headers=headers, allow_redirects=True)

        # Check the response status code
        if response.status_code == 416:
            return models.Result(True, data=0)  # Requested range not satisfiable
        elif response.status_code == 403:
            return models.Result(False, error_message="URL expired or access forbidden")

        # Try to get the file size from the response
        file_size = get_capacity_from_response(response)
        if file_size is None:
            return models.Result(False, error_message="Can't parse content-range from response")

        return models.Result(True, data=file_size)

    except Exception as e:
        return models.Result(False, error_message=str(e))

def get_capacity_from_response(response):
    # Check the Content-Range header
    content_range = response.headers.get('Content-Range')
    if content_range is None:
        return None

    # Parse the Content-Range header
    try:
        range_str = content_range
        pos = range_str.find("/")
        if pos == -1:
            return None
        size_str = range_str[pos + 1:]
        return int(size_str)  # Convert file size to integer
    except ValueError:
        return None


def splite_path(path_):
    path_list = []
    while path_:
        l = os.path.split(path_)
        path_ = l[0]
        if l[1]:
            path_list.insert(0, l[1])
    return path_list


def rename_path_text(path_):
    path_ = _path_preprocess(path_)
    path_without_ext, ext = os.path.splitext(path_)
    suffix_number = 1
    while os.path.exists(path_):
        path_ = "%s (%d)%s" % (path_without_ext, suffix_number, ext)
        suffix_number += 1
    return path_


def parse_cookies(cookie_str):
    cookies = {}
    cookie_strs = cookie_str.split(';')
    for _item in cookie_strs:
        _item = _item.strip()
        _pair = _item.split('=')
        if len(_pair) == 2:
            cookies[_pair[0]] = _pair[1]
    return cookies


def rename_path(src, dest):
    if not mkdir(os.path.dirname(dest)):
        return False
    try:
        shutil.move(src, dest)
    except Exception:
        return False
    return True


def count_files(root_path):
    total_files = 0
    if not os.path.exists(root_path):
        return total_files
    item_list = os.listdir(root_path)
    if len(item_list) == 0:
        return total_files
    for item in item_list:
        next_path = os.path.join(root_path, item)
        if os.path.isfile(next_path):
            total_files += 1
        else:
            total_files += count_files(next_path)
    return total_files


def read_file(file_path):
    with open(file_path, "r") as file_obj:
        return file_obj.read()


def write_file(file_path, data):
    with open(file_path, "w") as file_obj:
        file_obj.write(data)


def get_cache_dir(api_host):
    root = user_cache_dir(appname="arthub", opinion=False)
    d = os.path.join(root, api_host)
    try:
        os.makedirs(d)
    except Exception as e:
        pass
    return d


def get_token_cache_file(api_host):
    root = get_cache_dir(api_host)
    return os.path.join(root, "arthub_token")


def get_token_from_cache(api_host):
    token_file = get_token_cache_file(api_host)
    try:
        if os.path.exists(token_file):
            return read_file(token_file)
    except Exception as e:
        logger.warning("[TokenCache] get token from file \"%s\" error: %s",
                       token_file, str(e))
    return ""


def save_token_to_cache(token, api_host):
    token_file = get_token_cache_file(api_host)
    try:
        write_file(token_file, token)
    except Exception as e:
        logger.warning("[TokenCache] save token to file \"%s\" error: %s",
                       token_file, str(e))


def remove_token_cache_file(api_host):
    remove(get_token_cache_file(api_host))


def get_config_by_name(env, default_config=None):
    _env_map = {
        "oa": api_config_oa,
        "qq": api_config_qq,
        "oa_test": api_config_oa_test,
        "qq_test": api_config_qq_test,
        "public": api_config_public
    }
    c = _env_map.get(env)
    if not c:
        if default_config:
            return default_config
        return api_config_oa
    return c


def encrypt(public_key_str, plain_text):
    try:
        public_key_bytes = b64decode(public_key_str)
        public_key = RSA.import_key(public_key_bytes)

        block_size = 256 - 2 * 32 - 2
        plain_text_blocks = [plain_text[i:i + block_size] for i in range(0, len(plain_text), block_size)]

        encrypted_blocks = []
        for block in plain_text_blocks:
            cipher = PKCS1_OAEP.new(public_key, hashAlgo=SHA256)
            encrypted_block = cipher.encrypt(block.encode())
            encrypted_blocks.append(b64encode(encrypted_block).decode())

        encrypted_text = ".".join(encrypted_blocks)
        return encrypted_text
    except Exception as e:
        print("Error during encryption:", str(e))
        return ""
