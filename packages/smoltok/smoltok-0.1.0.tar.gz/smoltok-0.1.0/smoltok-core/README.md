# smoltok ğŸ¦€

Byte-Pair Encoding tokenizer implementation in Rust with Python bindings.

The main goal of this project is to practice Rust ğŸš€ and benchmark Rust vs. Python performance for the tokenization task. I put effort into building it as a clean, well-structured reference, but it's not meant to be a production library.

## Features

- Basic BPE tokenizer implementation
- BPE with regex-based split and special tokens handling
- Parallel regex-based tokenizer with rayon, processing each chunk in parallel after split
- Saving/loading of training tokenizers and visualization of learned merges
- Python bindings and benchmark scripts (any Hugging Face dataset or random Unicode data)
- High-level walkthrough to build your own tokenizer from scratch and re-implementing this project!

## Benchmark Results

### Wikitext

Here are results of training [Rust RegexBPETokenizer](smoltok-core/src/regex/config.rs) vs. [Rust ParallelRegexBPETokenizer](smoltok-core/src/regex/config_parallel.rs) vs. [Python RegexBPETokenizer](smoltok-py/py_impl/src/regex.py) on [Wikitext dataset](https://huggingface.co/datasets/Salesforce/wikitext/viewer/wikitext-103-raw-v1) test set (1.2 MB) on M2 Pro MacBook:

| Vocab size | Rust (s) | Rust Parallel (s) | Python (s) | Rust vs Python |
|------------|----------|-------------------|------------|----------------|
| 512        | 3.83     | 3.40              | 94.87      | 24.8Ã—          |
| 1024       | 9.32     | 9.80              | 271.26     | 29.1Ã—          |
| 2048       | 18.92    | 22.12             | 589.53     | 31.2Ã—          |

![Wikitext benchmark test set](assets/bench-wikitest-test.png)

Rust provides **~25â€“31Ã— speedup** as vocab grows from 512 â†’ 2048. Scaling with vocab size is much better in Rust: mildly superlinear vs. clearly more superlinear in Python. For this small dataset with many merges, the parallel version is slower due to overhead!; it starts to make more sense on larger inputs:

![Wikitext benchmark train set](assets/bench-wikitest-train.png)

Even a 1 MB dataset with 1k merges is enough to learn realistic full-word tokens:

```
...
 A + ug:  Aug
 c + ould:  could
 f + ound:  found
in + ed: ined
er + ies: eries
 l + ike:  like
 w + ind:  wind
h + n: hn
 or + d:  ord
 al + ong:  along
all + ed: alled
 m + ain:  main
 Aug + ust:  August
...
```

To reproduce, run `cd smoltok-py && make bench-download && make bench-wikitext`.

### Random Data

On random Unicode data, Rust provides ~8Ã— speedup with similar scaling characteristics:

| Operation | Python        | Rust         | Rust Parallel | Rust vs Python | Parallel vs Python |
|-----------|---------------|--------------|---------------|----------------|--------------------|
| Train     | 1.58s Â± 111ms | 192ms Â± 2ms  | 435ms Â± 21ms  | 8.2Ã—           | 3.6Ã—               |
| Encode    | 55ms Â± 11ms   | 12ms Â± 193Âµs | 4.5ms Â± 189Âµs | 4.5Ã—           | 12.3Ã—              |
| Decode    | 1.4ms Â± 80Âµs  | 847Âµs Â± 16Âµs | 1.1ms Â± 112Âµs | 1.7Ã—           | 1.3Ã—               |

Once again, in this setup parallel training hurts due to many very small chunks (since it's random data), but parallel encoding provides benefits.

![Random data benchmark](assets/bench-random-data.png)

For context, random multilingual sample is:

```
äºœÃ‰ê³¾à¤­Ó˜à¥³Íµä¸–ê±™à¸°à¹†Ù’ê²™Ñ”à¥¥Ã©Ö¨Î€ê²‡Û·ÙµÓ­ä¸…Ñ„Ğ»ä»†6Ğ¹ê²×ÓÑ¡ê±–ãƒãƒ†Û˜ğŸ˜‡ãƒ”à¸’×Ó™ä¸¶äº›ÙŠÚŸÄÄ½ä»ºÎ§ä¹¤äºÖ×ê²œäº²äº•ê³œà¹’Ùµê³ ÎŒê³¡;
Ğ°Ö»Óê±©äº¿à¸†Ã›ã‚—ê³¤ã‚ŒÒ½ÎÙ‚Ù¸Ù­Ú‡à¹—t×ªÏ¥ÙğŸ˜´Îµê²§Ä¬ãƒ½Î¼sãƒ‡Ó³Ú¤Í³Î–ÙšğŸ™ƒÄ…ã‚•à¸ŸlÅ•tä»»ãÄ—à¸¸Ä¬Ú¶Ó°Óˆê³§Ã­Ãˆ×´Î¢ä¸¼Ñ®ä¸†Ò³ÎšĞ¥äºŠÛ³Îºäº“Å¸à¸®)
Å­à¤¦ê±¯ê³˜Ğºà¥‚ä»µĞŸÚ•Ï™Ø´Øªê²œà¹ƒãƒ•Ï¬Ú­Å¯xÚ„~Å¯à¤¢à¸Œä»ƒÃ«à¥¾ÑˆğŸ™‹Å·ØØ¦ê±‡Ä«Uä»¼ÏÚ€Å§.ä¸±bäº¥Å¾ä»‚Ú€Í¶à¤¼×”äº‡à¤Ò Û˜Ï’à¤£ê±ãà¤ÎœÒ»ê³±ã£ãƒğŸ˜‚
```

So it's not realistic but interesting since it could allow you to potentially simulate various language distributions.

To reproduce run `cd smoltok-py && make bench-random-data`.
Explore the command to see options, such as sampling characters from different sets.

## Installation

### Python

_Coming soon..._

### Rust

```toml
[dependencies]
smoltok-core = "0.1"
```

```rust
use smoltok_core::{RegexBPETokenizerConfig, Tokenizer, Trainable};

fn main() {
    let text = "hello world hello world hello world hello world";
    // with default GPT-4 split pattern
    let config = RegexBPETokenizerConfig::build(512, None).unwrap();

    let tokenizer = config.train(text).unwrap();
    let tokens = tokenizer.encode("Hello, world!");
    
    println!("Encoded:");
    for (i, &token) in tokens.iter().enumerate() {
        let decoded_token = tokenizer.decode(&[token]).unwrap();
        println!("- Token {} (ID: {}): {:?}", i, token, decoded_token);
    }

    let decoded = tokenizer.decode(tokens.as_slice()).unwrap();
    println!("\nFull decoded text: {:#?}", decoded);
}
```

## Exercise

Building this was a fun exercise, and I encourage you to try it too! Check out [`exercise.md`](./exercise.md) for a high-level guide to implementing a BPE tokenizer in Rust from scratch.

The implementation is not as minimal as [minbpe](https://github.com/karpathy/minbpe), but I've tried to keep it clear, robust, and well-documented. One difference from other projects is the use of separate config classesâ€”a natural way to prevent calling `encode`/`decode` on an untrained tokenizer using Rust's type system.

If you're more comfortable with Python, feel free to explore the [Python implementation](smoltok-py/py_impl/src/base.py), but keep in mind it exists primarily for benchmarking and isn't a 1-to-1 mapping of the Rust code.

## Tools

- **Python** ğŸ: [uv](https://github.com/astral-sh/uv) for package management, [ruff](https://github.com/astral-sh/ruff) for linting & formatting, [ty](https://github.com/astral-sh/ty) for type checking
- **Bindings** ğŸ”—: [pyo3](https://github.com/PyO3/pyo3) & [maturin](https://github.com/PyO3/maturin)
- **Rust** ğŸ¦€: pure Rust with [rayon](https://github.com/rayon-rs/rayon) for parallel implementation

## Acknowledgments & Resources

This project is inspired by Andrej Karpathy's video on tokenization: [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE).

I also enjoyed reading [The Tokenizer section of HuggingFace Smol Training Playbook](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#the-tokenizer) and [The Bitter Lesson is coming for Tokenization post by lucalp](https://lucalp.dev/bitter-lesson-tokenization-and-blt/).
