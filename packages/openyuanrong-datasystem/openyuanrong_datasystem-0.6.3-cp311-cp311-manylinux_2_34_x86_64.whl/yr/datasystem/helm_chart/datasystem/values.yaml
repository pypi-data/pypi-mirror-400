global:
  namespace: "default"
  # If the namespace is not "default" or does not exist, set this configuration to true to enable automatic creation.
  autoCreatedNamespace: false

  # Image prefix. Docker registry URL. Empty means using Docker Hub.
  imageRegistry: ""
  images:
    # Image name and tag in 'name:tag' format
    datasystem: "openyuanrong-datasystem:0.5.0"

  # Config ETCD table prefix, the value should only contain english alphabetics (a-zA-Z), numbers (0-9) only.
  clusterName: "AZ1"

  etcd:
    # ETCD configuration
    # Configure ETCD server address. Should not be empty.
    etcdAddress: ""
    # Whether to enable ETCD auth.
    enableEtcdAuth: false
    # Set ETCD target name override for SSL host name checking.
    # The configuration value should be consistent with the DNS content of the Subject Alternate Names of the TLS certificate.
    etcdTargetNameOverride: ""
    # The CA certificate. No encryption required. Base64 encoding is required.
    etcdCa: ""
    # The client cert. No encryption required, base64 transcoding required.
    etcdCert: ""
    # The client private key.
    # Base64 transcoding is required, and whether encryption is required depends on whether passphrase is applied:
    etcdKey: ""
    # The path where the encrypted etcd certificate is mounted. This must be specified when etcd authentication is enabled.
    etcdCertDir: /home/yuanrong/datasystem/etcd_cert_dir
    # The value of passphrase. Encryption is required, base64 transcoding required.
    passphraseValue: ""
    # ETCD metadata async operation pool size.
    # If key size is large under WRITE_BACK_L2_CACHE mode, you may need to increase this value.
    etcdMetaPoolSize: 8

  # Port value (suggested range: 30000-32767).
  port:
    datasystemWorker: 31501

  resources:
    # Resource configuration for datasystem worker container.
    # For cpu: "1" means 1 vCPU core, "500m" means 0.5 core.
    # For memory: should specify unit like Mi/Gi(e.g. "4Gi", not just "4").
    datasystemWorker:
      # Max allowable CPU cores and memory.
      limits:
        cpu: "3"
        memory: "4Gi"
      # Minimum required resources.
      requests:
        cpu: "3"
        memory: "3Gi"

      # Upper limit of the shared memory, the default unit for shared memory is MB.
      # To prevent being rendered as scientific notation by helm, numbers with more than 5 digits should be configured as strings.
      sharedMemory: 2048
      # Upper limit of the stream cache local memory, the unit for shared memory is MB.
      scLocalCacheMemorySizeMb: 1024
      #Upper limit of the shared memory in percentage can be used by OC, must be within (0, 100]
      ocShmThresholdPercentage: 100
      #Upper limit of the shared memory in percentage can be used by SC, must be within (0, 100]
      scShmThresholdPercentage: 100
      # Maximum number of clients that can be connected to a worker.
      # Value range: [1, 10000], default value: 200.
      maxClientNum: 200
      # Number of cached pages. Higher number improve performance
      scCachePages: 16
      # The shared page size, should be in range [1, 16].
      scSharedPageSizeMb: 4
      # The shared page group count for each remote worker, should be in range [1, 64].
      scSharedPageGroupCount: 4
      # Size of the page used for caching worker files.
      # The valid range is 4096-1073741824.
      # Unit for page size is Bytes.
      pageSize: "1048576"

  spill:
    # The path of the spilling, empty means local_dick spill disabled.
    # It will create a new subdirectory("datasystem_spill_data") under the SPILL_DIRECTORY to store the spill file.
    # Example: If SPILL_DIRECTORY is "/home/spill", spill files will exist in the "/home/spill/datasystem_spill_data".
    spillDirectory: ""
    # Maximum amount of spilled data that can be stored in the spill directory. If spill is enable and spillSizeLimit is 0, spillSizeLimit will be set to 95% of the spill directory.
    # Unit for spillSizeLimit is Bytes.
    spillSizeLimit: "0"
    # It represents the maximum parallelism of writing files, more threads will consume more CPU and I/O resources.
    spillThreadNum: 8
    # The size limit of single spill file, spilling objects which lager than that value with one object per file.
    # If there are some big objects, you can increase this value to avoid run out of inodes quickly.
    # The valid range is 200-10240.
    spillFileMaxSizeMb: 200
    # The maximum number of open file descriptors about spill. If opened file exceed this value,
    # some files will be temporarily closed to prevent exceeding the maximum system limit. You need reduce this value if your system resources are limited.
    # The valid range is greater than or equal to 8.
    spillFileOpenLimit: 512
    # Disable readahead can mitigate the read amplification problem for offset read, default is true
    spillEnableReadahead: true
    # Thread number of eviction for object cache, suggested value range: (0, 4096].
    evictionThreadNum: 1
    # The reserved memory (MB) is determined by min(shared_memory_size_mb*0.2, eviction_reserve_mem_threshold_mb). Eviction begins when memory drops below this threshold.The valid range is 100-102400.
    evictionReserveMemThresholdMB: 10240

  log:
    # Log config.
    # The directory where log files are stored.
    logDir: /home/yuanrong/datasystem/logs
    # Size of async logger's message queue.
    logAsyncQueueSize: 65536
    # Vlog level, a larger value indicates more detailed logs. The value is between 0-3.
    logLevel: 0
    # Log messages below this level will not actually be recorded anywhere. The value is between 0-3.
    minLogLevel: 0
    # The maximum log file size (in MB), which must be greater than 0.
    maxLogSize: 400
    # Maximum number of log files to retain per severity level. And every log file size is limited by max log size.
    maxLogFileNum: 25
    # If log-retention-day is greater than 0,
    # any log file from your project whose last modified time is greater than log-retention-day days will be unlinked.
    # If log-retention-day is equal 0, will not unlink log file by time.
    logRetentionDay: 0
    # Flush log files with async mode.
    logAsync: true
    # Buffer log messages for at most this many seconds.
    logBufSecs: 10
    # Compress old log files in .gz format.
    # This parameter takes effect only when the size of the generated log is greater than max log size.
    logCompress: true
    # Prefix of log filename, default is program invocation short name. Use standard characters only.
    logFilename: ""
    # Interval between logging stream metrics
    scMetricsLogIntervalS: 60

  observability:
    # Record performance and resource logs
    logMonitor: true
    # Specify the type of exporter, [harddisk]. Takes effect only when logMonitor is true.
    logMonitorExporter: harddisk
    # The sleep time between iterations of observability collector scan, default value is 10000ms. (Must be greater than 0)
    logMonitorIntervalMs: 10000

  l2Cache:
    # Config the l2 cache type support obs, none by default. Optional value: 'obs', 'sfs', 'none'.
    l2CacheType: "none"

    # l2 cache delete threads number, it affects the parallelism of L2 data deletion. 
    # Increasing it will also cause CPU usage to increase.
    l2CacheDeleteThreadNum: 32

    # when we delete the object which is in uploading process, in this scenarios we need delay some time to retry.
    objectDelRetryDelaySec: 3600
    # Whether data read and write from the L2 cache daemon depend on metadata. Note: If set to false, it indicates that the metadata is not stored in etcd.
    ocIoFromL2CacheNeedMetadata: true

    obs:
      # The access key for obs AK/SK authentication.
      obsAccessKey: ""
      # The secret key for obs AK/SK authentication.
      obsSecretKey: ""
      # OBS endpoint. Example: "xxx.hwcloudtest.cn"
      obsEndpoint: ""
      # OBS bucket name.
      obsBucket: ""
      # Whether to enable the https in obs. false: use HTTP (default), true: use HTTPS
      obsHttpsEnabled: false
      # Use cloud service token rotation to connect obs.
      cloudServiceTokenRotation:
        # Whether to use ccms credential rotation mode to access OBS, default is false. If it is enabled, need to specify
        # iamHostName, identityProvider, projectId, regionId at least.
        # In addition, obsEndpoint and obsBucket need to be specified.
        enable: false
        # Domain name of the IAM token to be obtained. Example:  iam.example.com.
        iamHostName: ""
        # Provider that provides permissions for the ds-worker. Example: csms-datasystem.
        identityProvider: ""
        # Project id of the OBS to be accessed. Example: fb6a00ff7ae54a5fbb8ff855d0841d00.
        projectId: ""
        # Region id of the OBS to be accessed. Example: cn-beijing-4.
        regionId: ""
        # Whether to access OBS of other accounts by agency, default is false. If is true, need to specify tokenAgencyName
        # and tokenAgencyDomain.
        enableTokenByAgency: false
        # Agency name for proxy access to other accounts. Example: obs_access.
        tokenAgencyName: ""
        # Agency domain for proxy access to other accounts. Example: op_svc_cff.
        tokenAgencyDomain: ""

    sfsTurbo:
      # Endpoint of sfs-turbo, which is used to concatenate the shared path in sfs-turbo. such as '172.21.7.239'.
      endpoint: ""
      # Sfs-turbo sub-path mounted to ds-worker. If this parameter is not specified, the root directory '/' of sfs-turbo
      # is mounted by default.
      subPath: ""
      # Specifies the sfs-turbo ID, which can be viewed on the sfs-turbo page.
      id: "0"
      # Specifies the sfs-turbo enterprise project ID, which can be viewed on the sfs-turbo page.
      projectId: "0"
      # Specifies the capacity of using sfs-turbo. Note that the size must be smaller than the size of sfsTurbo.
      capacity: "500Gi"

  metadata:
    # Controls whether to enable multiple meta replica
    enableMetaReplica: false
    # Config MASTER back store directory and must specify in rocksdb scenario.
    # The rocksdb database is used to persistently store the metadata stored in the master
    # so that the metadata before the restart can be re-obtained when the master restarts.
    rocksdbStoreDir: /home/yuanrong/datasystem/rocksdb
    # Number of open files that can be used by the rocksdb. The default value is 128, which is suitable for general
    # small and medium-sized applications. This value can be modified according to system limitations.
    rocksdbMaxOpenFile: 128
    # Number of background threads rocksdb can use for flushing and compacting, default value is 16. The value can be
    # modified according to cpu limitations but should be greater than 0 to ensure that there are backend threads to handle tasks.
    rocksdbBackgroundThreads: 16
    # Config the rocksdb support none, sync or async, async by default. Optional value: 'none', 'sync', 'async'. This represents the method of writing metadata to rocksdb.
    rocksdbWriteMode: "async"
    # Enable query meta redirect when scale up or voluntary scale down, default is true
    enableRedirect: "true"
    # Allow data's replica to be cached locally, default is true
    enableDataReplication: "true"

  rpc:
    # Whether to enable the authentication function between components(worker, master)
    enableCurveZmq: false
    # The directory to find ZMQ curve key files.
    # This path must be specified when zmq authentication is enabled.
    curveKeyDir: /home/yuanrong/datasystem/curve_key_dir
    curveZmqKey:
      # Control whether try to get data from other AZ's worker firstly, if false then get data from L2 cache directly.
      # The service mapping, public key and private key are configured with base64 to avoid the special characters problem when restore to files.
      # These keys must be replaced.
      # The format example: workerPublicKey: V11lTG59ZC5wYX0lTkNrMmlrUFZXJkBTRz5LVHtpMjhBWVhaTlA5Kw==
      # Worker's public key in the curve encryption environment.
      workerPublicKey: ""
      # Worker's private key in the curve encryption environment.
      workerSecretKey: ""               
      # Client's public key in the curve encryption environment.
      clientPublicKey: ""

    # A direct TCP/IP port for worker-to-worker scenarios to improve latency.
    # Acceptable value:0, or some positive integer. 0 means disabled.
    ocWorkerWorkerDirectPort: 0
    # Number of parallel connections from worker to worker scenarios to improve throughput.
    # OC_WORKER_WORKER_DIRECT_PORT must be enabled to take effect.
    ocWorkerWorkerPoolSize: 3
    # A direct tcp/ip port for worker to workers scenarios to improve latency.
    # Acceptable values:0, or some positive integer. 0 means disabled.
    scWorkerWorkerDirectPort: 0
    # Number of parallel connections from worker to worker scenarios to improve throughput.
    # SC_WORKER_WORKER_DIRECT_PORT must be enabled to take effect.
    scWorkerWorkerPoolSize: 3

    # The payload threshold to batch get objects, unit is MB. Setting to 0 will indicate no split.
    batchGetThresholdMb: 100
    # Minimum payload size in bytes to trigger direct write into shared memory.
    # May incur extra network cost
    payloadNocopyThreshold: "104857600"
    # Config rpc server thread number, must be greater than 0.
    rpcThreadNum: 128
    # The number of worker service for object cache.
    ocThreadNum: 64
    # Optimize the performance of the customer. Default server is 5.
    # The higher the throughput, the higher the value, but should be in range [1, 32]
    zmqServerIoContext: 5
    # Optimize the performance of the client stub. Default value is 5.
    # The higher the throughput, the higher the value, but should be in range [1, 32]
    zmqClientIoContext: 5
    # Parallel payload split chunk size. Default to 1048756 bytes.
    zmqChunkSz: 1048576
    # Maximum number of sessions that can be cached, must be within [512, 10'000]
    maxRpcSessionNum: 2048
    # The num of threads used to scan new elements in shared memory.
    scScanThreadNum: 16
    # Number of partitions for scanning streams.
    scScanNumBuckets: 1024
    # The number of regular backend socket for stream cache.
    scRegularSocketNum: 64
    # The number of stream backend socket for stream cache.
    scStreamSocketNum: 64
    # The maximum number of threads for non-rpc tasks in the master.
    masterScThreadNum: 128
    # The num of threads used to send elements to remote worker.
    remoteSendThreadNum: 8
    
    # Memory resource clean up interval in milliseconds
    scGcIntervalMs: 50
    # Remote send interval in milliseconds
    scScanIntervalMs: 10
    # stream idle time. default 300s (5 minutes)
    streamIdleTimes: 300

  ipc:
    # Determines whether the shared memory feature is enabled.
    ipcThroughSharedMemory: true
    # Unix domain socket (UDS) file directory with 80-character path limit
    udsDir: /home/uds

  reliability:
    # Client reconnect wait seconds, default value is 5 seconds. (Must be greater than 0)
    clientReconnectWaitS: 5
    # Maximum time interval for the worker to determine client death, value range: [15, UINT64_MAX).
    clientDeadTimeoutS: 120
    # Time interval between worker and etcd heartbeats.
    heartbeatIntervalMs: 1000
    # Maximum time interval before a node is considered lost, the unit is second. (Must be greater than 5s)
    nodeTimeoutS: 60
    # Maximum time interval for the etcd to determine node death, the unit is second. (Must be greater than nodeTimeoutS)
    nodeDeadTimeoutS: 300
    # Control whether to enable reconciliation, default is true.
    enableReconciliation: true
    # Whether to support self-healing when the hash ring is in an abnormal state.
    enableHashRingSelfHealing: false
    # Timeout interval of kubernetes liveness probe, default is 150 seconds.
    livenessProbeTimeoutS: 150
    # Time to wait for the first node that wants to join a working hash ring.
    addNodeWaitTimeS: 60
    # Decide whether to remove the node from hash ring or not when node is dead
    autoDelDeadNode: true
    # Whether to support distributed master, default is true.
    enableDistributedMaster: true
    # Option to verify if data from a producer is out of order.
    enableStreamDataVerification: false

  gracefulShutdown:
    # Scale in taint, format is key=value:effect.
    # Supports configuring multiple types of taints, and multiple taints are only separated by ",".
    scaleInTaint: "datasystem/offline=true:NoExecute"
    # Decide whether to migrate data to other nodes or not when current node exits. If this is the only node in the cluster, exits directly and the data will be lost.
    enableLosslessDataExitMode: false
    # The worker ensures a certain period of time that the asynchronous queues for sending messages to ETCD and
    # L2 cache remain empty before it can exit properly.
    checkAsyncQueueEmptyTimeS: 15
    # Data migration rate limit for every node when scaling down.
    dataMigrateRateLimitMb: 40
    # Overwrite the global default terminationGracePeriodSeconds when liveness probe failed, enable when greater than 0.
    livenessProbeTerminationGracePeriodSeconds: 0

  performance:
    # This is controlled by the flag of mmap(MAP_HUGETLB) which can improve memory access and reducing the overhead of page table, default is disable .
    # Huge pages require system-level configuration. use command "echo [pageNums] > /proc/sys/vm/nr_hugepages" to config the number of huge page.
    # If the system does not have enough physical memory to allocate huge pages, memory allocation will fail.
    enableHugeTlb: false
    #Due to Kubernetes' (k8s) resource calculation policies, shared memory is sometimes counted twice, which can lead to client crashes.
    #To address this issue, fallocate is employed to link the client and worker nodes for shared memory,
    #thus correcting the memory calculation errors. By default, fallocate is enabled.
    #Enabling fallocate will lower the efficiency of memory allocation
    enableFallocate: true
    # Avoiding page faults during copying improves runtime performance but may result in longer worker startup times (depending on sharedMemory).
    # If set to true, it must be ensured that 'arenaPerTenant' is 1 and 'enableFallocate' is false.
    sharedMemoryPopulate: false
    # Control this process by enabling transparent huge pages, default is disabled.
    # Enabling Transparent Huge Pages (THP) can enhance performance and reduce page table overhead,
    # but it may also lead to increased memory usage, leading to worker being terminated by the OOM Killer.
    enableThp: false

    # The arena count for each tenant. Multiple arenas can improve the performance of share memory allocation for the first time,
    # but each arena will use one more fd, value range: [1, 32]
    arenaPerTenant: 16
    # The memory reclamation time after free, default is 600 seconds.
    memoryReclamationTimeSecond: 600

    # Set whether to delete object asynchronously. If set to true, master will notify workers to delete objects asynchronously.
    # Client doesn't need to wait for all workers to delete objects.
    # The default value is false.
    asyncDelete: false
    # Heterogeneous object transfer protocol Enables p2ptransfer
    enableP2pTransfer: false
    # Enable worker->worker OC batch get, default false.
    enableWorkerWorkerBatchGet: false
    # The data threshold to transfer obj data between client and worker via shm, unit is KB.
    ocShmTransferThresholdKB: 500
    # Option to turn on urma for OC worker to worker data transfer, default false.
    enableUrma: false
    # Option to enable URMA over IB or UB, default UB to run with URMA over UB.
    urmaMode: "UB"
    # Number of complete record to poll at a time, 16 is the max this device can poll.
    urmaPollSize: 8
    # Register the whole arena as segment during init, otherwise, register each object as a segment.
    urmaRegisterWholeArena: true
    # Number of jfs and jfr pair.
    urmaConnectionSize: 16
    # Uses interrupt mode to poll completion events.
    urmaEventMode: false
    # Disk cache data placement directory, default value is empty, indicating that disk cache is not enabled.
    sharedDiskDirectory: ""
    # Upper limit of the shared disk, the unit is mb.
    sharedDiskSize: 0
    # The number of disk cache Arena for each tenant. Multiple arenas can improve the performance of shared disk allocation for the first time, but each arena will use one more fd. The valid range is 0 to 32.
    sharedDiskArenaPerTenant: 8
    # Option to turn on rdma for OC worker to worker data transfer, default false.
    enableRdma: false
    # Register the whole arena as segment during init, otherwise, register each object as a segment.
    rdmaRegisterWholeArena: true
    # Target batch size for worker worker responses, default is 2MB.
    ocWorkerAggregateMergeSize: 2097152
    # Max single item size for batching worker worker batch rsp, default is 64KB.
    ocWorkerAggregateSingleMax: 65536
    # Min data count for parallel worker worker batch rsp, default is 100.
    ocWorkerWorkerParallelMin: 100
    # Worker worker batch rsp control nums, 0 means unlimited.
    ocWorkerWorkerParallelNums: 16

  crossAz:
    # Specify other az names using the same etcd. Only split by ','
    otherClusterNames: ""
    # Control whether to try to get data from other AZ's worker first. If false, data will be retrieved directly from the L2 cache.
    crossAzGetDataFromWorker: true
    # Control whether to get meta data from other AZ's worker, if false then get meta data from local AZ.
    crossAzGetMetaFromWorker: false

  akSk:
    # The access key used by the system.
    systemAccessKey: ""
    # The secret key used by the system.
    systemSecretKey: ""
    # The data key for system encrypte and decrypt secert key.
    systemDataKey: ""
    # The access key used by the tenant.
    tenantAccessKey: ""
    # The secret key used by the tenant.
    tenantSecretKey: ""
    # Request expiration time in seconds, the maximum value is 300s.
    requestExpireTimeS: 300
    # Skip authentication for worker requests
    skipAuthenticate: false

  tenantAuth:
    # Indicates whether to enable the tenant authentication, default is false.
    # Tenant authorization is forbidden in the case of stream cache.
    authorizationEnable: false
    # The type of iam kit, none is default, value range: [none, yuanrong_iam].
    iamKit: "none"
    yuanrong:
      # The access yuanrong token or accessKey server url. Example: http(s)://xxx.xxx , for different auth methods, the following URI will be automatically added.
      yuanrongIamUrl: ""
      # The access yuanrong token or accessKey server url.
      # yuanrongIamCa, yuanrongIamCert, yuanrongIamKey, yuanrongIamPassphrase are configured with base64 to avoid the special characters problem when restore to files.
      # root yuanrong iam server certificate, default is none.
      # if ca , cert, key is all empty, connect with yuanrong iam without tls.
      yuanrongIamCa: ""
      # Client's yuanrong iam server certificate, default is none.
      yuanrongIamCert: ""
      # Encrypted client's yuanrong iam server key, default is none.
      yuanrongIamKey: ""
      # Encrypted yuanrong iam passphrase
      yuanrongIamPassphrase: ""
      # The path that mounts yuanrong cert ciphertext. Must be specified when enable kmc yuanrong token auth.
      yuanrongIamDir: "/home/sn/datasystem/yuanrong_iam"
    
  # fsGroup configuration
  # All processes of the container are also part of the supplementary group ID.
  fsGid: "1002"

  # To support hybrid deployment.
  # For example:
  # multiSpec:
  # - name: "ds-worker-mid"
  #   affinityLabel: "mid"
  #   resources:
  #     limits:
  #       cpu: "3"
  #       memory: "4Gi"
  #     requests:
  #       cpu: "3"
  #       memory: "4Gi"
  #   workerResources:
  #     sharedMemory: 1024
  # - name: "ds-worker-small"
  #   affinityLabel: "small"
  #   resources:
  #     limits:
  #       cpu: "2"
  #       memory: "3Gi"
  #     requests:
  #       cpu: "1"
  #       memory: "2Gi"
  #   workerResources:
  #     sharedMemory: 1024
  # - name: "ds-worker-big"
  #   affinityLabel: "big"
  #   resources:
  #     limits:
  #       cpu: "5"
  #       memory: "6Gi"
  #     requests:
  #       cpu: "4"
  #       memory: "5Gi"
  #   workerResources:
  #     sharedMemory: 2048
  multiSpec: []

  # Mount the host path into the container.
  # Take effect when both HOST_PATH and MOUNT_PATH are non-empty.
  # Example for mount multiple directories:
  # MOUNT:
  # - HOST_PATH: "/host/ssd-1"
  #   MOUNT_PATH: "/home/spill-1"
  # - HOST_PATH: "/host/ssd-2"
  #   MOUNT_PATH: "/home/spill-2"
  # Example 2:
  # Use ssd to store spill file, you can configure SPILL_DIRECTORY: "/home/ssd/spill", and MOUNT_PATH is as follows:
  # MOUNT:
  # - HOST_PATH: "/host/ssd"
  #   MOUNT_PATH: "/home/ssd"
  # Example 3:
  # Use ssd to store rocksdb file, you can configure ROCKSDB_STORE_DIR: "/home/ssd/rocksdb", and
  # MOUNT_PATH is as follows:
  # MOUNT:
  # - HOST_PATH: "/host/ssd"
  #   MOUNT_PATH: "/home/ssd"
  mount:
    - hostPath: ""
      # The path that you want to mount to your pods.
      mountPath: ""

  # K8s affinity configuration.
  # When configuring the affinity attribute, nodeSelector and nodeAffinity will take effect at the same time.
  # When the two configurations conflict, the pending component may not be pulled up.
  # It is recommended to configure only one.
  # For example:
  # affinity:
  #   nodeSelector:
  #     node-role.kubernetes.io/datasystem: "openyuanrong-datasystem:0.5.0"
  #   tolerations:
  #   - key: "testAffinity"
  #     operator: "Equal"
  #     value: "true"
  #     effect: "NoSchedule"
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchExpressions:
  #             - key: "datasystem"
  #               operator: "In"
  #               values:
  #                 - "true"
  affinity:
    nodeSelector: {}
    tolerations: []
    nodeAffinity: {}

  # K8s meta annotation
  # For example:
  # annotations:
  #   cluster-autoscaler.kubernetes.io/enable-ds-eviction: "true"
  annotations: {}

  # Configure priorityClass.
  # If the value is false, the default priorityClass is system-cluster-critical.
  # If the value is true, a priorityClass with preemptionPolicy Never is created.
  enableNonPreemptive: false

  # Used to define how long Kubernetes will give a Pod a grace period before it is terminated. 
  # This grace period is intended to allow the Pod sufficient time to gracefully shut down its running processes and services. 
  terminationGracePeriodSeconds: 1800

  # Used to control how many Pods can be in an unavailable state during a rolling update.
  maxUnavailable: "100%"

  # Maximum duration of the rolling upgrade, default value is 1800 seconds.
  rollingUpdateTimeoutS: 1800

  # Security Configuration
  security:
    # The encrypted secret key for stream cache. The key length is up to 1024 bytes and must be 32 bytes after decryption.
    scEncryptSecretKey: ""