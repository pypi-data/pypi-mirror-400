#!/usr/bin/env python3
"""LLM service management commands for SAGE.

All LLM services should be managed through sageLLM (LLMAPIServer),
NOT by directly calling vLLM entrypoints.
"""

from __future__ import annotations

import json
import time
from pathlib import Path
from typing import Any
from urllib.parse import urlparse

import httpx
import typer
import yaml
from rich.console import Console
from rich.table import Table

from sage.common.config import ensure_hf_mirror_configured
from sage.common.config.ports import SagePorts
from sage.common.model_registry import fetch_recommended_models, vllm_registry
from sage.llm.presets import (
    EnginePreset,
    get_builtin_preset,
    list_builtin_presets,
    load_preset_file,
)

try:  # Optional dependency: middleware is not required for every CLI install
    from sage.llm import VLLMService
except Exception:  # pragma: no cover - handled gracefully at runtime
    VLLMService = None  # type: ignore

try:
    from sage.llm import (
        LLMAPIServer,
        LLMLauncher,
        LLMServerConfig,
    )
except Exception:  # pragma: no cover
    LLMAPIServer = None  # type: ignore
    LLMLauncher = None  # type: ignore
    LLMServerConfig = None  # type: ignore

try:
    from sage.llm import (
        BackendInstanceConfig,
        UnifiedAPIServer,
        UnifiedServerConfig,
    )
except Exception:  # pragma: no cover
    UnifiedAPIServer = None  # type: ignore
    UnifiedServerConfig = None  # type: ignore
    BackendInstanceConfig = None  # type: ignore

# Import config subcommands
from sage.cli.commands.platform.llm_config import app as config_app

console = Console()
app = typer.Typer(help="ğŸ¤– LLM æœåŠ¡ç®¡ç†")
model_app = typer.Typer(help="ğŸ“¦ æ¨¡å‹ç®¡ç†")
engine_app = typer.Typer(help="âš™ï¸ å¼•æ“ç®¡ç†")
preset_app = typer.Typer(help="ğŸ›ï¸ é¢„è®¾ç¼–æ’")

# PID file for tracking background service
SAGE_DIR = Path.home() / ".sage"
LOG_DIR = SAGE_DIR / "logs"


def _ensure_dirs():
    """Ensure required directories exist."""
    SAGE_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)


def _resolve_api_base(api_base: str | None, port: int | None) -> str:
    """Return the control plane base URL (including /v1)."""
    if api_base:
        return api_base.rstrip("/")
    target_port = port or SagePorts.GATEWAY_DEFAULT
    return f"http://localhost:{target_port}/v1"


def _print_management_api_hint(api_base: str) -> None:
    """Provide guidance when the management API cannot be reached."""

    parsed = urlparse(api_base)
    host = parsed.hostname or "localhost"
    port = parsed.port or SagePorts.GATEWAY_DEFAULT

    console.print(
        "[yellow]ğŸ’¡ æ§åˆ¶å¹³é¢ç®¡ç† API æœªè¿è¡Œæˆ–ä¸å¯è¾¾ã€‚[/yellow]",
    )
    console.print(
        "   è¯·å…ˆå¯åŠ¨ Unified API Serverï¼ˆgatewayï¼‰ï¼Œä¾‹å¦‚è¿è¡Œ [cyan]sage llm serve[/cyan]",
    )
    console.print(
        f"   é»˜è®¤ç®¡ç†åœ°å€: http://{host}:{port}/v1ï¼Œå¯ç”¨ --api-port æˆ– --api-base è‡ªè¡Œè¦†ç›–ã€‚",
    )


def _extract_error_detail(resp: httpx.Response) -> str:
    try:
        payload = resp.json()
    except ValueError:
        return resp.text.strip() or resp.reason_phrase

    if isinstance(payload, dict):
        for key in ("detail", "message", "error"):
            if key in payload:
                value = payload[key]
                if isinstance(value, (dict, list)):
                    return json.dumps(value, ensure_ascii=False)
                return str(value)
        return json.dumps(payload, ensure_ascii=False)
    return str(payload)


def _management_request(
    method: str,
    endpoint: str,
    *,
    api_base: str,
    timeout: float,
    payload: dict[str, Any] | None = None,
) -> dict[str, Any]:
    endpoint_path = endpoint if endpoint.startswith("/") else f"/{endpoint}"
    url = f"{api_base.rstrip('/')}{endpoint_path}"

    request_kwargs: dict[str, Any] = {"timeout": timeout}
    if payload is not None:
        request_kwargs["json"] = payload

    try:
        response = httpx.request(method, url, **request_kwargs)
    except httpx.RequestError as exc:
        console.print(f"[red]âŒ æ— æ³•è¿æ¥åˆ°ç®¡ç† API: {exc}[/red]")
        _print_management_api_hint(api_base)
        raise typer.Exit(1) from exc

    if response.status_code >= 400:
        detail = _extract_error_detail(response)
        console.print(f"[red]âŒ ç®¡ç† API è¯·æ±‚å¤±è´¥ ({response.status_code}): {detail}[/red]")
        raise typer.Exit(1)

    if not response.content:
        return {}

    try:
        return response.json()
    except ValueError as exc:  # pragma: no cover - defensive
        console.print(f"[red]âŒ æ— æ³•è§£ææœåŠ¡å“åº”: {exc}[/red]")
        raise typer.Exit(1)


def _load_preset_source(name: str | None, file_path: Path | None) -> EnginePreset:
    """Resolve preset definition from builtin registry or local file."""

    if file_path is not None:
        return load_preset_file(file_path)
    if name:
        preset = get_builtin_preset(name)
        if preset is None:
            console.print(f"[red]æœªçŸ¥é¢„è®¾ '{name}'ã€‚ä½¿ç”¨ 'sage llm preset list' æŸ¥çœ‹å¯ç”¨é¡¹ã€‚[/red]")
            raise typer.Exit(1)
        return preset
    console.print("[red]è¯·æŒ‡å®šé¢„è®¾åç§°æˆ– --fileã€‚[/red]")
    raise typer.Exit(1)


def _print_preset_plan(preset: EnginePreset) -> None:
    table = Table(show_header=True, header_style="bold", title=f"é¢„è®¾: {preset.name}")
    table.add_column("åºå·", justify="center")
    table.add_column("åç§°", overflow="fold")
    table.add_column("ç±»å‹", justify="center")
    table.add_column("æ¨¡å‹", overflow="fold")
    table.add_column("TP/PP", justify="center")
    table.add_column("ç«¯å£", justify="center")
    table.add_column("æ ‡ç­¾", overflow="fold")
    for idx, engine in enumerate(preset.engines, start=1):
        table.add_row(
            str(idx),
            engine.name,
            engine.kind,
            engine.model,
            f"{engine.tensor_parallel}/{engine.pipeline_parallel}",
            str(engine.port or "auto"),
            engine.label or "-",
        )
    console.print(table)


def _fetch_cluster_status(api_base: str, timeout: float) -> dict[str, Any]:
    return _management_request(
        "GET",
        "/management/status",
        api_base=api_base,
        timeout=timeout,
    )


def _ensure_dict_list(data: Any) -> list[dict[str, Any]]:
    if isinstance(data, list):
        return [item for item in data if isinstance(item, dict)]
    if isinstance(data, dict):
        return [item for item in data.values() if isinstance(item, dict)]
    return []


def _normalize_memory_gb(value: Any) -> float | None:
    if value is None:
        return None
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return None

    if numeric > 1_000_000:  # assume bytes
        return numeric / (1024**3)
    return numeric


def _format_memory_gb(value: Any) -> str:
    amount = _normalize_memory_gb(value)
    if amount is None:
        return "-"
    return f"{amount:.1f} GB"


def _format_uptime(value: Any) -> str:
    try:
        seconds = float(value)
    except (TypeError, ValueError):
        return "-"

    if seconds < 60:
        return f"{int(seconds)}s"

    minutes, remaining = divmod(int(seconds), 60)
    if minutes < 60:
        return f"{minutes}m{remaining:02d}s"

    hours, minutes = divmod(minutes, 60)
    return f"{hours}h{minutes:02d}m"


# Add subcommands
app.add_typer(config_app, name="config")
app.add_typer(model_app, name="model")
app.add_typer(engine_app, name="engine")
app.add_typer(preset_app, name="preset")


# ---------------------------------------------------------------------------
# Preset orchestration commands
# ---------------------------------------------------------------------------
@preset_app.command("list")
def list_presets(json_output: bool = typer.Option(False, "--json", help="JSON è¾“å‡º")):
    """åˆ—å‡ºå†…ç½®é¢„è®¾ã€‚"""

    presets = list_builtin_presets()
    if not presets:
        console.print("[yellow]å½“å‰æ²¡æœ‰å®šä¹‰ä»»ä½•å†…ç½®é¢„è®¾ã€‚[/yellow]")
        return

    if json_output:
        typer.echo(
            json.dumps([preset.to_dict() for preset in presets], ensure_ascii=False, indent=2)
        )
        return

    table = Table(show_header=True, header_style="bold", title="LLM é¢„è®¾åˆ—è¡¨")
    table.add_column("åç§°", overflow="fold")
    table.add_column("æè¿°", overflow="fold")
    table.add_column("å¼•æ“æ•°é‡", justify="center")

    for preset in presets:
        table.add_row(
            preset.name,
            preset.description or "-",
            str(len(preset.engines)),
        )

    console.print(table)


@preset_app.command("show")
def show_preset(
    name: str | None = typer.Option(None, "--name", "-n", help="é¢„è®¾åç§°"),
    file: Path | None = typer.Option(None, "--file", "-f", help="è‡ªå®šä¹‰é¢„è®¾æ–‡ä»¶"),
    json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON è¾“å‡º"),
):
    """å±•ç¤ºé¢„è®¾è¯¦æƒ…ã€‚"""

    preset = _load_preset_source(name, file)
    data = preset.to_dict()
    if json_output:
        typer.echo(json.dumps(data, ensure_ascii=False, indent=2))
    else:
        typer.echo(yaml.safe_dump(data, sort_keys=False, allow_unicode=True))


def _rollback_engines(engine_ids: list[str], api_base: str, timeout: float) -> None:
    for engine_id in engine_ids:
        try:
            _management_request(
                "DELETE",
                f"/management/engines/{engine_id}",
                api_base=api_base,
                timeout=timeout,
            )
            console.print(f"[yellow]â†©ï¸ å·²å›æ»šå¼•æ“ {engine_id}[/yellow]")
        except typer.Exit:
            console.print(f"[red]âš ï¸ å›æ»š {engine_id} å¤±è´¥[/red]")


@preset_app.command("apply")
def apply_preset(
    name: str | None = typer.Option(None, "--name", "-n", help="é¢„è®¾åç§°"),
    file: Path | None = typer.Option(None, "--file", "-f", help="è‡ªå®šä¹‰é¢„è®¾æ–‡ä»¶"),
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(None, "--api-base", help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€"),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
    assume_yes: bool = typer.Option(False, "--yes", "-y", help="æ— éœ€ç¡®è®¤ç›´æ¥æ‰§è¡Œ"),
    dry_run: bool = typer.Option(False, "--dry-run", help="ä»…å±•ç¤ºè®¡åˆ’ï¼Œä¸æ‰§è¡Œ"),
    no_rollback: bool = typer.Option(False, "--no-rollback", help="å¤±è´¥æ—¶ä¸å›æ»šå·²å¯åŠ¨çš„å¼•æ“"),
):
    """æ ¹æ®é¢„è®¾å¯åŠ¨ä¸€ç»„å¼•æ“ã€‚"""

    preset = _load_preset_source(name, file)
    _print_preset_plan(preset)

    if dry_run:
        console.print("[blue]ğŸ” Dry-run æ¨¡å¼ï¼Œä»…å±•ç¤ºè®¡åˆ’ã€‚[/blue]")
        return

    if not assume_yes and not typer.confirm("ç¡®è®¤æŒ‰ç…§ä»¥ä¸Šè®¡åˆ’å¯åŠ¨å¼•æ“?", default=True):
        typer.echo("å·²å–æ¶ˆã€‚")
        return

    base_url = _resolve_api_base(api_base, api_port)
    started_ids: list[str] = []
    results: list[dict[str, Any]] = []
    rollback_enabled = not no_rollback

    for engine in preset.engines:
        console.print(f"[cyan]ğŸš€ å¯åŠ¨ {engine.name} ({engine.kind}) -> {engine.model}[/cyan]")
        payload = engine.to_payload()
        try:
            response = _management_request(
                "POST",
                "/management/engines",
                api_base=base_url,
                timeout=timeout,
                payload=payload,
            )
        except typer.Exit as exc:
            if rollback_enabled and started_ids:
                console.print("[yellow]âš ï¸ å¯åŠ¨å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š...[/yellow]")
                _rollback_engines(started_ids, base_url, timeout)
            raise exc

        engine_id = response.get("engine_id") or response.get("id")
        if engine_id:
            started_ids.append(engine_id)
        results.append(
            {
                "engine_id": engine_id or "(pending)",
                "model": response.get("model_id") or engine.model,
                "port": response.get("port") or payload.get("port") or "auto",
                "status": response.get("status") or "STARTING",
                "kind": response.get("engine_kind") or engine.kind,
            }
        )

    table = Table(show_header=True, header_style="bold", title="å¯åŠ¨ç»“æœ")
    table.add_column("Engine ID", overflow="fold")
    table.add_column("ç±»å‹", justify="center")
    table.add_column("æ¨¡å‹", overflow="fold")
    table.add_column("ç«¯å£", justify="center")
    table.add_column("çŠ¶æ€", justify="center")

    for item in results:
        table.add_row(
            item["engine_id"],
            item["kind"],
            item["model"],
            str(item["port"]),
            item["status"],
        )

    console.print("[green]âœ… é¢„è®¾å·²åº”ç”¨ã€‚[/green]")
    console.print(table)


# ---------------------------------------------------------------------------
# Model management commands
# ---------------------------------------------------------------------------
@model_app.command("show")
def show_models(json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º")):
    """åˆ—å‡ºæœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    infos = vllm_registry.list_models()
    if json_output:
        payload = [
            {
                "model_id": info.model_id,
                "revision": info.revision,
                "path": str(info.path),
                "size_bytes": info.size_bytes,
                "size_mb": round(info.size_mb, 2),
                "last_used": info.last_used_iso,
                "tags": info.tags,
            }
            for info in infos
        ]
        typer.echo(json.dumps(payload, ensure_ascii=False, indent=2))
        return

    if not infos:
        typer.echo(
            "ğŸ“­ æœ¬åœ°å°šæœªç¼“å­˜ä»»ä½• vLLM æ¨¡å‹ã€‚ä½¿ç”¨ 'sage llm model download --model <name>' å¼€å§‹ä¸‹è½½ã€‚"
        )
        return

    header = f"{'æ¨¡å‹ID':48} {'Revision':12} {'Size(MB)':>10} {'Last Used':>20}"
    typer.echo(header)
    typer.echo("-" * len(header))
    for info in infos:
        typer.echo(
            f"{info.model_id[:48]:48} {str(info.revision or '-'):12} {info.size_mb:>10.2f} {info.last_used_iso or '-':>20}"
        )


@model_app.command("list-remote")
def list_remote_models(
    json_output: bool = typer.Option(False, "--json", help="ä»¥ JSON æ ¼å¼è¾“å‡º"),
    timeout: float = typer.Option(5.0, "--timeout", help="è¿œç¨‹è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """å±•ç¤ºå®˜æ–¹æ¨èçš„å¸¸ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆè‡ªåŠ¨ä» GitHub æ‹‰å–ï¼‰ã€‚"""

    models = fetch_recommended_models(timeout=timeout)
    if not models:
        typer.echo("âš ï¸ æœªèƒ½è·å–æ¨èæ¨¡å‹åˆ—è¡¨ã€‚è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œã€‚")
        return

    if json_output:
        typer.echo(json.dumps(models, ensure_ascii=False, indent=2))
        return

    table = Table(show_header=True, header_style="bold")
    table.add_column("æ¨¡å‹ID", overflow="fold")
    table.add_column("æ˜¾å­˜éœ€æ±‚", justify="center")
    table.add_column("æ ‡ç­¾", justify="center")
    table.add_column("ç®€ä»‹", overflow="fold")

    for item in models:
        tags = ", ".join(item.get("tags", [])) or "-"
        memory = item.get("min_gpu_memory_gb")
        memory_str = f"{memory} GB" if memory else "-"
        table.add_row(
            item.get("model_id", "-"),
            memory_str,
            tags,
            item.get("description", ""),
        )

    console.print(table)
    typer.echo(
        "ğŸ’¡ å¦‚éœ€æ·»åŠ æ–°çš„æ¨èæ¨¡å‹ï¼Œè¯·æ›´æ–° packages/sage-common/src/sage/common/model_registry/recommended_llm_models.jsonï¼Œ"
        "æˆ–è®¾ç½® SAGE_LLM_MODEL_INDEX_URL æŒ‡å‘è‡ªå®šä¹‰ JSONã€‚"
    )


@model_app.command("download")
def download_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦ä¸‹è½½çš„æ¨¡å‹åç§°"),
    revision: str | None = typer.Option(None, "--revision", help="æ¨¡å‹ revision"),
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½"),
    no_progress: bool = typer.Option(False, "--no-progress", help="éšè—ä¸‹è½½è¿›åº¦"),
):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ã€‚"""

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    try:
        info = vllm_registry.download_model(
            model,
            revision=revision,
            force=force,
            progress=not no_progress,
        )
    except Exception as exc:  # pragma: no cover - huggingface errors
        typer.echo(f"âŒ ä¸‹è½½å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo("âœ… ä¸‹è½½å®Œæˆ")
    typer.echo(f"ğŸ“ è·¯å¾„: {info.path}")
    typer.echo(f"ğŸ“¦ å¤§å°: {info.size_mb:.2f} MB")


@model_app.command("delete")
def delete_model(
    model: str = typer.Option(..., "--model", "-m", help="è¦åˆ é™¤çš„æ¨¡å‹åç§°"),
    assume_yes: bool = typer.Option(False, "--yes", "-y", help="æ— éœ€ç¡®è®¤ç›´æ¥åˆ é™¤"),
):
    """åˆ é™¤æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ã€‚"""

    if not assume_yes and not typer.confirm(f"ç¡®è®¤åˆ é™¤æœ¬åœ°æ¨¡å‹ '{model}'?"):
        raise typer.Exit(0)

    try:
        vllm_registry.delete_model(model)
    except Exception as exc:  # pragma: no cover - filesystem errors
        typer.echo(f"âš ï¸ åˆ é™¤å¤±è´¥: {exc}")
        raise typer.Exit(1)

    typer.echo(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ {model}")


# ---------------------------------------------------------------------------
# Engine management commands
# ---------------------------------------------------------------------------


@engine_app.command("list")
def list_engines(
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€ (é»˜è®¤ http://localhost:<api-port>/v1)",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """åˆ—å‡ºå½“å‰ç”±æ§åˆ¶å¹³é¢ç®¡ç†çš„å¼•æ“ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    cluster_status = _fetch_cluster_status(base_url, timeout)
    engines = _ensure_dict_list(
        cluster_status.get("engines")
        or cluster_status.get("engine_instances")
        or cluster_status.get("instances")
        or []
    )

    if not engines:
        console.print("[yellow]å½“å‰æ²¡æœ‰ç”±æ§åˆ¶å¹³é¢ç®¡ç†çš„å¼•æ“ã€‚[/yellow]")
        return

    table = Table(show_header=True, header_style="bold")
    table.add_column("Engine ID", overflow="fold")
    table.add_column("æ¨¡å‹", overflow="fold")
    table.add_column("ç±»å‹", justify="center")
    table.add_column("çŠ¶æ€", justify="center")
    table.add_column("ç«¯å£", justify="center")
    table.add_column("GPU", justify="center")
    table.add_column("PID", justify="center")
    table.add_column("Uptime", justify="center")

    for engine in engines:
        engine_id = engine.get("engine_id") or engine.get("id") or "-"
        model_name = engine.get("model_id") or engine.get("model") or "-"
        runtime_kind = engine.get("runtime") or engine.get("engine_kind")
        if not runtime_kind:
            metadata = engine.get("metadata") or {}
            runtime_kind = metadata.get("engine_kind")
        runtime_kind = runtime_kind or "llm"
        status_text = engine.get("status") or engine.get("state") or "-"
        listen_port = engine.get("port") or engine.get("listen_port") or "-"
        pid = engine.get("pid") or engine.get("process_id") or "-"
        uptime = engine.get("uptime_seconds") or engine.get("uptime") or engine.get("uptime_s")

        gpu_ids = engine.get("gpu_ids") or engine.get("gpus") or engine.get("devices")
        if isinstance(gpu_ids, list):
            gpu_text = ",".join(str(item) for item in gpu_ids) if gpu_ids else "CPU"
        else:
            gpu_text = str(gpu_ids) if gpu_ids is not None else "CPU"

        table.add_row(
            str(engine_id),
            str(model_name),
            str(runtime_kind),
            str(status_text),
            str(listen_port),
            gpu_text,
            str(pid),
            _format_uptime(uptime),
        )

    console.print(table)
    console.print(f"[green]å…± {len(engines)} ä¸ªå¼•æ“ã€‚[/green]")


@engine_app.command("start")
def start_engine(
    model_id: str = typer.Argument(..., help="è¦å¯åŠ¨çš„æ¨¡å‹ ID"),
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
    engine_port: int | None = typer.Option(
        None,
        "--engine-port",
        help="æ˜¾å¼æŒ‡å®šæ–°å¼•æ“ç›‘å¬ç«¯å£",
    ),
    tensor_parallel: int | None = typer.Option(
        None,
        "--tensor-parallel",
        "-tp",
        help="Tensor å¹¶è¡Œåº¦ (ç›´æ¥é€ä¼ ç»™æ§åˆ¶å¹³é¢)",
    ),
    required_memory_gb: float | None = typer.Option(
        None,
        "--required-memory-gb",
        help="æœŸæœ›çš„æ˜¾å­˜éœ€æ±‚ (GB)",
    ),
    engine_label: str | None = typer.Option(
        None,
        "--label",
        help="è‡ªå®šä¹‰æ ‡ç­¾ï¼Œä¾¿äºè¯†åˆ«å¼•æ“",
    ),
    pipeline_parallel: int | None = typer.Option(
        None,
        "--pipeline-parallel",
        "-pp",
        help="Pipeline å¹¶è¡Œåº¦",
    ),
    max_concurrent: int | None = typer.Option(
        None,
        "--max-concurrent",
        help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤ 256)",
    ),
    engine_kind: str = typer.Option(
        "llm",
        "--engine-kind",
        help="å¼•æ“ç±»å‹ (llm, embedding, æˆ– finetune)",
    ),
    use_gpu: bool | None = typer.Option(
        None,
        "--use-gpu/--no-gpu",
        help="æ˜¾å¼æŒ‡å®šæ˜¯å¦ä½¿ç”¨ GPU (é»˜è®¤: LLM ä½¿ç”¨ GPU, Embedding ä¸ä½¿ç”¨)",
    ),
    # Finetune-specific parameters
    dataset_path: str | None = typer.Option(
        None,
        "--dataset",
        help="Fine-tune æ•°æ®é›†è·¯å¾„ (JSON/JSONL) [finetune å¿…éœ€]",
    ),
    output_dir: str | None = typer.Option(
        None,
        "--output",
        help="Fine-tune è¾“å‡ºç›®å½• (ä¿å­˜ checkpoint) [finetune å¿…éœ€]",
    ),
    lora_rank: int = typer.Option(
        8,
        "--lora-rank",
        help="LoRA rank (1-128) [finetune]",
    ),
    lora_alpha: int = typer.Option(
        16,
        "--lora-alpha",
        help="LoRA alpha (1-256) [finetune]",
    ),
    learning_rate: float = typer.Option(
        5e-5,
        "--learning-rate",
        help="å­¦ä¹ ç‡ [finetune]",
    ),
    epochs: int = typer.Option(
        3,
        "--epochs",
        help="è®­ç»ƒè½®æ•° [finetune]",
    ),
    batch_size: int = typer.Option(
        4,
        "--batch-size",
        help="æ‰¹æ¬¡å¤§å° [finetune]",
    ),
    gradient_accumulation_steps: int = typer.Option(
        1,
        "--gradient-accumulation",
        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•° [finetune]",
    ),
    max_seq_length: int | None = typer.Option(
        None,
        "--max-seq-length",
        help="æœ€å¤§åºåˆ—é•¿åº¦ [finetune]",
    ),
    use_flash_attention: bool = typer.Option(
        False,
        "--flash-attention/--no-flash-attention",
        help="ä½¿ç”¨ Flash Attention [finetune]",
    ),
    quantization_bits: int | None = typer.Option(
        None,
        "--quantization-bits",
        help="é‡åŒ–ä½æ•° (4/8) [finetune]",
    ),
    auto_download: bool = typer.Option(
        True,
        "--auto-download/--no-auto-download",
        help="è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ [finetune]",
    ),
):
    """è¯·æ±‚å¯åŠ¨æ–°çš„ LLM, Embedding, æˆ– Finetune å¼•æ“ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    payload: dict[str, Any] = {"model_id": model_id}
    engine_kind_value = engine_kind.strip().lower()
    if engine_kind_value not in {"llm", "embedding", "finetune"}:
        console.print("[red]engine-kind ä»…æ”¯æŒ 'llm', 'embedding', æˆ– 'finetune'.[/red]")
        raise typer.Exit(1)

    # Validate finetune-specific requirements
    if engine_kind_value == "finetune":
        if not dataset_path:
            console.print("[red]âŒ --dataset æ˜¯ finetune å¼•æ“çš„å¿…éœ€å‚æ•°.[/red]")
            raise typer.Exit(1)
        if not output_dir:
            console.print("[red]âŒ --output æ˜¯ finetune å¼•æ“çš„å¿…éœ€å‚æ•°.[/red]")
            raise typer.Exit(1)

        # Add finetune-specific parameters to payload
        payload["dataset_path"] = dataset_path
        payload["output_dir"] = output_dir
        payload["lora_rank"] = lora_rank
        payload["lora_alpha"] = lora_alpha
        payload["learning_rate"] = learning_rate
        payload["epochs"] = epochs
        payload["batch_size"] = batch_size
        payload["gradient_accumulation_steps"] = gradient_accumulation_steps
        if max_seq_length is not None:
            payload["max_seq_length"] = max_seq_length
        payload["use_flash_attention"] = use_flash_attention
        if quantization_bits is not None:
            payload["quantization_bits"] = quantization_bits
        payload["auto_download"] = auto_download

    if engine_port is not None:
        payload["port"] = engine_port
    if tensor_parallel is not None:
        payload["tensor_parallel_size"] = tensor_parallel
    if pipeline_parallel is not None:
        payload["pipeline_parallel_size"] = pipeline_parallel
    if required_memory_gb is not None:
        payload["required_memory_gb"] = required_memory_gb
    if engine_label:
        payload["engine_label"] = engine_label
    if max_concurrent is not None:
        payload["max_concurrent_requests"] = max_concurrent
    payload["engine_kind"] = engine_kind_value
    if use_gpu is not None:
        payload["use_gpu"] = use_gpu

    response = _management_request(
        "POST",
        "/management/engines",
        api_base=base_url,
        timeout=timeout,
        payload=payload,
    )

    engine_id = response.get("engine_id") or response.get("id") or "(pending)"
    model_name = response.get("model_id") or model_id
    status_text = response.get("status") or response.get("state") or "CREATED"
    assigned_port = response.get("port") or response.get("listen_port") or payload.get("port")

    console.print("[green]âœ… å·²æäº¤å¼•æ“å¯åŠ¨è¯·æ±‚[/green]")
    console.print(f"  Engine ID : {engine_id}")
    console.print(f"  æ¨¡å‹       : {model_name}")
    console.print(f"  çŠ¶æ€       : {status_text}")
    console.print(f"  ç«¯å£       : {assigned_port or '-'}")


@engine_app.command("stop")
def stop_engine(
    engine_id: str = typer.Argument(..., help="è¦åœæ­¢çš„å¼•æ“ ID"),
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """è¯·æ±‚åœæ­¢æŒ‡å®šçš„ LLM å¼•æ“ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    response = _management_request(
        "DELETE",
        f"/management/engines/{engine_id}",
        api_base=base_url,
        timeout=timeout,
    )

    status_text = response.get("status") or response.get("state") or "STOPPED"
    console.print(f"[green]âœ… å·²è¯·æ±‚åœæ­¢å¼•æ“ {engine_id} (çŠ¶æ€: {status_text}).[/green]")


@engine_app.command("prune")
def prune_engines(
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """æ¸…ç†æ‰€æœ‰å·²åœæ­¢æˆ–å¤±è´¥çš„å¼•æ“è®°å½•ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    response = _management_request(
        "POST",
        "/management/engines/prune",
        api_base=base_url,
        timeout=timeout,
    )

    pruned_count = response.get("pruned_count", 0)
    console.print(f"[green]âœ… å·²æ¸…ç† {pruned_count} ä¸ªå·²åœæ­¢/å¤±è´¥çš„å¼•æ“è®°å½•ã€‚[/green]")


@app.command("gpu")
def gpu_status(
    api_port: int = typer.Option(
        SagePorts.GATEWAY_DEFAULT,
        "--api-port",
        help=f"æ§åˆ¶å¹³é¢ç«¯å£ (é»˜è®¤ {SagePorts.GATEWAY_DEFAULT})",
    ),
    api_base: str | None = typer.Option(
        None,
        "--api-base",
        help="è¦†ç›–æ§åˆ¶å¹³é¢ API åŸºåœ°å€",
    ),
    timeout: float = typer.Option(5.0, "--timeout", help="HTTP è¶…æ—¶æ—¶é—´ (ç§’)"),
):
    """å±•ç¤ºæ§åˆ¶å¹³é¢æ„ŸçŸ¥åˆ°çš„ GPU çŠ¶æ€ã€‚"""

    base_url = _resolve_api_base(api_base, api_port)
    cluster_status = _fetch_cluster_status(base_url, timeout)
    gpu_entries = _ensure_dict_list(
        cluster_status.get("gpus")
        or cluster_status.get("gpu_status")
        or cluster_status.get("system_status")
        or cluster_status.get("gpu")
        or []
    )

    if not gpu_entries:
        console.print("[yellow]æ§åˆ¶å¹³é¢æœªè¿”å› GPU ä¿¡æ¯ã€‚[/yellow]")
        return

    table = Table(title="GPU èµ„æº", show_header=True, header_style="bold")
    table.add_column("GPU", overflow="fold")
    table.add_column("å†…å­˜ (å·²ç”¨/æ€»é‡)", justify="center")
    table.add_column("ç©ºé—²", justify="center")
    table.add_column("åˆ©ç”¨ç‡", justify="center")
    table.add_column("å…³è”å¼•æ“", overflow="fold")

    for gpu in gpu_entries:
        idx = gpu.get("index")
        name = gpu.get("name") or "GPU"
        label = f"{idx}: {name}" if idx is not None else name

        used = gpu.get("memory_used_gb") or gpu.get("memory_used")
        total = gpu.get("memory_total_gb") or gpu.get("memory_total")
        free = gpu.get("memory_free_gb") or gpu.get("memory_free")

        util = gpu.get("utilization") or gpu.get("gpu_utilization")
        if isinstance(util, (int, float)):
            util_str = f"{util:.0f}%"
        else:
            util_str = str(util) if util is not None else "-"

        engines = gpu.get("engines") or gpu.get("engine_ids") or gpu.get("allocations")
        if isinstance(engines, list):
            engines_str = ", ".join(str(item) for item in engines) or "-"
        else:
            engines_str = str(engines) if engines is not None else "-"

        table.add_row(
            label,
            f"{_format_memory_gb(used)} / {_format_memory_gb(total)}",
            _format_memory_gb(free),
            util_str,
            engines_str,
        )

    console.print(table)


# ---------------------------------------------------------------------------
# Blocking service runner & fine-tune stub
# ---------------------------------------------------------------------------
@app.command("run")
def run_vllm_service(
    model: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model", "-m", help="ç”Ÿæˆæ¨¡å‹"),
    speculative_model: str | None = typer.Option(
        None, "--speculative-model", help="æŠ•æœºé‡‡æ ·æ¨¡å‹ (Draft Model)"
    ),
    embedding_model: str | None = typer.Option(
        None, "--embedding-model", help="åµŒå…¥æ¨¡å‹ï¼ˆé»˜è®¤åŒç”Ÿæˆæ¨¡å‹ï¼‰"
    ),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="ç¼ºå¤±æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"
    ),
    temperature: float = typer.Option(0.7, "--temperature", help="é‡‡æ ·æ¸©åº¦"),
    top_p: float = typer.Option(0.95, "--top-p", help="Top-p é‡‡æ ·"),
    max_tokens: int = typer.Option(512, "--max-tokens", help="æœ€å¤§ç”Ÿæˆ token æ•°"),
):
    """ä»¥é˜»å¡æ¨¡å¼è¿è¡Œ vLLM æœåŠ¡ï¼Œå¹¶æä¾›äº¤äº’å¼ä½“éªŒã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•åŠ è½½å†…ç½®æœåŠ¡ã€‚")
        typer.echo("   è¯·è¿è¡Œ `pip install isage-common[vllm]` åé‡è¯•ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    config_dict: dict[str, Any] = {
        "model_id": model,
        "speculative_model_id": speculative_model,
        "embedding_model_id": embedding_model,
        "auto_download": auto_download,
        "sampling": {
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
        },
    }

    service = VLLMService(config_dict)

    try:
        service.setup()
        typer.echo("âœ… vLLM æœåŠ¡å·²åŠ è½½å®Œæˆã€‚è¾“å…¥ç©ºè¡Œé€€å‡ºï¼Œæˆ– Ctrl+C ç»“æŸã€‚")
        while True:
            prompt = typer.prompt("ğŸ’¬ Prompt", default="")
            if not prompt.strip():
                break
            outputs = service.generate(prompt)
            if not outputs:
                typer.echo("âš ï¸ æœªè·å¾—ç”Ÿæˆç»“æœã€‚")
                continue
            choice = outputs[0]["generations"][0]
            typer.echo(f"ğŸ§  {choice['text'].strip()}")
    except KeyboardInterrupt:
        typer.echo("\nğŸ›‘ å·²ä¸­æ–­ã€‚")
    except Exception as exc:
        typer.echo(f"âŒ è¿è¡Œå¤±è´¥: {exc}")
        raise typer.Exit(1)
    finally:
        try:
            service.cleanup()
        except Exception:  # pragma: no cover - cleanup best-effort
            pass


@app.command("fine-tune")
def fine_tune_stub(
    base_model: str = typer.Option(..., "--base-model", help="åŸºç¡€æ¨¡å‹åç§°"),
    dataset_path: str = typer.Option(..., "--dataset", help="è®­ç»ƒæ•°æ®è·¯å¾„"),
    output_dir: str = typer.Option(..., "--output", help="è¾“å‡ºç›®å½•"),
    auto_download: bool = typer.Option(
        True, "--auto-download/--no-auto-download", help="è‡ªåŠ¨ç¡®ä¿åŸºç¡€æ¨¡å‹å°±ç»ª"
    ),
):
    """æäº¤ fine-tune è¯·æ±‚ï¼ˆå½“å‰ä¸ºå ä½å®ç°ï¼‰ã€‚"""

    if VLLMService is None:  # pragma: no cover - dependency guard
        typer.echo("âŒ å½“å‰ç¯å¢ƒæœªå®‰è£… isage-common[vllm]ï¼Œæ— æ³•è°ƒç”¨ fine-tune æ¥å£ã€‚")
        raise typer.Exit(1)

    # Auto-configure HuggingFace mirror for China mainland users
    ensure_hf_mirror_configured()

    service = VLLMService({"model_id": base_model, "auto_download": auto_download})
    try:
        try:
            service.fine_tune(
                {
                    "base_model": base_model,
                    "dataset_path": dataset_path,
                    "output_dir": output_dir,
                }
            )
        except NotImplementedError as exc:
            typer.echo(f"â„¹ï¸ {exc}")
        else:
            typer.echo("âœ… fine-tune è¯·æ±‚å·²æäº¤")
    finally:
        service.cleanup()


# ---------------------------------------------------------------------------
# Service lifecycle commands (via sageLLM LLMAPIServer)
# ---------------------------------------------------------------------------
@app.command("serve")
def serve_llm(
    model: str = typer.Option(
        "Qwen/Qwen2.5-0.5B-Instruct",
        "--model",
        "-m",
        help="LLM æ¨¡å‹åç§°",
    ),
    port: int = typer.Option(
        SagePorts.BENCHMARK_LLM,
        "--port",
        "-p",
        help=f"æœåŠ¡ç«¯å£ (é»˜è®¤: {SagePorts.BENCHMARK_LLM})",
    ),
    host: str = typer.Option(
        "0.0.0.0",
        "--host",
        help="æœåŠ¡ä¸»æœºåœ°å€",
    ),
    gpu_memory: float = typer.Option(
        0.7,
        "--gpu-memory",
        help="GPU å†…å­˜ä½¿ç”¨ç‡ (0.1-1.0)ï¼Œé»˜è®¤ 0.7 ä»¥å…¼å®¹æ¶ˆè´¹çº§æ˜¾å¡",
    ),
    max_model_len: int = typer.Option(
        4096,
        "--max-model-len",
        help="æœ€å¤§æ¨¡å‹åºåˆ—é•¿åº¦",
    ),
    tensor_parallel: int = typer.Option(
        1,
        "--tensor-parallel",
        "-tp",
        help="Tensor å¹¶è¡Œ GPU æ•°é‡",
    ),
    speculative_model: str = typer.Option(
        None,
        "--speculative-model",
        help="æŠ•æœºé‡‡æ ·ï¼ˆSpeculative Decodingï¼‰ä½¿ç”¨çš„è‰ç¨¿æ¨¡å‹ (Draft Model)",
    ),
    background: bool = typer.Option(
        True,
        "--background/--foreground",
        help="åå°è¿è¡Œï¼ˆé»˜è®¤ï¼‰æˆ–å‰å°è¿è¡Œ",
    ),
    with_embedding: bool = typer.Option(
        True,
        "--with-embedding/--no-embedding",
        help="åŒæ—¶å¯åŠ¨ Embedding æœåŠ¡ï¼ˆé»˜è®¤å¯ç”¨ï¼‰",
    ),
    embedding_model: str = typer.Option(
        "BAAI/bge-small-zh-v1.5",
        "--embedding-model",
        "-e",
        help="Embedding æ¨¡å‹åç§°",
    ),
    embedding_port: int = typer.Option(
        SagePorts.EMBEDDING_DEFAULT,
        "--embedding-port",
        help=f"Embedding æœåŠ¡ç«¯å£ (é»˜è®¤: {SagePorts.EMBEDDING_DEFAULT})",
    ),
):
    """å¯åŠ¨ LLM æ¨ç†æœåŠ¡ï¼ˆé€šè¿‡ sageLLMï¼‰ã€‚

    ä½¿ç”¨ sageLLM çš„ LLMAPIServer å¯åŠ¨ OpenAI å…¼å®¹çš„ LLM æœåŠ¡ã€‚
    é»˜è®¤åå°è¿è¡Œï¼Œå¯é€šè¿‡ 'sage llm stop' åœæ­¢ã€‚

    ç¤ºä¾‹:
        sage llm serve                           # å¯åŠ¨ LLM + Embedding æœåŠ¡
        sage llm serve -m Qwen/Qwen2.5-7B-Instruct  # æŒ‡å®šæ¨¡å‹
        sage llm serve --no-embedding            # ä»…å¯åŠ¨ LLMï¼Œä¸å¯åŠ¨ Embedding
        sage llm serve --foreground              # å‰å°è¿è¡Œï¼ˆé˜»å¡ï¼‰

    å¯åŠ¨åå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨:

        from sage.llm import UnifiedInferenceClient

        client = UnifiedInferenceClient.create()
        response = client.chat([{"role": "user", "content": "Hello"}])
    """
    if LLMLauncher is None:
        console.print("[red]âŒ LLMLauncher ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿å·²å®‰è£… sage-common[/red]")
        raise typer.Exit(1)

    # Launch LLM service using unified launcher
    result = LLMLauncher.launch(
        model=model,
        port=port,
        host=host,
        gpu_memory=gpu_memory,
        max_model_len=max_model_len,
        tensor_parallel=tensor_parallel,
        speculative_model=speculative_model,
        background=background,
        verbose=True,
    )

    if not result.success:
        if result.error and "already running" not in result.error:
            console.print(f"[dim]è¯·æ£€æŸ¥æ—¥å¿—: {LOG_DIR / f'llm_api_server_{port}.log'}[/dim]")
        raise typer.Exit(1)

    if background:
        console.print("\n[dim]ä½¿ç”¨ 'sage llm status' æŸ¥çœ‹çŠ¶æ€[/dim]")
        console.print("[dim]ä½¿ç”¨ 'sage llm stop' åœæ­¢æœåŠ¡[/dim]")
    else:
        # Foreground mode completed
        pass

    # Optionally start Embedding service
    if with_embedding:
        console.print("\n[blue]ğŸ¯ å¯åŠ¨ Embedding æœåŠ¡[/blue]")
        console.print(f"   æ¨¡å‹: {embedding_model}")
        console.print(f"   ç«¯å£: {embedding_port}")

        import subprocess
        import sys

        embedding_log = LOG_DIR / "embedding.log"
        embedding_cmd = [
            sys.executable,
            "-m",
            "sage.common.components.sage_embedding.embedding_server",
            "--model",
            embedding_model,
            "--port",
            str(embedding_port),
        ]

        with open(embedding_log, "w") as log_file:
            proc = subprocess.Popen(
                embedding_cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                start_new_session=True,
            )

        console.print(f"   [green]âœ“[/green] Embedding æœåŠ¡å·²å¯åŠ¨ (PID: {proc.pid})")
        console.print(f"   æ—¥å¿—: {embedding_log}")

        # Update service info with embedding PID
        if background:
            pid, config = LLMLauncher.load_service_info()
            if pid and config:
                config["embedding_pid"] = proc.pid
                config["embedding_port"] = embedding_port
                config["embedding_model"] = embedding_model
                LLMLauncher.save_service_info(pid, config)


@app.command("stop")
def stop_llm(
    force: bool = typer.Option(False, "--force", "-f", help="å¼ºåˆ¶åœæ­¢ (åŒ…æ‹¬æœªè®°å½•çš„å­¤å„¿æœåŠ¡)"),
):
    """åœæ­¢ LLM æ¨ç†æœåŠ¡ã€‚"""
    if LLMLauncher is None:
        console.print("[red]âŒ LLMLauncher ä¸å¯ç”¨[/red]")
        raise typer.Exit(1)

    success = LLMLauncher.stop(verbose=True, force=force)
    if not success:
        raise typer.Exit(1)


@app.command("restart")
def restart_llm():
    """é‡å¯ LLM æ¨ç†æœåŠ¡ï¼ˆä½¿ç”¨ä¸Šæ¬¡çš„é…ç½®ï¼‰ã€‚"""
    if LLMLauncher is None:
        console.print("[red]âŒ LLMLauncher ä¸å¯ç”¨[/red]")
        raise typer.Exit(1)

    # è·å–å½“å‰é…ç½®
    pid, config = LLMLauncher.load_service_info()
    if not config:
        console.print("[yellow]âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä¹‹å‰çš„æœåŠ¡é…ç½®ï¼Œè¯·ä½¿ç”¨ 'sage llm serve' å¯åŠ¨[/yellow]")
        raise typer.Exit(1)

    console.print("[blue]ğŸ”„ é‡å¯ LLM æœåŠ¡...[/blue]")

    # åœæ­¢æœåŠ¡
    LLMLauncher.stop(verbose=False)
    time.sleep(1)  # ç­‰å¾…ç«¯å£é‡Šæ”¾

    # ä½¿ç”¨ä¿å­˜çš„é…ç½®é‡æ–°å¯åŠ¨
    model = config.get("model", "Qwen/Qwen2.5-0.5B-Instruct")
    port = config.get("port", SagePorts.BENCHMARK_LLM)

    result = LLMLauncher.launch(
        model=model,
        port=port,
        background=True,
        verbose=True,
    )

    if result.success:
        console.print("[green]âœ… LLM æœåŠ¡é‡å¯æˆåŠŸ[/green]")
    else:
        console.print(f"[red]âŒ é‡å¯å¤±è´¥: {result.error}[/red]")
        raise typer.Exit(1)


@app.command("status")
def status_llm():
    """æŸ¥çœ‹ LLM æœåŠ¡çŠ¶æ€ã€‚"""

    import psutil

    if LLMLauncher is None:
        console.print("[red]âŒ LLMLauncher ä¸å¯ç”¨[/red]")
        raise typer.Exit(1)

    pid, config = LLMLauncher.load_service_info()

    table = Table(title="LLM æœåŠ¡çŠ¶æ€", show_header=True, header_style="bold")
    table.add_column("å±æ€§")
    table.add_column("å€¼")

    # Check process status based on saved PID
    saved_pid_running = False
    if pid and psutil.pid_exists(pid):
        try:
            proc = psutil.Process(pid)
            saved_pid_running = proc.is_running()
        except psutil.NoSuchProcess:
            pass

    # Check port status
    port = config.get("port", SagePorts.BENCHMARK_LLM) if config else SagePorts.BENCHMARK_LLM
    from sage.common.utils.system.network import is_port_occupied

    port_in_use = is_port_occupied("localhost", port)

    # Try to get actual service info via HTTP if port is in use
    actual_model = None
    service_healthy = False
    if port_in_use:
        try:
            import httpx

            resp = httpx.get(f"http://localhost:{port}/v1/models", timeout=5)
            if resp.status_code == 200:
                service_healthy = True
                models = resp.json().get("data", [])
                if models:
                    actual_model = models[0].get("id", "unknown")
        except Exception:
            pass

    # Determine overall status
    if saved_pid_running and port_in_use and service_healthy:
        status = "[green]è¿è¡Œä¸­[/green]"
    elif port_in_use and service_healthy:
        # Service is running but PID file is stale
        status = "[green]è¿è¡Œä¸­[/green] [dim](PID æ–‡ä»¶å·²è¿‡æ—¶)[/dim]"
    elif port_in_use:
        # Port occupied but service not responding (may be starting)
        status = "[yellow]å¯åŠ¨ä¸­...[/yellow]"
    else:
        status = "[red]å·²åœæ­¢[/red]"

    table.add_row("çŠ¶æ€", status)
    table.add_row("PID", str(pid) if pid else "-")
    table.add_row("ç«¯å£", str(port))

    # Show model info - prefer actual model from API if available
    if actual_model:
        table.add_row("æ¨¡å‹", actual_model)
    elif config:
        table.add_row("æ¨¡å‹", config.get("model", "-"))
    else:
        table.add_row("æ¨¡å‹", "-")

    if config:
        table.add_row("æ—¥å¿—", config.get("log_file", "-"))
    table.add_row("API ç«¯ç‚¹", f"http://localhost:{port}/v1")

    console.print(table)

    # Health check summary
    if service_healthy:
        console.print("\n[green]âœ“[/green] å¥åº·æ£€æŸ¥é€šè¿‡")
        if actual_model:
            console.print(f"  åŠ è½½çš„æ¨¡å‹: {actual_model}")
    elif port_in_use:
        console.print("\n[yellow]âš ï¸  æœåŠ¡æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨å€™...[/yellow]")

    # Check Embedding service status
    _show_embedding_status()


def _show_embedding_status():
    """æ˜¾ç¤º Embedding æœåŠ¡çŠ¶æ€ã€‚"""

    embedding_port = SagePorts.EMBEDDING_DEFAULT
    embedding_log = LOG_DIR / "embedding.log"

    # Check port status
    from sage.common.utils.system.network import is_port_occupied

    embedding_port_in_use = is_port_occupied("localhost", embedding_port)

    # Build table
    embed_table = Table(title="Embedding æœåŠ¡çŠ¶æ€", show_header=True, header_style="bold")
    embed_table.add_column("å±æ€§")
    embed_table.add_column("å€¼")

    if embedding_port_in_use:
        embed_status = "[green]è¿è¡Œä¸­[/green]"
    else:
        embed_status = "[red]å·²åœæ­¢[/red]"

    embed_table.add_row("çŠ¶æ€", embed_status)
    embed_table.add_row("ç«¯å£", str(embedding_port))
    embed_table.add_row("æ—¥å¿—", str(embedding_log) if embedding_log.exists() else "-")
    embed_table.add_row("API ç«¯ç‚¹", f"http://localhost:{embedding_port}/v1")

    console.print()
    console.print(embed_table)

    # Health check for embedding
    if embedding_port_in_use:
        try:
            import httpx

            resp = httpx.get(f"http://localhost:{embedding_port}/v1/models", timeout=5)
            if resp.status_code == 200:
                models = resp.json().get("data", [])
                if models:
                    console.print("\n[green]âœ“[/green] Embedding å¥åº·æ£€æŸ¥é€šè¿‡")
                    console.print(f"  åŠ è½½çš„æ¨¡å‹: {models[0].get('id', 'unknown')}")
        except Exception as e:
            console.print(f"\n[yellow]âš ï¸  Embedding å¥åº·æ£€æŸ¥å¤±è´¥: {e}[/yellow]")


@app.command("logs")
def view_logs(
    follow: bool = typer.Option(False, "--follow", "-f", help="å®æ—¶è·Ÿè¸ªæ—¥å¿—"),
    lines: int = typer.Option(50, "--lines", "-n", help="æ˜¾ç¤ºæœ€å N è¡Œ"),
):
    """æŸ¥çœ‹ LLM æœåŠ¡æ—¥å¿—ã€‚"""
    import os

    if LLMLauncher is None:
        console.print("[red]âŒ LLMLauncher ä¸å¯ç”¨[/red]")
        raise typer.Exit(1)

    _, config = LLMLauncher.load_service_info()

    if config and config.get("log_file"):
        log_file = Path(config["log_file"])
    else:
        # Try default log file
        log_file = LOG_DIR / f"llm_api_server_{SagePorts.BENCHMARK_LLM}.log"

    if not log_file.exists():
        console.print(f"[yellow]æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}[/yellow]")
        return

    console.print(f"[blue]ğŸ“„ æ—¥å¿—æ–‡ä»¶: {log_file}[/blue]\n")

    if follow:
        import shlex

        os.system(f"tail -f {shlex.quote(str(log_file))}")
    else:
        try:
            content = log_file.read_text()
            log_lines = content.strip().split("\n")
            for line in log_lines[-lines:]:
                console.print(line)
        except Exception as e:
            console.print(f"[red]æ— æ³•è¯»å–æ—¥å¿—: {e}[/red]")
