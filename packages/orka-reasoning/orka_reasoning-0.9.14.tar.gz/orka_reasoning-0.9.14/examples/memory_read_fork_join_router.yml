orchestrator:
  id: orka-ui
  strategy: parallel
  queue: orka:generated
  memory_preset: "semantic"  # Use semantic memory for knowledge processing
  agents:
    - memory-read_0
    - openai-answer_2
    - fork_3
    - join_9
    - openai-binary_10
    - router_11
    - memory-write_final
    - final_summary
agents:
  - id: memory-read_0
    type: memory
    memory_preset: "semantic"  # Facts and knowledge base (30 days)
    queue: orka:memory-read_0
    config:
      operation: read
      # ðŸŽ¯ Preset provides: limit=10, similarity_threshold=0.65, vector_weight=0.7,
      # text_weight=0.3, enable_hybrid_search=true, ef_runtime=15, etc.
      memory_category_filter: stored
      similarity_threshold: 0.75  # Override preset default of 0.65
    namespace: fact_validator
    prompt: Retrieve any stored memories about how the subject '{{ get_input() }}' was classified or understood in the past. Return "NONE" if nothing matches.
  - id: openai-answer_2
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.7
    queue: orka:openai-answer_2
    prompt: Given previous context {{ get_agent_response('memory-read_0') }}, provide an initial detailed answer to {{ get_input() }}. Be comprehensive and informative.
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
  - id: fork_3
    type: fork
    targets:
      - [openai-binary_4]  # Fixed: Each branch must be wrapped in a list
      - [openai-answer_7]
    depends_on:
      - openai-answer_2
  - id: openai-binary_4
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.1
    queue: orka:openai-binary_4
    prompt: Does the question {{ get_input() }} require factual validation? Answer with exactly 'true' or 'false' only.
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - fork_3
  - id: openai-answer_7
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.5
    queue: orka:openai-answer_7
    prompt: Provide a concise summary for the question {{ get_input() }}. Be clear and to the point.
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - fork_3
  - id: openai-classification_5
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.2
    queue: orka:openai-classification_5
    prompt: | 
      Classify the domain of the question {{ get_input() }}. Choose exactly one from: science, history, technology, geography, culture, general. Answer with only the domain name.
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - openai-binary_4
  - id: failover_11
    type: failover
    input: openai-answer_7
    children:
      - id: duckduckgo_12
        type: duckduckgo
        queue: orka:duckduckgo_12
        prompt: "{{ get_input() }}"
      - id: duckduckgo_13
        type: duckduckgo
        queue: orka:duckduckgo_13
        prompt: "{{ get_input() }}"
    depends_on:
      - openai-answer_7
      - duckduckgo_12
      - duckduckgo_13
  - id: openai-answer_6
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.8
    queue: orka:openai-answer_6
    prompt: "Provide an alternative perspective or deeper insight into the question {{ get_input() }} considering domain: {{ get_agent_response('openai-classification_5') }}. Be creative and thoughtful."
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - openai-classification_5
  - id: join_9
    type: join
    group: fork_3
  - id: openai-binary_10
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.1
    queue: orka:openai-binary_10
    prompt: "Is the provided information coherent and complete based on outputs: {{ get_agent_response('join_9') }}? Answer with exactly 'true' or 'false' only."
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - join_9
  - id: router_11
    type: router
    params:
      decision_key: openai-binary_10
      routing_map:
        "true":
          - openai-answer_14
          - memory-write_final
        "false":
          - openai-answer_15
          - memory-write_final
    depends_on:
      - openai-binary_10
  - id: openai-answer_14
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.6
    queue: orka:openai-answer_14
    prompt: Given confirmed coherent inputs {{ get_agent_response('join_9') }}, provide a polished final response to {{ get_input() }}. Be professional and comprehensive.
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - router_11
  - id: openai-answer_15
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.7
    queue: orka:openai-answer_15
    prompt: "Given identified gaps in coherence or completeness in {{ get_agent_response('join_9') }}, clarify or complete the information to fully answer {{ get_input() }}. Fill in the missing pieces."
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - router_11
  - id: memory-write_final
    type: memory
    memory_preset: "semantic"  # Facts and knowledge base (30 days)
    queue: orka:memory-write_final
    config:
      operation: write
      vector: true
      vector_field_name: "content_vector"
      force_recreate_index: false
    namespace: fact_validator
    prompt: |
      {
        "type": "final_result",
        "question": {{ get_input() | tojson }},
        "final_answer": {{ safe_get_response('openai-answer_14', get_agent_response('openai-answer_15')) | tojson }},
        "coherent": {{ (get_agent_response('openai-binary_10') == 'true') | tojson }},
        "sources": {{ {
          'memory': get_agent_response('memory-read_0'),
          'analysis': get_agent_response('openai-answer_2'),
          'classification': get_agent_response('openai-classification_5'),
          'alternative': get_agent_response('openai-answer_6')
        } | tojson }}
      }
    metadata:
      source: '{{ "openai-answer_14" if get_agent_response("openai-answer_14") else "openai-answer_15" }}'
      result: "{{ safe_get_response('openai-answer_14', get_agent_response('openai-answer_15')) }}"
      category: stored
    key_template: "{{ get_input() }}"
    depends_on:
      - router_11
  
  - id: final_summary
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.7
    prompt: |
      Based on the question "{{ get_input() }}", provide a comprehensive response using the available information:
      
      {% set memory_info = get_agent_response('memory-read_0') %}
      {% if memory_info %}
      Retrieved information: {{ memory_info }}
      {% endif %}
      
      {% set analysis = get_agent_response('openai-answer_2') %}
      {% if analysis %}
      Analysis: {{ analysis }}
      {% endif %}
      
      {% set validation = get_agent_response('openai-binary_4') %}
      {% set classification = get_agent_response('openai-classification_5') %}
      {% set alternative = get_agent_response('openai-answer_6') %}
      {% set coherence = get_agent_response('openai-binary_10') %}
      {% set final_answer = get_agent_response('openai-answer_14') or get_agent_response('openai-answer_15') %}
      
      {% if final_answer %}
      {{ final_answer }}
      {% endif %}
      
      Synthesize all available information to provide the most accurate and helpful response to the user's question.
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
    depends_on:
      - memory-write_final
