#!/usr/bin/env python3
"""
Two-Stage Judgment Pipeline: TinyLlama (Judge) + Mistral (Narrator)

NOTE: Excluded from Stage 1-only canon.
Reason: Uses LLM (TinyLlama) in Stage 1 judgment, inconsistent with LLM-free deterministic judgment definition.

ì—­í•  ë¶„ë¦¬:
- Stage 1 (TinyLlama): 1ì°¨ íŒì •ê¸° (VALUE/INDETERMINATE/STOP)
- Stage 2 (Mistral): 2ì°¨ ì„œìˆ ì/ê°ì‚¬ì (ì„¤ëª…ë§Œ, íŒì • ê¶Œí•œ ì—†ìŒ)

í•µì‹¬ ì›ì¹™:
1. íŒì • ê¶Œí•œì€ í•­ìƒ TinyLlamaì— ê³ ì •
2. Mistralì€ ì„¤ëª…ë§Œ ìˆ˜í–‰ (ì¬íŒì • ê¸ˆì§€)
3. ìƒì‹ ê¸°ë°˜ ìë™ ë³´ì • ì°¨ë‹¨
4. ê·¼ê±° ì¶œì²˜ ë¶ˆëª… ì‹œ ì¦‰ì‹œ STOP
"""

import json
import logging
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Optional, Literal, Tuple, Dict, Any
import re

from echo_engine.llm_router import get_default_router
from echo_engine.routing import InferenceContext

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


# Judgment states
JudgmentState = Literal["VALUE", "INDETERMINATE", "STOP"]


@dataclass
class ObservationRecord:
    """ê´€ì¸¡ ê¸°ë¡ (ê°œë… ì—†ìŒ)"""
    record_id: str
    timestamp: str
    estimated_protrusions: int
    convexity_defects: int
    contour_area: float
    hull_points: int
    bbox_width: int
    bbox_height: int
    aspect_ratio: float
    image_path: str
    processing_method: str


@dataclass
class Stage1JudgmentResult:
    """Stage 1: TinyLlama íŒì • ê²°ê³¼"""
    record_id: str
    timestamp: str

    # Judgment (íŒì •)
    state: JudgmentState  # VALUE, INDETERMINATE, STOP
    value: Optional[int]  # state == VALUEì¼ ë•Œë§Œ ìœ íš¨

    # Raw outputs
    raw_response: str

    # Metadata
    model: str
    latency_s: float
    reasoning_trace: str


@dataclass
class Stage2NarrativeResult:
    """Stage 2: Mistral ì„œìˆ  ê²°ê³¼"""
    record_id: str
    timestamp: str

    # Stage 1 ê²°ê³¼ (ì½ê¸° ì „ìš©)
    stage1_state: JudgmentState
    stage1_value: Optional[int]

    # Narrative (ì„¤ëª…)
    explanation: str

    # Prior intrusion detection
    prior_intrusion_detected: bool
    intrusion_evidence: str

    # Raw outputs
    raw_response: str

    # Metadata
    model: str
    latency_s: float


@dataclass
class TwoStagePipelineResult:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²°ê³¼"""
    record_id: str
    timestamp: str

    # Stage 1 (íŒì •)
    stage1_result: Stage1JudgmentResult

    # Stage 2 (ì„œìˆ ) - STOP/INDETERMINATEë©´ None
    stage2_result: Optional[Stage2NarrativeResult]

    # Final decision (í•­ìƒ Stage 1 íŒì •)
    final_state: JudgmentState
    final_value: Optional[int]

    # Quality signals
    pipeline_stopped_early: bool
    prior_intrusion_detected: bool


class TinyLlamaJudge:
    """Stage 1: ê²½ëŸ‰ LLM íŒì •ê¸° (VALUE/INDETERMINATE/STOP)

    NOTE: Migrated to LLMRouter architecture (2025-12-23).
    Direct HTTP calls replaced with router-based judgment context.
    """

    def __init__(
        self,
        router=None,
        model: str = "phi3:mini",  # phi3:mini > tinyllama for instruction following
    ):
        self.router = router or get_default_router()
        self.model = model

    def judge(self, observation: ObservationRecord) -> Stage1JudgmentResult:
        """
        1ì°¨ íŒì •: Observation Record â†’ VALUE/INDETERMINATE/STOP

        ì—­í• :
        - ê·œì¹™ ì ìš©
        - ë©ˆì¶¤ íŒë‹¨
        - ê·¼ê±° ì¶œì²˜ ì¶”ì  ì‹¤íŒ¨ ì‹œ STOP/INDETERMINATE

        ê¸ˆì§€:
        - ìƒì‹(prior) ì‚¬ìš©
        - ê°œë… ê¸°ë°˜ ì¶”ë¡ 
        - ì„ì˜ ë³´ì •
        """
        logger.info("=" * 80)
        logger.info("STAGE 1: TINYLLAMA JUDGMENT")
        logger.info("=" * 80)
        logger.info(f"Model: {self.model}")
        logger.info(f"Observation: {observation.record_id}")
        logger.info("")

        start_time = time.time()

        # ê´€ì¸¡ ê¸°ë¡ ì§ë ¬í™”
        obs_text = self._serialize_observation(observation)

        # íŒì • í”„ë¡¬í”„íŠ¸
        prompt = self._build_judgment_prompt(obs_text)

        logger.info("Calling TinyLlama for judgment...")
        logger.info("")

        # Ollama í˜¸ì¶œ
        raw_response = self._call_ollama(prompt)

        # ì‘ë‹µ íŒŒì‹±
        state, value, reasoning = self._parse_judgment(raw_response)

        latency = time.time() - start_time

        logger.info(f"State: {state}")
        logger.info(f"Value: {value}")
        logger.info(f"Reasoning: {reasoning}")
        logger.info(f"Latency: {latency:.2f}s")
        logger.info("=" * 80)
        logger.info("")

        return Stage1JudgmentResult(
            record_id=observation.record_id,
            timestamp=datetime.now().isoformat(),
            state=state,
            value=value,
            raw_response=raw_response,
            model=self.model,
            latency_s=round(latency, 2),
            reasoning_trace=reasoning,
        )

    def _serialize_observation(self, observation: ObservationRecord) -> str:
        """ê´€ì¸¡ ê¸°ë¡ â†’ í…ìŠ¤íŠ¸ (ê°œë… ì—†ìŒ)"""
        return f"""Observation Record: {observation.record_id}

Structural Measurements (NO concept labels):
- Estimated protrusions: {observation.estimated_protrusions}
- Convexity defects: {observation.convexity_defects}
- Contour area: {observation.contour_area:.0f} px
- Hull points: {observation.hull_points}
- Bounding box: {observation.bbox_width} x {observation.bbox_height}
- Aspect ratio: {observation.aspect_ratio:.2f}

Processing method: {observation.processing_method}"""

    def _build_judgment_prompt(self, obs_text: str) -> str:
        """íŒì • í”„ë¡¬í”„íŠ¸ (ìƒì‹ ì°¨ë‹¨) - TinyLlama ìµœì í™”"""
        # TinyLlamaëŠ” ë§¤ìš° ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ ê·¹ë„ë¡œ ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ í•„ìš”
        # ì§ì ‘ì ì¸ ì§€ì‹œ: "estimated_protrusions" ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥
        return f"""Read the observation data and output the "estimated_protrusions" value.

{obs_text}

Output ONLY the number.
Answer:"""

    def _call_ollama(self, prompt: str) -> str:
        """LLM inference via router (judgment context)"""
        ctx = InferenceContext.judgment()
        try:
            result = self.router.generate(
                prompt,
                context=ctx,
                model=self.model,
                temperature=0.0,  # ê²°ì •ë¡ ì 
                num_predict=10,   # ë§¤ìš° ì§§ì€ ì‘ë‹µ
            )
            return result.text.strip()

        except Exception as e:
            logger.error(f"Router inference error: {e}")
            return "ERROR"

    def _parse_judgment(self, response: str) -> Tuple[JudgmentState, Optional[int], str]:
        """ì‘ë‹µ íŒŒì‹±: (state, value, reasoning) - ìœ ì—°í•œ ì¶”ì¶œ"""
        response_clean = response.strip()

        # STOP ì²´í¬ (ì „ì²´ ì‘ë‹µì—ì„œ)
        if "STOP" in response_clean.upper():
            return "STOP", None, "Evidence source untraceable"

        # INDETERMINATE ì²´í¬ (ì „ì²´ ì‘ë‹µì—ì„œ)
        if "INDETERMINATE" in response_clean.upper():
            return "INDETERMINATE", None, "Insufficient evidence"

        # VALUE (ì •ìˆ˜) ì¶”ì¶œ - ì „ì²´ ì‘ë‹µì—ì„œ ì²« ë²ˆì§¸ ìˆ«ì ì°¾ê¸°
        numbers = re.findall(r'\b\d+\b', response_clean)
        if numbers:
            value = int(numbers[0])
            return "VALUE", value, f"Based on structural observation: {value}"

        # íŒŒì‹± ì‹¤íŒ¨ â†’ INDETERMINATE
        first_line = response_clean.split("\n")[0][:50]  # ì²˜ìŒ 50ìë§Œ
        return "INDETERMINATE", None, f"Parse failed: {first_line}"


class MistralNarrator:
    """Stage 2: Mistral ì„œìˆ ì (ì„¤ëª…ë§Œ, íŒì • ê¶Œí•œ ì—†ìŒ)

    NOTE: Migrated to LLMRouter architecture (2025-12-23).
    Direct HTTP calls replaced with router-based judgment context.
    """

    def __init__(
        self,
        router=None,
        model: str = "mistral:instruct",
    ):
        self.router = router or get_default_router()
        self.model = model

    def narrate(
        self,
        observation: ObservationRecord,
        stage1_result: Stage1JudgmentResult,
    ) -> Stage2NarrativeResult:
        """
        2ì°¨ ì„œìˆ : Stage 1 íŒì • ê²°ê³¼ ì„¤ëª…

        ì—­í• :
        - íŒì • ê²°ê³¼ ì„¤ëª…
        - ìƒì‹ ì¹¨íˆ¬ ê°ì§€
        - ê°ì‚¬ì ì—­í• 

        ê¸ˆì§€:
        - íŒì • ê²°ê³¼ ìˆ˜ì •
        - ì¬íŒì •
        - íŒì • ê¶Œí•œ í–‰ì‚¬
        """
        logger.info("=" * 80)
        logger.info("STAGE 2: MISTRAL NARRATIVE")
        logger.info("=" * 80)
        logger.info(f"Model: {self.model}")
        logger.info(f"Stage 1 Result: {stage1_result.state} = {stage1_result.value}")
        logger.info("")

        start_time = time.time()

        # ê´€ì¸¡ ê¸°ë¡ + Stage 1 ê²°ê³¼
        context = self._build_narrative_context(observation, stage1_result)

        # ì„œìˆ  í”„ë¡¬í”„íŠ¸
        prompt = self._build_narrative_prompt(context)

        logger.info("Calling Mistral for explanation...")
        logger.info("")

        # Ollama í˜¸ì¶œ
        raw_response = self._call_ollama(prompt)

        # ìƒì‹ ì¹¨íˆ¬ ê°ì§€
        prior_detected, evidence = self._detect_prior_intrusion(raw_response)

        latency = time.time() - start_time

        logger.info(f"Explanation: {raw_response[:200]}...")
        logger.info(f"Prior Intrusion: {prior_detected}")
        if prior_detected:
            logger.info(f"  Evidence: {evidence}")
        logger.info(f"Latency: {latency:.2f}s")
        logger.info("=" * 80)
        logger.info("")

        return Stage2NarrativeResult(
            record_id=observation.record_id,
            timestamp=datetime.now().isoformat(),
            stage1_state=stage1_result.state,
            stage1_value=stage1_result.value,
            explanation=raw_response,
            prior_intrusion_detected=prior_detected,
            intrusion_evidence=evidence,
            raw_response=raw_response,
            model=self.model,
            latency_s=round(latency, 2),
        )

    def _build_narrative_context(
        self,
        observation: ObservationRecord,
        stage1_result: Stage1JudgmentResult,
    ) -> str:
        """ì„œìˆ  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        judge_helper = TinyLlamaJudge(router=self.router)
        return f"""OBSERVATION RECORD:
{judge_helper._serialize_observation(observation)}

STAGE 1 JUDGMENT (PRIMARY JUDGE - TinyLlama):
- State: {stage1_result.state}
- Value: {stage1_result.value}
- Reasoning: {stage1_result.reasoning_trace}"""

    def _build_narrative_prompt(self, context: str) -> str:
        """ì„œìˆ  í”„ë¡¬í”„íŠ¸ (íŒì • ê¶Œí•œ ì—†ìŒ ëª…ì‹œ)"""
        return f"""You are a NARRATOR/AUDITOR. Your role is to EXPLAIN the judgment, NOT to make or modify it.

CRITICAL RULES:
1. You have NO authority to change the judgment
2. The PRIMARY JUDGE's decision is FINAL and READ-ONLY
3. Your task: Explain WHY the judgment was made based on observation data
4. If you find yourself using common sense (e.g., "this is a hand", "fingers"), STOP and acknowledge it
5. Focus on structural data, NOT semantic interpretation

CONTEXT:
{context}

YOUR TASK:
Explain the PRIMARY JUDGE's decision based ONLY on the structural measurements provided.
If you notice yourself using prior knowledge (common sense), explicitly mention it as "PRIOR_INTRUSION".

EXPLANATION:"""

    def _call_ollama(self, prompt: str) -> str:
        """LLM inference via router (judgment context)"""
        ctx = InferenceContext.judgment()
        try:
            result = self.router.generate(
                prompt,
                context=ctx,
                model=self.model,
                temperature=0.3,
                num_predict=200,
            )
            return result.text.strip()

        except Exception as e:
            logger.error(f"Router inference error: {e}")
            return "ERROR"

    def _detect_prior_intrusion(self, response: str) -> Tuple[bool, str]:
        """ìƒì‹ ì¹¨íˆ¬ ê°ì§€"""

        # ëª…ì‹œì  PRIOR_INTRUSION ì„ ì–¸
        if "PRIOR_INTRUSION" in response.upper():
            return True, "Explicitly acknowledged by narrator"

        # ê°œë… ë¼ë²¨ ì‚¬ìš© ê°ì§€
        concept_keywords = [
            "hand", "finger", "thumb", "palm", "digit",
            "ì†", "ì†ê°€ë½", "ì—„ì§€", "ì†ë°”ë‹¥",
        ]

        response_lower = response.lower()
        found_concepts = [kw for kw in concept_keywords if kw in response_lower]

        if found_concepts:
            return True, f"Concept labels used: {', '.join(found_concepts)}"

        # ìƒì‹ ê¸°ë°˜ ì¶”ë¡  íŒ¨í„´
        prior_patterns = [
            "normally", "usually", "typically", "common",
            "ë³´í†µ", "ì¼ë°˜ì ìœ¼ë¡œ", "ëŒ€ê°œ",
        ]

        found_priors = [p for p in prior_patterns if p in response_lower]

        if found_priors:
            return True, f"Prior-based reasoning: {', '.join(found_priors)}"

        return False, ""


class TwoStageJudgmentPipeline:
    """2ë‹¨ê³„ íŒë‹¨ íŒŒì´í”„ë¼ì¸: TinyLlama (íŒì •) + Mistral (ì„œìˆ )

    NOTE: Migrated to LLMRouter architecture (2025-12-23).
    Both stages now route through judgment context.
    """

    def __init__(self, router=None):
        self.router = router or get_default_router()
        self.judge = TinyLlamaJudge(router=self.router)
        self.narrator = MistralNarrator(router=self.router)

    def execute(
        self,
        observation: ObservationRecord,
    ) -> TwoStagePipelineResult:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        íë¦„:
        1. Stage 1 (TinyLlama): íŒì •
        2. If STOP/INDETERMINATE â†’ ì¢…ë£Œ
        3. If VALUE â†’ Stage 2 (Mistral): ì„œìˆ 
        """
        logger.info("\n")
        logger.info("â•”" + "=" * 78 + "â•—")
        logger.info("â•‘" + " " * 20 + "TWO-STAGE JUDGMENT PIPELINE" + " " * 30 + "â•‘")
        logger.info("â•š" + "=" * 78 + "â•")
        logger.info("\n")

        # Stage 1: TinyLlama íŒì •
        stage1_result = self.judge.judge(observation)

        # Early termination check
        if stage1_result.state in ["STOP", "INDETERMINATE"]:
            logger.info(f"ğŸ›‘ Pipeline stopped early: {stage1_result.state}")
            logger.info(f"   Reason: {stage1_result.reasoning_trace}")
            logger.info("")

            return TwoStagePipelineResult(
                record_id=observation.record_id,
                timestamp=datetime.now().isoformat(),
                stage1_result=stage1_result,
                stage2_result=None,
                final_state=stage1_result.state,
                final_value=None,
                pipeline_stopped_early=True,
                prior_intrusion_detected=False,
            )

        # Stage 2: Mistral ì„œìˆ  (VALUEì¸ ê²½ìš°ë§Œ)
        logger.info(f"âœ… Stage 1 completed: VALUE = {stage1_result.value}")
        logger.info(f"   Proceeding to Stage 2 (Narrative)...")
        logger.info("")

        stage2_result = self.narrator.narrate(observation, stage1_result)

        # Final result
        logger.info("=" * 80)
        logger.info("PIPELINE COMPLETE")
        logger.info("=" * 80)
        logger.info(f"Final State: {stage1_result.state}")
        logger.info(f"Final Value: {stage1_result.value}")
        logger.info(f"Prior Intrusion: {stage2_result.prior_intrusion_detected}")
        logger.info("=" * 80)
        logger.info("")

        return TwoStagePipelineResult(
            record_id=observation.record_id,
            timestamp=datetime.now().isoformat(),
            stage1_result=stage1_result,
            stage2_result=stage2_result,
            final_state=stage1_result.state,
            final_value=stage1_result.value,
            pipeline_stopped_early=False,
            prior_intrusion_detected=stage2_result.prior_intrusion_detected,
        )

    def save_result(self, result: TwoStagePipelineResult, filepath: Path):
        """ê²°ê³¼ ì €ì¥"""
        with filepath.open("w", encoding="utf-8") as f:
            json.dump(asdict(result), f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… Result saved: {filepath}")


def test_reproducibility(
    pipeline: TwoStageJudgmentPipeline,
    observation: ObservationRecord,
    n_runs: int = 3,
) -> bool:
    """ì¬í˜„ì„± í…ŒìŠ¤íŠ¸: ë™ì¼ ì…ë ¥ â†’ ë™ì¼ íŒì •"""

    logger.info("\n")
    logger.info("=" * 80)
    logger.info("REPRODUCIBILITY TEST")
    logger.info("=" * 80)
    logger.info(f"Running {n_runs} times with same observation...")
    logger.info("")

    results = []

    for i in range(n_runs):
        logger.info(f"Run {i+1}/{n_runs}")
        result = pipeline.execute(observation)
        results.append((result.final_state, result.final_value))
        logger.info(f"  Result: {result.final_state} = {result.final_value}")
        logger.info("")

    # ëª¨ë‘ ë™ì¼í•œì§€ í™•ì¸
    all_same = len(set(results)) == 1

    logger.info("Results:")
    logger.info(f"  {results}")
    logger.info(f"  All same: {all_same}")

    if all_same:
        logger.info("  âœ… PASS: Reproducible")
    else:
        logger.info("  âŒ FAIL: Not reproducible")

    logger.info("=" * 80)
    logger.info("")

    return all_same


def main():
    """ë©”ì¸ ì‹¤í–‰"""

    # Load observation record from previous run
    obs_file = Path("observation_record_real.json")

    if not obs_file.exists():
        logger.error(f"âŒ Observation record not found: {obs_file}")
        logger.error("   Please run real_image_finger_counter.py first")
        return 1

    with obs_file.open("r", encoding="utf-8") as f:
        obs_data = json.load(f)

    observation = ObservationRecord(**obs_data)

    logger.info(f"âœ… Loaded observation: {observation.record_id}")
    logger.info(f"   Estimated protrusions: {observation.estimated_protrusions}")
    logger.info("")

    # Create pipeline
    pipeline = TwoStageJudgmentPipeline()

    # Execute
    result = pipeline.execute(observation)

    # Save
    output_file = Path("two_stage_result.json")
    pipeline.save_result(result, output_file)

    # Reproducibility test
    logger.info("Running reproducibility test...")
    reproducible = test_reproducibility(pipeline, observation, n_runs=3)

    # Final summary
    logger.info("\n")
    logger.info("=" * 80)
    logger.info("FINAL SUMMARY")
    logger.info("=" * 80)
    logger.info(f"Observation: {observation.record_id}")
    logger.info(f"Estimated Protrusions: {observation.estimated_protrusions}")
    logger.info("")
    logger.info("STAGE 1 (TinyLlama - JUDGE):")
    logger.info(f"  State: {result.final_state}")
    logger.info(f"  Value: {result.final_value}")
    logger.info("")

    if result.stage2_result:
        logger.info("STAGE 2 (Mistral - NARRATOR):")
        logger.info(f"  Explanation: {result.stage2_result.explanation[:100]}...")
        logger.info(f"  Prior Intrusion: {result.prior_intrusion_detected}")
        if result.prior_intrusion_detected:
            logger.info(f"    Evidence: {result.stage2_result.intrusion_evidence}")
    else:
        logger.info("STAGE 2: Skipped (early termination)")

    logger.info("")
    logger.info("Success Criteria:")
    logger.info(f"  âœ… Judgment authority fixed to TinyLlama: YES")
    logger.info(f"  âœ… Mistral performs explanation only: YES")
    logger.info(f"  {'âœ…' if reproducible else 'âŒ'} Reproducibility: {'PASS' if reproducible else 'FAIL'}")
    logger.info(f"  {'âš ï¸ ' if result.prior_intrusion_detected else 'âœ…'} Prior intrusion prevented: {'WARNING' if result.prior_intrusion_detected else 'PASS'}")
    logger.info("=" * 80)
    logger.info("")

    return 0 if reproducible else 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
