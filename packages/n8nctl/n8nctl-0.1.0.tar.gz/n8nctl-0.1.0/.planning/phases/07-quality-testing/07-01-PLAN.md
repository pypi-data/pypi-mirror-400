---
phase: 07-quality-testing
plan: 01
type: execute
---

<objective>
Improve test coverage across all CLI commands and utilities to 90%+ (currently 79% overall).

Purpose: Ensure comprehensive test coverage with focus on error paths, edge cases, and verbosity output that are currently untested. Following established patterns (pytest-httpx mocking, minimal duplication).

Output: Test coverage increased from 79% to 90%+ with all command modules at 90%+ coverage, producing high-quality tests that document behavior and catch regressions.

**Current Coverage Gaps:**
- commands/member.py: 60% (missing error paths, edge cases)
- commands/user.py: 70% (missing validation, error handling)
- commands/project.py: 74% (missing edge cases, verbosity)
- commands/workflow.py: 74% (missing file operations, complex scenarios)
- commands/execution.py: 78% (missing error paths, filtering)
- completion.py: 72% (missing error scenarios)
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Test patterns to follow:**
- Use pytest-httpx for API mocking (httpx_mock.add_response)
- Mock typer.confirm() for confirmation prompts
- Test error paths with appropriate exceptions
- Test verbosity levels (verbose=0, 1, 2, 3, 4)
- Follow existing test structure in each test file
- Minimal mocking - only mock external APIs and user input

**Existing test files to extend:**
- tests/commands/test_member.py
- tests/commands/test_user.py
- tests/commands/test_project.py
- tests/commands/test_workflow.py
- tests/commands/test_execution.py
- tests/test_completion.py
- tests/test_utils.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Improve member, user, project command test coverage to 90%+</name>
  <files>tests/commands/test_member.py, tests/commands/test_user.py, tests/commands/test_project.py</files>
  <action>
Add test cases for uncovered branches identified in coverage report:

**test_member.py (60% → 90%+):**
- Error scenarios: API failures (HTTPError, ServerError), network errors
- Edge cases: Empty member lists, member not found, project not found
- Verbosity levels: Test -v and -vv output differences
- Confirmation handling: Test member remove with mocked confirm (yes/no)
- Validation errors: Invalid email format, invalid role

**test_user.py (70% → 90%+):**
- Error scenarios: User invite failures, duplicate email handling
- Edge cases: Empty user lists, user not found
- Verbosity levels: Test -v output for list command
- Confirmation prompts: Test user remove with confirm mock
- Validation: Invalid email handling

**test_project.py (74% → 90%+):**
- Error scenarios: Project not found, API failures
- Edge cases: Empty project lists, missing member data
- Verbosity levels: Test -v and -vv output
- Member placeholder: Verify "member management not yet implemented" message

Follow existing patterns: pytest-httpx for API mocking, mock_api_client fixture, typer.confirm mocking for prompts. Keep tests focused on one behavior per test function.
  </action>
  <verify>uv run pytest tests/commands/test_member.py tests/commands/test_user.py tests/commands/test_project.py --cov=src/n8n_cli/commands/member.py --cov=src/n8n_cli/commands/user.py --cov=src/n8n_cli/commands/project.py --cov-report=term-missing shows all three modules at 90%+ coverage</verify>
  <done>member.py, user.py, and project.py all show 90%+ coverage with new tests for error paths, edge cases, and verbosity levels</done>
</task>

<task type="auto">
  <name>Task 2: Improve workflow and execution command test coverage to 90%+</name>
  <files>tests/commands/test_workflow.py, tests/commands/test_execution.py</files>
  <action>
Add test cases for uncovered branches (workflow.py has 124 missing lines - largest gap):

**test_workflow.py (74% → 90%+):**
- File operations: Test pull/push with file write errors, invalid JSON, missing directories
- Diff scenarios: Test different verbosity levels (0-4), no changes detected, API errors
- Move command: Test placeholder message for deferred functionality
- Error paths: Workflow not found by name/ID, duplicate workflow names, file not found
- Edge cases: Empty workflow lists, workflows with no active flag
- Create command: Test with invalid workflow JSON, file read errors
- Delete confirmation: Test with confirm mock (yes/no paths)
- Open command: Test URL construction, different instance URL formats

**test_execution.py (78% → 90%+):**
- Status filtering: Test different status values (success, error, running, waiting)
- Error scenarios: Execution not found, API failures during retry
- Download command: Test with custom filenames, file write errors, missing execution data
- Verbosity levels: Test -v through -vvvv output differences
- Edge cases: Empty execution lists, executions without workflow data

Follow established patterns: Use pytest-httpx, mock file operations with tmp_path fixture, test one scenario per function. Avoid over-mocking - only mock external APIs and file I/O.
  </action>
  <verify>uv run pytest tests/commands/test_workflow.py tests/commands/test_execution.py --cov=src/n8n_cli/commands/workflow.py --cov=src/n8n_cli/commands/execution.py --cov-report=term-missing shows both modules at 90%+ coverage</verify>
  <done>workflow.py and execution.py both show 90%+ coverage with comprehensive tests for file operations, error handling, and complex scenarios</done>
</task>

<task type="auto">
  <name>Task 3: Improve completion and utility test coverage to 95%+</name>
  <files>tests/test_completion.py, tests/test_utils.py</files>
  <action>
Add test cases for remaining uncovered edge cases:

**test_completion.py (72% → 95%+):**
- Error scenarios: API client creation failure, network errors during completion
- Empty responses: No workflows/projects returned from API
- Incomplete context: Missing params in completion context
- File completion: Test with different directory structures, missing directories
- Silent error handling: Verify completion always returns list (never raises)

**test_utils.py (92% → 95%+):**
- Edge cases: Missing .n8n directory, invalid config.json format
- Directory creation: Test workflows dir creation when missing
- File write errors: Test save_workflow with permission errors
- Path resolution: Test with various project structures

Keep tests minimal and focused. Each test should verify one specific behavior. Follow existing pytest patterns in both test files.
  </action>
  <verify>uv run pytest tests/test_completion.py tests/test_utils.py --cov=src/n8n_cli/completion.py --cov=src/n8n_cli/utils.py --cov-report=term-missing shows both modules at 95%+ coverage</verify>
  <done>completion.py and utils.py both show 95%+ coverage with comprehensive edge case and error scenario tests</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `uv run pytest` passes all tests (existing + new)
- [ ] `uv run pytest --cov=src/n8n_cli --cov-report=term-missing` shows 90%+ overall coverage
- [ ] All command modules (member, user, project, workflow, execution) at 90%+ coverage
- [ ] completion.py and utils.py at 95%+ coverage
- [ ] `uv run mypy src/` passes with no errors
- [ ] `uv run ruff check` passes with no errors
- [ ] No test duplication introduced (each test covers unique scenario)
</verification>

<success_criteria>

- All tasks completed
- Overall test coverage increased from 79% to 90%+
- All CLI command modules at 90%+ coverage minimum
- completion.py and utils.py at 95%+ coverage
- All new tests follow established patterns (pytest-httpx, minimal mocking)
- No regressions introduced (all existing tests still pass)
- Quality gates pass (pytest, mypy, ruff)
</success_criteria>

<output>
After completion, create `.planning/phases/07-quality-testing/07-01-SUMMARY.md`:

# Phase 7 Plan 1: Test Coverage Improvements Summary

**Test coverage increased from 79% to 90%+ across all CLI commands and utilities**

## Performance

- **Duration:** [X] min
- **Started:** [timestamp]
- **Completed:** [timestamp]
- **Tasks:** 3
- **Files modified:** 7 test files

## Accomplishments

- Improved member, user, project command coverage (60%/70%/74% → 90%+)
- Improved workflow, execution command coverage (74%/78% → 90%+)
- Improved completion, utils coverage (72%/92% → 95%+)
- Added [X] new test cases covering error paths, edge cases, verbosity levels
- Overall coverage: 79% → 90%+

## Files Created/Modified

- `tests/commands/test_member.py` - Added [X] tests for error scenarios and edge cases
- `tests/commands/test_user.py` - Added [X] tests for validation and error handling
- `tests/commands/test_project.py` - Added [X] tests for verbosity and edge cases
- `tests/commands/test_workflow.py` - Added [X] tests for file operations and complex scenarios
- `tests/commands/test_execution.py` - Added [X] tests for filtering and error paths
- `tests/test_completion.py` - Added [X] tests for error scenarios
- `tests/test_utils.py` - Added [X] tests for edge cases

## Decisions Made

[Document any testing patterns or approaches chosen, or "None"]

## Deviations from Plan

[Document any deviations or "None - plan executed as written"]

## Issues Encountered

[Document any issues or "None"]

## Next Step

Phase 7 Plan 1 complete. Consider additional plans if needed:
- Test quality review (minimize mocking, reduce duplication)
- Integration tests (end-to-end command flows)
- Performance tests (large workflow handling)

Or proceed to next milestone if quality is sufficient.

---
*Phase: 07-quality-testing*
*Completed: [date]*
</output>
