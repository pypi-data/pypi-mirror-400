"""Search Catalog MCP Tool - Query local catalog artifacts."""

from __future__ import annotations

import time
from collections.abc import Iterable, Sequence
from pathlib import Path
from typing import Any

from igloo_mcp.catalog import CatalogIndex
from igloo_mcp.mcp.exceptions import (
    MCPSelectorError,
    MCPValidationError,
)
from igloo_mcp.path_utils import (
    resolve_catalog_path,
    resolve_catalog_root,
    validate_safe_path,
)

from .base import MCPTool, ensure_request_id, tool_error_handler
from .schema_utils import (
    integer_schema,
    snowflake_identifier_schema,
    string_schema,
)

try:
    from fastmcp.utilities.logging import get_logger
except ImportError:
    from mcp.server.fastmcp.utilities.logging import get_logger

logger = get_logger(__name__)


class SearchCatalogTool(MCPTool):
    """MCP tool for searching catalog artifacts generated by build_catalog."""

    def __init__(self) -> None:
        # No shared state required; catalog files are read per invocation
        pass

    @property
    def name(self) -> str:
        return "search_catalog"

    @property
    def description(self) -> str:
        return (
            "Search offline catalog for tables/columns without querying Snowflake. "
            "Use for rapid schema discovery BEFORE writing queries. "
            "Combine name_contains and column_contains for targeted searches."
        )

    @property
    def category(self) -> str:
        return "metadata"

    @property
    def tags(self) -> list[str]:
        return ["catalog", "metadata", "search", "discovery"]

    @property
    def usage_examples(self) -> list[dict[str, Any]]:
        return [
            {
                "description": "Find tables containing the word 'customers'",
                "parameters": {
                    "name_contains": "customers",
                    "object_types": ["table"],
                },
            },
            {
                "description": "Search for columns mentioning 'revenue' in ANALYTICS database",
                "parameters": {
                    "database": "ANALYTICS",
                    "column_contains": "revenue",
                },
            },
        ]

    @tool_error_handler("search_catalog")
    async def execute(
        self,
        catalog_dir: str = "./data_catalogue",
        object_types: list[str] | None = None,
        database: str | None = None,
        schema: str | None = None,
        name_contains: str | None = None,
        column_contains: str | None = None,
        limit: int = 20,
        search_all_databases: bool = False,
        request_id: str | None = None,
        **kwargs: Any,
    ) -> dict[str, Any]:
        # Timing and request correlation
        start_time = time.time()
        request_id = ensure_request_id(request_id)

        # Warnings collection
        warnings: list[dict[str, Any]] = []

        # If catalog_dir is default and search_all_databases is False, resolve to unified storage
        if catalog_dir == "./data_catalogue" and not search_all_databases:
            try:
                resolved_path = resolve_catalog_path(
                    database=database,
                    account_scope=False,
                )
                catalog_dir = str(resolved_path)
            except Exception:
                # If resolution fails, use default path (backward compatibility)
                pass

        logger.info(
            "search_catalog_started",
            extra={
                "catalog_dir": catalog_dir,
                "database": database,
                "schema": schema,
                "search_all_databases": search_all_databases,
                "request_id": request_id,
            },
        )

        # Validate catalog directory path (prevent path traversal)
        # Skip validation for unified storage paths (absolute paths from resolve_catalog_path)
        # Only validate relative paths
        catalog_path = Path(catalog_dir)
        if not catalog_path.is_absolute():
            try:
                validated_catalog_dir = validate_safe_path(
                    catalog_dir,
                    reject_parent_dirs=True,
                )
                catalog_dir = str(validated_catalog_dir)
            except MCPValidationError:
                raise  # Re-raise validation errors
            except Exception as e:
                raise MCPValidationError(
                    f"Invalid catalog directory path: {e!s}",
                    validation_errors=[f"Path validation failed: {catalog_dir}"],
                    hints=[
                        "Use a relative path within the current directory",
                        "Do not use '..' in paths",
                    ],
                ) from e

        # Support searching across multiple databases in unified storage
        if search_all_databases and Path(catalog_dir).name == "data_catalogue":
            catalog_root = resolve_catalog_root()
            all_results = []
            total_matches = 0
            searched_dirs = []

            # Timing: Search operation
            search_start = time.time()

            # Search all database folders in unified storage
            if catalog_root.exists():
                for db_dir in catalog_root.iterdir():
                    if db_dir.is_dir() and db_dir.name != "account":
                        try:
                            index = CatalogIndex(str(db_dir))
                            results, matches, _ = index.search(
                                object_types=_normalize_types(object_types),
                                database=database,  # Filter by database if specified
                                schema=schema,
                                name_contains=name_contains,
                                column_contains=column_contains,
                                limit=limit,
                            )
                            all_results.extend(results)
                            total_matches += matches
                            searched_dirs.append(str(db_dir))
                        except FileNotFoundError:
                            # Skip directories without catalog files
                            continue

            # Sort and limit results
            all_results = all_results[:limit]
            search_duration = (time.time() - search_start) * 1000

            # Calculate total duration
            total_duration = (time.time() - start_time) * 1000

            logger.info(
                "search_catalog_completed",
                extra={
                    "catalog_dir": f"{catalog_root} (all databases)",
                    "searched_dirs": searched_dirs,
                    "total_matches": total_matches,
                    "results_count": len(all_results),
                    "request_id": request_id,
                    "search_duration_ms": search_duration,
                    "total_duration_ms": total_duration,
                },
            )

            return {
                "status": "success",
                "request_id": request_id,
                "warnings": warnings,
                "timing": {
                    "search_duration_ms": round(search_duration, 2),
                    "total_duration_ms": round(total_duration, 2),
                },
                "catalog_dir": str(catalog_root),
                "searched_databases": searched_dirs,
                "metadata": {},
                "total_matches": total_matches,
                "limit": limit,
                "results": [
                    {
                        "object_type": obj.object_type,
                        "database": obj.database,
                        "schema": obj.schema,
                        "name": obj.name,
                        "comment": obj.comment,
                        "columns": obj.columns,
                        "raw": obj.raw,
                    }
                    for obj in all_results
                ],
            }

        try:
            # Timing: Search operation
            search_start = time.time()
            index = CatalogIndex(catalog_dir)
            results, total_matches, metadata = index.search(
                object_types=_normalize_types(object_types),
                database=database,
                schema=schema,
                name_contains=name_contains,
                column_contains=column_contains,
                limit=max(1, limit),
            )
            search_duration = (time.time() - search_start) * 1000

            # Calculate total duration
            total_duration = (time.time() - start_time) * 1000

            logger.info(
                "search_catalog_completed",
                extra={
                    "catalog_dir": catalog_dir,
                    "total_matches": total_matches,
                    "results_count": len(results),
                    "request_id": request_id,
                    "search_duration_ms": search_duration,
                    "total_duration_ms": total_duration,
                },
            )

            return {
                "status": "success",
                "request_id": request_id,
                "warnings": warnings,
                "timing": {
                    "search_duration_ms": round(search_duration, 2),
                    "total_duration_ms": round(total_duration, 2),
                },
                "catalog_dir": catalog_dir,
                "metadata": metadata,
                "total_matches": total_matches,
                "limit": limit,
                "results": [
                    {
                        "object_type": obj.object_type,
                        "database": obj.database,
                        "schema": obj.schema,
                        "name": obj.name,
                        "comment": obj.comment,
                        "columns": obj.columns,
                        "raw": obj.raw,
                    }
                    for obj in results
                ],
            }
        except FileNotFoundError as exc:
            logger.warning(
                "search_catalog_not_found",
                extra={
                    "catalog_dir": catalog_dir,
                    "error": str(exc),
                    "request_id": request_id,
                },
            )

            raise MCPSelectorError(
                f"Catalog directory not found: {exc!s}",
                selector=catalog_dir,
                error="not_found",
                hints=[
                    f"Verify catalog_dir exists: {catalog_dir}",
                    "Run build_catalog first to create catalog artifacts",
                ],
            ) from exc

    def get_parameter_schema(self) -> dict[str, Any]:
        return {
            "title": "Search Catalog Parameters",
            "type": "object",
            "additionalProperties": False,
            "properties": {
                "catalog_dir": string_schema(
                    "Directory containing catalog artifacts (catalog.json or catalog.jsonl). "
                    "Defaults to './data_catalogue' which resolves to unified storage "
                    "at ~/.igloo_mcp/catalogs/{database}/. Specify a custom path to override.",
                    title="Catalog Directory",
                    default="./data_catalogue",
                    examples=["./data_catalogue", "./artifacts/catalog"],
                ),
                "search_all_databases": {
                    "type": "boolean",
                    "description": (
                        "If True and catalog_dir is default, search across all database catalogs in unified storage."
                    ),
                    "title": "Search All Databases",
                    "default": False,
                },
                "object_types": {
                    "type": "array",
                    "title": "Object Types",
                    "items": {
                        "type": "string",
                        "enum": list(_SUPPORTED_OBJECT_TYPES),
                    },
                    "description": "Optional list of object types to include (e.g., table, view, function).",
                },
                "database": snowflake_identifier_schema(
                    "Filter results to a specific database.",
                    title="Database",
                    examples=["ANALYTICS", "PROD_DB"],
                ),
                "schema": snowflake_identifier_schema(
                    "Filter results to a specific schema.",
                    title="Schema",
                    examples=["PUBLIC", "REPORTING"],
                ),
                "name_contains": string_schema(
                    "Substring match on object name (case-insensitive).",
                    title="Name Contains",
                    examples=["customer", "sales"],
                ),
                "column_contains": string_schema(
                    "Substring match on column name (case-insensitive).",
                    title="Column Contains",
                    examples=["revenue", "id"],
                ),
                "limit": integer_schema(
                    "Maximum number of results to return.",
                    minimum=1,
                    maximum=500,
                    default=20,
                    examples=[10, 20, 50],
                ),
                "request_id": {
                    "type": "string",
                    "description": "Optional request correlation ID for tracing (auto-generated if not provided)",
                },
            },
        }


_SUPPORTED_OBJECT_TYPES: Sequence[str] = (
    "database",
    "schema",
    "table",
    "view",
    "materialized_view",
    "dynamic_table",
    "task",
    "function",
    "procedure",
)


def _normalize_types(types: Iterable[str] | None) -> list[str] | None:
    if not types:
        return None
    normalized: list[str] = []
    for value in types:
        if not value:
            continue
        lower = value.lower()
        if lower in _SUPPORTED_OBJECT_TYPES:
            normalized.append(lower)
    return normalized or None
