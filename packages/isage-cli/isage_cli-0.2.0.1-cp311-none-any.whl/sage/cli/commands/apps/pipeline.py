#!/usr/bin/env python3
"""Interactive pipeline builder powered by LLMs."""

from __future__ import annotations

import importlib
import json
import os
import re
import textwrap
from collections.abc import Sequence
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import typer
import yaml  # type: ignore[import-untyped]
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table

from sage.cli import templates
from sage.cli.commands.apps.pipeline_domain import load_custom_contexts, load_domain_contexts
from sage.cli.commands.apps.pipeline_knowledge import (
    PipelineKnowledgeBase,
    build_query_payload,
    get_default_knowledge_base,
)
from sage.cli.core.exceptions import CLIException
from sage.cli.templates import pipeline_blueprints as blueprints
from sage.common.config.output_paths import get_sage_paths
from sage.kernel.api.base_environment import BaseEnvironment
from sage.kernel.api.local_environment import LocalEnvironment

try:  # pragma: no cover - optional dependency at runtime only
    from sage.llm import UnifiedInferenceClient

    OPENAI_AVAILABLE = True
    OPENAI_IMPORT_ERROR: Exception | None = None
except Exception as exc:  # pragma: no cover - runtime check
    OPENAI_AVAILABLE = False
    OPENAI_IMPORT_ERROR = exc
    UnifiedInferenceClient = object  # type: ignore


DEFAULT_BACKEND = os.getenv("SAGE_PIPELINE_BUILDER_BACKEND", "openai")
DEFAULT_MODEL = os.getenv("SAGE_PIPELINE_BUILDER_MODEL") or os.getenv(
    "TEMP_GENERATOR_MODEL",
    "qwen-turbo-2025-02-11",
)
DEFAULT_BASE_URL = os.getenv("SAGE_PIPELINE_BUILDER_BASE_URL") or os.getenv(
    "TEMP_GENERATOR_BASE_URL"
)
DEFAULT_API_KEY = os.getenv("SAGE_PIPELINE_BUILDER_API_KEY") or os.getenv("TEMP_GENERATOR_API_KEY")


SYSTEM_PROMPT = textwrap.dedent(
    """
    You are SAGE Pipeline Builder, an expert in configuring Streaming-Augmented Generative Execution pipelines.
    Produce a *single* JSON object that can be saved as a YAML config for the SAGE CLI.

    Required JSON structure:
    {
      "pipeline": {
        "name": str,
        "description": str,
        "version": "1.0.0",
        "type": "local" | "remote"
      },
      "source": { ... },
      "stages": [
        {
          "id": str,
          "kind": "map" | "batch" | "service",
          "class": str,  # Python class path within SAGE
          "params": { ... },
          "summary": str
        }
      ],
      "sink": { ... },
      "services": [
        {
          "name": str,
          "class": str,
          "params": { ... }
        }
      ],
      "monitors": [ ... ],
      "notes": [str]
    }

    Rules:
    - Populate concrete SAGE component class paths (e.g. "sage.libs.rag.retriever.Wiki18FAISSRetriever").
    - When unsure, choose sensible defaults that work out-of-the-box.
    - Always include at least one stage and a sink.
    - Ensure identifiers are slugified (lowercase, hyphen-separated).
    - Parameters must be JSON-serializable and concise.
    - Never wrap the JSON in markdown fences or add commentary outside the JSON.
    """
).strip()


GRAPH_SYSTEM_PROMPT = textwrap.dedent(
    """
    You are SAGE Pipeline Architect, an expert agent workflow designer.
    Create expressive multi-stage graph pipelines that can include agents, tools, services,
    and messaging channels. Support branching, multi-agent orchestration, shared memories,
    and control flows whenever helpful.

    Produce a single JSON object with the structure:
    {
        "pipeline": {
            "name": str,
            "description": str,
            "version": "1.0.0",
            "type": "local" | "remote"
        },
        "graph": {
            "nodes": [
                {
                    "id": str,
                    "title": str,
                    "kind": "source" | "agent" | "tool" | "service" | "sink" | "router",
                    "class": str,
                    "params": { ... },
                    "inputs": [str],  # upstream node IDs
                    "outputs": [str],
                    "metadata": { ... }
                }
            ],
            "channels": [
                {
                    "id": str,
                    "type": "memory" | "event" | "queue" | "stream",
                    "description": str,
                    "participants": [str]
                }
            ]
        },
        "agents": [
            {
                "id": str,
                "role": str,
                "goals": [str],
                "tools": [str],
                "memory": {
                    "type": str,
                    "config": { ... }
                }
            }
        ],
        "services": [ ... ],
        "monitors": [ ... ],
        "notes": [str]
    }

    Guidelines:
    - Encourage the use of multiple agents when the task benefits from specialization.
    - Use inputs/outputs to express the DAG; omit when not relevant.
    - Fill params with concrete configuration that can run on SAGE where possible.
    - Include channels when agents need to coordinate or share state.
    - Ensure node IDs are unique kebab-case strings. Outputs list may be omitted when obvious.
    - Prefer referencing existing SAGE components (e.g. sage.libs.rag.*, examples.*) but
      custom classes are allowed if necessaryâ€”describe them in notes.
    """
).strip()


console = Console()
app = typer.Typer(help="ğŸ§  ä½¿ç”¨å¤§æ¨¡å‹äº¤äº’å¼åˆ›å»º SAGE pipeline é…ç½®")


def _render_blueprint_panel(
    matches: Sequence[tuple[blueprints.PipelineBlueprint, float]],
) -> Panel:
    lines: list[str] = []
    for index, (blueprint, score) in enumerate(matches, start=1):
        lines.append(
            textwrap.dedent(
                f"""
                [{index}] {blueprint.title} ({blueprint.id})
                åŒ¹é…åº¦: {score:.2f} | å…³é”®è¯: {", ".join(blueprint.keywords) or "é€šç”¨"}
                åœºæ™¯: {blueprint.description}
                """
            ).strip()
        )
    body = "\n\n".join(lines) or "æš‚æ— å¯ç”¨è“å›¾"
    return Panel(body, title="è“å›¾åº“å€™é€‰", style="magenta")


def _render_template_panel(
    matches: Sequence[templates.TemplateMatch],
) -> Panel:
    lines: list[str] = []
    for index, match in enumerate(matches, start=1):
        template = match.template
        lines.append(
            textwrap.dedent(
                f"""
                [{index}] {template.title} ({template.id})
                åŒ¹é…åº¦: {match.score:.2f} | æ ‡ç­¾: {", ".join(template.tags) or "é€šç”¨"}
                ç¤ºä¾‹: {template.example_path}
                åœºæ™¯: {template.description}
                """
            ).strip()
        )
    body = "\n\n".join(lines) or "æš‚æ— åº”ç”¨æ¨¡æ¿"
    return Panel(body, title="åº”ç”¨æ¨¡æ¿æ¨è", style="green")


def _blueprint_contexts(
    matches: Sequence[tuple[blueprints.PipelineBlueprint, float]],
) -> tuple[str, ...]:
    return tuple(
        blueprints.render_blueprint_prompt(blueprint, score) for blueprint, score in matches
    )


def _template_contexts(
    matches: Sequence[templates.TemplateMatch],
) -> tuple[str, ...]:
    return tuple(match.template.render_prompt(match.score) for match in matches)


class PipelineBuilderError(RuntimeError):
    """Raised when the builder cannot produce a valid plan."""


def _slugify(value: str) -> str:
    slug = re.sub(r"[^a-zA-Z0-9]+", "-", value.lower()).strip("-")
    return slug or "pipeline"


def _extract_json_object(text: str) -> dict[str, Any]:
    console.print(f"[dim]LLM response (first 500 chars): {text[:500]}[/dim]")  # Debug
    candidate = text.strip()
    if candidate.startswith("```"):
        candidate = re.sub(r"^```(?:json)?", "", candidate, count=1).strip()
        candidate = re.sub(r"```$", "", candidate, count=1).strip()

    try:
        result = json.loads(candidate)
        console.print(f"[dim]Parsed JSON keys: {list(result.keys())}[/dim]")  # Debug
        return result
    except json.JSONDecodeError:
        pass

    brace_match = re.search(r"\{.*\}", candidate, re.DOTALL)
    if brace_match:
        try:
            result = json.loads(brace_match.group())
            console.print(f"[dim]Parsed JSON keys (from brace match): {list(result.keys())}[/dim]")
            return result
        except json.JSONDecodeError as exc:  # pragma: no cover - defensive
            raise PipelineBuilderError(f"LLM returned invalid JSON: {exc}") from exc

    raise PipelineBuilderError("æ— æ³•è§£æå¤§æ¨¡å‹è¿”å›çš„ JSONï¼Œè¯·é‡è¯•æˆ–è°ƒæ•´æè¿°ã€‚")


def _validate_plan(plan: dict[str, Any]) -> None:
    if "pipeline" not in plan or "stages" not in plan or "sink" not in plan:
        raise PipelineBuilderError(
            "ç”Ÿæˆçš„é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ (pipeline/stages/sink)ã€‚è¯·å°è¯•æä¾›æ›´å¤šéœ€æ±‚ç»†èŠ‚ã€‚"
        )

    if not isinstance(plan["stages"], list) or not plan["stages"]:
        raise PipelineBuilderError("stages å­—æ®µå¿…é¡»æ˜¯éç©ºåˆ—è¡¨ã€‚")

    if not isinstance(plan.get("sink"), dict) or not plan["sink"].get("class"):
        raise PipelineBuilderError("sink å­—æ®µå¿…é¡»æ˜¯åŒ…å« class çš„å¯¹è±¡ã€‚")

    source = plan.get("source")
    if source is not None and not isinstance(source, dict):
        raise PipelineBuilderError("source å­—æ®µå¿…é¡»æ˜¯å¯¹è±¡ã€‚")

    pipeline_block = plan["pipeline"]
    if "name" not in pipeline_block:
        pipeline_block["name"] = "untitled-pipeline"
    if "type" not in pipeline_block:
        pipeline_block["type"] = "local"
    if "version" not in pipeline_block:
        pipeline_block["version"] = "1.0.0"

    for stage in plan["stages"]:
        if not isinstance(stage, dict):
            raise PipelineBuilderError("stages åˆ—è¡¨ä¸­çš„å…ƒç´ å¿…é¡»æ˜¯å¯¹è±¡ã€‚")
        stage_id = stage.get("id")
        stage["id"] = _slugify(str(stage_id)) if stage_id else _slugify(stage.get("class", "stage"))
        if not stage.get("class"):
            raise PipelineBuilderError("æ¯ä¸ª stage å¿…é¡»åŒ…å« class å­—æ®µã€‚")
        params = stage.get("params", {})
        if params is None:
            stage["params"] = {}
        elif not isinstance(params, dict):
            raise PipelineBuilderError("stage çš„ params å¿…é¡»æ˜¯å¯¹è±¡ (key/value)ã€‚")


def _validate_graph_plan(plan: dict[str, Any]) -> None:
    pipeline_meta = plan.get("pipeline")
    graph = plan.get("graph")

    if not isinstance(pipeline_meta, dict):
        raise PipelineBuilderError("graph é…ç½®ç¼ºå°‘ pipeline ä¿¡æ¯ã€‚")
    if not isinstance(graph, dict):
        raise PipelineBuilderError("graph é…ç½®ç¼ºå°‘ graph èŠ‚ç‚¹å®šä¹‰ã€‚")

    nodes = graph.get("nodes")
    if not isinstance(nodes, list) or not nodes:
        raise PipelineBuilderError("graph.nodes å¿…é¡»æ˜¯éç©ºåˆ—è¡¨ã€‚")

    seen_ids: set[str] = set()
    for node in nodes:
        if not isinstance(node, dict):
            raise PipelineBuilderError("graph.nodes ä¸­çš„å…ƒç´ å¿…é¡»æ˜¯å¯¹è±¡ã€‚")
        node_id = node.get("id")
        if not node_id:
            raise PipelineBuilderError("æ¯ä¸ªèŠ‚ç‚¹éƒ½éœ€è¦ idã€‚")
        slugified = _slugify(str(node_id))
        node["id"] = slugified
        if slugified in seen_ids:
            raise PipelineBuilderError(f"èŠ‚ç‚¹ id é‡å¤ : {slugified}")
        seen_ids.add(slugified)

        if not node.get("class"):
            raise PipelineBuilderError(f"èŠ‚ç‚¹ {slugified} ç¼ºå°‘ class å­—æ®µã€‚")

        for key in ("inputs", "outputs"):
            if key in node and node[key] is not None and not isinstance(node[key], list):
                raise PipelineBuilderError(f"èŠ‚ç‚¹ {slugified} çš„ {key} å­—æ®µå¿…é¡»æ˜¯åˆ—è¡¨ã€‚")

    channels = graph.get("channels") or []
    if not isinstance(channels, list):
        raise PipelineBuilderError("graph.channels å¿…é¡»æ˜¯åˆ—è¡¨ã€‚")
    for channel in channels:
        if not isinstance(channel, dict):
            raise PipelineBuilderError("graph.channels ä¸­çš„å…ƒç´ å¿…é¡»æ˜¯å¯¹è±¡ã€‚")
        if not channel.get("id"):
            raise PipelineBuilderError("æ¯ä¸ª channel éœ€è¦ idã€‚")

    for block_name in ("agents", "services", "monitors"):
        if (
            block_name in plan
            and plan[block_name] is not None
            and not isinstance(plan[block_name], list)
        ):
            raise PipelineBuilderError(f"{block_name} å­—æ®µå¿…é¡»æ˜¯åˆ—è¡¨ã€‚")

    notes = plan.get("notes")
    if notes is not None and not isinstance(notes, list):
        raise PipelineBuilderError("notes å­—æ®µå¿…é¡»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ã€‚")


def _expand_params(value: Any) -> Any:
    if isinstance(value, dict):
        return {key: _expand_params(val) for key, val in value.items()}
    if isinstance(value, list):
        return [_expand_params(item) for item in value]
    if isinstance(value, str):
        return os.path.expandvars(os.path.expanduser(value))
    return value


def _import_attr(path: str) -> Any:
    try:
        module_name, attr_name = path.rsplit(".", 1)
    except ValueError as exc:  # pragma: no cover - defensive
        raise CLIException(f"Invalid class path: {path}") from exc

    try:
        module = importlib.import_module(module_name)
    except ImportError as exc:
        raise CLIException(f"æ— æ³•å¯¼å…¥æ¨¡å— {module_name}: {exc}") from exc

    try:
        return getattr(module, attr_name)
    except AttributeError as exc:
        raise CLIException(f"æ¨¡å— {module_name} ä¸åŒ…å« {attr_name}") from exc


def _ensure_pipeline_dict(data: dict[str, Any]) -> dict[str, Any]:
    if not isinstance(data, dict):
        raise CLIException("Pipeline é…ç½®å¿…é¡»æ˜¯å­—å…¸ç»“æ„ã€‚")
    return data


def _create_environment(
    plan: dict[str, Any], host: str | None, port: int | None
) -> BaseEnvironment:
    pipeline_meta = plan.get("pipeline") or {}
    pipeline_name = pipeline_meta.get("name", "sage-pipeline")
    env_settings = plan.get("environment") or {}
    env_config = _expand_params(env_settings.get("config") or {})

    env_type = (pipeline_meta.get("type") or "local").lower()
    if env_type == "remote":
        from sage.kernel.api.remote_environment import RemoteEnvironment  # import lazily

        resolved_host = host or env_settings.get("host") or "127.0.0.1"
        resolved_port = port or env_settings.get("port") or 19001
        return RemoteEnvironment(
            name=pipeline_name,
            config=env_config,
            host=resolved_host,
            port=int(resolved_port),
        )

    return LocalEnvironment(name=pipeline_name, config=env_config)


def _register_services(env: BaseEnvironment, services: list[dict[str, Any]]) -> None:
    for service in services or []:
        name = service.get("name")
        class_path = service.get("class")
        if not name or not class_path:
            raise CLIException("æ¯ä¸ª service éœ€è¦ name å’Œ class å­—æ®µã€‚")

        service_class = _import_attr(class_path)
        args = service.get("args") or []
        if not isinstance(args, list):
            raise CLIException(f"Service {name} çš„ args å¿…é¡»æ˜¯æ•°ç»„ã€‚")
        params = _expand_params(service.get("params") or {})
        env.register_service(name, service_class, *args, **params)


def _apply_source(env: BaseEnvironment, source: dict[str, Any]):
    if not source:
        raise CLIException("Pipeline ç¼ºå°‘ source å®šä¹‰ã€‚")

    class_path = source.get("class")
    if not class_path:
        raise CLIException("source éœ€è¦æä¾› class å­—æ®µã€‚")

    function_class = _import_attr(class_path)
    args = source.get("args") or []
    if not isinstance(args, list):
        raise CLIException("source çš„ args å¿…é¡»æ˜¯æ•°ç»„ã€‚")
    params = _expand_params(source.get("params") or {})
    kind = (source.get("kind") or "batch").lower()

    if kind in {"batch", "collection"}:
        return env.from_batch(function_class, *args, **params)
    if kind in {"source", "stream"}:
        return env.from_source(function_class, *args, **params)
    if kind == "future":
        future_name = params.get("name") or source.get("id") or "future"
        return env.from_future(future_name)

    # Default to from_source for unknown kinds
    return env.from_source(function_class, *args, **params)


def _apply_stage(stream, stage: dict[str, Any]):
    class_path = stage.get("class")
    if not class_path:
        raise CLIException("stage ç¼ºå°‘ class å­—æ®µã€‚")

    function_class = _import_attr(class_path)
    args = stage.get("args") or []
    if not isinstance(args, list):
        raise CLIException(f"stage {stage.get('id')} çš„ args å¿…é¡»æ˜¯æ•°ç»„ã€‚")
    params = _expand_params(stage.get("params") or {})
    kind = (stage.get("kind") or "map").lower()

    if kind in {"map", "service", "batch"}:
        return stream.map(function_class, *args, **params)
    if kind == "flatmap":
        return stream.flatmap(function_class, *args, **params)
    if kind == "filter":
        return stream.filter(function_class, *args, **params)
    if kind == "keyby":
        strategy = params.pop("strategy", "hash")
        return stream.keyby(function_class, strategy=strategy, *args, **params)
    if kind == "sink":
        stream.sink(function_class, *args, **params)
        return stream

    console.print(f"[yellow]âš ï¸ æœªçŸ¥çš„ stage ç±»å‹ {kind}ï¼Œé»˜è®¤ä½¿ç”¨ mapã€‚[/yellow]")
    return stream.map(function_class, *args, **params)


def _apply_sink(stream, sink: dict[str, Any]):
    if not sink:
        raise CLIException("Pipeline ç¼ºå°‘ sink å®šä¹‰ã€‚")

    class_path = sink.get("class")
    if not class_path:
        raise CLIException("sink éœ€è¦æä¾› class å­—æ®µã€‚")

    function_class = _import_attr(class_path)
    args = sink.get("args") or []
    if not isinstance(args, list):
        raise CLIException("sink çš„ args å¿…é¡»æ˜¯æ•°ç»„ã€‚")
    params = _expand_params(sink.get("params") or {})
    stream.sink(function_class, *args, **params)


def _load_pipeline_file(path: Path) -> dict[str, Any]:
    try:
        content = path.read_text(encoding="utf-8")
    except FileNotFoundError as exc:
        raise CLIException(f"æ‰¾ä¸åˆ° pipeline é…ç½®æ–‡ä»¶: {path}") from exc
    except OSError as exc:
        raise CLIException(f"è¯»å– pipeline æ–‡ä»¶å¤±è´¥: {exc}") from exc

    try:
        data = yaml.safe_load(content) or {}
    except yaml.YAMLError as exc:
        raise CLIException(f"è§£æ YAML å¤±è´¥: {exc}") from exc

    return _ensure_pipeline_dict(data)


@dataclass
class BuilderConfig:
    backend: str
    model: str | None
    base_url: str | None
    api_key: str | None
    domain_contexts: tuple[str, ...] = ()
    knowledge_base: PipelineKnowledgeBase | None = None
    knowledge_top_k: int = 6
    show_knowledge: bool = False


@dataclass
class GraphBuilderConfig:
    backend: str
    model: str
    base_url: str | None
    api_key: str | None
    domain_contexts: tuple[str, ...] = ()
    knowledge_base: PipelineKnowledgeBase | None = None
    knowledge_top_k: int = 6
    show_knowledge: bool = False


class PipelinePlanGenerator:
    def __init__(self, config: BuilderConfig) -> None:
        self.config = config
        self._client: Any | None = None
        self._last_knowledge_contexts: tuple[str, ...] = ()
        self._blueprint_matches: tuple[tuple[blueprints.PipelineBlueprint, float], ...] = ()
        self._last_blueprint_contexts: tuple[str, ...] = ()
        self._template_matches: tuple[templates.TemplateMatch, ...] = ()
        self._last_template_contexts: tuple[str, ...] = ()

        if self.config.backend != "mock":
            if not OPENAI_AVAILABLE:
                message = f"æœªèƒ½å¯¼å…¥ UnifiedInferenceClientï¼š{OPENAI_IMPORT_ERROR}"
                raise PipelineBuilderError(message)
            # ä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»º UnifiedInferenceClient
            self._client = UnifiedInferenceClient.create(
                default_llm_model=self.config.model,
            )

    def generate(
        self,
        requirements: dict[str, Any],
        previous_plan: dict[str, Any] | None = None,
        feedback: str | None = None,
    ) -> dict[str, Any]:
        knowledge_contexts: tuple[str, ...] = ()
        if self.config.knowledge_base is not None:
            try:
                query_payload = build_query_payload(requirements, previous_plan, feedback)
                results = self.config.knowledge_base.search(
                    query_payload,
                    top_k=self.config.knowledge_top_k,
                )
                knowledge_contexts = tuple(item.text for item in results)
                self._last_knowledge_contexts = knowledge_contexts
            except Exception as exc:  # pragma: no cover - defensive
                console.print(f"[yellow]æ£€ç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™ï¼Œå°†ç»§ç»­ä½¿ç”¨å†…å»ºä¸Šä¸‹æ–‡: {exc}[/yellow]")
                self._last_knowledge_contexts = ()

        if self.config.show_knowledge and knowledge_contexts:
            console.print(
                Panel(
                    "\n\n".join(knowledge_contexts),
                    title="çŸ¥è¯†åº“æ£€ç´¢ç»“æœ",
                    style="blue",
                )
            )

        self._template_matches = tuple(templates.match_templates(requirements, top_k=3))
        self._last_template_contexts = _template_contexts(self._template_matches)
        if self._template_matches and self.config.show_knowledge:
            console.print(_render_template_panel(self._template_matches))

        if self.config.backend == "mock":
            self._blueprint_matches = tuple(blueprints.match_blueprints(requirements))
            self._last_blueprint_contexts = _blueprint_contexts(self._blueprint_matches)
            if self._blueprint_matches and self.config.show_knowledge:
                console.print(_render_blueprint_panel(self._blueprint_matches))
            return self._blueprint_plan(requirements, previous_plan, feedback)

        self._blueprint_matches = tuple(blueprints.match_blueprints(requirements))
        if self._blueprint_matches and self.config.show_knowledge:
            console.print(_render_blueprint_panel(self._blueprint_matches))
        self._last_blueprint_contexts = _blueprint_contexts(self._blueprint_matches)

        assert self._client is not None  # for type checker
        user_prompt = self._build_prompt(
            requirements,
            previous_plan,
            feedback,
            knowledge_contexts,
            self._last_template_contexts,
            self._last_blueprint_contexts,
        )
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        console.print("ğŸ¤– æ­£åœ¨è¯·æ±‚å¤§æ¨¡å‹ç”Ÿæˆé…ç½®...", style="cyan")
        response = self._client.chat(messages, max_tokens=1200, temperature=0.2)
        plan = _extract_json_object(response)
        _validate_plan(plan)
        return plan

    def _build_prompt(
        self,
        requirements: dict[str, Any],
        previous_plan: dict[str, Any] | None,
        feedback: str | None,
        knowledge_contexts: tuple[str, ...],
        template_contexts: tuple[str, ...],
        blueprint_contexts: tuple[str, ...],
    ) -> str:
        blocks = [
            "è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚ç”Ÿæˆç¬¦åˆ SAGE æ¡†æ¶çš„ pipeline é…ç½® JSONï¼š",
            json.dumps(requirements, ensure_ascii=False, indent=2),
        ]

        if template_contexts:
            blocks.append("ä»¥ä¸‹åº”ç”¨æ¨¡æ¿ä»…ä½œçµæ„Ÿå‚è€ƒï¼Œè¯·ç»“åˆéœ€æ±‚è‡ªè¡Œè®¾è®¡ï¼š")
            for idx, snippet in enumerate(template_contexts, start=1):
                blocks.append(f"æ¨¡æ¿[{idx}]:\n{snippet.strip()}")

        if blueprint_contexts:
            blocks.append("ä»¥ä¸‹è“å›¾å¯ç›´æ¥å¤ç”¨æˆ–åœ¨æ­¤åŸºç¡€ä¸Šæ‰©å±•ï¼š")
            for idx, snippet in enumerate(blueprint_contexts, start=1):
                blocks.append(f"è“å›¾[{idx}]:\n{snippet.strip()}")

        if knowledge_contexts:
            blocks.append("ä»¥ä¸‹æ˜¯ä» SAGE çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„å‚è€ƒä¿¡æ¯ï¼š")
            for idx, snippet in enumerate(knowledge_contexts, start=1):
                blocks.append(f"çŸ¥è¯†[{idx}]:\n{snippet.strip()}")

        if self.config.domain_contexts:
            blocks.append("ä»¥ä¸‹æ˜¯ä¸ SAGE ç®¡é“æ„å»ºç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼š")
            for idx, snippet in enumerate(self.config.domain_contexts, start=1):
                blocks.append(f"å‚è€ƒ[{idx}]:\n{snippet.strip()}")

        if previous_plan:
            blocks.append("è¿™æ˜¯ä¸Šä¸€ç‰ˆé…ç½®ä¾›å‚è€ƒï¼š")
            blocks.append(json.dumps(previous_plan, ensure_ascii=False, indent=2))

        if feedback:
            blocks.append("è¯·éµå¾ªä»¥ä¸‹ä¿®æ”¹æ„è§æ›´æ–°é…ç½®ï¼š")
            blocks.append(feedback.strip())

        blocks.append("ä¸¥æ ¼è¾“å‡ºå•ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å« markdownã€æ³¨é‡Šæˆ–å¤šä½™æ–‡å­—ã€‚")
        return "\n\n".join(blocks)

    def _blueprint_plan(
        self,
        requirements: dict[str, Any],
        previous_plan: dict[str, Any] | None,
        feedback: str | None,
    ) -> dict[str, Any]:
        blueprint = (
            self._blueprint_matches[0][0]
            if self._blueprint_matches
            else blueprints.DEFAULT_BLUEPRINT
        )
        return blueprints.build_pipeline_plan(blueprint, requirements, feedback)


class GraphPlanGenerator:
    def __init__(self, config: GraphBuilderConfig) -> None:
        self.config = config
        self._client: Any | None = None
        self._last_knowledge_contexts: tuple[str, ...] = ()
        self._blueprint_matches: tuple[tuple[blueprints.PipelineBlueprint, float], ...] = ()
        self._last_blueprint_contexts: tuple[str, ...] = ()

        if self.config.backend != "mock":
            if not OPENAI_AVAILABLE:
                message = f"æœªèƒ½å¯¼å…¥ UnifiedInferenceClientï¼š{OPENAI_IMPORT_ERROR}"
                raise PipelineBuilderError(message)
            # ä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»º UnifiedInferenceClient
            self._client = UnifiedInferenceClient.create(
                default_llm_model=self.config.model,
            )

    def generate(
        self,
        requirements: dict[str, Any],
        previous_plan: dict[str, Any] | None = None,
        feedback: str | None = None,
    ) -> dict[str, Any]:
        knowledge_contexts: tuple[str, ...] = ()
        if self.config.knowledge_base is not None:
            try:
                query_payload = build_query_payload(requirements, previous_plan, feedback)
                results = self.config.knowledge_base.search(
                    query_payload, top_k=self.config.knowledge_top_k
                )
                knowledge_contexts = tuple(item.text for item in results)
                self._last_knowledge_contexts = knowledge_contexts
            except Exception as exc:  # pragma: no cover - defensive
                console.print(f"[yellow]æ£€ç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™ï¼Œå°†ç»§ç»­ä½¿ç”¨é™æ€ä¸Šä¸‹æ–‡: {exc}[/yellow]")
                self._last_knowledge_contexts = ()

        if self.config.show_knowledge and knowledge_contexts:
            console.print(
                Panel(
                    "\n\n".join(knowledge_contexts),
                    title="çŸ¥è¯†åº“æ£€ç´¢ç»“æœ",
                    style="blue",
                )
            )

        self._template_matches = tuple(templates.match_templates(requirements, top_k=4))
        self._last_template_contexts = _template_contexts(self._template_matches)
        if self._template_matches and self.config.show_knowledge:
            console.print(_render_template_panel(self._template_matches))

        if self.config.backend == "mock":
            self._blueprint_matches = tuple(blueprints.match_blueprints(requirements))
            self._last_blueprint_contexts = _blueprint_contexts(self._blueprint_matches)
            if self._blueprint_matches and self.config.show_knowledge:
                console.print(_render_blueprint_panel(self._blueprint_matches))
            return self._blueprint_plan(requirements, previous_plan, feedback)

        self._blueprint_matches = tuple(blueprints.match_blueprints(requirements))
        if self._blueprint_matches and self.config.show_knowledge:
            console.print(_render_blueprint_panel(self._blueprint_matches))
        self._last_blueprint_contexts = _blueprint_contexts(self._blueprint_matches)

        assert self._client is not None
        user_prompt = self._build_prompt(
            requirements,
            previous_plan,
            feedback,
            knowledge_contexts,
            self._last_template_contexts,
            self._last_blueprint_contexts,
        )
        messages = [
            {"role": "system", "content": GRAPH_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        console.print("ğŸ¤– æ­£åœ¨è¯·æ±‚å¤§æ¨¡å‹è®¾è®¡å›¾è°±...", style="cyan")
        response = self._client.chat(messages, max_tokens=1600, temperature=0.35)
        plan = _extract_json_object(response)
        return plan

    def _build_prompt(
        self,
        requirements: dict[str, Any],
        previous_plan: dict[str, Any] | None,
        feedback: str | None,
        knowledge_contexts: tuple[str, ...],
        template_contexts: tuple[str, ...],
        blueprint_contexts: tuple[str, ...],
    ) -> str:
        blocks: list[str] = [
            "è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚è®¾è®¡ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ SAGE pipeline å›¾è°±ï¼š",
            json.dumps(requirements, ensure_ascii=False, indent=2),
        ]

        if template_contexts:
            blocks.append("ä»¥ä¸‹åº”ç”¨æ¨¡æ¿å¯ä½œä¸ºå‚è€ƒçµæ„Ÿï¼Œè¯·ä¸»åŠ¨è§„åˆ’åˆé€‚çš„å¤šæ™ºèƒ½ä½“ç»“æ„ï¼š")
            for idx, snippet in enumerate(template_contexts, start=1):
                blocks.append(f"æ¨¡æ¿[{idx}]:\n{snippet.strip()}")

        if blueprint_contexts:
            blocks.append("ä»¥ä¸‹è“å›¾å¯ä½œä¸ºèµ·ç‚¹è¿›è¡Œæ‰©å±•ï¼š")
            for idx, snippet in enumerate(blueprint_contexts, start=1):
                blocks.append(f"è“å›¾[{idx}]:\n{snippet.strip()}")

        if knowledge_contexts:
            blocks.append("ä»¥ä¸‹æ˜¯ä» SAGE çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„å‚è€ƒä¿¡æ¯ï¼š")
            for idx, snippet in enumerate(knowledge_contexts, start=1):
                blocks.append(f"çŸ¥è¯†[{idx}]:\n{snippet.strip()}")

        if self.config.domain_contexts:
            blocks.append("ä»¥ä¸‹æ˜¯ä¸ SAGE ç»„ä»¶ç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼š")
            for idx, snippet in enumerate(self.config.domain_contexts, start=1):
                blocks.append(f"å‚è€ƒ[{idx}]:\n{snippet.strip()}")

        if previous_plan:
            blocks.append("ä¸Šä¸€ç‰ˆå›¾è°±ç»“æ„ä¾›å‚è€ƒï¼š")
            blocks.append(json.dumps(previous_plan, ensure_ascii=False, indent=2))

        if feedback:
            blocks.append("è¯·ä¾æ®ä»¥ä¸‹åé¦ˆè°ƒæ•´å›¾è°±ï¼š")
            blocks.append(feedback.strip())

        blocks.append("ä¸¥æ ¼è¾“å‡ºå•ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å« markdownã€æ³¨é‡Šæˆ–å¤šä½™æ–‡å­—ã€‚")
        return "\n\n".join(blocks)

    def _blueprint_plan(
        self,
        requirements: dict[str, Any],
        previous_plan: dict[str, Any] | None,
        feedback: str | None,
    ) -> dict[str, Any]:
        blueprint = (
            self._blueprint_matches[0][0]
            if self._blueprint_matches
            else blueprints.DEFAULT_BLUEPRINT
        )
        return blueprints.build_graph_plan(blueprint, requirements, feedback)


def _render_plan(plan: dict[str, Any]) -> None:
    pipeline_meta = plan.get("pipeline", {})
    console.print(
        Panel.fit(
            f"åç§°: [cyan]{pipeline_meta.get('name', '-')}[/cyan]\n"
            f"æè¿°: {pipeline_meta.get('description', '-')}\n"
            f"ç±»å‹: {pipeline_meta.get('type', '-')}",
            title="Pipeline å…ƒä¿¡æ¯",
            style="green",
        )
    )

    table = Table(title="é˜¶æ®µæ¦‚è§ˆ", show_header=True, header_style="bold blue")
    table.add_column("ID", style="cyan")
    table.add_column("ç±»å‹")
    table.add_column("ç±»è·¯å¾„")
    table.add_column("æ‘˜è¦")

    for stage in plan.get("stages", []):
        table.add_row(
            stage.get("id", "-"),
            stage.get("kind", "-"),
            stage.get("class", "-"),
            stage.get("summary", ""),
        )
    console.print(table)

    notes = plan.get("notes") or []
    if notes:
        console.print(Panel("\n".join(f"â€¢ {note}" for note in notes), title="Notes"))


def _plan_to_yaml(plan: dict[str, Any]) -> str:
    data = dict(plan)
    stages = data.pop("stages", [])

    # Flatten stages into numbered keys for readability in YAML
    data["stages"] = stages
    return yaml.safe_dump(data, allow_unicode=True, sort_keys=False)


def render_pipeline_plan(plan: dict[str, Any]) -> None:
    _render_plan(plan)


def _graph_plan_to_yaml(plan: dict[str, Any]) -> str:
    return yaml.safe_dump(plan, allow_unicode=True, sort_keys=False)


def _save_plan(plan: dict[str, Any], output: Path | None, overwrite: bool) -> Path:
    yaml_text = _plan_to_yaml(plan)
    if output is None:
        default_name = _slugify(plan.get("pipeline", {}).get("name", "pipeline"))
        output_dir = get_sage_paths().output_dir / "pipelines"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"{default_name}.yaml"
    else:
        output_path = Path(output).expanduser().resolve()
        if output_path.is_dir():
            default_name = _slugify(plan.get("pipeline", {}).get("name", "pipeline"))
            output_path = output_path / f"{default_name}.yaml"
        output_path.parent.mkdir(parents=True, exist_ok=True)

    if output_path.exists() and not overwrite:
        raise PipelineBuilderError(f"æ–‡ä»¶å·²å­˜åœ¨: {output_path}ã€‚ä½¿ç”¨ --overwrite å¼ºåˆ¶è¦†ç›–ã€‚")

    output_path.write_text(yaml_text, encoding="utf-8")
    return output_path


def _preview_yaml(yaml_text: str) -> None:
    syntax = Syntax(yaml_text, "yaml", theme="monokai", line_numbers=False)
    console.print(Panel(syntax, title="YAML é¢„è§ˆ"))


def pipeline_plan_to_yaml(plan: dict[str, Any]) -> str:
    return _plan_to_yaml(plan)


def preview_pipeline_plan(plan: dict[str, Any]) -> None:
    yaml_text = _plan_to_yaml(plan)
    _preview_yaml(yaml_text)


def save_pipeline_plan(plan: dict[str, Any], output: Path | None, overwrite: bool) -> Path:
    return _save_plan(plan, output, overwrite)


def execute_pipeline_plan(
    plan: dict[str, Any],
    autostop: bool = True,
    host: str | None = None,
    port: int | None = None,
    *,
    console_override: Console | None = None,
) -> str | None:
    """Apply a pipeline configuration and submit it to the target environment."""

    log_console = console_override or console

    env = _create_environment(plan, host, port)

    services = plan.get("services") or []
    if services:
        log_console.print(f"ğŸ”§ æ³¨å†Œ {len(services)} ä¸ªæœåŠ¡...")
        _register_services(env, services)

    source = plan.get("source")
    if not source:
        raise PipelineBuilderError("Pipeline plan must include a 'source' configuration")
    log_console.print("ğŸš° é…ç½® source")
    stream = _apply_source(env, source)

    stages = plan.get("stages") or []
    for stage in stages:
        stage_id = stage.get("id", "stage")
        log_console.print(f"â¡ï¸  åº”ç”¨é˜¶æ®µ: {stage_id}")
        stream = _apply_stage(stream, stage)

    sink = plan.get("sink")
    if not sink:
        raise PipelineBuilderError("Pipeline plan must include a 'sink' configuration")
    log_console.print("ğŸ›¬ é…ç½®ç»ˆç«¯ sink")
    _apply_sink(stream, sink)

    if plan.get("monitors"):
        log_console.print("[yellow]ğŸ“ˆ å½“å‰ç‰ˆæœ¬æš‚æœªè‡ªåŠ¨é…ç½® monitorsï¼Œéœ€æ‰‹åŠ¨é›†æˆã€‚[/yellow]")

    log_console.print("ğŸš€ æäº¤ pipeline...")
    job_uuid = env.submit(autostop=autostop)  # type: ignore[call-arg]

    if job_uuid:
        log_console.print(f"âœ… Pipeline å·²æäº¤ï¼Œä½œä¸š UUID: [green]{job_uuid}[/green]")
    else:
        log_console.print("âœ… Pipeline å·²æäº¤ã€‚")

    if autostop:
        log_console.print("ğŸ‰ æ‰¹å¤„ç†å®Œæˆå¹¶è‡ªåŠ¨æ¸…ç†ã€‚")
    else:
        log_console.print("â³ Pipeline æ­£åœ¨è¿è¡Œï¼Œå¯ä½¿ç”¨ 'sage job list' æŸ¥çœ‹çŠ¶æ€ã€‚")

    return job_uuid


def _collect_requirements(
    name: str | None,
    goal: str | None,
    requirements_path: Path | None,
    interactive: bool,
) -> dict[str, Any]:
    requirements: dict[str, Any] = {}

    if requirements_path:
        path = Path(requirements_path).expanduser().resolve()
        if not path.exists():
            raise PipelineBuilderError(f"æ‰¾ä¸åˆ°éœ€æ±‚æ–‡ä»¶: {path}")
        requirements = json.loads(path.read_text(encoding="utf-8"))

    if name:
        requirements["name"] = name
    if goal:
        requirements["goal"] = goal

    if not interactive:
        missing = [key for key in ("name", "goal") if key not in requirements]
        if missing:
            raise PipelineBuilderError(f"éäº¤äº’æ¨¡å¼ä¸‹å¿…é¡»æä¾›: {', '.join(missing)}")
        return requirements

    if "name" not in requirements:
        requirements["name"] = typer.prompt("Pipeline åç§°", default="My Pipeline")
    if "goal" not in requirements:
        requirements["goal"] = typer.prompt("ä¸»è¦ç›®æ ‡", default="æ„å»ºä¸€ä¸ªé—®ç­”å‹ RAG pipeline")

    if "data_sources" not in requirements:
        requirements["data_sources"] = typer.prompt("æ•°æ®æ¥æº (å¯ç•™ç©º)", default="æ–‡æ¡£çŸ¥è¯†åº“")
    if "latency_budget" not in requirements:
        requirements["latency_budget"] = typer.prompt(
            "å»¶è¿Ÿ/ååéœ€æ±‚ (å¯ç•™ç©º)", default="å®æ—¶ä½“éªŒä¼˜å…ˆ"
        )
    if "constraints" not in requirements:
        requirements["constraints"] = typer.prompt("ç‰¹æ®Šçº¦æŸ (å¯ç•™ç©º)", default="")

    return requirements


@app.command("build")
def build_pipeline(  # noqa: D401 - Typer handles CLI docs
    name: str | None = typer.Option(None, help="Pipeline åç§°"),
    goal: str | None = typer.Option(None, help="Pipeline ç›®æ ‡æè¿°"),
    backend: str = typer.Option(
        DEFAULT_BACKEND,
        help="LLM åç«¯ (openai/compatible/mock)",
    ),
    model: str | None = typer.Option(None, help="LLM æ¨¡å‹åç§°"),
    base_url: str | None = typer.Option(None, help="LLM Base URL"),
    api_key: str | None = typer.Option(None, help="LLM API Key"),
    requirements_path: Path | None = typer.Option(
        None,
        exists=False,
        help="éœ€æ±‚ JSON æ–‡ä»¶è·¯å¾„ï¼Œæä¾›å·²æœ‰è¾“å…¥ä»¥è·³è¿‡äº¤äº’",
    ),
    output: Path | None = typer.Option(
        None,
        help="è¾“å‡º YAML æ–‡ä»¶è·¯å¾„ (å¯ä¸ºç›®å½•)",
    ),
    overwrite: bool = typer.Option(False, help="å…è®¸è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶"),
    non_interactive: bool = typer.Option(False, help="éäº¤äº’æ¨¡å¼ (éœ€è¦åŒæ—¶æä¾›åç§°å’Œç›®æ ‡)"),
    context_limit: int = typer.Option(
        4,
        "--context-limit",
        min=0,
        max=12,
        help="æç¤ºä¸­åŒ…å«çš„ç¤ºä¾‹é…ç½®æ•°é‡",
    ),
    context_file: list[Path] = typer.Option(
        [],
        "--context-file",
        "-c",
        help="é¢å¤–ä¸Šä¸‹æ–‡æ–‡ä»¶ (çº¯æ–‡æœ¬)ï¼Œå¯é‡å¤æŒ‡å®š",
        exists=True,
        file_okay=True,
        dir_okay=False,
        resolve_path=True,
    ),
    show_contexts: bool = typer.Option(False, "--show-contexts", help="æ‰“å°ç”¨äºæç¤ºçš„å¤§æ¨¡å‹ä¸Šä¸‹æ–‡"),
    disable_knowledge: bool = typer.Option(
        False,
        "--no-knowledge",
        help="ç¦ç”¨ä»æœ¬åœ°çŸ¥è¯†åº“è‡ªåŠ¨æ£€ç´¢ä¸Šä¸‹æ–‡",
    ),
    knowledge_top_k: int = typer.Option(
        5,
        "--knowledge-top-k",
        min=1,
        max=12,
        help="æ¯æ¬¡æ£€ç´¢è¿”å›çš„çŸ¥è¯†ç‰‡æ®µæ•°é‡",
    ),
    show_knowledge: bool = typer.Option(
        False,
        "--show-knowledge",
        help="æ‰“å°çŸ¥è¯†åº“æ£€ç´¢ç»“æœ",
    ),
    embedding_method: str | None = typer.Option(
        None,
        "--embedding-method",
        "-e",
        help="çŸ¥è¯†åº“æ£€ç´¢ä½¿ç”¨çš„ embedding æ–¹æ³• (hash/openai/hf/zhipu ç­‰)",
    ),
    embedding_model: str | None = typer.Option(
        None,
        "--embedding-model",
        help="Embedding æ¨¡å‹åç§° (å¦‚ text-embedding-3-small)",
    ),
) -> None:
    """ä½¿ç”¨å¤§æ¨¡å‹äº¤äº’å¼ç”Ÿæˆ SAGE pipeline é…ç½®ã€‚"""

    resolved_model = model or DEFAULT_MODEL
    resolved_base_url = base_url or DEFAULT_BASE_URL
    resolved_api_key = api_key or DEFAULT_API_KEY

    if backend != "mock" and not resolved_api_key:
        raise PipelineBuilderError(
            "æœªæä¾› API Keyã€‚è¯·é€šè¿‡ --api-key æˆ–ç¯å¢ƒå˜é‡ SAGE_PIPELINE_BUILDER_API_KEY/TEMP_GENERATOR_API_KEY è®¾ç½®ã€‚"
        )

    requirements = _collect_requirements(
        name,
        goal,
        requirements_path,
        interactive=not non_interactive,
    )

    try:
        domain_contexts = list(load_domain_contexts(limit=context_limit))
    except Exception as exc:  # pragma: no cover - defensive
        raise PipelineBuilderError(f"åŠ è½½é»˜è®¤ä¸Šä¸‹æ–‡å¤±è´¥: {exc}") from exc

    if context_file:
        try:
            custom_contexts = load_custom_contexts(tuple(context_file))
            domain_contexts.extend(custom_contexts)
        except RuntimeError as exc:
            raise PipelineBuilderError(str(exc)) from exc

    if show_contexts and domain_contexts:
        console.print(
            Panel(
                "\n\n".join(domain_contexts),
                title="LLM æç¤ºä¸Šä¸‹æ–‡",
                style="magenta",
            )
        )

    knowledge_base: PipelineKnowledgeBase | None = None
    if not disable_knowledge:
        try:
            knowledge_base = get_default_knowledge_base(
                embedding_method=embedding_method,
                embedding_model=embedding_model,
            )
            # Show which embedding method is being used
            method_name = embedding_method or os.getenv("SAGE_PIPELINE_EMBEDDING_METHOD", "hash")
            console.print(f"ğŸ¯ çŸ¥è¯†åº“ä½¿ç”¨ [cyan]{method_name}[/cyan] embedding æ–¹æ³•", style="dim")
        except Exception as exc:
            console.print(f"[yellow]åˆå§‹åŒ–çŸ¥è¯†åº“å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨é™æ€ä¸Šä¸‹æ–‡: {exc}[/yellow]")

    config = BuilderConfig(
        backend=backend,
        model=resolved_model,
        base_url=resolved_base_url,
        api_key=resolved_api_key,
        domain_contexts=tuple(domain_contexts),
        knowledge_base=knowledge_base,
        knowledge_top_k=knowledge_top_k,
        show_knowledge=show_knowledge,
    )

    generator = PipelinePlanGenerator(config)

    plan: dict[str, Any] | None = None
    feedback: str | None = None

    for _iteration in range(1, 6):
        try:
            plan = generator.generate(requirements, plan, feedback)
        except PipelineBuilderError as exc:
            console.print(f"[red]ç”Ÿæˆå¤±è´¥: {exc}[/red]")
            if non_interactive:
                raise
            if not typer.confirm("æ˜¯å¦é‡æ–°å°è¯•ç”Ÿæˆï¼Ÿ", default=True):
                raise
            feedback = typer.prompt("è¯·æä¾›æ›´è¯¦ç»†çš„éœ€æ±‚æˆ–ä¿®æ”¹å»ºè®®")
            continue

        _render_plan(plan)

        if non_interactive:
            break

        if typer.confirm("å¯¹é…ç½®æ»¡æ„å—ï¼Ÿ", default=True):
            break

        feedback = typer.prompt(
            "è¯·è¾“å…¥éœ€è¦è°ƒæ•´çš„ç‚¹ï¼ˆä¾‹å¦‚ä¿®æ”¹æŸä¸€é˜¶æ®µ/æ›¿æ¢ç»„ä»¶ï¼‰",
            default="",
        )
        if not feedback or not feedback.strip():
            console.print("æœªæä¾›ä¿®æ”¹æ„è§ï¼Œä¿æŒå½“å‰ç‰ˆæœ¬ã€‚", style="yellow")
            break

    if plan is None:
        raise PipelineBuilderError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ pipeline é…ç½®ã€‚")

    yaml_text = _plan_to_yaml(plan)
    _preview_yaml(yaml_text)

    if not non_interactive and not typer.confirm("æ˜¯å¦ä¿å­˜è¯¥é…ç½®?", default=True):
        console.print("æ“ä½œå·²å–æ¶ˆï¼Œæœªå†™å…¥æ–‡ä»¶ã€‚", style="yellow")
        return

    output_path = _save_plan(plan, output, overwrite)
    console.print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: [green]{output_path}[/green]")


@app.command("run")
def run_pipeline(
    config: Path = typer.Argument(..., exists=False, help="Pipeline YAML é…ç½®æ–‡ä»¶"),
    autostop: bool = typer.Option(
        True, "--autostop/--no-autostop", help="æäº¤åæ˜¯å¦ç­‰å¾…æ‰¹å¤„ç†å®Œæˆ"
    ),
    host: str | None = typer.Option(
        None,
        "--host",
        help="è¿œç¨‹ç¯å¢ƒ JobManager ä¸»æœº (ä»…å½“ pipeline.type=remote æ—¶ç”Ÿæ•ˆ)",
    ),
    port: int | None = typer.Option(
        None,
        "--port",
        min=1,
        max=65535,
        help="è¿œç¨‹ç¯å¢ƒ JobManager ç«¯å£ (ä»…å½“ pipeline.type=remote æ—¶ç”Ÿæ•ˆ)",
    ),
) -> None:
    """åŠ è½½ YAML é…ç½®å¹¶è¿è¡Œ SAGE pipelineã€‚"""

    try:
        config_path = Path(config).expanduser().resolve()
        plan = _load_pipeline_file(config_path)

        pipeline_meta = plan.get("pipeline") or {}
        pipeline_name = pipeline_meta.get("name", config_path.stem)

        console.print(
            Panel.fit(
                f"åç§°: [cyan]{pipeline_name}[/cyan]\nç±»å‹: {pipeline_meta.get('type', 'local')}\næ¥æº: {config_path}",
                title="è¿è¡Œ Pipeline",
                style="blue",
            )
        )

        execute_pipeline_plan(
            plan,
            autostop=autostop,
            host=host,
            port=port,
            console_override=console,
        )

    except CLIException as exc:
        console.print(f"[red]âŒ {exc}[/red]")
        raise typer.Exit(1) from exc


@app.command("analyze-embedding")
def analyze_embedding_methods(
    query: str = typer.Argument(..., help="æµ‹è¯•æŸ¥è¯¢æ–‡æœ¬"),
    top_k: int = typer.Option(3, "--top-k", "-k", min=1, max=10, help="è¿”å› Top-K ç»“æœæ•°é‡"),
    methods: list[str] | None = typer.Option(
        None,
        "--method",
        "-m",
        help="æŒ‡å®šè¦æ¯”è¾ƒçš„ embedding æ–¹æ³•ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰",
    ),
    show_vectors: bool = typer.Option(False, "--show-vectors", help="æ˜¾ç¤ºå‘é‡è¯¦æƒ…"),
) -> None:
    """åˆ†æå’Œæ¯”è¾ƒä¸åŒ embedding æ–¹æ³•åœ¨ Pipeline Builder çŸ¥è¯†åº“ä¸Šçš„æ£€ç´¢æ•ˆæœã€‚

    è¿™ä¸ªå‘½ä»¤å¸®åŠ©ä½ é€‰æ‹©æœ€é€‚åˆä½ åœºæ™¯çš„ embedding æ–¹æ³•ã€‚

    ç¤ºä¾‹:
        sage pipeline analyze-embedding "å¦‚ä½•æ„å»º RAG pipeline"
        sage pipeline analyze-embedding "å‘é‡æ£€ç´¢" -m hash -m openai -m hf
    """
    from sage.common.components.sage_embedding.registry import EmbeddingRegistry

    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–¹æ³•ï¼Œä½¿ç”¨é»˜è®¤çš„å‡ ä¸ªå¸¸ç”¨æ–¹æ³•
    if not methods:
        all_methods = EmbeddingRegistry.list_methods()
        # ä¼˜å…ˆé€‰æ‹©å…è´¹/æœ¬åœ°æ–¹æ³•
        default_methods = []
        for m in ["hash", "mockembedder", "hf"]:
            if m in all_methods:
                default_methods.append(m)
        methods = default_methods[:3] if default_methods else all_methods[:3]

    console.print(
        Panel(
            f"ğŸ” æŸ¥è¯¢: [cyan]{query}[/cyan]\n"
            f"ğŸ“Š å¯¹æ¯”æ–¹æ³•: {', '.join(methods)}\n"
            f"ğŸ“š çŸ¥è¯†åº“: SAGE Pipeline Builder",
            title="Embedding æ–¹æ³•åˆ†æ",
            style="blue",
        )
    )

    results_by_method = {}

    for method in methods:
        try:
            console.print(f"\nâš™ï¸  æµ‹è¯•æ–¹æ³•: [cyan]{method}[/cyan]")

            # åˆ›å»ºä½¿ç”¨è¯¥ embedding æ–¹æ³•çš„çŸ¥è¯†åº“
            kb = PipelineKnowledgeBase(
                max_chunks=500,  # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†åŠ å¿«æµ‹è¯•
                allow_download=False,
                embedding_method=method,
            )

            # æ‰§è¡Œæ£€ç´¢
            import time

            start = time.time()
            search_results = kb.search(query, top_k=top_k)
            elapsed = time.time() - start

            results_by_method[method] = {
                "results": search_results,
                "time": elapsed,
                "dimension": (
                    len(search_results[0].vector)
                    if search_results and search_results[0].vector
                    else 0
                ),
            }

            console.print(
                f"   âœ“ æ£€ç´¢å®Œæˆ (è€—æ—¶: {elapsed * 1000:.2f}ms, ç»´åº¦: {results_by_method[method]['dimension']})"
            )

        except Exception as exc:
            console.print(f"   âœ— [red]{method} å¤±è´¥: {exc}[/red]")
            continue

    if not results_by_method:
        console.print("[red]æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚[/red]")
        raise typer.Exit(1)

    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    console.print("\n" + "=" * 80)
    console.print("[bold green]ğŸ“Š æ£€ç´¢ç»“æœå¯¹æ¯”[/bold green]\n")

    for method, data in results_by_method.items():
        console.print(f"[bold cyan]â”â”â” {method.upper()} â”â”â”[/bold cyan]")
        console.print(f"â±ï¸  è€—æ—¶: {data['time'] * 1000:.2f}ms | ğŸ“ ç»´åº¦: {data['dimension']}")

        table = Table(show_header=True, header_style="bold magenta", box=None)
        table.add_column("æ’å", style="dim", width=4)
        table.add_column("å¾—åˆ†", justify="right", width=8)
        table.add_column("ç±»å‹", width=8)
        table.add_column("æ–‡æœ¬ç‰‡æ®µ", width=60)

        for idx, chunk in enumerate(data["results"], 1):
            preview = (
                chunk.text[:100].replace("\n", " ") + "..."
                if len(chunk.text) > 100
                else chunk.text.replace("\n", " ")
            )
            table.add_row(
                f"#{idx}",
                f"{chunk.score:.4f}",
                chunk.kind,
                preview,
            )

        console.print(table)

        if show_vectors and data["results"]:
            first_vec = data["results"][0].vector
            if first_vec:
                vec_preview = str(first_vec[:10])[:-1] + ", ...]"
                console.print(f"   å‘é‡ç¤ºä¾‹: {vec_preview}\n")

        console.print()

    # æ¨èæœ€ä½³æ–¹æ³•
    console.print("[bold yellow]ğŸ’¡ æ¨èå»ºè®®:[/bold yellow]\n")

    fastest = min(results_by_method.items(), key=lambda x: x[1]["time"])
    console.print(f"âš¡ æœ€å¿«æ–¹æ³•: [green]{fastest[0]}[/green] ({fastest[1]['time'] * 1000:.2f}ms)")

    # ç®€å•çš„ç›¸å…³æ€§è¯„ä¼°ï¼ˆåŸºäºå¹³å‡å¾—åˆ†ï¼‰
    avg_scores = {
        method: (
            sum(r.score for r in data["results"]) / len(data["results"]) if data["results"] else 0
        )
        for method, data in results_by_method.items()
    }
    best_relevance = max(avg_scores.items(), key=lambda x: x[1])
    console.print(
        f"ğŸ¯ æœ€ç›¸å…³æ–¹æ³•: [green]{best_relevance[0]}[/green] (å¹³å‡å¾—åˆ†: {best_relevance[1]:.4f})"
    )

    console.print(
        f"\nğŸ’¡ [dim]ä½¿ç”¨æ¨èæ–¹æ³•:[/dim] "
        f"[cyan]sage pipeline build --embedding-method {best_relevance[0]}[/cyan]"
    )


@app.command("create-embedding")
def create_embedding_pipeline(
    template: str = typer.Option(
        "rag",
        "--template",
        "-t",
        help="Pipeline æ¨¡æ¿ç±»å‹: rag, knowledge-base, hybrid-search, multi-strategy",
    ),
    embedding_method: str = typer.Option(
        "hf",
        "--embedding-method",
        "-e",
        help="Embedding æ–¹æ³• (hf/openai/jina/zhipu/cohere/bedrock/ollama/siliconcloud/nvidia_openai/vllm)",
    ),
    embedding_model: str | None = typer.Option(
        None,
        "--embedding-model",
        "-m",
        help="Embedding æ¨¡å‹åç§°ï¼ˆæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤ï¼‰",
    ),
    use_vllm: bool = typer.Option(
        False,
        "--vllm",
        help="ä½¿ç”¨ vLLM æœåŠ¡è¿›è¡Œé«˜æ€§èƒ½ embedding",
    ),
    llm_model: str | None = typer.Option(
        None,
        "--llm-model",
        help="LLM æ¨¡å‹åç§°ï¼ˆRAG æ¨¡æ¿éœ€è¦ï¼‰",
    ),
    dense_method: str | None = typer.Option(
        None,
        "--dense-method",
        help="Hybrid æ¨¡æ¿ï¼šDense embedding æ–¹æ³•",
    ),
    sparse_method: str | None = typer.Option(
        None,
        "--sparse-method",
        help="Hybrid æ¨¡æ¿ï¼šSparse embedding æ–¹æ³•ï¼ˆé»˜è®¤ bm25sï¼‰",
    ),
    query_method: str | None = typer.Option(
        None,
        "--query-method",
        help="Multi-strategy æ¨¡æ¿ï¼šæŸ¥è¯¢ç”¨ embedding æ–¹æ³•ï¼ˆå¿«é€Ÿï¼‰",
    ),
    doc_method: str | None = typer.Option(
        None,
        "--doc-method",
        help="Multi-strategy æ¨¡æ¿ï¼šæ–‡æ¡£ç”¨ embedding æ–¹æ³•ï¼ˆé«˜è´¨é‡ï¼‰",
    ),
    batch_method: str | None = typer.Option(
        None,
        "--batch-method",
        help="Multi-strategy æ¨¡æ¿ï¼šæ‰¹é‡å¤„ç†ç”¨ embedding æ–¹æ³•",
    ),
    chunk_size: int = typer.Option(512, "--chunk-size", help="æ–‡æ¡£åˆ†å—å¤§å°"),
    chunk_overlap: int = typer.Option(50, "--chunk-overlap", help="åˆ†å—é‡å å¤§å°"),
    batch_size: int = typer.Option(32, "--batch-size", help="æ‰¹å¤„ç†å¤§å°"),
    enable_cache: bool = typer.Option(True, "--cache/--no-cache", help="å¯ç”¨ç¼“å­˜"),
    normalize: bool = typer.Option(True, "--normalize/--no-normalize", help="å‘é‡å½’ä¸€åŒ–"),
    output: Path | None = typer.Option(
        None,
        "--output",
        "-o",
        help="è¾“å‡º YAML æ–‡ä»¶è·¯å¾„",
    ),
    overwrite: bool = typer.Option(False, "--overwrite", help="è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶"),
    interactive: bool = typer.Option(
        False,
        "--interactive",
        "-i",
        help="äº¤äº’å¼é…ç½®æ¨¡æ¿å‚æ•°",
    ),
) -> None:
    """ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿åˆ›å»ºåŸºäº EmbeddingService çš„ pipelineã€‚

    æ”¯æŒçš„æ¨¡æ¿:
    - rag: RAG pipeline with embedding service
    - knowledge-base: é«˜ååé‡çŸ¥è¯†åº“æ„å»º
    - hybrid-search: Dense + Sparse æ··åˆæ£€ç´¢
    - multi-strategy: æ™ºèƒ½è·¯ç”±å¤šç­–ç•¥ embedding

    ç¤ºä¾‹:
        # åˆ›å»º HuggingFace RAG pipeline
        sage pipeline create-embedding -t rag -e hf -m BAAI/bge-small-zh-v1.5

        # åˆ›å»º vLLM é«˜æ€§èƒ½çŸ¥è¯†åº“æ„å»º
        sage pipeline create-embedding -t knowledge-base --vllm

        # åˆ›å»ºæ··åˆæ£€ç´¢ pipeline
        sage pipeline create-embedding -t hybrid-search --dense-method openai --sparse-method bm25s

        # åˆ›å»ºå¤šç­–ç•¥æ™ºèƒ½è·¯ç”±
        sage pipeline create-embedding -t multi-strategy --query-method hash --doc-method openai
    """
    from .pipeline_embedding import generate_embedding_pipeline

    # äº¤äº’å¼é…ç½®
    if interactive:
        console.print(
            Panel(
                "ğŸ¯ äº¤äº’å¼ Embedding Pipeline é…ç½®å‘å¯¼",
                style="cyan",
            )
        )

        template_choices = ["rag", "knowledge-base", "hybrid-search", "multi-strategy"]
        template = typer.prompt(
            "é€‰æ‹©æ¨¡æ¿ç±»å‹",
            type=str,
            default=template,
            show_choices=True,
        )

        if template not in template_choices:
            console.print(f"[red]æ— æ•ˆçš„æ¨¡æ¿: {template}[/red]")
            raise typer.Exit(1)

        embedding_method = typer.prompt(
            "Embedding æ–¹æ³• (hf/openai/jina/zhipu/cohere/bedrock/ollama/siliconcloud/nvidia_openai/vllm)",
            type=str,
            default=embedding_method,
        )

        if embedding_method not in ["vllm", "hash", "mockembedder"]:
            embedding_model = typer.prompt(
                "Embedding æ¨¡å‹åç§°",
                type=str,
                default=embedding_model or "",
            )

        use_vllm = typer.confirm("ä½¿ç”¨ vLLM æœåŠ¡?", default=use_vllm)

        if template == "rag":
            llm_model = typer.prompt(
                "LLM æ¨¡å‹åç§°",
                type=str,
                default=llm_model or "Qwen/Qwen2.5-7B-Instruct",
            )
        elif template == "hybrid-search":
            dense_method = typer.prompt(
                "Dense embedding æ–¹æ³•",
                type=str,
                default=dense_method or embedding_method,
            )
            sparse_method = typer.prompt(
                "Sparse embedding æ–¹æ³•",
                type=str,
                default=sparse_method or "bm25s",
            )
        elif template == "multi-strategy":
            query_method = typer.prompt(
                "æŸ¥è¯¢ç”¨ embedding æ–¹æ³• (å¿«é€Ÿ)",
                type=str,
                default=query_method or "hash",
            )
            doc_method = typer.prompt(
                "æ–‡æ¡£ç”¨ embedding æ–¹æ³• (é«˜è´¨é‡)",
                type=str,
                default=doc_method or embedding_method,
            )
            batch_method = typer.prompt(
                "æ‰¹é‡å¤„ç†ç”¨ embedding æ–¹æ³•",
                type=str,
                default=batch_method or "vllm" if use_vllm else embedding_method,
            )

    # æ„å»ºå‚æ•°
    kwargs = {
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "batch_size": batch_size,
        "enable_cache": enable_cache,
        "normalize": normalize,
    }

    # æ ¹æ®æ¨¡æ¿ç±»å‹æ·»åŠ ç‰¹å®šå‚æ•°
    if template == "rag":
        if not llm_model:
            llm_model = "Qwen/Qwen2.5-7B-Instruct"
        kwargs["llm_model"] = llm_model
    elif template == "hybrid-search":
        if not dense_method:
            dense_method = embedding_method
        if not sparse_method:
            sparse_method = "bm25s"
        kwargs["dense_method"] = dense_method
        kwargs["sparse_method"] = sparse_method
        # dense_model ä½¿ç”¨ embedding_model
        if embedding_model:
            kwargs["dense_model"] = embedding_model
    elif template == "multi-strategy":
        if not query_method:
            query_method = "hash"
        if not doc_method:
            doc_method = embedding_method
        if not batch_method:
            batch_method = "vllm" if use_vllm else embedding_method
        kwargs["query_method"] = query_method
        kwargs["doc_method"] = doc_method
        kwargs["batch_method"] = batch_method

    # ç”Ÿæˆé…ç½®
    console.print(
        Panel(
            f"ğŸ“‹ æ¨¡æ¿: [cyan]{template}[/cyan]\n"
            f"ğŸ”§ Embedding: [cyan]{embedding_method}[/cyan]\n"
            f"ğŸš€ vLLM: [cyan]{use_vllm}[/cyan]",
            title="ç”Ÿæˆ Pipeline é…ç½®",
            style="blue",
        )
    )

    try:
        plan = generate_embedding_pipeline(
            use_case=template,
            embedding_method=embedding_method,
            embedding_model=embedding_model,
            use_vllm=use_vllm,
            **kwargs,
        )
    except ValueError as exc:
        console.print(f"[red]ç”Ÿæˆå¤±è´¥: {exc}[/red]")
        raise typer.Exit(1) from exc

    # æ˜¾ç¤ºé…ç½®
    _render_plan(plan)

    # é¢„è§ˆ YAML
    yaml_text = _plan_to_yaml(plan)
    _preview_yaml(yaml_text)

    # ä¿å­˜
    if not interactive or typer.confirm("ä¿å­˜é…ç½®?", default=True):
        output_path = _save_plan(plan, output, overwrite)
        console.print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: [green]{output_path}[/green]")

        # æç¤ºå¦‚ä½•è¿è¡Œ
        console.print(f"\nğŸ’¡ è¿è¡Œæ­¤ pipeline:\n   [cyan]sage pipeline run {output_path}[/cyan]")
    else:
        console.print("[yellow]æœªä¿å­˜é…ç½®ã€‚[/yellow]")


__all__ = [
    "app",
    "BuilderConfig",
    "GraphBuilderConfig",
    "PipelinePlanGenerator",
    "GraphPlanGenerator",
    "PipelineBuilderError",
    "render_pipeline_plan",
    "pipeline_plan_to_yaml",
    "preview_pipeline_plan",
    "save_pipeline_plan",
    "execute_pipeline_plan",
    "create_embedding_pipeline",
    "analyze_embedding_methods",
]
