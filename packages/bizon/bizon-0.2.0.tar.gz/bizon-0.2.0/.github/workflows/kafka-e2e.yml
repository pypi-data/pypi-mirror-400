name: Kafka Connector E2E Tests

on:
  pull_request:
    paths:
      - 'bizon/connectors/sources/kafka/**'
      - 'tests/e2e/kafka/**'
      - 'tests/connectors/sources/kafka/**'
      - '.github/workflows/kafka-e2e.yml'

permissions:
  contents: read

jobs:
  kafka-e2e-test:
    name: Kafka connector E2E tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Start Kafka with Docker Compose
      run: |
        echo "Starting Kafka with Docker Compose..."
        cat > docker-compose.yml << 'EOF'
        version: '3.8'
        services:
          kafka:
            image: confluentinc/confluent-local:7.5.0
            hostname: kafka
            container_name: kafka
            ports:
              - "9092:9092"
            environment:
              KAFKA_NODE_ID: 1
              KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
              KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
              KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
              KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
              KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
              KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
              KAFKA_JMX_PORT: 9101
              KAFKA_JMX_HOSTNAME: localhost
              KAFKA_PROCESS_ROLES: 'broker,controller'
              KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
              KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
              KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
              KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
              KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
              CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
        EOF
        docker compose up -d
        echo "Waiting for container to start..."
        sleep 5
        docker compose ps

    - name: Wait for Kafka to be ready
      run: |
        echo "Waiting for Kafka to be ready..."
        # Check container logs
        echo "Container logs:"
        docker compose logs kafka | tail -20
        # Wait for Kafka port to be available
        echo "Waiting for Kafka port 9092..."
        timeout 120s bash -c 'until nc -z localhost 9092; do echo "Waiting..."; sleep 3; done'
        # Additional wait for Kafka to fully initialize
        echo "Waiting for Kafka to fully initialize..."
        sleep 15
        # Check logs again
        echo "Final container logs:"
        docker compose logs kafka | tail -10
        echo "Kafka should be ready!"

    - name: Install uv
      uses: astral-sh/setup-uv@v4

    - name: Install dependencies
      run: uv sync --all-extras --group test

    - name: Verify Kafka connection
      run: |
        echo "Verifying Kafka connection with retries..."
        uv run python -c "
        from confluent_kafka import Producer
        import time
        import sys

        for attempt in range(1, 6):
            try:
                print(f'Attempt {attempt}/5: Testing Kafka connection...')
                producer = Producer({
                    'bootstrap.servers': 'localhost:9092',
                    'client.id': 'verification-client',
                    'api.version.request': True,
                    'api.version.request.timeout.ms': 10000
                })
                producer.produce('test-topic', 'test-message')
                producer.flush(timeout=10)
                print('âœ… Kafka connection verified!')
                break
            except Exception as e:
                print(f'âŒ Attempt {attempt} failed: {e}')
                if attempt == 5:
                    print('Failed to connect after 5 attempts')
                    sys.exit(1)
                time.sleep(5)
        "

    - name: Create test topics and produce data
      run: |
        uv run python -c "
        import json
        import time
        from confluent_kafka import Producer
        from confluent_kafka.admin import AdminClient, NewTopic

        print('ðŸŽ¯ Setting up Kafka test environment...')
        # Create topics
        admin_client = AdminClient({'bootstrap.servers': 'localhost:9092'})
        topics = [
            NewTopic('async-test-high-volume', num_partitions=3, replication_factor=1),
            NewTopic('async-test-low-volume', num_partitions=1, replication_factor=1)
        ]

        futures = admin_client.create_topics(topics)
        for topic_name, future in futures.items():
            try:
                future.result(timeout=10)
                print(f'âœ… Created topic: {topic_name}')
            except Exception as e:
                if 'already exists' in str(e).lower():
                    print(f'â„¹ï¸  Topic {topic_name} already exists')
                else:
                    print(f'âŒ Failed to create topic {topic_name}: {e}')
                    raise
        # Produce test data
        producer = Producer({
            'bootstrap.servers': 'localhost:9092',
            'client.id': 'ci-test-producer'
        })

        print('ðŸš€ Producing test data to simulate starvation scenario...')
        # High-volume topic - simulate heavy load (200 messages)
        for i in range(200):
            message = {
                'id': f'high-vol-{i}',
                'type': 'high_volume',
                'data': f'High volume message {i}',
                'timestamp': int(time.time() * 1000),
                'batch': i // 20
            }

            producer.produce(
                'async-test-high-volume',
                key=json.dumps(f'high-vol-{i}'),
                value=json.dumps(message),
                partition=i % 3
            )

            if i % 50 == 0:
                producer.flush()
        # Low-volume topic - simulate critical but infrequent messages (10 messages)
        for i in range(10):
            message = {
                'id': f'low-vol-{i}',
                'type': 'low_volume',
                'data': f'Critical message {i}',
                'priority': 'HIGH',
                'timestamp': int(time.time() * 1000),
                'sequence': i
            }

            producer.produce(
                'async-test-low-volume',
                key=json.dumps(f'low-vol-{i}'),
                value=json.dumps(message),
                partition=0
            )

        producer.flush()
        print('ðŸ“¦ Test data produced successfully!')
        print('   - High-volume topic: 200 messages across 3 partitions')
        print('   - Low-volume topic: 10 critical messages in 1 partition')
        "

    - name: Run Kafka Authentication Failure Tests
      env:
        KAFKA_E2E_TESTS: "1"
      run: |
        echo "ðŸ” Running Kafka authentication failure tests..."
        uv run pytest tests/e2e/kafka/test_e2e_kafka_auth_failures.py -v --tb=short
        echo "âœ… Authentication failure tests completed!"

    - name: Run Standard Kafka E2E Tests
      run: |
        echo "ðŸš€ Running standard Kafka e2e tests..."
        # Add any existing e2e tests here if they exist
        echo "âœ… Standard e2e tests completed!"

    - name: Cleanup
      if: always()
      run: |
        echo "ðŸ§¹ Cleaning up..."
        docker compose down -v
        docker compose logs kafka | tail -20
