#!/usr/bin/env python3
"""
Quality Validation System for Greeum v2.0.5
- Automatically assesses memory quality before storage
- Provides quality scores and improvement suggestions
- Integrates with MCP server for enhanced user experience
"""

import re
import logging
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from enum import Enum

logger = logging.getLogger(__name__)

class QualityLevel(Enum):
    """Memory quality classification levels"""
    EXCELLENT = "excellent"    # 0.9-1.0
    GOOD = "good"             # 0.7-0.9
    ACCEPTABLE = "acceptable" # 0.5-0.7
    POOR = "poor"            # 0.3-0.5
    VERY_POOR = "very_poor"  # 0.0-0.3

class QualityValidator:
    """ì§€ëŠ¥ì  ë©”ëª¨ë¦¬ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """í’ˆì§ˆ ê²€ì¦ê¸° ì´ˆê¸°í™”"""
        self.min_length = 10
        self.max_length = 10000
        self.stop_words = {
            'english': {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'},
            'korean': {'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë“¤', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„'},
            'common': {'hello', 'hi', 'bye', 'thanks', 'thank', 'you', 'ok', 'okay', 'yes', 'no', 'sure'}
        }
        
    def validate_memory_quality(self, content: str, importance: float = 0.5, 
                              context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ í’ˆì§ˆ ì¢…í•© ê²€ì¦
        
        Args:
            content: ê²€ì¦í•  ë©”ëª¨ë¦¬ ë‚´ìš©
            importance: ì‚¬ìš©ì ì§€ì • ì¤‘ìš”ë„
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            
        Returns:
            {
                "quality_score": float,        # 0.0-1.0 í’ˆì§ˆ ì ìˆ˜
                "quality_level": str,          # excellent/good/acceptable/poor/very_poor
                "quality_factors": Dict,       # ì„¸ë¶€ í’ˆì§ˆ ìš”ì†Œë“¤
                "suggestions": List[str],      # ê°œì„  ì œì•ˆì‚¬í•­
                "should_store": bool,          # ì €ì¥ ê¶Œì¥ ì—¬ë¶€
                "adjusted_importance": float,  # í’ˆì§ˆ ê¸°ë°˜ ì¡°ì •ëœ ì¤‘ìš”ë„
                "warnings": List[str]          # ê²½ê³  ì‚¬í•­ë“¤
            }
        """
        try:
            # 1. ê¸°ë³¸ í’ˆì§ˆ ê²€ì‚¬
            quality_factors = self._assess_quality_factors(content)
            
            # 2. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_quality_score(quality_factors, importance)
            
            # 3. í’ˆì§ˆ ë“±ê¸‰ ë¶„ë¥˜
            quality_level = self._classify_quality_level(quality_score)
            
            # 4. ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±
            suggestions = self._generate_suggestions(quality_factors, content)
            
            # 5. ì €ì¥ ê¶Œì¥ ì—¬ë¶€ ê²°ì •
            should_store = self._should_store_memory(quality_score, quality_level)
            
            # 6. ì¤‘ìš”ë„ ì¡°ì •
            adjusted_importance = self._adjust_importance(importance, quality_score)
            
            # 7. ê²½ê³ ì‚¬í•­ ìƒì„±
            warnings = self._generate_warnings(quality_factors, content)
            
            return {
                "quality_score": round(quality_score, 3),
                "quality_level": quality_level.value,
                "quality_factors": quality_factors,
                "suggestions": suggestions,
                "should_store": should_store,
                "adjusted_importance": adjusted_importance,
                "warnings": warnings,
                "timestamp": datetime.now().isoformat(),
                "validation_version": "2.1.0"
            }
            
        except Exception as e:
            logger.error(f"Quality validation failed: {e}")
            return self._create_fallback_result(content, importance, str(e))
    
    def _assess_quality_factors(self, content: str) -> Dict[str, Any]:
        """í’ˆì§ˆ ìš”ì†Œë“¤ ì„¸ë¶€ í‰ê°€"""
        factors = {}
        
        # 1. ê¸¸ì´ í‰ê°€
        factors['length'] = self._assess_length_quality(content)
        
        # 2. ë‚´ìš© í’ë¶€ë„ í‰ê°€
        factors['richness'] = self._assess_content_richness(content)
        
        # 3. êµ¬ì¡°ì  í’ˆì§ˆ í‰ê°€
        factors['structure'] = self._assess_structural_quality(content)
        
        # 4. ì–¸ì–´ í’ˆì§ˆ í‰ê°€
        factors['language'] = self._assess_language_quality(content)
        
        # 5. ì •ë³´ ë°€ë„ í‰ê°€
        factors['information_density'] = self._assess_information_density(content)
        
        # 6. ê²€ìƒ‰ ê°€ëŠ¥ì„± í‰ê°€
        factors['searchability'] = self._assess_searchability(content)
        
        # 7. ì‹œê°„ ê´€ë ¨ì„± í‰ê°€
        factors['temporal_relevance'] = self._assess_temporal_relevance(content)
        
        return factors
    
    def _assess_length_quality(self, content: str) -> Dict[str, Any]:
        """ê¸¸ì´ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
        length = len(content.strip())
        
        if length < self.min_length:
            return {"score": 0.1, "issue": "too_short", "actual_length": length}
        elif length > self.max_length:
            return {"score": 0.3, "issue": "too_long", "actual_length": length}
        elif self.min_length <= length <= 50:
            return {"score": 0.5, "issue": "minimal", "actual_length": length}
        elif 50 < length <= 200:
            return {"score": 0.8, "issue": None, "actual_length": length}
        elif 200 < length <= 1000:
            return {"score": 1.0, "issue": None, "actual_length": length}
        else:
            return {"score": 0.7, "issue": "verbose", "actual_length": length}
    
    def _is_meaningful_word(self, word: str) -> bool:
        """Check if word is meaningful (not stop word)"""
        return (word not in self.stop_words['english'] and 
                word not in self.stop_words['korean'] and 
                word not in self.stop_words['common'] and 
                len(word) > 2 and word.isalpha())
    
    def _assess_content_richness(self, content: str) -> Dict[str, Any]:
        """Memory-efficient content richness evaluation"""
        # Process words incrementally to save memory
        word_count = 0
        unique_words = set()
        meaningful_words = set()
        
        # Early limit to prevent DoS attacks
        max_words = 10000
        words_processed = 0
        
        for word in content.lower().split():
            if words_processed >= max_words:
                break
            
            word_count += 1
            words_processed += 1
            unique_words.add(word)
            
            if self._is_meaningful_word(word):
                meaningful_words.add(word)
        
        # Calculate ratios safely
        richness_ratio = len(meaningful_words) / word_count if word_count > 0 else 0.0
        lexical_diversity = len(unique_words) / word_count if word_count > 0 else 0.0
        
        # Comprehensive score
        richness_score = (richness_ratio * 0.6 + lexical_diversity * 0.4)
        
        return {
            "score": min(richness_score * 1.2, 1.0),  # ì•½ê°„ì˜ ë³´ë„ˆìŠ¤
            "meaningful_word_ratio": richness_ratio,
            "lexical_diversity": lexical_diversity,
            "total_words": word_count,
            "unique_words": len(unique_words),
            "meaningful_words": len(meaningful_words),
            "truncated": words_processed >= max_words
        }
    
    def _assess_structural_quality(self, content: str) -> Dict[str, Any]:
        """êµ¬ì¡°ì  í’ˆì§ˆ í‰ê°€"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        issues = []
        
        # ë¬¸ì¥ êµ¬ì¡° í™•ì¸
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) > 1:
            score += 0.2  # ì—¬ëŸ¬ ë¬¸ì¥ ë³´ë„ˆìŠ¤
        
        # êµ¬ë‘ì  ì‚¬ìš© í™•ì¸
        punctuation_count = len(re.findall(r'[.!?,:;]', content))
        if punctuation_count > 0:
            score += 0.1
        
        # ëŒ€ì†Œë¬¸ì í˜¼ìš© í™•ì¸ (ì˜ì–´ì˜ ê²½ìš°)
        if re.search(r'[A-Z]', content) and re.search(r'[a-z]', content):
            score += 0.1
        
        # ë‹¨ë½ êµ¬ë¶„ í™•ì¸
        paragraphs = content.split('\n\n')
        if len(paragraphs) > 1:
            score += 0.1
        
        # ë„ˆë¬´ ë°˜ë³µì ì¸ íŒ¨í„´ ê²€ì‚¬
        words = content.lower().split()
        if len(words) > 5:
            word_counts = {}
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
            
            max_repeat = max(word_counts.values())
            if max_repeat > len(words) * 0.3:  # 30% ì´ìƒ ë°˜ë³µ
                score -= 0.3
                issues.append("excessive_repetition")
        
        return {
            "score": min(score, 1.0),
            "sentence_count": len(sentences),
            "punctuation_count": punctuation_count,
            "paragraph_count": len(paragraphs),
            "issues": issues
        }
    
    def _assess_language_quality(self, content: str) -> Dict[str, Any]:
        """ì–¸ì–´ í’ˆì§ˆ í‰ê°€"""
        score = 0.7  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸°ë³¸ì ì¸ ì–¸ì–´ í’ˆì§ˆ ì§€í‘œë“¤
        
        # 1. ì—°ì† ê³µë°± í™•ì¸
        if '  ' in content:
            score -= 0.1
        
        # 2. íŠ¹ìˆ˜ë¬¸ì ë‚¨ìš© í™•ì¸
        special_char_ratio = len(re.findall(r'[!@#$%^&*()_+=\[\]{}|;:,.<>?]', content)) / len(content)
        if special_char_ratio > 0.1:
            score -= 0.2
        
        # 3. ìˆ«ìì™€ í…ìŠ¤íŠ¸ì˜ ê· í˜•
        digit_ratio = len(re.findall(r'\d', content)) / len(content)
        if digit_ratio > 0.5:  # ìˆ«ìê°€ 50% ì´ìƒ
            score -= 0.1
        
        # 4. ì „ì²´ ëŒ€ë¬¸ì ë˜ëŠ” ì†Œë¬¸ì í™•ì¸
        if content.isupper() and len(content) > 20:
            score -= 0.2
        elif content.islower() and len(content) > 50:
            score -= 0.1
        
        return {
            "score": max(score, 0.0),
            "special_char_ratio": special_char_ratio,
            "digit_ratio": digit_ratio,
            "has_mixed_case": not (content.isupper() or content.islower())
        }
    
    def _assess_information_density(self, content: str) -> Dict[str, Any]:
        """ì •ë³´ ë°€ë„ í‰ê°€"""
        words = content.split()
        
        # ì •ë³´ê°€ ë‹´ê¸´ íŒ¨í„´ë“¤ ê²€ì‚¬
        info_patterns = [
            r'\d+',                    # ìˆ«ì
            r'[A-Z][a-z]+',           # ê³ ìœ ëª…ì‚¬
            r'[a-zA-Z]+\.[a-zA-Z]+',  # ë„ë©”ì¸/í™•ì¥ì
            r'@[a-zA-Z0-9]+',         # ë©˜ì…˜
            r'#[a-zA-Z0-9]+',         # í•´ì‹œíƒœê·¸
            r'\b[A-Z]{2,}\b',         # ì•½ì–´
            r'\d{4}-\d{2}-\d{2}',     # ë‚ ì§œ
        ]
        
        info_matches = 0
        for pattern in info_patterns:
            info_matches += len(re.findall(pattern, content))
        
        if len(words) == 0:
            density = 0.0
        else:
            density = info_matches / len(words)
        
        # ë°€ë„ ì ìˆ˜ ê³„ì‚°
        if density == 0:
            score = 0.3
        elif density < 0.1:
            score = 0.5
        elif density < 0.3:
            score = 0.8
        else:
            score = 1.0
            
        return {
            "score": score,
            "density": density,
            "info_matches": info_matches,
            "word_count": len(words)
        }
    
    def _assess_searchability(self, content: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê°€ëŠ¥ì„± í‰ê°€"""
        score = 0.5
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ê°€ëŠ¥ì„±
        words = content.lower().split()
        potential_keywords = [w for w in words if len(w) > 3 and w.isalpha()]
        
        if len(potential_keywords) >= 3:
            score += 0.3
        elif len(potential_keywords) >= 1:
            score += 0.1
            
        # ê³ ìœ í•œ ì‹ë³„ì í¬í•¨
        unique_identifiers = len(re.findall(r'\b[A-Z][a-z]+\b|\b\d+\b|[a-zA-Z]+\.[a-zA-Z]+', content))
        if unique_identifiers > 0:
            score += 0.2
            
        return {
            "score": min(score, 1.0),
            "potential_keywords": len(potential_keywords),
            "unique_identifiers": unique_identifiers
        }
    
    def _assess_temporal_relevance(self, content: str) -> Dict[str, Any]:
        """ì‹œê°„ ê´€ë ¨ì„± í‰ê°€"""
        score = 0.6  # ê¸°ë³¸ ì ìˆ˜
        
        # ì‹œê°„ ê´€ë ¨ í‘œí˜„ ê²€ì‚¬
        temporal_patterns = [
            r'\b\d{4}ë…„?\b',                    # ë…„ë„
            r'\b\d{1,2}ì›”\b',                   # ì›”
            r'\b\d{1,2}ì¼\b',                   # ì¼
            r'\b\d{4}-\d{2}-\d{2}\b',          # ISO ë‚ ì§œ
            r'\b(ì˜¤ëŠ˜|ì–´ì œ|ë‚´ì¼|ì´ë²ˆì£¼|ë‹¤ìŒì£¼)\b',      # í•œêµ­ì–´ ì‹œê°„ í‘œí˜„
            r'\b(today|yesterday|tomorrow|this week|next week)\b',  # ì˜ì–´ ì‹œê°„ í‘œí˜„
            r'\b(ìµœê·¼|ì˜ˆì „|ê³¼ê±°|ë¯¸ë˜)\b',             # ì‹œê°„ ê´€ë ¨ í˜•ìš©ì‚¬
            r'\b(recently|previously|future|past)\b'
        ]
        
        temporal_matches = 0
        for pattern in temporal_patterns:
            temporal_matches += len(re.findall(pattern, content, re.IGNORECASE))
        
        if temporal_matches > 0:
            score += 0.2
        
        # í˜„ì¬ ì‹œì ê³¼ì˜ ê´€ë ¨ì„±
        current_time_words = ['ì§€ê¸ˆ', 'í˜„ì¬', 'ì´ì œ', 'now', 'current', 'currently']
        for word in current_time_words:
            if word in content.lower():
                score += 0.1
                break
                
        return {
            "score": min(score, 1.0),
            "temporal_matches": temporal_matches,
            "has_current_context": any(word in content.lower() for word in current_time_words)
        }
    
    def _calculate_quality_score(self, quality_factors: Dict[str, Any], importance: float) -> float:
        """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {
            'length': 0.15,
            'richness': 0.25,
            'structure': 0.15,
            'language': 0.15,
            'information_density': 0.15,
            'searchability': 0.10,
            'temporal_relevance': 0.05
        }
        
        weighted_score = 0.0
        for factor, weight in weights.items():
            if factor in quality_factors:
                weighted_score += quality_factors[factor]['score'] * weight
        
        # ì‚¬ìš©ì ì¤‘ìš”ë„ë¥¼ ì•½ê°„ ë°˜ì˜
        final_score = weighted_score * 0.85 + importance * 0.15
        
        return min(final_score, 1.0)
    
    def _classify_quality_level(self, quality_score: float) -> QualityLevel:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë¶„ë¥˜"""
        if quality_score >= 0.9:
            return QualityLevel.EXCELLENT
        elif quality_score >= 0.7:
            return QualityLevel.GOOD
        elif quality_score >= 0.5:
            return QualityLevel.ACCEPTABLE
        elif quality_score >= 0.3:
            return QualityLevel.POOR
        else:
            return QualityLevel.VERY_POOR
    
    def _generate_suggestions(self, quality_factors: Dict[str, Any], content: str) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±"""
        suggestions = []
        
        # ê¸¸ì´ ê´€ë ¨ ì œì•ˆ
        length_factor = quality_factors.get('length', {})
        if length_factor.get('issue') == 'too_short':
            suggestions.append("ğŸ’¡ Content is too short. Add more context or details to make it more meaningful.")
        elif length_factor.get('issue') == 'too_long':
            suggestions.append("âœ‚ï¸ Content is very long. Consider breaking it into smaller, focused memories.")
        elif length_factor.get('issue') == 'minimal':
            suggestions.append("[NOTE] Content is quite brief. Adding more context would improve searchability.")
        
        # í’ë¶€ë„ ê´€ë ¨ ì œì•ˆ
        richness = quality_factors.get('richness', {})
        if richness.get('meaningful_word_ratio', 0) < 0.3:
            suggestions.append("ğŸ¯ Add more specific and meaningful details to increase content value.")
        
        # êµ¬ì¡° ê´€ë ¨ ì œì•ˆ
        structure = quality_factors.get('structure', {})
        if 'excessive_repetition' in structure.get('issues', []):
            suggestions.append("[PROCESS] Reduce repetitive content to improve clarity and conciseness.")
        if structure.get('sentence_count', 0) <= 1 and len(content) > 50:
            suggestions.append("ğŸ“– Break long content into multiple sentences for better readability.")
        
        # ê²€ìƒ‰ ê°€ëŠ¥ì„± ê´€ë ¨ ì œì•ˆ
        searchability = quality_factors.get('searchability', {})
        if searchability.get('potential_keywords', 0) < 2:
            suggestions.append("ğŸ” Include more specific keywords to improve future searchability.")
        
        # ì •ë³´ ë°€ë„ ê´€ë ¨ ì œì•ˆ
        info_density = quality_factors.get('information_density', {})
        if info_density.get('density', 0) < 0.1:
            suggestions.append("ğŸ“Š Add specific details like names, dates, or numbers to increase information value.")
        
        # ì¼ë°˜ì ì¸ í’ˆì§ˆ ì œì•ˆ
        if not suggestions:
            suggestions.append("âœ… Content quality looks good! Consider adding more context if relevant.")
        
        return suggestions[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
    
    def _should_store_memory(self, quality_score: float, quality_level: QualityLevel) -> bool:
        """ì €ì¥ ê¶Œì¥ ì—¬ë¶€ ê²°ì •"""
        if quality_level in [QualityLevel.EXCELLENT, QualityLevel.GOOD]:
            return True
        elif quality_level == QualityLevel.ACCEPTABLE:
            return True  # ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ
        elif quality_level == QualityLevel.POOR:
            return False  # í’ˆì§ˆì´ ë‚®ì•„ ì €ì¥ ë¹„ê¶Œì¥
        else:  # VERY_POOR
            return False
    
    def _adjust_importance(self, original_importance: float, quality_score: float) -> float:
        """í’ˆì§ˆì— ê¸°ë°˜í•œ ì¤‘ìš”ë„ ì¡°ì •"""
        # í’ˆì§ˆì´ ì¢‹ìœ¼ë©´ ì¤‘ìš”ë„ ìƒí–¥, ë‚˜ì˜ë©´ í•˜í–¥ ì¡°ì •
        adjustment_factor = (quality_score - 0.5) * 0.2  # -0.1 ~ +0.1 ë²”ìœ„
        adjusted = original_importance + adjustment_factor
        return max(0.0, min(1.0, adjusted))
    
    def _generate_warnings(self, quality_factors: Dict[str, Any], content: str) -> List[str]:
        """ê²½ê³ ì‚¬í•­ ìƒì„±"""
        warnings = []
        
        # ê¸¸ì´ ê²½ê³ 
        length_factor = quality_factors.get('length', {})
        if length_factor.get('issue') == 'too_short':
            warnings.append("âš ï¸ Content may be too brief to be useful for future reference.")
        
        # ì–¸ì–´ í’ˆì§ˆ ê²½ê³ 
        language_factor = quality_factors.get('language', {})
        if language_factor.get('score', 1.0) < 0.4:
            warnings.append("âš ï¸ Content may have formatting or language quality issues.")
        
        # ì •ë³´ ë°€ë„ ê²½ê³ 
        info_density = quality_factors.get('information_density', {})
        if info_density.get('density', 0) < 0.05:
            warnings.append("âš ï¸ Content appears to have low information density.")
        
        # ê²€ìƒ‰ ê°€ëŠ¥ì„± ê²½ê³ 
        searchability = quality_factors.get('searchability', {})
        if searchability.get('potential_keywords', 0) == 0:
            warnings.append("âš ï¸ Content may be difficult to search for in the future.")
        
        return warnings
    
    def _create_fallback_result(self, content: str, importance: float, error: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜"""
        return {
            "quality_score": 0.5,
            "quality_level": QualityLevel.ACCEPTABLE.value,
            "quality_factors": {"error": error},
            "suggestions": ["âš ï¸ Quality validation encountered an error. Manual review recommended."],
            "should_store": True,  # ì•ˆì „í•˜ê²Œ ì €ì¥ í—ˆìš©
            "adjusted_importance": importance,
            "warnings": [f"Quality validation error: {error}"],
            "timestamp": datetime.now().isoformat(),
            "validation_version": "2.1.0"
        }
    
    def validate_batch_memories(self, memories: List[Tuple[str, float]]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ë©”ëª¨ë¦¬ í’ˆì§ˆ ê²€ì¦"""
        results = []
        for content, importance in memories:
            result = self.validate_memory_quality(content, importance)
            results.append(result)
        return results
    
    def get_quality_statistics(self, validations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ í†µê³„ ìƒì„±"""
        if not validations:
            return {"error": "No validation results provided"}
        
        total_count = len(validations)
        quality_levels = {}
        total_score = 0.0
        should_store_count = 0
        
        for validation in validations:
            level = validation.get('quality_level', 'unknown')
            quality_levels[level] = quality_levels.get(level, 0) + 1
            total_score += validation.get('quality_score', 0.0)
            if validation.get('should_store', False):
                should_store_count += 1
        
        return {
            "total_validations": total_count,
            "average_quality_score": round(total_score / total_count, 3),
            "quality_level_distribution": quality_levels,
            "storage_recommendation_rate": round(should_store_count / total_count, 3),
            "generated_at": datetime.now().isoformat()
        }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    validator = QualityValidator()
    
    test_cases = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ í”„ë¡œì íŠ¸ íšŒì˜ì—ì„œ ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë ¸ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œì„ ìœ„í•´ Reactë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆê³ , ë°ì´í„°ë² ì´ìŠ¤ëŠ” PostgreSQLë¡œ ì„ ì •í–ˆìŠµë‹ˆë‹¤.",
        "!!!!!!!!!!!!",
        "Machine learning model training completed successfully with 95% accuracy on validation set. Model deployed to production environment at 2025-07-31 10:30 AM."
    ]
    
    print("âœ… QualityValidator module loaded successfully")
    print("ğŸ§ª Running test cases...")
    
    for i, content in enumerate(test_cases, 1):
        result = validator.validate_memory_quality(content)
        print(f"\nTest {i}: {content[:50]}...")
        print(f"Quality Score: {result['quality_score']}")
        print(f"Quality Level: {result['quality_level']}")
        print(f"Should Store: {result['should_store']}")
        print(f"Suggestions: {result['suggestions'][0] if result['suggestions'] else 'None'}")