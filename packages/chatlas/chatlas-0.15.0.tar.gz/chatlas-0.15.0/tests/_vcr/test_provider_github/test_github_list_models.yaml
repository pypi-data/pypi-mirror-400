interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
    method: GET
    uri: https://models.github.ai/catalog/models
  response:
    body:
      string: '[{"id":"openai/gpt-4.1","name":"OpenAI GPT-4.1","publisher":"OpenAI","summary":"gpt-4.1
        outperforms gpt-4o across the board, with major gains in coding, instruction
        following, and long-context understanding","rate_limit_tier":"high","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-04-14","capabilities":["agents","streaming","tool-calling","agentsV2"],"limits":{"max_input_tokens":1048576,"max_output_tokens":32768},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-4-1"},{"id":"openai/gpt-4.1-mini","name":"OpenAI
        GPT-4.1-mini","publisher":"OpenAI","summary":"gpt-4.1-mini outperform gpt-4o-mini
        across the board, with major gains in coding, instruction following, and long-context
        handling","rate_limit_tier":"low","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-04-14","capabilities":["agents","streaming","tool-calling","agentsV2"],"limits":{"max_input_tokens":1048576,"max_output_tokens":32768},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-4-1-mini"},{"id":"openai/gpt-4.1-nano","name":"OpenAI
        GPT-4.1-nano","publisher":"OpenAI","summary":"gpt-4.1-nano provides gains
        in coding, instruction following, and long-context handling along with lower
        latency and cost","rate_limit_tier":"low","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-04-14","capabilities":["agents","streaming","tool-calling","agentsV2"],"limits":{"max_input_tokens":1048576,"max_output_tokens":32768},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-4-1-nano"},{"id":"openai/gpt-4o","name":"OpenAI
        GPT-4o","publisher":"OpenAI","summary":"OpenAI''s most advanced multimodal
        model in the gpt-4o family. Can handle both text and image inputs.","rate_limit_tier":"high","supported_input_modalities":["text","image","audio"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2024-11-20","capabilities":["agents","assistants","streaming","tool-calling","agentsV2"],"limits":{"max_input_tokens":131072,"max_output_tokens":16384},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-4o"},{"id":"openai/gpt-4o-mini","name":"OpenAI
        GPT-4o mini","publisher":"OpenAI","summary":"An affordable, efficient AI solution
        for diverse text and image tasks.","rate_limit_tier":"low","supported_input_modalities":["text","image","audio"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2024-07-18","capabilities":["agents","assistants","streaming","tool-calling","agentsV2"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-4o-mini"},{"id":"openai/gpt-5","name":"OpenAI
        gpt-5","publisher":"OpenAI","summary":"gpt-5 is designed for logic-heavy and
        multi-step tasks. ","rate_limit_tier":"custom","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-08-07","capabilities":["agents","agentsV2","reasoning","tool-calling","streaming"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-5"},{"id":"openai/gpt-5-chat","name":"OpenAI
        gpt-5-chat (preview)","publisher":"OpenAI","summary":"gpt-5-chat (preview)
        is an advanced, natural, multimodal, and context-aware conversations for enterprise
        applications.","rate_limit_tier":"custom","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-10-03","capabilities":["agents","agentsV2","reasoning","tool-calling","streaming"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-5-chat"},{"id":"openai/gpt-5-mini","name":"OpenAI
        gpt-5-mini","publisher":"OpenAI","summary":"gpt-5-mini is a lightweight version
        for cost-sensitive applications.","rate_limit_tier":"custom","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-08-07","capabilities":["agents","agentsV2","reasoning","tool-calling","streaming"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-5-mini"},{"id":"openai/gpt-5-nano","name":"OpenAI
        gpt-5-nano","publisher":"OpenAI","summary":"gpt-5-nano is optimized for speed,
        ideal for applications requiring low latency. ","rate_limit_tier":"custom","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-08-07","capabilities":["agents","agentsV2","reasoning","tool-calling","streaming"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/gpt-5-nano"},{"id":"openai/o1","name":"OpenAI
        o1","publisher":"OpenAI","summary":"Focused on advanced reasoning and solving
        complex problems, including math and science tasks. Ideal for applications
        that require deep contextual understanding and agentic workflows.","rate_limit_tier":"custom","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["reasoning","multilingual","coding"],"registry":"azure-openai","version":"2024-12-17","capabilities":["agents","reasoning","tool-calling","agentsV2"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/o1"},{"id":"openai/o1-mini","name":"OpenAI
        o1-mini","publisher":"OpenAI","summary":"Smaller, faster, and 80% cheaper
        than o1-preview, performs well at code generation and small context operations.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","multilingual","coding"],"registry":"azure-openai","version":"2024-09-12","capabilities":["reasoning","streaming","agentsV2"],"limits":{"max_input_tokens":128000,"max_output_tokens":65536},"html_url":"https://github.com/marketplace/models/azure-openai/o1-mini"},{"id":"openai/o1-preview","name":"OpenAI
        o1-preview","publisher":"OpenAI","summary":"Focused on advanced reasoning
        and solving complex problems, including math and science tasks. Ideal for
        applications that require deep contextual understanding and agentic workflows.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","multilingual","coding"],"registry":"azure-openai","version":"1","capabilities":["agentsV2","reasoning"],"limits":{"max_input_tokens":128000,"max_output_tokens":32768},"html_url":"https://github.com/marketplace/models/azure-openai/o1-preview"},{"id":"openai/o3","name":"OpenAI
        o3","publisher":"OpenAI","summary":"o3 includes significant improvements on
        quality and safety while supporting the existing features of o1 and delivering
        comparable or better performance.","rate_limit_tier":"custom","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-04-16","capabilities":["agents","agentsV2","reasoning","streaming","tool-calling"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/o3"},{"id":"openai/o3-mini","name":"OpenAI
        o3-mini","publisher":"OpenAI","summary":"o3-mini includes the o1 features
        with significant cost-efficiencies for scenarios requiring high performance.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","multilingual","coding"],"registry":"azure-openai","version":"2025-01-31","capabilities":["agents","reasoning","streaming","tool-calling","agentsV2"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/o3-mini"},{"id":"openai/o4-mini","name":"OpenAI
        o4-mini","publisher":"OpenAI","summary":"o4-mini includes significant improvements
        on quality and safety while supporting the existing features of o3-mini and
        delivering comparable or better performance.","rate_limit_tier":"custom","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","multilingual","multimodal"],"registry":"azure-openai","version":"2025-04-16","capabilities":["agents","agentsV2","reasoning","tool-calling","streaming"],"limits":{"max_input_tokens":200000,"max_output_tokens":100000},"html_url":"https://github.com/marketplace/models/azure-openai/o4-mini"},{"id":"openai/text-embedding-3-large","name":"OpenAI
        Text Embedding 3 (large)","publisher":"OpenAI","summary":"Text-embedding-3
        series models are the latest and most capable embedding model from OpenAI.","rate_limit_tier":"embeddings","supported_input_modalities":["text"],"supported_output_modalities":["embeddings"],"tags":["rag"],"registry":"azure-openai","version":"1","capabilities":[],"limits":{"max_input_tokens":8191,"max_output_tokens":null},"html_url":"https://github.com/marketplace/models/azure-openai/text-embedding-3-large"},{"id":"openai/text-embedding-3-small","name":"OpenAI
        Text Embedding 3 (small)","publisher":"OpenAI","summary":"Text-embedding-3
        series models are the latest and most capable embedding model from OpenAI.","rate_limit_tier":"embeddings","supported_input_modalities":["text"],"supported_output_modalities":["embeddings"],"tags":["rag"],"registry":"azure-openai","version":"1","capabilities":[],"limits":{"max_input_tokens":8191,"max_output_tokens":null},"html_url":"https://github.com/marketplace/models/azure-openai/text-embedding-3-small"},{"id":"ai21-labs/ai21-jamba-1.5-large","name":"AI21
        Jamba 1.5 Large","publisher":"AI21 Labs","summary":"A 398B parameters (94B
        active) multilingual model, offering a 256K long context window, function
        calling, structured output, and grounded generation.","rate_limit_tier":"high","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["rag","multilingual","large
        context"],"registry":"azureml-ai21","version":"1","capabilities":["streaming","tool-calling"],"limits":{"max_input_tokens":262144,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-ai21/AI21-Jamba-1-5-Large"},{"id":"cohere/cohere-command-a","name":"Cohere
        Command A","publisher":"Cohere","summary":"Command A is a highly efficient
        generative model that excels at agentic and multilingual use cases.","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["rag","multilingual"],"registry":"azureml-cohere","version":"3","capabilities":[],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-cohere/cohere-command-a"},{"id":"cohere/cohere-command-r-08-2024","name":"Cohere
        Command R 08-2024","publisher":"Cohere","summary":"Command R is a scalable
        generative model targeting RAG and Tool Use to enable production-scale AI
        for enterprise.","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["rag","multilingual"],"registry":"azureml-cohere","version":"1","capabilities":["streaming"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-cohere/Cohere-command-r-08-2024"},{"id":"cohere/cohere-command-r-plus-08-2024","name":"Cohere
        Command R+ 08-2024","publisher":"Cohere","summary":"Command R+ is a state-of-the-art
        RAG-optimized model designed to tackle enterprise-grade workloads.","rate_limit_tier":"high","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["rag","multilingual"],"registry":"azureml-cohere","version":"1","capabilities":["streaming","tool-calling"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-cohere/Cohere-command-r-plus-08-2024"},{"id":"deepseek/deepseek-r1","name":"DeepSeek-R1","publisher":"DeepSeek","summary":"DeepSeek-R1
        excels at reasoning tasks using a step-by-step training process, such as language,
        scientific reasoning, and coding tasks.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","coding","agents"],"registry":"azureml-deepseek","version":"1","capabilities":["reasoning","streaming","tool-calling"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-deepseek/DeepSeek-R1"},{"id":"deepseek/deepseek-r1-0528","name":"DeepSeek-R1-0528","publisher":"DeepSeek","summary":"The
        DeepSeek R1 0528 model has improved reasoning capabilities, this version also
        offers a reduced hallucination rate, enhanced support for function calling,
        and better experience for vibe coding.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","coding","agents"],"registry":"azureml-deepseek","version":"1","capabilities":["agentsV2","reasoning","streaming","tool-calling"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-deepseek/DeepSeek-R1-0528"},{"id":"deepseek/deepseek-v3-0324","name":"DeepSeek-V3-0324","publisher":"DeepSeek","summary":"DeepSeek-V3-0324
        demonstrates notable improvements over its predecessor, DeepSeek-V3, in several
        key aspects, including enhanced reasoning, improved function calling, and
        superior code generation capabilities.","rate_limit_tier":"high","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["coding","agents"],"registry":"azureml-deepseek","version":"1","capabilities":["agentsV2","streaming","tool-calling"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-deepseek/DeepSeek-V3-0324"},{"id":"meta/llama-3.2-11b-vision-instruct","name":"Llama-3.2-11B-Vision-Instruct","publisher":"Meta","summary":"Excels
        in image reasoning capabilities on high-res images for visual understanding
        apps.","rate_limit_tier":"low","supported_input_modalities":["text","image","audio"],"supported_output_modalities":["text"],"tags":["multimodal","reasoning","conversation"],"registry":"azureml-meta","version":"6","capabilities":["streaming"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-meta/Llama-3-2-11B-Vision-Instruct"},{"id":"meta/llama-3.2-90b-vision-instruct","name":"Llama-3.2-90B-Vision-Instruct","publisher":"Meta","summary":"Advanced
        image reasoning capabilities for visual understanding agentic apps.","rate_limit_tier":"high","supported_input_modalities":["text","image","audio"],"supported_output_modalities":["text"],"tags":["multimodal","reasoning","conversation"],"registry":"azureml-meta","version":"5","capabilities":["streaming"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-meta/Llama-3-2-90B-Vision-Instruct"},{"id":"meta/llama-3.3-70b-instruct","name":"Llama-3.3-70B-Instruct","publisher":"Meta","summary":"Llama
        3.3 70B Instruct offers enhanced reasoning, math, and instruction following
        with performance comparable to Llama 3.1 405B.","rate_limit_tier":"high","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["conversation"],"registry":"azureml-meta","version":"8","capabilities":["agentsV2","streaming"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-meta/Llama-3-3-70B-Instruct"},{"id":"meta/llama-4-maverick-17b-128e-instruct-fp8","name":"Llama
        4 Maverick 17B 128E Instruct FP8","publisher":"Meta","summary":"Llama 4 Maverick
        17B 128E Instruct FP8 is great at precise image understanding and creative
        writing, offering high quality at a lower price compared to Llama 3.3 70B","rate_limit_tier":"high","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multimodal","conversation","multilingual"],"registry":"azureml-meta","version":"1","capabilities":["agents","agentsV2","assistants","streaming","tool-calling"],"limits":{"max_input_tokens":1000000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-meta/Llama-4-Maverick-17B-128E-Instruct-FP8"},{"id":"meta/llama-4-scout-17b-16e-instruct","name":"Llama
        4 Scout 17B 16E Instruct","publisher":"Meta","summary":"Llama 4 Scout 17B
        16E Instruct is great at multi-document summarization, parsing extensive user
        activity for personalized tasks, and reasoning over vast codebases.","rate_limit_tier":"high","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multimodal","conversation","multilingual"],"registry":"azureml-meta","version":"2","capabilities":["agents","assistants","streaming","tool-calling"],"limits":{"max_input_tokens":10000000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-meta/Llama-4-Scout-17B-16E-Instruct"},{"id":"meta/meta-llama-3.1-405b-instruct","name":"Meta-Llama-3.1-405B-Instruct","publisher":"Meta","summary":"The
        Llama 3.1 instruction tuned text only models are optimized for multilingual
        dialogue use cases and outperform many of the available open source and closed
        chat models on common industry benchmarks.","rate_limit_tier":"high","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["conversation"],"registry":"azureml-meta","version":"1","capabilities":["agents"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-meta/Meta-Llama-3-1-405B-Instruct"},{"id":"meta/meta-llama-3.1-8b-instruct","name":"Meta-Llama-3.1-8B-Instruct","publisher":"Meta","summary":"The
        Llama 3.1 instruction tuned text only models are optimized for multilingual
        dialogue use cases and outperform many of the available open source and closed
        chat models on common industry benchmarks.","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["conversation"],"registry":"azureml-meta","version":"5","capabilities":["streaming"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-meta/Meta-Llama-3-1-8B-Instruct"},{"id":"mistral-ai/codestral-2501","name":"Codestral
        25.01","publisher":"Mistral AI","summary":"Codestral 25.01 by Mistral AI is
        designed for code generation, supporting 80+ programming languages, and optimized
        for tasks like code completion and fill-in-the-middle","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","coding"],"registry":"azureml-mistral","version":"2","capabilities":["streaming"],"limits":{"max_input_tokens":256000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-mistral/Codestral-2501"},{"id":"mistral-ai/ministral-3b","name":"Ministral
        3B","publisher":"Mistral AI","summary":"Ministral 3B is a state-of-the-art
        Small Language Model (SLM) optimized for edge computing and on-device applications.
        As it is designed for low-latency and compute-efficient inference, it it also
        the perfect model for standard GenAI applications that have","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["low
        latency","agents","reasoning"],"registry":"azureml-mistral","version":"1","capabilities":["streaming","tool-calling"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-mistral/Ministral-3B"},{"id":"mistral-ai/mistral-medium-2505","name":"Mistral
        Medium 3 (25.05)","publisher":"Mistral AI","summary":"Mistral Medium 3 is
        an advanced Large Language Model (LLM) with state-of-the-art reasoning, knowledge,
        coding and vision capabilities.","rate_limit_tier":"low","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","vision"],"registry":"azureml-mistral","version":"1","capabilities":["streaming","tool-calling"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-mistral/mistral-medium-2505"},{"id":"mistral-ai/mistral-small-2503","name":"Mistral
        Small 3.1","publisher":"Mistral AI","summary":"Enhanced Mistral Small 3 with
        multimodal capabilities and a 128k context length.","rate_limit_tier":"low","supported_input_modalities":["text","image"],"supported_output_modalities":["text"],"tags":["multipurpose","vision","multimodal"],"registry":"azureml-mistral","version":"1","capabilities":["agents","assistants","streaming","tool-calling"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-mistral/mistral-small-2503"},{"id":"xai/grok-3","name":"Grok
        3","publisher":"xAI","summary":"Grok 3 is xAI''s debut model, pretrained by
        Colossus at supermassive scale to excel in specialized domains like finance,
        healthcare, and the law.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["understanding","instruction","summarization"],"registry":"azureml-xai","version":"1","capabilities":["agentsV2"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-xai/grok-3"},{"id":"xai/grok-3-mini","name":"Grok
        3 Mini","publisher":"xAI","summary":"Grok 3 Mini is a lightweight model that
        thinks before responding. Trained on mathematic and scientific problems, it
        is great for logic-based tasks.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["agents","reasoning","coding"],"registry":"azureml-xai","version":"1","capabilities":["agentsV2"],"limits":{"max_input_tokens":131072,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml-xai/grok-3-mini"},{"id":"microsoft/mai-ds-r1","name":"MAI-DS-R1","publisher":"Microsoft","summary":"MAI-DS-R1
        is a DeepSeek-R1 reasoning model that has been post-trained by the Microsoft
        AI team to fill in information gaps in the previous version of the model and
        improve its harm protections while maintaining R1 reasoning capabilities.","rate_limit_tier":"custom","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","coding","agents"],"registry":"azureml","version":"1","capabilities":["agentsV2","reasoning","streaming"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml/MAI-DS-R1"},{"id":"microsoft/phi-4","name":"Phi-4","publisher":"Microsoft","summary":"Phi-4
        14B, a highly capable model for low latency scenarios.","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","understanding","low
        latency"],"registry":"azureml","version":"8","capabilities":[],"limits":{"max_input_tokens":16384,"max_output_tokens":16384},"html_url":"https://github.com/marketplace/models/azureml/Phi-4"},{"id":"microsoft/phi-4-mini-instruct","name":"Phi-4-mini-instruct","publisher":"Microsoft","summary":"3.8B
        parameters Small Language Model outperforming larger models in reasoning,
        math, coding, and function-calling","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","agents","multilingual"],"registry":"azureml","version":"1","capabilities":[],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml/Phi-4-mini-instruct"},{"id":"microsoft/phi-4-mini-reasoning","name":"Phi-4-mini-reasoning","publisher":"Microsoft","summary":"Lightweight
        math reasoning model optimized for multi-step problem solving","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","large
        context","low latency"],"registry":"azureml","version":"1","capabilities":["reasoning"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml/Phi-4-mini-reasoning"},{"id":"microsoft/phi-4-multimodal-instruct","name":"Phi-4-multimodal-instruct","publisher":"Microsoft","summary":"First
        small multimodal model to have 3 modality inputs (text, audio, image), excelling
        in quality and efficiency","rate_limit_tier":"low","supported_input_modalities":["audio","image","text"],"supported_output_modalities":["text"],"tags":["vision","audio","summarization"],"registry":"azureml","version":"2","capabilities":["streaming"],"limits":{"max_input_tokens":128000,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml/Phi-4-multimodal-instruct"},{"id":"microsoft/phi-4-reasoning","name":"Phi-4-reasoning","publisher":"Microsoft","summary":"State-of-the-art
        open-weight reasoning model.","rate_limit_tier":"low","supported_input_modalities":["text"],"supported_output_modalities":["text"],"tags":["reasoning","large
        context","low latency"],"registry":"azureml","version":"1","capabilities":["reasoning","streaming"],"limits":{"max_input_tokens":32768,"max_output_tokens":4096},"html_url":"https://github.com/marketplace/models/azureml/Phi-4-reasoning"}]'
    headers:
      Accept-Ranges:
      - bytes
      Cache-Control:
      - max-age=0, private, must-revalidate
      Content-Security-Policy:
      - 'default-src ''none''; base-uri ''self''; child-src github.githubassets.com
        github.com/assets-cdn/worker/ github.com/assets/ gist.github.com/assets-cdn/worker/;
        connect-src ''self'' uploads.github.com www.githubstatus.com collector.github.com
        raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com
        github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com
        *.rel.tunnels.api.visualstudio.com wss://*.rel.tunnels.api.visualstudio.com
        github.githubassets.com objects-origin.githubusercontent.com copilot-proxy.githubusercontent.com
        proxy.individual.githubcopilot.com proxy.business.githubcopilot.com proxy.enterprise.githubcopilot.com
        *.actions.githubusercontent.com wss://*.actions.githubusercontent.com productionresultssa0.blob.core.windows.net/
        productionresultssa1.blob.core.windows.net/ productionresultssa2.blob.core.windows.net/
        productionresultssa3.blob.core.windows.net/ productionresultssa4.blob.core.windows.net/
        productionresultssa5.blob.core.windows.net/ productionresultssa6.blob.core.windows.net/
        productionresultssa7.blob.core.windows.net/ productionresultssa8.blob.core.windows.net/
        productionresultssa9.blob.core.windows.net/ productionresultssa10.blob.core.windows.net/
        productionresultssa11.blob.core.windows.net/ productionresultssa12.blob.core.windows.net/
        productionresultssa13.blob.core.windows.net/ productionresultssa14.blob.core.windows.net/
        productionresultssa15.blob.core.windows.net/ productionresultssa16.blob.core.windows.net/
        productionresultssa17.blob.core.windows.net/ productionresultssa18.blob.core.windows.net/
        productionresultssa19.blob.core.windows.net/ github-production-repository-image-32fea6.s3.amazonaws.com
        github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com
        wss://alive.github.com wss://alive-staging.github.com api.githubcopilot.com
        api.individual.githubcopilot.com api.business.githubcopilot.com api.enterprise.githubcopilot.com;
        font-src github.githubassets.com; form-action ''self'' github.com gist.github.com
        copilot-workspace.githubnext.com objects-origin.githubusercontent.com; frame-ancestors
        ''none''; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com;
        img-src ''self'' data: blob: github.githubassets.com media.githubusercontent.com
        camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com
        private-avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com
        release-assets.githubusercontent.com secured-user-images.githubusercontent.com/
        user-images.githubusercontent.com/ private-user-images.githubusercontent.com
        opengraph.githubassets.com marketplace-screenshots.githubusercontent.com/
        copilotprodattachments.blob.core.windows.net/github-production-copilot-attachments/
        github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com
        spotlights-feed.github.com objects-origin.githubusercontent.com *.githubusercontent.com;
        manifest-src ''self''; media-src github.com user-images.githubusercontent.com/
        secured-user-images.githubusercontent.com/ private-user-images.githubusercontent.com
        github-production-user-asset-6210df.s3.amazonaws.com gist.github.com github.githubassets.com;
        script-src github.githubassets.com; style-src ''unsafe-inline'' github.githubassets.com;
        upgrade-insecure-requests; worker-src github.githubassets.com github.com/assets-cdn/worker/
        github.com/assets/ gist.github.com/assets-cdn/worker/'
      Content-Type:
      - application/json; charset=utf-8
      Date:
      - Wed, 31 Dec 2025 20:40:05 GMT
      ETag:
      - W/"e466693951a9c5ca75f789f5bf8a3464"
      Referrer-Policy:
      - origin-when-cross-origin, strict-origin-when-cross-origin
      Server:
      - github.com
      Strict-Transport-Security:
      - max-age=31536000; includeSubdomains; preload
      Transfer-Encoding:
      - chunked
      Vary:
      - X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, X-Requested-With,Accept-Encoding,
        Accept, X-Requested-With
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - deny
      X-GitHub-Request-Id:
      - D82C:305E60:52AD031:69B522D:69558A25
      X-XSS-Protection:
      - '0'
      x-github-backend:
      - Kubernetes
    status:
      code: 200
      message: OK
version: 1
