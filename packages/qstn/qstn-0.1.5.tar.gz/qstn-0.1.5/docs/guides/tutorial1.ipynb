{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68673fba",
   "metadata": {},
   "source": [
    "# Tutorial 1: Multiple Choice Questionnaires\n",
    "\n",
    "Often when inferencing LLMs, e.g., when annotating data into labels, we want to have answers to multiple choice questions or we want models to give answers on a scale. QSTN supports various checks to ensure that the output of the model is robust. In this tutorial we will take a look at how to use QSTN to produce robust results and how to use different response generation methods, such as restricting the model output, logprobs and open answer formats.\n",
    "\n",
    "\n",
    "For now we keep our example from the [quickstart tutorial](https://qstn.readthedocs.io/en/latest/quickstart.html) but we want our models to answer on a Likert Scale from \"1: Strongly Dislike to \"5: Strongly Like\".\n",
    "\n",
    "\n",
    "Let's setup our two questionnaires again with our two different personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca191fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import LLMPrompt\n",
    "from qstn.utilities import placeholder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "personas = {\n",
    "    \"New York\": \"a black middle aged man from New York\",\n",
    "    \"Texas\": \"a white middle aged man from Texas\",\n",
    "}\n",
    "\n",
    "system_prompt = \"Act as if you were {persona}!\"\n",
    "prompt = f\"Please tell us how you feel about the following parties:\\n{placeholder.PROMPT_QUESTIONS}\"\n",
    "\n",
    "\n",
    "questionnaire = [\n",
    "    {\"questionnaire_item_id\": 1, \"question_content\": \"The Republican Party?\"},\n",
    "    {\"questionnaire_item_id\": 2, \"question_content\": \"The Democratic Party?\"},\n",
    "]\n",
    "\n",
    "party_questionnaire = pd.DataFrame(questionnaire)\n",
    "\n",
    "questionnaires: list[LLMPrompt] = []\n",
    "\n",
    "for name, persona in personas.items():\n",
    "    questionnaires.append(\n",
    "        LLMPrompt(\n",
    "            questionnaire_name=name,\n",
    "            questionnaire_source=party_questionnaire,\n",
    "            system_prompt=system_prompt.format_map({\"persona\": persona}),\n",
    "            prompt=prompt,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b098fc",
   "metadata": {},
   "source": [
    "So far we have not specified how the LLM should answer so it will be a free text answer. The first thing we can now adjust is to simply prompt the model to only respond in a very specific way. QSTN allows us to do this in a controlled manner, so we can later easily adjust our prompts or our response generation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0e36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import generate_likert_options\n",
    "\n",
    "likert_scale = [\n",
    "    \"Strongly Dislike\",\n",
    "    \"Dislike\",\n",
    "    \"Neiter Dislike nor Like\",\n",
    "    \"Like\",\n",
    "    \"Strongly Like\",\n",
    "]\n",
    "\n",
    "options = generate_likert_options(\n",
    "    n=5,\n",
    "    answer_texts=likert_scale,\n",
    "    random_order=False,\n",
    "    reversed_order=False,\n",
    "    idx_type=\"integer\",\n",
    "    options_separator=\"|\",\n",
    "    only_from_to_scale=False,\n",
    "    start_idx=1,\n",
    "    list_prompt_template=\"Only respond with one of the following {options}.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5506d2",
   "metadata": {},
   "source": [
    "\n",
    "With QSTN we can easily create a likert scala. If we want to control for prompt pertubations, for example random or reverse order of options, we simply set a flag to change them. \n",
    "\n",
    "For now let's keep the options as they are. To include these options in the prompt we simply can use the ``prepare_questionnaire`` function of ``LLMPrompt`` to specify how our questions should be asked. We can specify placeholders to define at which point in the prompt the options or questions should be specified. \n",
    "\n",
    "Note that ``placeholder.PROMPT_OPTIONS`` can also be specified in the system prompt or prompt above, if we do want to place it independently from the questions. Again we can simply set a flag, if we want our questions to be in a random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af0840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.utilities import placeholder\n",
    "\n",
    "for questionnaire in questionnaires:\n",
    "    questionnaire.prepare_prompt(\n",
    "        question_stem=f\"How do you feel towards {placeholder.QUESTION_CONTENT} {placeholder.PROMPT_OPTIONS}\",\n",
    "        answer_options=options,\n",
    "        randomized_item_order=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b852ef",
   "metadata": {},
   "source": [
    "Let's take a look at our current prompts. Depending on how we present the questionnaire to the model, the prompts will contain all questions or just one of them. For now let's assume we give each question in a new context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae59565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act as if you were a black middle aged man from New York!\n",
      "Please tell us how you feel about the following parties:\n",
      "How do you feel towards The Republican Party? Only respond with one of the following 1: Strongly Dislike|2: Dislike|3: Neiter Dislike nor Like|4: Like|5: Strongly Like.\n"
     ]
    }
   ],
   "source": [
    "system_prompt, prompt = questionnaires[0].get_prompt_for_questionnaire_type()\n",
    "print(system_prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5972bbf",
   "metadata": {},
   "source": [
    "Looks good! Now we prompt the model to only respond with one of the possible answers. Let's see how a small Llama model responds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model = LLM(model_id, max_model_len=5000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import survey_manager\n",
    "\n",
    "results = survey_manager.conduct_survey_single_item(\n",
    "    model,\n",
    "    questionnaire,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=False,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fccfb97",
   "metadata": {},
   "source": [
    "We get the following output for our \"survey participants\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33621a7",
   "metadata": {},
   "source": [
    "|questionnaire_name|questionnaire_item_id|question                                                                                                                                                                        |llm_response                                                                               |logprobs|reasoning|\n",
    "|--------------|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|--------|---------|\n",
    "|New York      |2                |How do you feel towards The Republican Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|Ain't no party for me, I go with Neutral Dislike.                                          |        |         |\n",
    "|New York      |1                |How do you feel towards The Democratic Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|Ain't no party I like more than da Democratic Party, we stand up for da people, ya hear me?|        |         |\n",
    "|Texas         |2                |How do you feel towards The Republican Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|I'd say I Like it, partner, 'cause we seem to be alignin' on a whole lotta issues.         |        |         |\n",
    "|Texas         |1                |How do you feel towards The Democratic Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|I'd say 3: Neither Like nor Dislike.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86731a",
   "metadata": {},
   "source": [
    "Well, this is not what we wanted. The small model took the \"Only answer with exactly\" quite liberally, sometimes not even including one of the labels, making it quite hard to interpret the output. We could now go three ways, if we do not want to involve manual labor or finetuning: \n",
    "1. *Try to find a different prompt.* Generally this is not preferable, because while it might be possible to find a prompt that works for this specific case, it might not work for a different model/different setting.\n",
    "2. *Use LLM-as-a-judge.* We can give the output to another LLM and try to get our labels this way.\n",
    "3. *Use Guided Decoding.* We can restrict the models output to ensure that our models only respond in a certain way.\n",
    "\n",
    "### Restricting Model Output\n",
    "\n",
    "QSTN supports all of these methods. For now, let's take a look at restricting the models output. For this we can simply give another option to the ``SurveyOptionGenerator`` we used before and prepare the questionnaires again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c84bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.inference import ChoiceResponseGenerationMethod\n",
    "from qstn import utilities\n",
    "\n",
    "choice_rgm = ChoiceResponseGenerationMethod(\n",
    "    utilities.constants.OPTIONS_ADJUST, output_index_only=False\n",
    ")\n",
    "\n",
    "options = generate_likert_options(\n",
    "    n=5,\n",
    "    answer_texts=likert_scale,\n",
    "    response_generation_method=choice_rgm, # We add our Response Generation\n",
    "    random_order=False,\n",
    "    reversed_order=False,\n",
    "    idx_type=\"integer\",\n",
    "    options_separator=\"|\",\n",
    "    only_from_to_scale=False,\n",
    "    start_idx=1,\n",
    "    list_prompt_template=\"Only respond with exactly one of the following {options}.\",\n",
    ")\n",
    "\n",
    "for questionnaire in questionnaires:\n",
    "    questionnaire.prepare_prompt(\n",
    "        question_stem=f\"How do you feel towards {placeholder.QUESTION_CONTENT} {placeholder.PROMPT_OPTIONS}\",\n",
    "        answer_options=options,\n",
    "        randomized_item_order=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6113a4",
   "metadata": {},
   "source": [
    "We get the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9caf832",
   "metadata": {},
   "source": [
    "|questionnaire_name|questionnaire_item_id|question                                                                                                                                                                        |llm_response                                                                               |logprobs|reasoning|\n",
    "|--------------|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|--------|---------|\n",
    "|New York      |1                |How do you feel towards The Democratic Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|2: Dislike                                                                                 |        |         |\n",
    "|New York      |2                |How do you feel towards The Republican Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|2: Dislike                                                                                 |        |         |\n",
    "|Texas         |1                |How do you feel towards The Democratic Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|2: Dislike                                                                                 |        |         |\n",
    "|Texas         |2                |How do you feel towards The Republican Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|3: Neiter Dislike nor Like      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8b661",
   "metadata": {},
   "source": [
    "While we now get exactly the output format that we specified, the answers themselves are not really reflecting the answers that we got from our free text responses from before. One way to get more meaningful output, especially with smaller models, is to first let the model generate a reasoning string and only then answer with one of the options. If we still want to easily parse the answers, we can instruct and restrict the model to only answer in JSON.\n",
    "\n",
    "For this we have to slightly modify our prompt, but QSTN offers easy automatic adjustment depending on the output method via placeholders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.inference import (\n",
    "    JSONReasoningResponseGenerationMethod,\n",
    ")\n",
    "\n",
    "# We now use a new placeholder to automatically adjust for our output format\n",
    "for questionnaire in questionnaires:\n",
    "    questionnaire.prompt = f\"Please tell us how you feel about the following parties:\\n{placeholder.PROMPT_QUESTIONS}\\n{placeholder.PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS}\"\n",
    "\n",
    "\n",
    "# Define our Response Generation Method (If we want to, we can also adjust our automatic output instructions here)\n",
    "reasoning_rgm = JSONReasoningResponseGenerationMethod()\n",
    "\n",
    "# Adjust the options to now include the reasoning\n",
    "options = generate_likert_options(\n",
    "    n=5,\n",
    "    answer_texts=likert_scale,\n",
    "    response_generation_method=reasoning_rgm, # We add our Response Generation\n",
    "    random_order=False,\n",
    "    reversed_order=False,\n",
    "    idx_type=\"integer\",\n",
    "    options_separator=\"|\",\n",
    "    only_from_to_scale=False,\n",
    "    start_idx=1,\n",
    "    list_prompt_template=\"These are the options: {options}.\",\n",
    ")\n",
    "\n",
    "for questionnaire in questionnaires:\n",
    "    questionnaire.prepare_questionnaire(\n",
    "        question_stem=f\"How do you feel towards {placeholder.QUESTION_CONTENT} {placeholder.PROMPT_OPTIONS}\",\n",
    "        answer_options=options,\n",
    "        randomized_item_order=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f7a5a",
   "metadata": {},
   "source": [
    "Finally we get our following prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cd5fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act as if you were a black middle aged man from New York!\n",
      "Please tell us how you feel about the following parties:\n",
      "How do you feel towards The Democratic Party? These are the options: 1: Strongly Dislike|2: Dislike|3: Neiter Dislike nor Like|4: Like|5: Strongly Like.\n",
      "You always reason about the possible answer options first.\n",
      "You respond with your reasoning and the most probable answer option in the following JSON format:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": <your reasoning about the answer options>,\n",
      "  \"answer\": <1: Strongly Dislike, 2: Dislike, 3: Neiter Dislike nor Like, 4: Like, 5: Strongly Like>\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_prompt, prompt = questionnaires[0].get_prompt_for_questionnaire_type()\n",
    "print(system_prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eee04b",
   "metadata": {},
   "source": [
    "We run our survey again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = survey_manager.conduct_survey_single_item(\n",
    "    model,\n",
    "    questionnaire,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=False,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41287f6a",
   "metadata": {},
   "source": [
    "QSTN allows us to easily parse the json output of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import parser\n",
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "parsed_results = parser.parse_json(results)\n",
    "df = create_one_dataframe(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde91e7f",
   "metadata": {},
   "source": [
    "And now we get answers that are more aligned with the free text answers from before and are also easily parsable. If you are interested in which Response Generation Methods reflect public opinion most closely and which are most efficient to use, we encourage you to check out this paper: [Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models](https://arxiv.org/abs/2510.11586).\n",
    "\n",
    "|questionnaire_name|questionnaire_item_id|question                                                                                                                                                                        |reasoning                                                                                  |answer                    |\n",
    "|--------------|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|--------------------------|\n",
    "|New York      |2                |How do you feel towards The Republican Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|The Republican Party's views on issues like income inequality and access to healthcare don't align with my experiences and values as a black New Yorker, making it unlikely I'd support them strongly, but I also don't feel strongly opposed to their existence.|3: Neiter Dislike nor Like|\n",
    "|New York      |1                |How do you feel towards The Democratic Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|The Democratic Party has historically represented the values and priorities of my community, particularly on issues like social justice, education, and economic equality, making it more likely that I'd have a strong positive association with the party.|5: Strongly Like          |\n",
    "|Texas         |2                |How do you feel towards The Republican Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|The Republican Party aligns more closely with my conservative values and the politics of Texas, where I reside, so I'd likely favor the party.|4: Like                   |\n",
    "|Texas         |1                |How do you feel towards The Democratic Party? Only respond with exactly one of the following 1: Strongly Dislike&#124;2: Dislike&#124;3: Neiter Dislike nor Like&#124;4: Like&#124;5: Strongly Like.|The Democratic Party's views on issues like gun control, climate change, and taxation are often at odds with my own conservative values, so I'm leanin' towards dislikin' 'em, but not strongly dislikin' 'em, since I still believe we can have a civil discussion about our differences|2: Dislike                |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qstn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
