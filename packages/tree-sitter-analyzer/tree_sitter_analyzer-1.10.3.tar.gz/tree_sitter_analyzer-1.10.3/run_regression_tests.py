#!/usr/bin/env python3
"""Regression test runner for async QueryService fix"""

import json
import subprocess
import sys
import time
from pathlib import Path


class RegressionTestRunner:
    """å›å¸°ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.passed = 0
        self.failed = 0
        self.skipped = 0
        self.results = []

    def run_command(self, cmd, description, timeout=60):
        """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã¨ãƒ­ã‚°å‡ºåŠ›"""
        print(f"\nğŸ”§ {description}")
        print(f"Command: {' '.join(cmd)}")

        start_time = time.time()
        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=timeout, cwd=Path.cwd()
            )
            duration = time.time() - start_time

            if result.returncode == 0:
                print(f"âœ… {description} passed ({duration:.2f}s)")
                self.passed += 1
                self.results.append(
                    {
                        "test": description,
                        "status": "PASSED",
                        "duration": duration,
                        "output": result.stdout[:500] if result.stdout else "",
                    }
                )
                return True
            else:
                print(f"âŒ {description} failed ({duration:.2f}s)")
                print(f"STDOUT: {result.stdout[:500]}")
                print(f"STDERR: {result.stderr[:500]}")
                self.failed += 1
                self.results.append(
                    {
                        "test": description,
                        "status": "FAILED",
                        "duration": duration,
                        "stdout": result.stdout[:500],
                        "stderr": result.stderr[:500],
                    }
                )
                return False

        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            print(f"â° {description} timed out ({duration:.2f}s)")
            self.failed += 1
            self.results.append(
                {"test": description, "status": "TIMEOUT", "duration": duration}
            )
            return False
        except Exception as e:
            duration = time.time() - start_time
            print(f"ğŸ’¥ {description} error: {e} ({duration:.2f}s)")
            self.failed += 1
            self.results.append(
                {
                    "test": description,
                    "status": "ERROR",
                    "duration": duration,
                    "error": str(e),
                }
            )
            return False

    def run_pytest_test(self, test_path, description, markers="", timeout=120):
        """pytestãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        cmd = ["python", "-m", "pytest", test_path, "-v", "--tb=short"]
        if markers:
            cmd.extend(["-m", markers])

        return self.run_command(cmd, description, timeout)

    def check_dependencies(self):
        """ä¾å­˜é–¢ä¿‚ã®ç¢ºèª"""
        print("ğŸ” Checking dependencies...")

        dependencies = [
            (["python", "-c", "import pytest"], "pytest availability"),
            (["python", "-c", "import asyncio"], "asyncio availability"),
            (
                ["python", "-c", "import tree_sitter_analyzer"],
                "tree_sitter_analyzer import",
            ),
            (
                [
                    "python",
                    "-c",
                    "from tree_sitter_analyzer.core.query_service import QueryService",
                ],
                "QueryService import",
            ),
            (
                [
                    "python",
                    "-c",
                    "from tree_sitter_analyzer.mcp.tools.query_tool import QueryTool",
                ],
                "QueryTool import",
            ),
        ]

        for cmd, desc in dependencies:
            if not self.run_command(cmd, desc, timeout=10):
                print(f"âŒ Dependency check failed: {desc}")
                return False

        print("âœ… All dependencies available")
        return True

    def run_new_async_tests(self):
        """æ–°è¦éåŒæœŸãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("\nğŸ“‹ Running new async tests...")

        tests = [
            ("tests/test_async_query_service.py", "Async QueryService tests"),
            ("tests/test_cli_async_integration.py", "CLI async integration tests"),
            ("tests/test_mcp_async_integration.py", "MCP async integration tests"),
            ("tests/test_async_performance.py", "Async performance tests"),
        ]

        all_passed = True
        for test_path, description in tests:
            if Path(test_path).exists():
                if not self.run_pytest_test(test_path, description):
                    all_passed = False
            else:
                print(f"âš ï¸ Test file not found: {test_path}")
                self.skipped += 1

        return all_passed

    def run_existing_core_tests(self):
        """æ—¢å­˜ã®ã‚³ã‚¢ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("\nğŸ“‹ Running existing core tests...")

        # é‡è¦ãªæ—¢å­˜ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        core_tests = [
            ("tests/test_query_service.py", "Core QueryService tests"),
            (
                "tests/test_mcp_query_tool_definition.py",
                "MCP QueryTool definition tests",
            ),
            ("tests/test_mcp_server.py", "MCP server tests"),
            ("tests/test_engine.py", "Analysis engine tests"),
            ("tests/test_main_entry.py", "Main entry tests"),
        ]

        all_passed = True
        for test_path, description in core_tests:
            if Path(test_path).exists():
                if not self.run_pytest_test(test_path, description):
                    all_passed = False
            else:
                print(f"âš ï¸ Core test file not found: {test_path}")
                self.skipped += 1

        return all_passed

    def run_integration_tests(self):
        """çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("\nğŸ“‹ Running integration tests...")

        integration_tests = [
            ("tests/test_mcp_tools_integration.py", "MCP tools integration tests"),
            ("tests/test_tree_sitter_integration.py", "Tree-sitter integration tests"),
        ]

        all_passed = True
        for test_path, description in integration_tests:
            if Path(test_path).exists():
                if not self.run_pytest_test(test_path, description):
                    all_passed = False
            else:
                print(f"âš ï¸ Integration test file not found: {test_path}")
                self.skipped += 1

        return all_passed

    def run_emergency_fix_verification(self):
        """ç·Šæ€¥ä¿®æ­£ã®æ¤œè¨¼"""
        print("\nğŸ“‹ Running emergency fix verification...")

        if Path("test_emergency_fix.py").exists():
            return self.run_command(
                ["python", "test_emergency_fix.py"],
                "Emergency fix verification",
                timeout=30,
            )
        else:
            print("âš ï¸ Emergency fix test not found")
            self.skipped += 1
            return True

    def run_sample_queries(self):
        """ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ“‹ Running sample query tests...")

        sample_tests = [
            (
                [
                    "python",
                    "-m",
                    "tree_sitter_analyzer",
                    "query",
                    "--file-path",
                    "examples/sample.py",
                    "--query-key",
                    "function",
                ],
                "Sample Python function query",
            ),
            (
                [
                    "python",
                    "-m",
                    "tree_sitter_analyzer",
                    "query",
                    "--file-path",
                    "examples/ModernJavaScript.js",
                    "--query-key",
                    "function",
                ],
                "Sample JavaScript function query",
            ),
        ]

        all_passed = True
        for cmd, description in sample_tests:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
            file_path = cmd[cmd.index("--file-path") + 1]
            if Path(file_path).exists():
                if not self.run_command(cmd, description, timeout=30):
                    all_passed = False
            else:
                print(f"âš ï¸ Sample file not found: {file_path}")
                self.skipped += 1

        return all_passed

    def run_comprehensive_test_suite(self):
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ"""
        print("\nğŸ“‹ Running comprehensive test suite...")

        # å…¨ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆå¤±æ•—æ™‚ã«åœæ­¢ã—ãªã„ï¼‰
        return self.run_command(
            ["python", "-m", "pytest", "tests/", "--tb=short", "-x"],
            "Comprehensive test suite",
            timeout=300,  # 5åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )

    def generate_report(self):
        """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        print("\nğŸ“Š Regression test results:")
        print(f"âœ… Passed: {self.passed}")
        print(f"âŒ Failed: {self.failed}")
        print(f"âš ï¸ Skipped: {self.skipped}")

        total_tests = self.passed + self.failed
        if total_tests > 0:
            success_rate = (self.passed / total_tests) * 100
            print(f"ğŸ“ˆ Success rate: {success_rate:.1f}%")

        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report = {
            "summary": {
                "passed": self.passed,
                "failed": self.failed,
                "skipped": self.skipped,
                "success_rate": (
                    (self.passed / (self.passed + self.failed)) * 100
                    if (self.passed + self.failed) > 0
                    else 0
                ),
            },
            "results": self.results,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }

        try:
            with open("regression_test_report.json", "w") as f:
                json.dump(report, f, indent=2)
            print("ğŸ“„ Detailed report saved to: regression_test_report.json")
        except Exception as e:
            print(f"âš ï¸ Could not save report: {e}")

        return self.failed == 0

    def run_all_tests(self):
        """å…¨ã¦ã®å›å¸°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print(
            "ğŸš€ Starting comprehensive regression tests for async QueryService fix..."
        )
        print(f"Working directory: {Path.cwd()}")

        # 1. ä¾å­˜é–¢ä¿‚ã®ç¢ºèª
        if not self.check_dependencies():
            print("ğŸ’¥ Dependency check failed! Cannot proceed with tests.")
            return False

        # 2. ç·Šæ€¥ä¿®æ­£ã®æ¤œè¨¼
        self.run_emergency_fix_verification()

        # 3. æ–°è¦éåŒæœŸãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        self.run_new_async_tests()

        # 4. æ—¢å­˜ã‚³ã‚¢ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        self.run_existing_core_tests()

        # 5. çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        self.run_integration_tests()

        # 6. ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
        self.run_sample_queries()

        # 7. åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        print("\nğŸ“‹ Running final comprehensive test suite...")
        self.run_comprehensive_test_suite()

        # 8. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        success = self.generate_report()

        if success:
            print("ğŸ‰ All regression tests passed!")
            print("âœ… Async QueryService fix is ready for production!")
        else:
            print("ğŸ’¥ Some regression tests failed!")
            print("âŒ Please review the failures before proceeding.")

        return success


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = RegressionTestRunner()
    success = runner.run_all_tests()
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
