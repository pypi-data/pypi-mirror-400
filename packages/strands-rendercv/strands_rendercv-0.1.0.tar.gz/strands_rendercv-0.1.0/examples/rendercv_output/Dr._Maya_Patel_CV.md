# Dr. Maya Patel's CV

- Email: [maya.patel@stanford.edu](mailto:maya.patel@stanford.edu)
- Location: Stanford, CA
- Website: [mayapatel.ai](https://mayapatel.ai/)
- LinkedIn: [drmayapatel](https://linkedin.com/in/drmayapatel)
- GitHub: [mayapatel](https://github.com/mayapatel)
- Google Scholar: [maya-patel](https://scholar.google.com/citations?user=maya-patel)
- X: [mayapatelai](https://x.com/mayapatelai)


# Summary
Machine Learning Researcher specializing in foundation models, multimodal learning, and AI alignment. Published 18 peer-reviewed papers with 2500+ citations. Currently postdoctoral researcher at Stanford AI Lab working on next-generation vision-language models. Passionate about making AI systems more interpretable, robust, and beneficial to humanity.

# Work Experience
## **Stanford University**, Postdoctoral Researcher, Stanford AI Lab

Stanford, CA

Sept 2023 – present

- Leading research on multimodal foundation models combining vision, language, and audio

- Developed novel attention mechanism improving vision-language alignment by 23 percent on VQA benchmarks

- Collaborating with Meta AI and Google DeepMind on joint research projects

- Mentoring 4 PhD students and 6 master's students on research projects

- Published 3 papers at top-tier conferences: NeurIPS, ICML, CVPR

- Secured 500K USD grant from NSF for research on interpretable multimodal AI



## **OpenAI**, Research Scientist Intern

San Francisco, CA

June 2022 – Dec 2022

- Contributed to GPT-4 vision capabilities development

- Developed novel dataset of 10M image-text pairs for improved visual reasoning

- Improved image captioning quality by 18 percent through reinforcement learning from human feedback

- Collaborated with safety team on multimodal alignment techniques

- Presented research findings to 50+ person research team in weekly seminars



## **Google Brain**, Student Researcher

Mountain View, CA

May 2021 – Aug 2021

- Researched efficient training methods for large-scale vision transformers

- Reduced training time by 40 percent through gradient checkpointing optimizations

- Published findings at ICLR 2022 (Oral presentation, top 5 percent of submissions)

- Collaborated with Google Cloud AI team to deploy research prototypes



# Education
## **Massachusetts Institute of Technology (MIT)**, Computer Science

**PhD**

Sept 2018 – June 2023

- Dissertation: Towards More Robust and Interpretable Multimodal Foundation Models

- Advisor: Prof. Regina Barzilay (Turing Award Winner)

- GPA: 4.0 out of 4.0

- NSF Graduate Research Fellowship (2019-2023)

- MIT Presidential Fellowship (2018-2019)

- Best Paper Award at NeurIPS 2022 for dissertation work



## **Stanford University**, Computer Science, minor in Statistics

**BS**

Sept 2014 – June 2018

- Summa Cum Laude, GPA: 3.95 out of 4.0

- Phi Beta Kappa Honor Society

- Stanford AI Lab Undergraduate Researcher (2016-2018)

- President's Award for Academic Excellence



# Publications
## **CLIP-Turbo: Efficient Vision-Language Pre-training with Knowledge Distillation**

Jan 2024

Maya Patel, James Chen, Sarah Williams, David Kim

[10.48550/arXiv.2312.12345](https://doi.org/10.48550/arXiv.2312.12345) (International Conference on Learning Representations (ICLR 2024) - Spotlight)



## **Multimodal Chain-of-Thought Reasoning for Visual Question Answering**

Oct 2023

Maya Patel, Regina Barzilay

[10.48550/arXiv.2310.98765](https://doi.org/10.48550/arXiv.2310.98765) (Neural Information Processing Systems (NeurIPS 2023))



## **Understanding and Mitigating Spurious Correlations in Vision-Language Models**

June 2023

Maya Patel, Alex Johnson, Regina Barzilay

[10.48550/arXiv.2306.54321](https://doi.org/10.48550/arXiv.2306.54321) (International Conference on Machine Learning (ICML 2023) - Oral)



## **Robust Multimodal Representations via Contrastive Learning**

Dec 2022

Maya Patel, Sarah Kim, Michael Chen

[10.48550/arXiv.2212.11111](https://doi.org/10.48550/arXiv.2212.11111) (Conference on Computer Vision and Pattern Recognition (CVPR 2023))



## **Scaling Laws for Vision Transformers: An Empirical Study**

Feb 2022

Maya Patel, David Lee, James Wilson

[10.48550/arXiv.2202.22222](https://doi.org/10.48550/arXiv.2202.22222) (International Conference on Learning Representations (ICLR 2022) - Oral)



## **Attention Mechanisms for Multimodal Fusion: A Survey**

Sept 2021

Maya Patel, Regina Barzilay

[10.48550/arXiv.2109.33333](https://doi.org/10.48550/arXiv.2109.33333) (IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI))



# Skills
**Research Areas:** Foundation Models, Multimodal Learning, Vision-Language Models, AI Alignment, Interpretability, Robustness

**Machine Learning:** Deep Learning, Transformers, Diffusion Models, Reinforcement Learning, Meta-Learning, Few-Shot Learning

**Programming:** Python, PyTorch, JAX, TensorFlow, C++, CUDA, Triton

**Infrastructure:** Distributed Training, Multi-GPU/TPU, Ray, Kubernetes, Docker, Weights & Biases, TensorBoard

**Mathematics:** Linear Algebra, Probability Theory, Optimization, Information Theory, Statistics

# Additional Experience and Awards
**Conference Service:** Area Chair: ICLR 2025, Reviewer: NeurIPS (2020-2024), ICML (2021-2024), CVPR (2022-2024), ICLR (2021-2024)

**Invited Talks:** MIT CSAIL Seminar (2024), Berkeley AI Research Lab (2023), Google Brain (2023), Stanford HAI (2023), Meta AI (2022)

**Awards & Honors:** NeurIPS Best Paper Award (2022), ICLR Outstanding Paper Award (2022), Forbes 30 Under 30 in AI (2023), MIT Sprowls Award for Best PhD Thesis (2023)

**Teaching Experience:** Guest Lecturer at Stanford CS231n (Computer Vision), MIT 6.S191 (Deep Learning), Teaching Assistant for MIT 6.867 (Machine Learning) - 4 semesters

**Open Source:** Creator of MultiModalBench (5K+ stars), Contributor to PyTorch Vision, Hugging Face Transformers (15+ merged PRs)

**Press Coverage:** Featured in MIT News, Stanford News, VentureBeat, TechCrunch AI, The Gradient

**Grants & Funding:** NSF CAREER Award Nominee (2024), NSF Grant (500K USD, 2023-2026), MIT Presidential Fellowship (2018-2019), NSF GRFP (2019-2023)
