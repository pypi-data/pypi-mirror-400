# LangChain Agent Architecture

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì „ì²´ êµ¬ì¡°](#ì „ì²´-êµ¬ì¡°)
3. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#ë””ë ‰í† ë¦¬-êµ¬ì¡°)
4. [í•µì‹¬ ì»´í¬ë„ŒíŠ¸](#í•µì‹¬-ì»´í¬ë„ŒíŠ¸)
5. [ë°ì´í„° íë¦„](#ë°ì´í„°-íë¦„)
6. [ë¯¸ë“¤ì›¨ì–´ ì‹œìŠ¤í…œ](#ë¯¸ë“¤ì›¨ì–´-ì‹œìŠ¤í…œ)
7. [ë„êµ¬ ì‹œìŠ¤í…œ](#ë„êµ¬-ì‹œìŠ¤í…œ)
8. [ì‹¤í–‰ íë¦„](#ì‹¤í–‰-íë¦„)

---

## ê°œìš”

ì´ ì‹œìŠ¤í…œì€ LangChainì„ ê¸°ë°˜ìœ¼ë¡œ Jupyter ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ë™ì‘í•˜ëŠ” ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•:

- **Human-in-the-Loop (HITL)**: ì½”ë“œ ì‹¤í–‰ ì „ ì‚¬ìš©ì ìŠ¹ì¸ í•„ìš”
- **TodoList ê¸°ë°˜ ì‘ì—… ê´€ë¦¬**: ì‘ì—…ì„ ë‹¨ê³„ë³„ë¡œ ì¶”ì 
- **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**: ì‹¤ì‹œê°„ìœ¼ë¡œ ì—ì´ì „íŠ¸ ìƒíƒœ ì „ë‹¬
- **ë©€í‹° ëª¨ë¸ ì§€ì›**: Gemini, OpenAI, vLLM
- **ì»¤ìŠ¤í…€ ë¯¸ë“¤ì›¨ì–´**: ë¹ˆ ì‘ë‹µ ì²˜ë¦¬, continuation í”„ë¡¬í”„íŠ¸ ì£¼ì… ë“±

---

## ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Jupyter Extension (Frontend)              â”‚
â”‚  - AgentPanel.tsx: UI ì»´í¬ë„ŒíŠ¸                               â”‚
â”‚  - ApiService.ts: API í†µì‹                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP/SSE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Jupyter Extension (Backend)                     â”‚
â”‚  - handlers.py: HTTP í•¸ë“¤ëŸ¬                                  â”‚
â”‚    - ChatStreamHandler: ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¬ë°                    â”‚
â”‚    - ExecuteCommandHandler: ì‰˜ ëª…ë ¹ ì‹¤í–‰                     â”‚
â”‚    - CheckResourceHandler: ë¦¬ì†ŒìŠ¤ í™•ì¸                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP POST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Agent Server (FastAPI)                        â”‚
â”‚  - langchain_agent.py: ë¼ìš°í„°                                â”‚
â”‚    - stream_agent(): ì´ˆê¸° ìš”ì²­ ì²˜ë¦¬                          â”‚
â”‚    - resume_agent(): ì¸í„°ëŸ½íŠ¸ ì¬ê°œ                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangChain Agent (agent.py)                      â”‚
â”‚  - create_simple_chat_agent(): ì—ì´ì „íŠ¸ ìƒì„±                 â”‚
â”‚  - Middleware ì²´ì¸                                           â”‚
â”‚  - Tools ë“±ë¡                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Middleware    â”‚ â”‚   Tools    â”‚ â”‚ LLM Factory  â”‚
â”‚                 â”‚ â”‚            â”‚ â”‚              â”‚
â”‚ - Empty Responseâ”‚ â”‚ - Jupyter  â”‚ â”‚ - Gemini     â”‚
â”‚ - Continuation  â”‚ â”‚ - File I/O â”‚ â”‚ - OpenAI     â”‚
â”‚ - HITL          â”‚ â”‚ - Search   â”‚ â”‚ - vLLM       â”‚
â”‚ - TodoList      â”‚ â”‚ - Shell    â”‚ â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

### `agent_server/langchain/`

```
langchain/
â”œâ”€â”€ __init__.py                 # ëª¨ë“ˆ ì´ˆê¸°í™”
â”œâ”€â”€ agent.py                    # ì—ì´ì „íŠ¸ ìƒì„± ë° ì„¤ì •
â”œâ”€â”€ custom_middleware.py        # ì»¤ìŠ¤í…€ ë¯¸ë“¤ì›¨ì–´ ì •ì˜
â”œâ”€â”€ hitl_config.py              # HITL ì„¤ì •
â”œâ”€â”€ llm_factory.py              # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
â”œâ”€â”€ logging_utils.py            # ë¡œê¹… ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ prompts.py                  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ìŠ¤í‚¤ë§ˆ
â”œâ”€â”€ state.py                    # ìƒíƒœ ì •ì˜ (TypedDict, dataclass)
â”œâ”€â”€ executors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ notebook_searcher.py   # ë…¸íŠ¸ë¶ ê²€ìƒ‰ ê¸°ëŠ¥
â””â”€â”€ tools/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ file_tools.py           # íŒŒì¼ ì½ê¸°/ì“°ê¸°/ëª©ë¡
    â”œâ”€â”€ jupyter_tools.py        # Jupyter ì…€ ì‹¤í–‰, ë§ˆí¬ë‹¤ìš´, final_answer
    â”œâ”€â”€ resource_tools.py       # ë¦¬ì†ŒìŠ¤ í™•ì¸ (íŒŒì¼ í¬ê¸°, ë©”ëª¨ë¦¬)
    â”œâ”€â”€ search_tools.py         # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ê²€ìƒ‰, ë…¸íŠ¸ë¶ ê²€ìƒ‰
    â””â”€â”€ shell_tools.py          # ì‰˜ ëª…ë ¹ ì‹¤í–‰
```

### `agent_server/routers/`

```
routers/
â””â”€â”€ langchain_agent.py          # FastAPI ë¼ìš°í„°
    - stream_agent()            # POST /agent/langchain/stream
    - resume_agent()            # POST /agent/langchain/resume
    - search_workspace()        # POST /agent/search/workspace
    - clear_agent_cache()       # POST /agent/langchain/clear
```

### `extensions/jupyter/jupyter_ext/`

```
jupyter_ext/
â””â”€â”€ handlers.py                 # Jupyter Extension í•¸ë“¤ëŸ¬
    - ChatStreamHandler         # GET /hdsp-agent/chat/stream
    - ExecuteCommandHandler     # POST /hdsp-agent/execute/command
    - CheckResourceHandler      # POST /hdsp-agent/check-resource
    - WriteFileHandler          # POST /hdsp-agent/write-file
    - (ê¸°íƒ€ í•¸ë“¤ëŸ¬)
```

---

## í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. Agent (`agent.py`)

#### `create_simple_chat_agent()`
ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤.

**ì£¼ìš” ì‘ì—…**:
1. LLM ìƒì„± (`llm_factory.create_llm()`)
2. Tools ë“±ë¡ (`_get_all_tools()`)
3. Middleware ì²´ì¸ êµ¬ì„±
4. Checkpointer ì„¤ì • (InMemorySaver)
5. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì • (Gemini-2.5-flash ì „ìš© í”„ë¡¬í”„íŠ¸ ì¶”ê°€)

**Middleware ìˆœì„œ**:
```python
1. handle_empty_response        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
2. limit_tool_calls             # í•œ ë²ˆì— 1ê°œ ë„êµ¬ë§Œ í˜¸ì¶œ
3. inject_continuation          # non-HITL ë„êµ¬ í›„ continuation í”„ë¡¬í”„íŠ¸
4. patch_tool_calls             # dangling tool call ìˆ˜ì •
5. TodoListMiddleware           # ì‘ì—… ëª©ë¡ ê´€ë¦¬
6. HumanInTheLoopMiddleware     # ì½”ë“œ ì‹¤í–‰ ì „ ì‚¬ìš©ì ìŠ¹ì¸
7. ModelCallLimitMiddleware     # LLM í˜¸ì¶œ íšŸìˆ˜ ì œí•œ (30íšŒ)
8. ToolCallLimitMiddleware      # íŠ¹ì • ë„êµ¬ í˜¸ì¶œ ì œí•œ
9. SummarizationMiddleware      # ëŒ€í™” ìš”ì•½
```

**Tools**:
```python
- jupyter_cell_tool            # Python ì½”ë“œ ì‹¤í–‰
- markdown_tool                # ë§ˆí¬ë‹¤ìš´ ì…€ ì¶”ê°€
- final_answer_tool            # ì‘ì—… ì™„ë£Œ ë° ìš”ì•½
- read_file_tool               # íŒŒì¼ ì½ê¸°
- write_file_tool              # íŒŒì¼ ì“°ê¸°
- list_files_tool              # ë””ë ‰í† ë¦¬ ëª©ë¡
- search_workspace_tool        # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ê²€ìƒ‰ (grep/rg)
- search_notebook_cells_tool   # ë…¸íŠ¸ë¶ ì…€ ê²€ìƒ‰
- execute_command_tool         # ì‰˜ ëª…ë ¹ ì‹¤í–‰
- check_resource_tool          # ë¦¬ì†ŒìŠ¤ í™•ì¸
```

---

### 2. Router (`langchain_agent.py`)

FastAPI ë¼ìš°í„°ë¡œ, í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ ë°›ì•„ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

#### `stream_agent()` - POST `/agent/langchain/stream`
ì´ˆê¸° ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.

**íë¦„**:
```
1. AgentRequest íŒŒì‹±
2. ì„¤ì • ì¤€ë¹„ (LLM config, workspace root, thread_id)
3. ì—ì´ì „íŠ¸ ìƒì„± (create_simple_chat_agent)
4. Checkpointer ìƒì„±/ì¡°íšŒ (InMemorySaver)
5. ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (agent.stream)
6. ì´ë²¤íŠ¸ ì²˜ë¦¬ ë£¨í”„:
   - todos: TodoList ì—…ë°ì´íŠ¸
   - messages: AIMessage/ToolMessage ì²˜ë¦¬
   - interrupt: HITL ì¸í„°ëŸ½íŠ¸ ë°œìƒ
7. SSE ì´ë²¤íŠ¸ ì „ì†¡:
   - event: todos           # Todo ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
   - event: token           # LLM ì‘ë‹µ í† í°
   - event: debug           # ë””ë²„ê·¸ ë©”ì‹œì§€
   - event: tool_call       # ë„êµ¬ í˜¸ì¶œ ìš”ì²­
   - event: interrupt       # HITL ì¸í„°ëŸ½íŠ¸
   - event: complete        # ì™„ë£Œ
```

**ì£¼ìš” ì²˜ë¦¬**:
- **ToolMessage (final_answer_tool)**: `final_answer` ì¶”ì¶œ, `summary` í•„ë“œì—ì„œ `next_items` JSON ì¶”ì¶œ í›„ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜
- **AIMessage**: tool_calls í™•ì¸, ë¹ˆ contentëŠ” í•„í„°ë§, ì¤‘ë³µ ì œê±°
- **Interrupt**: HITL ë„êµ¬ í˜¸ì¶œ ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì¼ì‹œ ì¤‘ì§€, í´ë¼ì´ì–¸íŠ¸ë¡œ interrupt ì´ë²¤íŠ¸ ì „ì†¡

#### `resume_agent()` - POST `/agent/langchain/resume`
HITL ì¸í„°ëŸ½íŠ¸ ì´í›„ ì‚¬ìš©ì ê²°ì •(ìŠ¹ì¸/ê±°ë¶€)ì„ ë°›ì•„ ì—ì´ì „íŠ¸ë¥¼ ì¬ê°œí•©ë‹ˆë‹¤.

**íë¦„**:
```
1. ResumeRequest íŒŒì‹± (thread_id, decision, execution_result)
2. Checkpointerì—ì„œ ê¸°ì¡´ ìƒíƒœ ì¡°íšŒ
3. ì¸í„°ëŸ½íŠ¸ ë©”ì‹œì§€ ì°¾ê¸°
4. ì‚¬ìš©ì ê²°ì •ì— ë”°ë¼ ì—…ë°ì´íŠ¸:
   - approved: execution_resultë¥¼ ToolMessage argumentsì— ì£¼ì…
   - rejected: rejection_reason ì¶”ê°€
5. ì—ì´ì „íŠ¸ ì¬ê°œ (agent.stream)
6. SSE ì´ë²¤íŠ¸ ì „ì†¡ (stream_agentì™€ ë™ì¼)
```

---

### 3. Handlers (`handlers.py`)

Jupyter Extensionì˜ ë°±ì—”ë“œ í•¸ë“¤ëŸ¬ë¡œ, í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ Agent Serverë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

#### `ChatStreamHandler` - GET `/hdsp-agent/chat/stream`
ì—ì´ì „íŠ¸ì™€ì˜ ëŒ€í™”ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

**íë¦„**:
```
1. GET íŒŒë¼ë¯¸í„° íŒŒì‹± (message, sessionId, mode ë“±)
2. Agent Serverë¡œ POST ìš”ì²­
   - URL: {AGENT_SERVER_URL}/agent/langchain/stream
   - Body: AgentRequest
3. SSE ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ë‹¬
4. event: tool_call ê°ì§€ ì‹œ:
   - jupyter_cell_tool: í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬ (HITL)
   - execute_command_tool: ì„œë²„ì—ì„œ ì‹¤í–‰ í›„ ê²°ê³¼ ë°˜í™˜
   - check_resource_tool: ì„œë²„ì—ì„œ ì‹¤í–‰ í›„ ê²°ê³¼ ë°˜í™˜
5. interrupt ì´ë²¤íŠ¸ ìˆ˜ì‹  ì‹œ: í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ë‹¬, ëŒ€ê¸°
```

#### `ExecuteCommandHandler` - POST `/hdsp-agent/execute/command`
ì‰˜ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

**íë¦„**:
```
1. POST body íŒŒì‹± (command, stdin, cwd, timeout)
2. subprocessë¡œ ëª…ë ¹ ì‹¤í–‰
3. stdout/stderr ìˆ˜ì§‘
4. ê²°ê³¼ ë°˜í™˜ (success, output, error)
```

#### `CheckResourceHandler` - POST `/hdsp-agent/check-resource`
íŒŒì¼ í¬ê¸° ë° DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•©ë‹ˆë‹¤.

**íë¦„**:
```
1. POST body íŒŒì‹± (files, dataframes)
2. íŒŒì¼ í¬ê¸° í™•ì¸ (subprocess: du -sh)
3. DataFrame ë©”ëª¨ë¦¬ í™•ì¸ (jupyter_cell_tool í˜¸ì¶œ)
4. ê²°ê³¼ ë°˜í™˜ (file_sizes, dataframe_memory)
```

---

## ë°ì´í„° íë¦„

### ì´ˆê¸° ìš”ì²­ íë¦„

```
[Client]
   â”‚
   â”œâ”€ message: "íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¶„ì„í•´ì¤˜"
   â”‚
   â–¼
[Jupyter Extension: ChatStreamHandler]
   â”‚
   â”œâ”€ POST /agent/langchain/stream
   â”‚  {
   â”‚    request: "íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¶„ì„í•´ì¤˜",
   â”‚    threadId: "uuid",
   â”‚    workspaceRoot: "/path/to/workspace",
   â”‚    llmConfig: { provider: "gemini", ... }
   â”‚  }
   â”‚
   â–¼
[Agent Server: stream_agent()]
   â”‚
   â”œâ”€ create_simple_chat_agent(llm_config, workspace_root)
   â”‚  â”‚
   â”‚  â”œâ”€ create_llm() â†’ ChatGoogleGenerativeAI
   â”‚  â”œâ”€ _get_all_tools() â†’ [jupyter_cell_tool, ...]
   â”‚  â”œâ”€ middleware ì²´ì¸ êµ¬ì„±
   â”‚  â””â”€ create_agent(model, tools, middleware, checkpointer)
   â”‚
   â”œâ”€ agent.stream(input, config)
   â”‚  â”‚
   â”‚  â”œâ”€ [TodoListMiddleware] â†’ write_todos í˜¸ì¶œ
   â”‚  â”‚  â†’ todos: [
   â”‚  â”‚      {content: "ë°ì´í„° ë¡œë“œ", status: "pending"},
   â”‚  â”‚      {content: "EDA", status: "pending"},
   â”‚  â”‚      {content: "ë‹¤ìŒ ë‹¨ê³„ ì œì‹œ", status: "pending"}
   â”‚  â”‚    ]
   â”‚  â”‚
   â”‚  â”œâ”€ [LLM] â†’ AIMessage with tool_calls
   â”‚  â”‚  â†’ tool_calls: [{
   â”‚  â”‚      name: "check_resource_tool",
   â”‚  â”‚      args: {files: ["titanic.csv"]}
   â”‚  â”‚    }]
   â”‚  â”‚
   â”‚  â”œâ”€ [HumanInTheLoopMiddleware] â†’ interrupt (non-HITLì´ë©´ í†µê³¼)
   â”‚  â”‚
   â”‚  â”œâ”€ [Tool Execution] â†’ check_resource_tool()
   â”‚  â”‚  â†’ {status: "pending_execution", ...}
   â”‚  â”‚
   â”‚  â””â”€ [Stream] â†’ SSE ì´ë²¤íŠ¸ ì „ì†¡
   â”‚     - event: todos
   â”‚     - event: debug (ğŸ”§ Tool ì‹¤í–‰: check_resource_tool)
   â”‚     - event: tool_call
   â”‚
   â–¼
[Jupyter Extension: ChatStreamHandler]
   â”‚
   â”œâ”€ tool_call ìˆ˜ì‹  (check_resource_tool)
   â”‚  â†’ CheckResourceHandler í˜¸ì¶œ
   â”‚  â†’ execution_result íšë“
   â”‚
   â”œâ”€ POST /agent/langchain/resume
   â”‚  {
   â”‚    threadId: "uuid",
   â”‚    decision: "approved",
   â”‚    execution_result: {...}
   â”‚  }
   â”‚
   â–¼
[Agent Server: resume_agent()]
   â”‚
   â”œâ”€ ì¸í„°ëŸ½íŠ¸ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (execution_result ì£¼ì…)
   â”‚
   â”œâ”€ agent.stream(None, config) â†’ ì¬ê°œ
   â”‚  â”‚
   â”‚  â”œâ”€ [LLM] â†’ ToolMessage ì²˜ë¦¬
   â”‚  â”‚  â†’ "íŒŒì¼ í¬ê¸°: 60KB"
   â”‚  â”‚
   â”‚  â”œâ”€ [inject_continuation] â†’ continuation í”„ë¡¬í”„íŠ¸ ì£¼ì…
   â”‚  â”‚  â†’ "[SYSTEM] Tool 'check_resource_tool' completed. Continue..."
   â”‚  â”‚
   â”‚  â”œâ”€ [LLM] â†’ AIMessage with tool_calls
   â”‚  â”‚  â†’ tool_calls: [{
   â”‚  â”‚      name: "jupyter_cell_tool",
   â”‚  â”‚      args: {code: "import pandas as pd\ndf = pd.read_csv('titanic.csv')"}
   â”‚  â”‚    }]
   â”‚  â”‚
   â”‚  â”œâ”€ [HumanInTheLoopMiddleware] â†’ interrupt (HITL)
   â”‚  â”‚
   â”‚  â””â”€ [Stream] â†’ SSE ì´ë²¤íŠ¸ ì „ì†¡
   â”‚     - event: interrupt
   â”‚
   â–¼
[Jupyter Extension: ChatStreamHandler]
   â”‚
   â”œâ”€ interrupt ìˆ˜ì‹  (jupyter_cell_tool)
   â”‚  â†’ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ë‹¬ (UIì—ì„œ ì‚¬ìš©ì ìŠ¹ì¸ ëŒ€ê¸°)
   â”‚
   â”œâ”€ ì‚¬ìš©ì ìŠ¹ì¸ í›„
   â”‚  â†’ Jupyter ì»¤ë„ì—ì„œ ì½”ë“œ ì‹¤í–‰
   â”‚  â†’ execution_result íšë“
   â”‚
   â”œâ”€ POST /agent/langchain/resume
   â”‚
   â–¼
... (ë°˜ë³µ)
```

### final_answer_tool ì²˜ë¦¬ íë¦„

```
[LLM]
   â”‚
   â”œâ”€ AIMessage with tool_calls
   â”‚  â†’ tool_calls: [{
   â”‚      name: "final_answer_tool",
   â”‚      args: {
   â”‚        answer: "ë¶„ì„ ì™„ë£Œ",
   â”‚        summary: '{"next_items": [...]}'  // JSON ë¬¸ìì—´
   â”‚      }
   â”‚    }]
   â”‚
   â–¼
[Tool Execution]
   â”‚
   â”œâ”€ final_answer_tool(answer, summary)
   â”‚  â†’ {
   â”‚      tool: "final_answer",
   â”‚      parameters: {answer: "...", summary: "..."},
   â”‚      status: "complete"
   â”‚    }
   â”‚
   â–¼
[Router: stream_agent()]
   â”‚
   â”œâ”€ ToolMessage ìˆ˜ì‹ 
   â”‚  â”‚
   â”‚  â”œâ”€ tool_result.get("answer")
   â”‚  â”œâ”€ summary = tool_result.get("summary")
   â”‚  â”‚
   â”‚  â”œâ”€ summaryê°€ JSON ë¬¸ìì—´ì´ë©´:
   â”‚  â”‚  â”‚
   â”‚  â”‚  â”œâ”€ summary_json = json.loads(summary)
   â”‚  â”‚  â”œâ”€ if "next_items" in summary_json:
   â”‚  â”‚  â”‚    next_items_block = f"\n\n```json\n{json.dumps(summary_json)}\n```"
   â”‚  â”‚  â”‚    final_answer = answer + next_items_block
   â”‚  â”‚  â”‚
   â”‚  â”‚  â””â”€ yield {"event": "token", "data": {"content": final_answer}}
   â”‚  â”‚
   â”‚  â”œâ”€ yield {"event": "todos", "data": {"todos": _complete_todos(todos)}}
   â”‚  â”œâ”€ yield {"event": "debug_clear"}
   â”‚  â””â”€ yield {"event": "complete"}
   â”‚
   â””â”€ return (ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ)
```

---

## ë¯¸ë“¤ì›¨ì–´ ì‹œìŠ¤í…œ

### 1. `handle_empty_response`
ë¹ˆ ì‘ë‹µ ë˜ëŠ” text-only ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

**ë™ì‘**:
1. LLM ì‘ë‹µ í™•ì¸:
   - `tool_calls` ìˆìœ¼ë©´ â†’ ì •ìƒ ì‘ë‹µ, í†µê³¼
   - `content`ì— JSONì´ ìˆìœ¼ë©´ â†’ íŒŒì‹±í•˜ì—¬ tool_call ìƒì„±
2. ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ `final_answer_tool` ê²°ê³¼ì´ë©´ â†’ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì—ì´ì „íŠ¸ ìì—° ì¢…ë£Œ)
3. ë¹ˆ ì‘ë‹µì´ë©´ â†’ JSON ìŠ¤í‚¤ë§ˆ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
4. ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ â†’ synthetic `final_answer_tool` ìƒì„±

**Gemini 2.5 Flash ëŒ€ì‘**:
- contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬ (`parse_json_tool_call`)
- multimodal ì‘ë‹µ ì§€ì›

### 2. `inject_continuation`
non-HITL ë„êµ¬ ì‹¤í–‰ í›„ continuation í”„ë¡¬í”„íŠ¸ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.

**ëŒ€ìƒ ë„êµ¬**:
```python
NON_HITL_TOOLS = {
    "markdown_tool",
    "read_file_tool",
    "list_files_tool",
    "search_workspace_tool",
    "search_notebook_cells_tool",
    "write_todos",
}
```

**ë™ì‘**:
1. ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ non-HITL ë„êµ¬ì˜ ToolMessageì¸ì§€ í™•ì¸
2. todos ìƒíƒœ í™•ì¸:
   - pending/in_progress ìˆìœ¼ë©´ â†’ "Continue with pending tasks: ..."
   - ëª¨ë‘ ì™„ë£Œì´ë©´ â†’ "Call final_answer_tool with a summary NOW."
3. HumanMessageë¡œ í”„ë¡¬í”„íŠ¸ ì£¼ì…

### 3. `limit_tool_calls`
í•œ ë²ˆì— 1ê°œ ë„êµ¬ë§Œ í˜¸ì¶œí•˜ë„ë¡ ì œí•œí•©ë‹ˆë‹¤.

**ë™ì‘**:
1. AIMessageì˜ `tool_calls` ê°œìˆ˜ í™•ì¸
2. 2ê°œ ì´ìƒì´ë©´ â†’ ì²« ë²ˆì§¸ë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ ì œê±°
3. ë¡œê·¸ ì¶œë ¥

### 4. `patch_tool_calls`
Dangling tool call (ì‹¤í–‰ë˜ì§€ ì•Šì€ ë„êµ¬ í˜¸ì¶œ)ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.

**ë™ì‘**:
1. ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ AIMessage with tool_callsì¸ì§€ í™•ì¸
2. ê·¸ ë‹¤ìŒ ë©”ì‹œì§€ê°€ ToolMessageê°€ ì•„ë‹ˆë©´ â†’ dangling
3. synthetic ToolMessage ìƒì„±í•˜ì—¬ ì£¼ì…

### 5. `TodoListMiddleware` (LangChain ë‚´ì¥)
ì‘ì—… ëª©ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

**ë™ì‘**:
1. `write_todos` ë„êµ¬ ë“±ë¡
2. LLMì´ `write_todos` í˜¸ì¶œí•˜ë©´ â†’ stateì— todos ì €ì¥
3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— todo ê´€ë¦¬ ì§€ì¹¨ ì¶”ê°€

### 6. `HumanInTheLoopMiddleware` (LangChain ë‚´ì¥)
ì‚¬ìš©ì ìŠ¹ì¸ì´ í•„ìš”í•œ ë„êµ¬ ì‹¤í–‰ ì „ ì¸í„°ëŸ½íŠ¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.

**ëŒ€ìƒ ë„êµ¬**:
```python
HITL_TOOLS = {
    "jupyter_cell_tool",
    "execute_command_tool",
    "write_file_tool",
}
```

**ë™ì‘**:
1. AIMessage with tool_calls ê°ì§€
2. tool_calls ì¤‘ HITL ë„êµ¬ê°€ ìˆìœ¼ë©´ â†’ interrupt ë°œìƒ
3. ì—ì´ì „íŠ¸ ì¼ì‹œ ì¤‘ì§€, í´ë¼ì´ì–¸íŠ¸ë¡œ ì œì–´ ë°˜í™˜

### 7. `ModelCallLimitMiddleware` (LangChain ë‚´ì¥)
LLM í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.

**ì„¤ì •**:
- `run_limit=30`: ìµœëŒ€ 30íšŒ LLM í˜¸ì¶œ
- `exit_behavior="end"`: ì œí•œ ë„ë‹¬ ì‹œ ì—ì´ì „íŠ¸ ì¢…ë£Œ

### 8. `ToolCallLimitMiddleware` (LangChain ë‚´ì¥)
íŠ¹ì • ë„êµ¬ì˜ í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.

**ì„¤ì •**:
```python
- write_todos: run_limit=5, exit_behavior="continue"
- list_files_tool: run_limit=5, exit_behavior="continue"
```

### 9. `SummarizationMiddleware` (LangChain ë‚´ì¥)
ëŒ€í™”ê°€ ê¸¸ì–´ì§€ë©´ ìš”ì•½í•©ë‹ˆë‹¤.

**ì„¤ì •**:
- `trigger`: tokens=8000 ë˜ëŠ” messages=30
- `keep`: ìµœê·¼ 10ê°œ ë©”ì‹œì§€ ìœ ì§€
- `summary_prefix`: "[ì´ì „ ëŒ€í™” ìš”ì•½]\n"

---

## ë„êµ¬ ì‹œìŠ¤í…œ

### Jupyter Tools (`jupyter_tools.py`)

#### `jupyter_cell_tool`
Python ì½”ë“œë¥¼ Jupyter ì…€ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `code`: Python ì½”ë“œ
- `description`: ì½”ë“œ ì„¤ëª… (ì„ íƒ)
- `execution_result`: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‹¤í–‰í•œ ê²°ê³¼ (HITL í›„)

**ë°˜í™˜**:
```python
{
    "tool": "jupyter_cell",
    "parameters": {"code": "...", "description": "..."},
    "status": "pending_execution",  # ë˜ëŠ” "complete"
    "message": "Code cell queued for execution...",
    "execution_result": {...}  # HITL í›„
}
```

**íŠ¹ì§•**:
- ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ë˜í¼ ì œê±°
- HITL ëŒ€ìƒ (ì‚¬ìš©ì ìŠ¹ì¸ í•„ìš”)

#### `markdown_tool`
ë§ˆí¬ë‹¤ìš´ ì…€ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `content`: ë§ˆí¬ë‹¤ìš´ ë‚´ìš©

**ë°˜í™˜**:
```python
{
    "tool": "markdown",
    "parameters": {"content": "..."},
    "status": "completed",
    "message": "Markdown cell added successfully."
}
```

**íŠ¹ì§•**:
- non-HITL (ì¦‰ì‹œ ì‹¤í–‰)

#### `final_answer_tool`
ì‘ì—…ì„ ì™„ë£Œí•˜ê³  ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `answer`: ìµœì¢… ë‹µë³€
- `summary`: ìš”ì•½ (ì„ íƒ, `next_items` JSON í¬í•¨ ê°€ëŠ¥)

**ë°˜í™˜**:
```python
{
    "tool": "final_answer",
    "parameters": {"answer": "...", "summary": "..."},
    "status": "complete",
    "message": "Task completed successfully"
}
```

**íŠ¹ì§•**:
- ì—ì´ì „íŠ¸ ì¢…ë£Œ ì‹ í˜¸
- `summary` í•„ë“œì— `next_items` JSON í¬í•¨ ê°€ëŠ¥ (Gemini)

---

### File Tools (`file_tools.py`)

#### `read_file_tool`
íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `path`: íŒŒì¼ ê²½ë¡œ

**ë°˜í™˜**:
```python
{
    "tool": "read_file",
    "parameters": {"path": "..."},
    "status": "completed",
    "content": "íŒŒì¼ ë‚´ìš©..."
}
```

**íŠ¹ì§•**:
- workspace_root ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
- ê²½ë¡œ ë²—ì–´ë‚˜ê¸° ë°©ì§€ (`_validate_path`)

#### `write_file_tool`
íŒŒì¼ì„ ì”ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `path`: íŒŒì¼ ê²½ë¡œ
- `content`: ë‚´ìš©
- `overwrite`: ë®ì–´ì“°ê¸° ì—¬ë¶€ (ê¸°ë³¸ False)

**ë°˜í™˜**:
```python
{
    "tool": "write_file",
    "parameters": {"path": "...", "content": "...", "overwrite": False},
    "status": "pending_execution",  # HITL
    "message": "File write queued..."
}
```

**íŠ¹ì§•**:
- HITL ëŒ€ìƒ (ì‚¬ìš©ì ìŠ¹ì¸ í•„ìš”)

#### `list_files_tool`
ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `path`: ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ ".")
- `recursive`: ì¬ê·€ íƒìƒ‰ ì—¬ë¶€ (ê¸°ë³¸ False)

**ë°˜í™˜**:
```python
{
    "tool": "list_files",
    "parameters": {"path": ".", "recursive": False},
    "status": "completed",
    "files": ["file1.py", "file2.csv", ...]
}
```

---

### Search Tools (`search_tools.py`)

#### `search_workspace_tool`
ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì—ì„œ íŒ¨í„´ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (grep/ripgrep).

**íŒŒë¼ë¯¸í„°**:
- `pattern`: ì •ê·œì‹ íŒ¨í„´
- `file_types`: íŒŒì¼ íƒ€ì… í•„í„° (ì˜ˆ: ["py", "md"])
- `path`: ê²€ìƒ‰ ê²½ë¡œ (ê¸°ë³¸ ".")

**ë°˜í™˜**:
```python
{
    "tool": "search_workspace",
    "parameters": {"pattern": "...", "file_types": ["py"], "path": "."},
    "status": "completed",
    "results": [
        {"file": "file1.py", "line_number": 10, "line": "..."},
        ...
    ],
    "command": "rg ... (ë˜ëŠ” grep ...)"
}
```

**íŠ¹ì§•**:
- ripgrep ìš°ì„  ì‚¬ìš© (ì†ë„)
- ì—†ìœ¼ë©´ grep ì‚¬ìš©

#### `search_notebook_cells_tool`
Jupyter ë…¸íŠ¸ë¶ ì…€ì—ì„œ íŒ¨í„´ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `pattern`: ì •ê·œì‹ íŒ¨í„´
- `notebook_path`: ë…¸íŠ¸ë¶ ê²½ë¡œ (ì„ íƒ, ì—†ìœ¼ë©´ ì „ì²´)

**ë°˜í™˜**:
```python
{
    "tool": "search_notebook_cells",
    "parameters": {"pattern": "...", "notebook_path": "..."},
    "status": "completed",
    "results": [
        {
            "notebook": "analysis.ipynb",
            "cell_index": 3,
            "cell_type": "code",
            "source": "...",
            "matches": [...]
        },
        ...
    ]
}
```

---

### Shell Tools (`shell_tools.py`)

#### `execute_command_tool`
ì‰˜ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `command`: ì‰˜ ëª…ë ¹
- `stdin`: ì¸í„°ë™í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ì…ë ¥ (ê¸°ë³¸ "y\n")
- `timeout`: íƒ€ì„ì•„ì›ƒ (ë°€ë¦¬ì´ˆ, ê¸°ë³¸ 600000)
- `execution_result`: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‹¤í–‰í•œ ê²°ê³¼ (HITL í›„)

**ë°˜í™˜**:
```python
{
    "tool": "execute_command_tool",
    "parameters": {"command": "...", "stdin": "y\n", "timeout": 600000},
    "status": "pending_execution",  # ë˜ëŠ” "complete"
    "message": "Shell command queued...",
    "execution_result": {...}  # HITL í›„
}
```

**íŠ¹ì§•**:
- HITL ëŒ€ìƒ (ì‚¬ìš©ì ìŠ¹ì¸ í•„ìš”)
- ì¥ì‹œê°„ ì‹¤í–‰ ëª…ë ¹ ê¸ˆì§€ (í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œ)

---

### Resource Tools (`resource_tools.py`)

#### `check_resource_tool`
íŒŒì¼ í¬ê¸° ë° DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°**:
- `files`: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
- `dataframes`: DataFrame ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸

**ë°˜í™˜**:
```python
{
    "tool": "check_resource_tool",
    "parameters": {"files": ["titanic.csv"], "dataframes": ["df"]},
    "status": "pending_execution",  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‹¤í–‰
    "message": "Resource check queued...",
    "execution_result": {
        "file_sizes": {"titanic.csv": "60KB"},
        "dataframe_memory": {"df": "2.5MB"}
    }
}
```

**íŠ¹ì§•**:
- í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‹¤í–‰ (Jupyter Extensionì˜ CheckResourceHandler)
- ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¡œë“œ ì „ í™•ì¸

---

## ì‹¤í–‰ íë¦„

### 1. ì´ˆê¸°í™” íë¦„

```python
# 1. ë¼ìš°í„°ì—ì„œ ì—ì´ì „íŠ¸ ìƒì„±
agent = create_simple_chat_agent(
    llm_config=llm_config,
    workspace_root=workspace_root,
    enable_hitl=True,
    enable_todo_list=True,
    checkpointer=checkpointer,
    system_prompt_override=None
)

# 2. create_simple_chat_agent ë‚´ë¶€
llm = create_llm(llm_config)  # Gemini/OpenAI/vLLM
tools = _get_all_tools()

# 3. Middleware ì²´ì¸ êµ¬ì„±
middleware = [
    handle_empty_response,
    limit_tool_calls,
    inject_continuation,
    patch_tool_calls,
    TodoListMiddleware(...),
    HumanInTheLoopMiddleware(...),
    ModelCallLimitMiddleware(run_limit=30),
    ToolCallLimitMiddleware(...),
    SummarizationMiddleware(...)
]

# 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
system_prompt = DEFAULT_SYSTEM_PROMPT
if "gemini-2.5-flash" in llm_config.get("gemini", {}).get("model", ""):
    system_prompt += GEMINI_CONTENT_PROMPT

# 5. ì—ì´ì „íŠ¸ ìƒì„±
agent = create_agent(
    model=llm,
    tools=tools,
    middleware=middleware,
    checkpointer=checkpointer,
    system_prompt=system_prompt
)
```

### 2. ìŠ¤íŠ¸ë¦¬ë° íë¦„

```python
# 1. ë¼ìš°í„°ì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
async for step in agent.stream(agent_input, config):
    # 2. stepì€ ë”•ì…”ë„ˆë¦¬ {"messages": [...], "todos": [...]}

    # 3. todos ì²˜ë¦¬
    if "todos" in step:
        todos = step["todos"]
        yield {"event": "todos", "data": json.dumps({"todos": todos})}

    # 4. messages ì²˜ë¦¬
    if "messages" in step:
        last_message = step["messages"][-1]

        # 5. ToolMessage ì²˜ë¦¬
        if isinstance(last_message, ToolMessage):
            tool_name = last_message.name

            if tool_name == "final_answer_tool":
                # 6. final_answer ì¶”ì¶œ
                tool_result = json.loads(last_message.content)
                final_answer = tool_result.get("answer")
                summary = tool_result.get("summary")

                # 7. summaryì—ì„œ next_items ì¶”ì¶œ
                if summary:
                    summary_json = json.loads(summary)
                    if "next_items" in summary_json:
                        next_items_block = f"\n\n```json\n{json.dumps(summary_json)}\n```"
                        final_answer += next_items_block

                # 8. ì‘ë‹µ ì „ì†¡
                yield {"event": "token", "data": {"content": final_answer}}
                yield {"event": "complete", "data": {"success": True}}
                return

        # 9. AIMessage ì²˜ë¦¬
        elif isinstance(last_message, AIMessage):
            # 10. tool_calls í™•ì¸
            if last_message.tool_calls:
                for tool_call in last_message.tool_calls:
                    # 11. ë””ë²„ê·¸ ì´ë²¤íŠ¸
                    yield {"event": "debug", "data": {"status": f"ğŸ”§ Tool ì‹¤í–‰: {tool_call['name']}"}}

                    # 12. HITL ë„êµ¬ì´ë©´ tool_call ì´ë²¤íŠ¸
                    if tool_call["name"] in HITL_TOOLS:
                        yield {"event": "tool_call", "data": tool_call}

            # 13. content ì „ì†¡
            if last_message.content:
                yield {"event": "token", "data": {"content": last_message.content}}
```

### 3. HITL ì¸í„°ëŸ½íŠ¸ íë¦„

```python
# 1. HumanInTheLoopMiddlewareì—ì„œ ì¸í„°ëŸ½íŠ¸ ë°œìƒ
# LangGraphëŠ” interruptë¥¼ stateì— ì €ì¥í•˜ê³  ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ

# 2. ë¼ìš°í„°ì—ì„œ ì¸í„°ëŸ½íŠ¸ ê°ì§€
if "__interrupt__" in step:
    interrupt_data = step["__interrupt__"]
    yield {"event": "interrupt", "data": interrupt_data}
    return  # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ

# 3. í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‚¬ìš©ì ê²°ì • ëŒ€ê¸°
# - jupyter_cell_tool: UIì—ì„œ ìŠ¹ì¸/ê±°ë¶€
# - execute_command_tool/check_resource_tool: ì„œë²„ì—ì„œ ì‹¤í–‰

# 4. í´ë¼ì´ì–¸íŠ¸ê°€ resume_agent() í˜¸ì¶œ
POST /agent/langchain/resume
{
    "threadId": "uuid",
    "decision": "approved",
    "execution_result": {...}
}

# 5. resume_agentì—ì„œ ì¸í„°ëŸ½íŠ¸ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
interrupt_message.args["execution_result"] = execution_result

# 6. ì—ì´ì „íŠ¸ ì¬ê°œ
agent.stream(None, config)  # Noneì€ ìƒˆ ì…ë ¥ ì—†ìŒì„ ì˜ë¯¸
```

### 4. ì—ì´ì „íŠ¸ ì¢…ë£Œ íë¦„

```python
# 1. LLMì´ final_answer_tool í˜¸ì¶œ
AIMessage(tool_calls=[{
    "name": "final_answer_tool",
    "args": {"answer": "...", "summary": "..."}
}])

# 2. final_answer_tool ì‹¤í–‰
result = {
    "tool": "final_answer",
    "parameters": {...},
    "status": "complete"
}

# 3. ToolMessage ìƒì„±
ToolMessage(name="final_answer_tool", content=json.dumps(result))

# 4. LangGraphê°€ ToolMessageë¥¼ LLMì— ì „ë‹¬
# LLMì´ ë¹ˆ ì‘ë‹µ ë°˜í™˜ (ë„êµ¬ í˜¸ì¶œ ì—†ìŒ)

# 5. handle_empty_response ë¯¸ë“¤ì›¨ì–´
# ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ final_answer_toolì´ë©´ â†’ ê·¸ëŒ€ë¡œ ë°˜í™˜
# synthetic answer ìƒì„±í•˜ì§€ ì•ŠìŒ

# 6. LangGraphê°€ ë„êµ¬ í˜¸ì¶œ ì—†ëŠ” ì‘ë‹µ ë°›ê³  ì¢…ë£Œ
# agent.stream() ë£¨í”„ ì¢…ë£Œ

# 7. ë¼ìš°í„°ì—ì„œ complete ì´ë²¤íŠ¸ ì „ì†¡
yield {"event": "complete", "data": {"success": True}}
return
```

---

## ì£¼ìš” ì„¤ê³„ ê²°ì • ì‚¬í•­

### 1. Gemini 2.5 Flash ëŒ€ì‘
- **ë¬¸ì œ**: content ë¹ˆê°’, multimodal ì‘ë‹µ (ë¦¬ìŠ¤íŠ¸)
- **í•´ê²°**:
  - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— content í¬í•¨ ì§€ì‹œ ì¶”ê°€
  - `parse_json_tool_call`ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬

### 2. final_answer_tool ë°˜ë³µ í˜¸ì¶œ ë°©ì§€
- **ë¬¸ì œ**: `final_answer_tool` í˜¸ì¶œ í›„ì—ë„ ì—ì´ì „íŠ¸ ê³„ì† ì‹¤í–‰
- **í•´ê²°**:
  - `ToolCallLimitMiddleware` ì œê±° (ìŠ¤ë ˆë“œ ì „ì²´ ì¹´ìš´íŠ¸ ë¬¸ì œ)
  - `handle_empty_response`ì—ì„œ `final_answer_tool` í›„ synthetic answer ìƒì„± ì•ˆí•¨
  - ì—ì´ì „íŠ¸ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì¢…ë£Œ

### 3. next_items UI ëˆ„ë½ ë¬¸ì œ
- **ë¬¸ì œ**: Geminiê°€ `summary` í•„ë“œì— JSON ë¬¸ìì—´ë¡œ `next_items` ì „ë‹¬
- **í•´ê²°**:
  - ë¼ìš°í„°ì—ì„œ `summary` í•„ë“œ íŒŒì‹±
  - `next_items` JSONì„ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜
  - UIì˜ `extractNextItemsBlock` í•¨ìˆ˜ê°€ íŒŒì‹±

### 4. HITL ë„êµ¬ vs non-HITL ë„êµ¬
- **HITL**: ì‚¬ìš©ì ìŠ¹ì¸ í•„ìš”
  - `jupyter_cell_tool`, `execute_command_tool`, `write_file_tool`
- **non-HITL**: ì¦‰ì‹œ ì‹¤í–‰
  - `markdown_tool`, `read_file_tool`, `list_files_tool`, `search_*_tool`
- **í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰**: ì„œë²„ì—ì„œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
  - `check_resource_tool`: CheckResourceHandlerì—ì„œ ì²˜ë¦¬

### 5. Checkpointer (InMemorySaver)
- ìŠ¤ë ˆë“œë³„ë¡œ ëŒ€í™” ìƒíƒœ ì €ì¥
- HITL ì¸í„°ëŸ½íŠ¸ ì¬ê°œì— í•„ìˆ˜
- ë©”ëª¨ë¦¬ ê¸°ë°˜ (ì„œë²„ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”)

### 6. SSE ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸
- `todos`: TodoList ì—…ë°ì´íŠ¸
- `token`: LLM ì‘ë‹µ í† í°
- `debug`: ë””ë²„ê·¸ ë©”ì‹œì§€ (ë„êµ¬ ì‹¤í–‰ ìƒíƒœ)
- `tool_call`: HITL ë„êµ¬ í˜¸ì¶œ ìš”ì²­
- `interrupt`: HITL ì¸í„°ëŸ½íŠ¸ ë°œìƒ
- `complete`: ì™„ë£Œ
- `debug_clear`: ë””ë²„ê·¸ ë©”ì‹œì§€ í´ë¦¬ì–´

---

## ë””ë²„ê¹… ê°€ì´ë“œ

### ë¡œê·¸ í™•ì¸

#### Agent Server
```bash
# ì „ì²´ ë¡œê·¸
tail -f agent-server.log

# LLM í˜¸ì¶œ ë¡œê·¸
grep "AGENT -> LLM PROMPT" agent-server.log

# ë¯¸ë“¤ì›¨ì–´ ë¡œê·¸
grep "Middleware:" agent-server.log

# ë„êµ¬ ì‹¤í–‰ ë¡œê·¸
grep "Tool ì‹¤í–‰:" agent-server.log
```

#### Jupyter Extension
```bash
# Jupyter ì„œë²„ ë¡œê·¸
jupyter lab --debug
```

### ì£¼ìš” ë¡œê·¸ íŒ¨í„´

#### 1. LLM í”„ë¡¬í”„íŠ¸
```
================================================================================================
AGENT -> LLM PROMPT SYSTEM (1521 chars)
================================================================================================
You are an expert Python data scientist...

================================================================================================
AGENT -> LLM PROMPT USER MESSAGES (batch=0)
================================================================================================
[0] HumanMessage
  "íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¶„ì„í•´ì¤˜"
```

#### 2. LLM ì‘ë‹µ
```
================================================================================================
AGENT <- LLM RESPONSE
================================================================================================
AIMessage
{
  "content": "ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê² ìŠµë‹ˆë‹¤.",
  "tool_calls": [
    {
      "name": "jupyter_cell_tool",
      "args": {"code": "import pandas as pd\ndf = pd.read_csv('titanic.csv')"}
    }
  ]
}
```

#### 3. ë¯¸ë“¤ì›¨ì–´ ì‹¤í–‰
```
Middleware: handle_empty_response [START]
handle_empty_response: attempt=1, type=AIMessage, content=True, tool_calls=True
Middleware: handle_empty_response [FINISH]

Middleware: inject_continuation_after_non_hitl_tool [START]
Injecting continuation prompt after non-HITL tool: write_todos
Middleware: inject_continuation_after_non_hitl_tool [FINISH]
```

#### 4. ë„êµ¬ í˜¸ì¶œ
```
SSE: Emitting debug event for tool: jupyter_cell_tool
ğŸ”§ Tool ì‹¤í–‰: jupyter_cell_tool
```

#### 5. HITL ì¸í„°ëŸ½íŠ¸
```
SimpleAgent interrupt detected with value: {...}
SSE: Sending interrupt event
```

### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### ë¬¸ì œ: ì—ì´ì „íŠ¸ê°€ ë¹ˆ ì‘ë‹µë§Œ ë°˜í™˜
- **ì›ì¸**: Gemini 2.5 Flashì˜ ë¹ˆ content
- **í™•ì¸**: `handle_empty_response` ë¡œê·¸ì—ì„œ `content=False, tool_calls=False`
- **í•´ê²°**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— content í¬í•¨ ì§€ì‹œ ì¶”ê°€ë¨

#### ë¬¸ì œ: final_answer_toolì´ ë°˜ë³µ í˜¸ì¶œ
- **ì›ì¸**: `handle_empty_response`ê°€ synthetic answer ìƒì„±
- **í™•ì¸**: ë¡œê·¸ì—ì„œ `"Synthesizing final_answer response."`
- **í•´ê²°**: `final_answer_tool` í›„ synthetic answer ìƒì„± ì•ˆí•˜ë„ë¡ ìˆ˜ì •ë¨

#### ë¬¸ì œ: next_items UIê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ
- **ì›ì¸**: Geminiê°€ `summary` í•„ë“œì— JSON ë¬¸ìì—´ë¡œ ì „ë‹¬
- **í™•ì¸**: ToolMessage contentì—ì„œ `"summary": "{\"next_items\": [...]}"` í™•ì¸
- **í•´ê²°**: ë¼ìš°í„°ì—ì„œ `summary` íŒŒì‹± ë¡œì§ ì¶”ê°€ë¨

#### ë¬¸ì œ: HITL ì¸í„°ëŸ½íŠ¸ í›„ ì¬ê°œë˜ì§€ ì•ŠìŒ
- **ì›ì¸**: Checkpointerì— ìƒíƒœ ì—†ìŒ
- **í™•ì¸**: `resume_agent`ì—ì„œ "No existing state for thread" ë¡œê·¸
- **í•´ê²°**: `stream_agent`ì—ì„œ Checkpointer ìƒì„± í™•ì¸

---

## í™•ì¥ ê°€ì´ë“œ

### ìƒˆ ë„êµ¬ ì¶”ê°€

1. `tools/` ë””ë ‰í† ë¦¬ì— íŒŒì¼ ìƒì„± (ì˜ˆ: `custom_tools.py`)
2. `@tool` ë°ì½”ë ˆì´í„°ë¡œ í•¨ìˆ˜ ì •ì˜
3. `tools/__init__.py`ì—ì„œ export
4. `agent.py`ì˜ `_get_all_tools()`ì— ì¶”ê°€

```python
# tools/custom_tools.py
from langchain_core.tools import tool
from pydantic import BaseModel, Field

class MyToolInput(BaseModel):
    param: str = Field(description="Parameter description")

@tool(args_schema=MyToolInput)
def my_tool(param: str) -> Dict[str, Any]:
    """Tool description for LLM."""
    return {
        "tool": "my_tool",
        "parameters": {"param": param},
        "status": "completed",
        "result": "..."
    }

# tools/__init__.py
from .custom_tools import my_tool

# agent.py
def _get_all_tools():
    return [
        jupyter_cell_tool,
        markdown_tool,
        final_answer_tool,
        my_tool,  # ì¶”ê°€
        ...
    ]
```

### ìƒˆ ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€

```python
# custom_middleware.py
def create_my_middleware(wrap_model_call):
    @wrap_model_call
    @_with_middleware_logging("my_middleware")
    def my_middleware(request, handler):
        # ì „ì²˜ë¦¬
        logger.info("Before LLM call")

        # LLM í˜¸ì¶œ
        response = handler(request)

        # í›„ì²˜ë¦¬
        logger.info("After LLM call")

        return response

    return my_middleware

# agent.py
def create_simple_chat_agent(...):
    ...
    my_middleware = create_my_middleware(wrap_model_call)
    middleware.append(my_middleware)
    ...
```

### ìƒˆ LLM Provider ì¶”ê°€

```python
# llm_factory.py
def _create_custom_llm(llm_config: Dict[str, Any], callbacks):
    from custom_llm_package import CustomLLM

    custom_config = llm_config.get("custom", {})
    api_key = custom_config.get("apiKey")
    model = custom_config.get("model", "default-model")

    return CustomLLM(
        model=model,
        api_key=api_key,
        temperature=0.0,
        callbacks=callbacks
    )

def create_llm(llm_config: Dict[str, Any]):
    provider = llm_config.get("provider", "gemini")

    if provider == "custom":
        return _create_custom_llm(llm_config, callbacks)
    ...
```

---

## ì°¸ê³  ìë£Œ

- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [LangChain Agent Middleware](https://python.langchain.com/docs/modules/agents/middleware/)
- [FastAPI SSE](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)
- [Jupyter Server Extension](https://jupyter-server.readthedocs.io/en/latest/developers/extensions.html)
