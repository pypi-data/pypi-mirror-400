# OrKa: Orchestrator Kit Agents
# by Marco Somma
#
# This file is part of OrKa – https://github.com/marcosomma/orka-reasoning
#
# Licensed under the Apache License, Version 2.0 (Apache 2.0).
#
# Full license: https://www.apache.org/licenses/LICENSE-2.0
#
# Attribution would be appreciated: OrKa by Marco Somma – https://github.com/marcosomma/orka-reasoning

"""
Validation and Structuring Agent
==============================

This module provides the ValidationAndStructuringAgent class, which is responsible for
validating answers and structuring them into a memory format. The agent ensures answers
are correct and contextually coherent, then extracts key information into a structured
memory object.

Classes
-------
ValidationAndStructuringAgent
    Agent that validates answers and structures them into memory objects.
"""

import json
import logging
from typing import Any, Dict, Optional, Union, cast

from jinja2 import Template

logger = logging.getLogger(__name__)

from .base_agent import BaseAgent, Context
from .llm_agents import OpenAIAnswerBuilder
from ..utils.json_parser import parse_llm_json, create_standard_schema


class ValidationAndStructuringAgent(BaseAgent):
    """
    Agent that validates answers and structures them into memory objects.

    This agent performs two main functions:
    1. Validates if an answer is correct and contextually coherent
    2. Structures valid answers into a memory object format

    The agent uses an LLM (Language Model) to perform validation and structuring.
    It returns a dictionary containing:
    - valid: Boolean indicating if the answer is valid
    - reason: Explanation of the validation decision
    - memory_object: Structured memory object if valid, None otherwise

    Parameters
    ----------
    params : Dict[str, Any], optional
        Configuration parameters for the agent, including:
        - prompt: The base prompt for the LLM
        - queue: Optional queue for async operations
        - agent_id: Unique identifier for the agent
        - store_structure: Optional template for memory object structure

    Attributes
    ----------
    llm_agent : OpenAIAnswerBuilder
        The LLM agent used for validation and structuring
    """

    def __init__(self, params: Dict[str, Any]):
        """Initialize the agent with an OpenAIAnswerBuilder for LLM calls."""
        _agent_id = params.get("agent_id", "validation_agent")
        super().__init__(
            agent_id=_agent_id,  # Pass agent_id to BaseAgent
            stream_key=_agent_id,  # Use agent_id as stream_key
            debug_keep_previous_outputs=False,  # Default value
            decay_config=None,  # Default value
        )
        # Initialize LLM agent with required parameters
        prompt = params.get("prompt", "")
        queue = params.get("queue")
        agent_id = params.get("agent_id", "validation_agent")
        self.llm_agent = OpenAIAnswerBuilder(
            agent_id=f"{agent_id}_llm",
            prompt=prompt,
            queue=queue,
        )

    def _parse_llm_output(
        self, raw_llm_output: str, prompt: str, formatted_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Parse the LLM output and extract the validation result.

        Args:
            raw_llm_output: The raw output from the LLM
            prompt: The prompt used to generate the output
            formatted_prompt: The formatted prompt (optional)

        Returns:
            Dict[str, Any]: The parsed validation result
        """
        # Create base response with common fields
        base_response = {
            "prompt": prompt,
            "formatted_prompt": formatted_prompt if formatted_prompt else prompt,
            "raw_llm_output": raw_llm_output,
        }

        # Define schema for validation output
        schema = create_standard_schema(
            required_fields=["valid"],
            optional_fields={
                "reason": "string",
                "memory_object": "object",
            },
        )

        # Use robust JSON parser
        result = parse_llm_json(
            raw_llm_output,
            schema=schema,
            strict=False,
            coerce_types=True,
            track_errors=True,
            agent_id=self.agent_id,
        )

        # Handle parsing failures
        if "error" in result and result.get("error") == "json_parse_failed":
            logger.error(
                f"[{self.agent_id}] JSON parsing failed: {result.get('message', 'Unknown error')}"
            )
            return {
                **base_response,
                "valid": False,
                "reason": f"Failed to parse JSON: {result.get('message', 'Unknown error')}",
                "memory_object": None,
            }

        # Ensure 'valid' field is boolean
        if "valid" in result and not isinstance(result["valid"], bool):
            # Try to coerce to boolean
            if isinstance(result["valid"], str):
                result["valid"] = result["valid"].lower() in ("true", "yes", "1")
            else:
                result["valid"] = bool(result["valid"])

        # Ensure required structure
        if "valid" not in result:
            logger.warning(f"[{self.agent_id}] Missing 'valid' field in parsed result")
            result["valid"] = False
            result["reason"] = "Invalid JSON structure - missing 'valid' field"

        if "reason" not in result:
            result["reason"] = "No reason provided"

        if "memory_object" not in result:
            result["memory_object"] = None

        # Add base response fields
        result.update(base_response)

        return result

    async def _run_impl(self, ctx: Context) -> Dict[str, Any]:
        """
        Process the input data to validate and structure the answer.

        Args:
            ctx: Context containing:
                - question: The original question
                - full_context: The context used to generate the answer
                - latest_answer: The answer to validate and structure
                - store_structure: Optional structure template for memory objects

        Returns:
            Dictionary containing:
                - valid: Boolean indicating if the answer is valid
                - reason: Explanation of validation decision
                - memory_object: Structured memory object if valid, None otherwise
        """
        # Convert ctx to dict if it's not already
        input_dict = dict(ctx) if isinstance(ctx, dict) else {"input": str(ctx)}

        question = input_dict.get("input", "")

        # Extract clean response text from complex agent outputs
        previous_outputs = input_dict.get("previous_outputs", {})
        context_output = (
            previous_outputs.get("context-collector", {})
            if isinstance(previous_outputs, dict)
            else {}
        )
        if isinstance(context_output, dict) and "result" in context_output:
            result = context_output["result"]
            context = result.get("response", "NONE") if isinstance(result, dict) else str(result)
        else:
            context = str(context_output) if context_output else "NONE"

        answer_output = (
            previous_outputs.get("answer-builder", {}) if isinstance(previous_outputs, dict) else {}
        )
        if isinstance(answer_output, dict) and "result" in answer_output:
            result = answer_output["result"]
            answer = result.get("response", "NONE") if isinstance(result, dict) else str(result)
        else:
            answer = str(answer_output) if answer_output else "NONE"

        store_structure = self.params.get("store_structure")

        # [OK] FIX: Check for pre-rendered prompt from execution engine first
        if isinstance(ctx, dict) and "formatted_prompt" in ctx and ctx["formatted_prompt"]:
            prompt = ctx["formatted_prompt"]
            logger.debug(f"Using pre-rendered prompt from execution engine (length: {len(prompt)})")
        # Check if we have a custom prompt that needs template rendering
        elif (
            hasattr(self.llm_agent, "prompt")
            and self.llm_agent.prompt
            and self.llm_agent.prompt.strip()
        ):
            # Use custom prompt with template rendering
            try:
                template = Template(self.llm_agent.prompt)
                prompt = template.render(**input_dict)
            except Exception:
                # Fallback to original prompt if rendering fails
                prompt = self.llm_agent.prompt
        else:
            # Use default prompt building logic
            prompt = self.build_prompt(str(question), str(context), str(answer), store_structure)

        # Ensure the prompt always instructs the model to return the required JSON schema.
        # If a user supplies a custom/pre-rendered prompt (common in YAML examples), we still
        # need to enforce the schema to avoid downstream parsing/validation failures.
        if (
            '"valid"' not in prompt
            and "'valid'" not in prompt
            and "memory_object" not in prompt
            and "Return your response in the following JSON format" not in prompt
        ):
            prompt = "\n".join(
                [
                    prompt.strip(),
                    "",
                    self._get_structure_instructions(store_structure),
                    "",
                    "Return your response in the following JSON format:",
                    "{",
                    '    "valid": true/false,',
                    '    "reason": "explanation of validation decision",',
                    '    "memory_object": {',
                    "        // structured memory object if valid, null if invalid",
                    "    }",
                    "}",
                    "Do not include any other text outside the JSON.",
                ]
            )

        # Create LLM input with prompt but disable automatic JSON parsing
        # We'll handle JSON parsing manually since we expect a different schema
        llm_input = {"prompt": prompt, "parse_json": False}

        # [OK] FIX: Pass the rendered prompt to the inner LLM agent
        if isinstance(ctx, dict) and "formatted_prompt" in ctx and ctx["formatted_prompt"]:
            llm_input["formatted_prompt"] = ctx["formatted_prompt"]
        else:
            llm_input["formatted_prompt"] = prompt

        # Get response from LLM
        response = await self.llm_agent.run(llm_input)

        # Extract the raw LLM output
        raw_llm_output = ""
        if isinstance(response, dict):
            # BaseAgent.run returns an OrkaResponse wrapper with the actual payload under `result`
            # (e.g., OpenAIAnswerBuilder returns {"response": "..."} as its internal result).
            inner = response.get("result")

            if isinstance(inner, dict):
                # Most common: {"result": {"response": "<llm_text>", ...}}
                raw_llm_output = str(inner.get("response", ""))
            elif isinstance(inner, str):
                # Sometimes components return a plain string as result
                raw_llm_output = inner
            else:
                # Backward-compat: some tests/mocks return {"response": "..."} directly
                raw_llm_output = str(response.get("response", "") or "")
        else:
            raw_llm_output = str(response)

        # Parse the LLM output - pass the correct formatted prompt
        formatted_prompt_to_use = (
            ctx["formatted_prompt"]
            if (isinstance(ctx, dict) and "formatted_prompt" in ctx and ctx["formatted_prompt"])
            else prompt
        )
        return self._parse_llm_output(
            str(raw_llm_output), str(prompt), str(formatted_prompt_to_use)
        )

    def build_prompt(
        self,
        question: str,
        context: str,
        answer: str,
        store_structure: Optional[str] = None,
    ) -> str:
        """
        Build the prompt for the validation and structuring task.

        Args:
            question: The original question
            context: The context used to generate the answer
            answer: The answer to validate and structure
            store_structure: Optional structure template for memory objects

        Returns:
            The complete prompt for the LLM
        """
        # Handle cases where context or answer is "NONE" or empty
        context = "No context available" if context in ["NONE", "", None] else context
        answer = "No answer provided" if answer in ["NONE", "", None] else answer

        # Build the prompt parts
        parts = [
            "Validate the following situation and structure it into a memory format.",
            f"\nQuestion: {question}",
            f"\nContext: {context}",
            f"\nAnswer to validate: {answer}",
        ]

        # Add special instructions for no-information cases
        if answer == "No answer provided" and context == "No context available":
            parts.extend(
                [
                    "\nThis appears to be a case where no information was found for the question. "
                    'Please validate this as a legitimate "no information available" response '
                    "and structure it appropriately.",
                    "\nIMPORTANT: You MUST respond with the exact JSON format specified below. "
                    "Do not use any other format.",
                    "\nFor cases where no information is available, you should:",
                    '1. Mark as valid=true (since "no information available" is a valid response)',
                    "2. Set confidence to 0.1 (low but not zero)",
                    "3. Create a memory object that captures the fact that no information was found",
                ]
            )
        else:
            parts.extend(
                [
                    "\nPlease validate if the answer is correct and contextually coherent. "
                    "Then structure the information into a memory object.",
                    "\nIMPORTANT: You MUST respond with the exact JSON format specified below. "
                    "Do not use any other format.",
                ]
            )

        # Add structure instructions
        parts.append(self._get_structure_instructions(store_structure))

        # Add response format
        parts.extend(
            [
                "\nReturn your response in the following JSON format:",
                "{",
                '    "valid": true/false,',
                '    "reason": "explanation of validation decision",',
                '    "memory_object": {',
                "        // structured memory object if valid, null if invalid",
                "    }",
                "}",
            ]
        )

        # Combine all parts
        return "\n".join(parts)

    def _get_structure_instructions(self, store_structure: Optional[str] = None) -> str:
        """
        Get the structure instructions for the memory object.

        Args:
            store_structure: Optional structure template for memory objects

        Returns:
            Instructions for structuring the memory object
        """
        if store_structure:
            return f"""Structure the memory object according to this template:
{store_structure}

Ensure all required fields are present and properly formatted."""
        else:
            return """Structure the memory object with these fields:
- fact: The validated fact or information
- category: The category or type of information (e.g., 'fact', 'opinion', 'data')
- confidence: A number between 0 and 1 indicating confidence in the fact
- source: The source of the information (e.g., 'context', 'answer', 'inferred')"""
