
# Autogenerated by mlir-tblgen; don't manually edit.

from ._ods_common import _cext as _ods_cext
from ._ods_common import (
    equally_sized_accessor as _ods_equally_sized_accessor,
    get_default_loc_context as _ods_get_default_loc_context,
    get_op_results_or_values as _get_op_results_or_values,
    segmented_accessor as _ods_segmented_accessor,
)
_ods_ir = _ods_cext.ir
_ods_cext.globals.register_traceback_file_exclusion(__file__)

import builtins
from typing import Sequence as _Sequence, Union as _Union, Optional as _Optional


@_ods_cext.register_dialect
class _Dialect(_ods_ir.Dialect):
  DIALECT_NAMESPACE = "amdgpu"

@_ods_cext.register_operation(_Dialect)
class DPPOp(_ods_ir.OpView):
  r"""
  This operation represents DPP functionality in a GPU program.
   DPP provides the following operations:
  - Full crossbar in a group of four (`quad_perm`)
  - Wavefront shift left by one lane (`wave_shl`)
  - Wavefront shift right by one lane (`wave_shr`)
  - Wavefront rotate right by one lane (`wave_ror`)
  - Wavefront rotate left by one lane (`wave_rol`)
  - Row shift left by 1–15 lanes (`row_shl`)
  - Row shift right by 1–15 lanes (`row_shr`)
  - Row rotate right by 1–15 lanes (`row_ror`)
  - Reverse within a row (`row_mirror`)
  - Reverse within a half-row (`row_half_mirror`)
  - Broadcast the 15th lane of each row to the next row (`row_bcast`)
  - Broadcast lane 31 to rows 2 and 3 (`row_bcast`)
  """

  OPERATION_NAME = "amdgpu.dpp"

  _ODS_REGIONS = (0, True)

  def __init__(self, old, src, kind, *, permArgument=None, row_mask=None, bank_mask=None, bound_ctrl=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(old)
    operands.append(src)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["kind"] = (kind if (
    isinstance(kind, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('AMDGPU_DPPPermAttr')) else
      _ods_ir.AttrBuilder.get('AMDGPU_DPPPermAttr')(kind, context=_ods_context))
    if permArgument is not None: attributes["permArgument"] = (permArgument if (
        isinstance(permArgument, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('anonymous_658')) else
          _ods_ir.AttrBuilder.get('anonymous_658')(permArgument, context=_ods_context))
    if row_mask is not None: attributes["row_mask"] = (row_mask if (
        isinstance(row_mask, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(row_mask, context=_ods_context))
    if bank_mask is not None: attributes["bank_mask"] = (bank_mask if (
        isinstance(bank_mask, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(bank_mask, context=_ods_context))
    if bound_ctrl is not None: attributes["bound_ctrl"] = (bound_ctrl if (
        isinstance(bound_ctrl, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(bound_ctrl, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def old(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def src(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def kind(self) -> _ods_ir.Attribute:
    return self.operation.attributes["kind"]

  @kind.setter
  def kind(self, value: _ods_ir.Attribute):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["kind"] = value

  @builtins.property
  def permArgument(self) -> _Optional[_ods_ir.Attribute]:
    if "permArgument" not in self.operation.attributes:
      return None
    return self.operation.attributes["permArgument"]

  @permArgument.setter
  def permArgument(self, value: _Optional[_ods_ir.Attribute]):
    if value is not None:
      self.operation.attributes["permArgument"] = value
    elif "permArgument" in self.operation.attributes:
      del self.operation.attributes["permArgument"]

  @permArgument.deleter
  def permArgument(self):
    del self.operation.attributes["permArgument"]

  @builtins.property
  def row_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["row_mask"]

  @row_mask.setter
  def row_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["row_mask"] = value

  @builtins.property
  def bank_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["bank_mask"]

  @bank_mask.setter
  def bank_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["bank_mask"] = value

  @builtins.property
  def bound_ctrl(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["bound_ctrl"]

  @bound_ctrl.setter
  def bound_ctrl(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["bound_ctrl"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def dpp(old, src, kind, *, perm_argument=None, row_mask=None, bank_mask=None, bound_ctrl=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return DPPOp(old=old, src=src, kind=kind, permArgument=perm_argument, row_mask=row_mask, bank_mask=bank_mask, bound_ctrl=bound_ctrl, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ExtPackedFp8Op(_ods_ir.OpView):
  r"""
  Extend one or two 8-bit floats in `source[index]` to a 32-bit float or
  two floats and return them.
  
  This rather unusual signature arises from the fact that AMD GPUs cannot
  easily work with sub 32-bit quantities, so the compiler intrinsics for
  extending 8-bit floats (which are, currently, the only way to work with
  this operation) take packed vectors of 4 such floats.
  
  If the passed-in vector has fewer than four elements, or the input is scalar,
  the remaining values in the <4 x i8> will be filled with
  undefined values as needed.
  """

  OPERATION_NAME = "amdgpu.ext_packed_fp8"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, index, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["index"] = (index if (
    isinstance(index, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(index, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def index(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["index"]

  @index.setter
  def index(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["index"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def ext_packed_fp8(res, source, index, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return ExtPackedFp8Op(res=res, source=source, index=index, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class FatRawBufferCastOp(_ods_ir.OpView):
  r"""
  Wraps the memory pointed to by `source` as a raw buffer fat pointer, or,
  in LLVM terms, a `ptr addrspace(7)`, returning a memref that has the same
  sizes and layout but the `#amdgpu.address_space<fat_raw_buffer>`
  address space.
  
  This memref can be used with standard memref operations like `memref.load`,
  `memref.store`, and `memref.atomicrmw`, which will be lowered to the relevant
  buffer intrinsics. (`vector.masked_load/store` will work once there's backend
  support for lowering them, and then this document will be updated)
  
  If `validBytes` is given, it is the number of bytes that will be valid as
  an offset to `out`. If it is not provided, this will be inferred from
  the size of the memref during lowering. This size is
  max_{d = 0 upto rank(source)} (sizes[d] * strides[d]) * sizeof(element type).
  
  The flags of the buffer descriptor will be set up to enable raw usage -
  for example, stride = 0, add_tid = 0, and so on. The `boundsCheck`
  property determines if bounds checking is enabled or not (on architectures
  where this can be controlled - that is, on RDNA chips).
  
  If `cacheSwizzleStride` is provided, L1 cache swizzling will be enabled
  on architectures that support it. This swizzling, unlike the main swizzling
  mode (whose usage makes a buffer non-raw) does not affect index calculation,
  but does affect cache behavior. Mixing access between cache-swizzled raw
  buffers and other forms of memory access, like ordinary pointer loads or
  unswizzled buffer pointers can cause incorrect behavior and must be avoided.
  
  This operation preserves the sizes, strides, and offset of the input
  memref - they'll be added in by `memref.load` later. However, if
  `resetOffset` is set, that offset will be added to the base pointer.
  If the value of the memref's offset is not uniform (independent of the lane/thread ID),
  this will lead to substantially decreased performance due to the need for
  a waterfall loop on the base address of the buffer resource.
  """

  OPERATION_NAME = "amdgpu.fat_raw_buffer_cast"

  _ODS_OPERAND_SEGMENTS = [1,0,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, source, *, validBytes=None, cacheSwizzleStride=None, boundsCheck=None, resetOffset=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(validBytes)
    operands.append(cacheSwizzleStride)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if bool(resetOffset): attributes["resetOffset"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def validBytes(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def cacheSwizzleStride(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def resetOffset(self) -> bool:
    return "resetOffset" in self.operation.attributes

  @resetOffset.setter
  def resetOffset(self, value):
    if bool(value):
      self.operation.attributes["resetOffset"] = _ods_ir.UnitAttr.get()
    elif "resetOffset" in self.operation.attributes:
      del self.operation.attributes["resetOffset"]

  @resetOffset.deleter
  def resetOffset(self):
    del self.operation.attributes["resetOffset"]

  @builtins.property
  def result(self) -> _ods_ir.OpResult[_ods_ir.MemRefType]:
    return self.operation.results[0]

def fat_raw_buffer_cast(source, *, valid_bytes=None, cache_swizzle_stride=None, bounds_check=None, reset_offset=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return FatRawBufferCastOp(source=source, validBytes=valid_bytes, cacheSwizzleStride=cache_swizzle_stride, boundsCheck=bounds_check, resetOffset=reset_offset, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class GatherToLDSOp(_ods_ir.OpView):
  r"""
  The `amdgpu.gather_to_lds` op is a wrapper around the `global_load_lds` instructions.
  
  Operands:
  * `$src`: global memory (including fat buffer) memref to read from.
  * `$srcIndices`: indices into `$src` to read from for this thread.
  * `$dst`: LDS memory memref to write to.
  * `$dstIndices`: base indices into `$dst` to write to for the subgroup of this thread.
    The elements gathered by the subgroup will be written contiguously in order of lane ID
    starting at `$dst[$dstIndices]`. Byte-sized (ex. i8) or short-sized (ex. i16)
    types will be zero-padded/extended to 32 bits before being written. 96-bit types
    (ex. vector<3xf32>) will be zero-padded to 128 bits before being written. Only the
    offsets held by lane 0 are used.
  * `$transferType`: type of the data to be transferred by each thread. This is used to determine
    the size of the data to be transferred and the number of threads in the subgroup.
    The transfer type must be a scalar type or a vector type with a single element type.
  
  The `$dst`, along with its indices, points to the memory location the subgroup of this thread
  will write to.
  
  Note: only supported on gfx9 and gfx10.
  """

  OPERATION_NAME = "amdgpu.gather_to_lds"

  _ODS_OPERAND_SEGMENTS = [1,-1,1,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, src, srcIndices, dst, dstIndices, transferType, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    operands.append(_get_op_results_or_values(srcIndices))
    operands.append(dst)
    operands.append(_get_op_results_or_values(dstIndices))
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["transferType"] = (transferType if (
    isinstance(transferType, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('TypeAttr')) else
      _ods_ir.AttrBuilder.get('TypeAttr')(transferType, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def srcIndices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def dst(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0]

  @builtins.property
  def dstIndices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def transferType(self) -> _ods_ir.TypeAttr:
    return self.operation.attributes["transferType"]

  @transferType.setter
  def transferType(self, value: _ods_ir.TypeAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["transferType"] = value

def gather_to_lds(src, src_indices, dst, dst_indices, transfer_type, *, loc=None, ip=None) -> GatherToLDSOp:
  return GatherToLDSOp(src=src, srcIndices=src_indices, dst=dst, dstIndices=dst_indices, transferType=transfer_type, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class LDSBarrierOp(_ods_ir.OpView):
  r"""
  `amdgpu.lds_barrier` is both a barrier (all workitems in a workgroup must reach
  the barrier before any of them may proceed past it) and a wait for all
  operations that affect the Local Data Store (LDS) issued from that wrokgroup
  to complete before the workgroup may continue. Since the LDS is per-workgroup
  memory, this barrier may be used, for example, to ensure all workitems have
  written data to LDS before any workitem attempts to read from it.
  
  Note that `lds_barrier` does **not** force reads to or from global memory
  to complete before execution continues. Therefore, it should be used when
  operations on global memory can be issued far in advance of when their results
  are used (for example, by writing them to LDS).
  
  WARNING: On architectures that do not support the BackOffBarrier feature,
  (those which will implement this barrier by emitting inline assembly),
  use of this operation will impede the usabiliity of memory watches (including
  breakpoints set on variables) when debugging.
  """

  OPERATION_NAME = "amdgpu.lds_barrier"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def lds_barrier(*, loc=None, ip=None) -> LDSBarrierOp:
  return LDSBarrierOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class MFMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.mfma` op is an MLIR wrapper around intrinsics
  for various `mfma` instructions in the CDNA architecture, which perform
  multiple outer products in order to allow fast matrix multiplication.
  
  The wrapper will select an appropriate `mfma` instruction, if one is available,
  based on the provided `m`, `k`, `n`, and `nBlks` attributes, along with the
  types of the source and destination arguments.
  
  For information on the layouts of the input and output matrices (which are stored
  in `sourceA`, `sourceB`, `destC`, and `destD`), see the CDNA ISA documentation.
  
  The `cbsz`, `abid`, and `blgp` parameters control how the lanes of the wave
  are permuted when matrix data is being loaded: `blgp` can be any number of
  fixed permutations, `cbsz` specifies the log_2 of the number of chunks the lanes
  holding sourceA are split into, and `abid` selects one of those chunks.
  
  Note, this wrapper allows specifying `vector<4Kxi8>` arguments to MFMA
  intrinsics that take an integer type of width `4K`. For example,
  one can provide a vector<4xi8> as an argument to an MFMA instruction that
  logically takes 4 i8s but whose intrinsics are specified to take an i32.
  In these cases, the bytes in the vector will be concatenated in little-endian
  order (that is, v[0] will go to arg[7:0], v[1] to arg[15:8] and so on).
  
  The negateA, negateB, and negateC flags are only supported for double-precision
  operations on gfx94x.
  
  Example:
  ```mlir
    %0 = amdgpu.mfma 16x16x16 %matA * %matB + %matC
      : vector<4xf16>, vector<4xf16>, vector<4xf32>
  
    %1 = amdgpu.mfma 32x32x1 %matD * %matE + %matF
      { abid = 1 : i32, cbsz = 1 : i32, blocks = 2 : i32 }
      blgp = bcast_second_32 : f32, f32, vector<32xf32>
  ```
  """

  OPERATION_NAME = "amdgpu.mfma"

  _ODS_REGIONS = (0, True)

  def __init__(self, m, n, k, sourceA, sourceB, destC, *, blocks=None, cbsz=None, abid=None, blgp=None, reducePrecision=None, negateA=None, negateB=None, negateC=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["m"] = (m if (
    isinstance(m, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(m, context=_ods_context))
    attributes["n"] = (n if (
    isinstance(n, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(n, context=_ods_context))
    attributes["k"] = (k if (
    isinstance(k, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(k, context=_ods_context))
    if blocks is not None: attributes["blocks"] = (blocks if (
        isinstance(blocks, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(blocks, context=_ods_context))
    if cbsz is not None: attributes["cbsz"] = (cbsz if (
        isinstance(cbsz, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(cbsz, context=_ods_context))
    if abid is not None: attributes["abid"] = (abid if (
        isinstance(abid, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(abid, context=_ods_context))
    if blgp is not None: attributes["blgp"] = (blgp if (
        isinstance(blgp, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('AMDGPU_MFMAPermBAttr')) else
          _ods_ir.AttrBuilder.get('AMDGPU_MFMAPermBAttr')(blgp, context=_ods_context))
    if bool(reducePrecision): attributes["reducePrecision"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(negateA): attributes["negateA"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(negateB): attributes["negateB"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(negateC): attributes["negateC"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def m(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["m"]

  @m.setter
  def m(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["m"] = value

  @builtins.property
  def n(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["n"]

  @n.setter
  def n(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["n"] = value

  @builtins.property
  def k(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["k"]

  @k.setter
  def k(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["k"] = value

  @builtins.property
  def blocks(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["blocks"]

  @blocks.setter
  def blocks(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["blocks"] = value

  @builtins.property
  def cbsz(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["cbsz"]

  @cbsz.setter
  def cbsz(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["cbsz"] = value

  @builtins.property
  def abid(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["abid"]

  @abid.setter
  def abid(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["abid"] = value

  @builtins.property
  def blgp(self) -> _ods_ir.Attribute:
    return self.operation.attributes["blgp"]

  @blgp.setter
  def blgp(self, value: _ods_ir.Attribute):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["blgp"] = value

  @builtins.property
  def reducePrecision(self) -> bool:
    return "reducePrecision" in self.operation.attributes

  @reducePrecision.setter
  def reducePrecision(self, value):
    if bool(value):
      self.operation.attributes["reducePrecision"] = _ods_ir.UnitAttr.get()
    elif "reducePrecision" in self.operation.attributes:
      del self.operation.attributes["reducePrecision"]

  @reducePrecision.deleter
  def reducePrecision(self):
    del self.operation.attributes["reducePrecision"]

  @builtins.property
  def negateA(self) -> bool:
    return "negateA" in self.operation.attributes

  @negateA.setter
  def negateA(self, value):
    if bool(value):
      self.operation.attributes["negateA"] = _ods_ir.UnitAttr.get()
    elif "negateA" in self.operation.attributes:
      del self.operation.attributes["negateA"]

  @negateA.deleter
  def negateA(self):
    del self.operation.attributes["negateA"]

  @builtins.property
  def negateB(self) -> bool:
    return "negateB" in self.operation.attributes

  @negateB.setter
  def negateB(self, value):
    if bool(value):
      self.operation.attributes["negateB"] = _ods_ir.UnitAttr.get()
    elif "negateB" in self.operation.attributes:
      del self.operation.attributes["negateB"]

  @negateB.deleter
  def negateB(self):
    del self.operation.attributes["negateB"]

  @builtins.property
  def negateC(self) -> bool:
    return "negateC" in self.operation.attributes

  @negateC.setter
  def negateC(self, value):
    if bool(value):
      self.operation.attributes["negateC"] = _ods_ir.UnitAttr.get()
    elif "negateC" in self.operation.attributes:
      del self.operation.attributes["negateC"]

  @negateC.deleter
  def negateC(self):
    del self.operation.attributes["negateC"]

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def mfma(m, n, k, source_a, source_b, dest_c, *, blocks=None, cbsz=None, abid=None, blgp=None, reduce_precision=None, negate_a=None, negate_b=None, negate_c=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return MFMAOp(m=m, n=n, k=k, sourceA=source_a, sourceB=source_b, destC=dest_c, blocks=blocks, cbsz=cbsz, abid=abid, blgp=blgp, reducePrecision=reduce_precision, negateA=negate_a, negateB=negate_b, negateC=negate_c, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class MakeDmaBaseOp(_ods_ir.OpView):
  r"""
  This operation creates a pair of addresses that will be used by tensor_load_to_lds
  and tensor_store_from_lds.
  
  This operation creates a value corresponding to the tensor descriptor (D#) group 0
  found in TensorLoadToLDSOp and TensorStoreFromLDSOp in the rocdl dialect.
  
  For example:
  
  ```mlir
    %base = amdgpu.make_dma_base %global[%idx0, %idx1], %lds[%idx2, %idx3] : memref<64x64xi32>, memref<64x64xi32, #gpu.address_space<workgroup>> -> !amdgpu.tdm_base<i32>
    %descriptor = amdgpu.make_dma_descriptor %base globalSize [2, 2] globalStride [2, 1] sharedSize [2, 2] : !amdgpu.tdm_base<i32> -> !amdgpu.tdm_descriptor
    amdgpu.tensor_load_to_lds %descriptor : !amdgpu.tdm_descriptor
  ```
  
  to
  
  ```mlir
    // pseudo-code
    %global_base = llvm.extractvalue %global_memref[1]
    %global_address = llvm.get_element_ptr ...
  
    %lds_base = llvm.extractvalue %lds_memref[1]
    %lds_address = llvm.get_element_ptr ...
  
    // Definition of %base
    %undef = llvm.mlir.undef : vector<4xi32>
    %v0 = llvm.insertelement %15, %undef[0] : vector<4xi32>
    %v1 = llvm.insertelement %lds_address, %v0[1] : vector<4xi32>
    %v2 = llvm.insertelement %global_address_low, %v1[2] : vector<4xi32>
    %base = llvm.insertelement %global_address_high, %v2[3] : vector<4xi32>
  
    rocdl.tensor.load.to.lds %base, %dgroup1, %dgroup2, %dgroup3 cachepolicy 0 : vector<4xi32>, vector<8xi32>
  ```
  
  These tensor DMA operations were introduced in gfx1250.
  """

  OPERATION_NAME = "amdgpu.make_dma_base"

  _ODS_OPERAND_SEGMENTS = [1,-1,1,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, base, global_, global_indices, lds, lds_indices, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(global_)
    operands.append(_get_op_results_or_values(global_indices))
    operands.append(lds)
    operands.append(_get_op_results_or_values(lds_indices))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(base)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def global_(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def global_indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def lds(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0]

  @builtins.property
  def lds_indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def base(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def make_dma_base(base, global_, global_indices, lds, lds_indices, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return MakeDmaBaseOp(base=base, global_=global_, global_indices=global_indices, lds=lds, lds_indices=lds_indices, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class MakeDmaDescriptorOp(_ods_ir.OpView):
  r"""
  Make all descriptor groups needed by tensor memory operations.
  
  The $base operand corresponds to the base pair addresses, one must be an address in LDS
  while the other must be a global memory location.
  
  $global_{static/dynamic}_sizes determine the size of the tensor.
  $global_{static/dynamic}_strides determine the strides of the tensor.
  $shared_{static/dynamic}_sizes determines the size of the tile.
  
  $workgroup_mask broadcast load to workgroups inside of a workgroup cluster
  (0 = do not broadcast result to workgroup, 1 = broadcast result to workgroup). Ignored for stores.
  An all zeros mask is interpreted as a non-broadcasted load.
  
  $early_timeout return data to requesters as soon as cache supplies it.
  
  Padding can be applied to the LDS address when copying from memory to LDS,
  but not when copying from LDS to memory.
  The values in the padded target addresses remain the same as before the operation was applied.
  $pad_interval must be a power of two contained in [2, 256].
  $pad_amount must be a value contained in [1, 128].
  
  $atomic_barrier_address must be aligned to 8 bytes.
  
  2D and 3D tensors may be iterated over by setting $global_increment, $lds_increment, and $iteration_count.
  $global_increment determines how much to increment the starting global memory address per iteration in units of the $base's element type.
  $lds_increment determines how much to increment the starting LDS address per iteration in units of the $base's element type.
  $iterate_count determines how many times to iterate, it must be a value in the inclusive interval [1, 256].
  
  ```mlir
   // Example of moving a two-dimensional tensor to LDS.
   %base = amdgpu.make_dma_base %global[0, 0], %lds[0, 0] : memref<64x64xi32>, memref<64x64xi32, #gpu.address_space<workgroup>> -> !amdgpu.tdm_base<i32>
   %descriptor = amdgpu.make_dma_descriptor %base globalSize [64, 64] globalStride [64, 1] sharedSize [64, 64] : !amdgpu.tdm_base<i32> -> !amdgpu.tdm_descriptor
   amdgpu.tensor_load_to_lds %descriptor : !amdgpu.tdm_descriptor
  
   // Example of moving a two dimension tensor to LDS where padding is applied after every integer.
   %base = amdgpu.make_dma_base %global[0, 0], %lds[0, 0] : memref<32x32xi32>, memref<64x64xi32, #gpu.address_space<workgroup>> -> !amdgpu.tdm_base<i32>
   %descriptor = amdgpu.make_dma_descriptor %base globalSize [32, 32] globalStride [32, 1] sharedSize [64, 64] padShared(%pad_amount every %pad_interval) : !amdgpu.tdm_base<i32> -> !amdgpu.tdm_descriptor
   amdgpu.tensor_load_to_lds %descriptor : !amdgpu.tdm_descriptor
  ```
  """

  OPERATION_NAME = "amdgpu.make_dma_descriptor"

  _ODS_OPERAND_SEGMENTS = [1,-1,-1,-1,0,0,0,0,0,-1,0,0,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, base, global_dynamic_sizes, global_static_sizes, global_dynamic_strides, global_static_strides, shared_dynamic_sizes, shared_static_sizes, atomic_barrier_indices, *, workgroup_mask=None, early_timeout=None, pad_amount=None, pad_interval=None, atomic_barrier_address=None, global_increment=None, lds_increment=None, iteration_count=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(base)
    operands.append(_get_op_results_or_values(global_dynamic_sizes))
    operands.append(_get_op_results_or_values(global_dynamic_strides))
    operands.append(_get_op_results_or_values(shared_dynamic_sizes))
    operands.append(workgroup_mask)
    operands.append(early_timeout)
    operands.append(pad_amount)
    operands.append(pad_interval)
    operands.append(atomic_barrier_address)
    operands.append(_get_op_results_or_values(atomic_barrier_indices))
    operands.append(global_increment)
    operands.append(lds_increment)
    operands.append(iteration_count)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["global_static_sizes"] = (global_static_sizes if (
    isinstance(global_static_sizes, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(global_static_sizes, context=_ods_context))
    attributes["global_static_strides"] = (global_static_strides if (
    isinstance(global_static_strides, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(global_static_strides, context=_ods_context))
    attributes["shared_static_sizes"] = (shared_static_sizes if (
    isinstance(shared_static_sizes, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(shared_static_sizes, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def base(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def global_dynamic_sizes(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def global_dynamic_strides(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def shared_dynamic_sizes(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def workgroup_mask(self) -> _Optional[_ods_ir.Value[_ods_ir.VectorType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 4)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def early_timeout(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 5)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def pad_amount(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 6)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def pad_interval(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 7)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def atomic_barrier_address(self) -> _Optional[_ods_ir.Value[_ods_ir.MemRefType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 8)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def atomic_barrier_indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 9)
    return operand_range

  @builtins.property
  def global_increment(self) -> _Optional[_ods_ir.Value[_ods_ir.IndexType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 10)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def lds_increment(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 11)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def iteration_count(self) -> _Optional[_ods_ir.Value[_ods_ir.IndexType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 12)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def global_static_sizes(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["global_static_sizes"]

  @global_static_sizes.setter
  def global_static_sizes(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["global_static_sizes"] = value

  @builtins.property
  def global_static_strides(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["global_static_strides"]

  @global_static_strides.setter
  def global_static_strides(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["global_static_strides"] = value

  @builtins.property
  def shared_static_sizes(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["shared_static_sizes"]

  @shared_static_sizes.setter
  def shared_static_sizes(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["shared_static_sizes"] = value

  @builtins.property
  def desc(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def make_dma_descriptor(base, global_dynamic_sizes, global_static_sizes, global_dynamic_strides, global_static_strides, shared_dynamic_sizes, shared_static_sizes, atomic_barrier_indices, *, workgroup_mask=None, early_timeout=None, pad_amount=None, pad_interval=None, atomic_barrier_address=None, global_increment=None, lds_increment=None, iteration_count=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return MakeDmaDescriptorOp(base=base, global_dynamic_sizes=global_dynamic_sizes, global_static_sizes=global_static_sizes, global_dynamic_strides=global_dynamic_strides, global_static_strides=global_static_strides, shared_dynamic_sizes=shared_dynamic_sizes, shared_static_sizes=shared_static_sizes, atomic_barrier_indices=atomic_barrier_indices, workgroup_mask=workgroup_mask, early_timeout=early_timeout, pad_amount=pad_amount, pad_interval=pad_interval, atomic_barrier_address=atomic_barrier_address, global_increment=global_increment, lds_increment=lds_increment, iteration_count=iteration_count, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class MakeGatherDmaBaseOp(_ods_ir.OpView):
  r"""
  This operation creates a pair of addresses that will be used by `tensor_load_to_lds`
  and `tensor_store_from_lds`.
  
  This operation creates a value corresponding to the tensor descriptor (D#) group 0
  found in TensorLoadToLDSOp and TensorStoreFromLDSOp in the rocdl dialect.
  
  Unlike `make_dma_base`, this operation returns `!amdgpu.tdm_gather_base<$element_type, $index_type>`
  which is only compatible with `make_gather_dma_descriptor`. Using the descriptor returned
  by `make_gather_dma_descriptor` will set the `tensor_load_to_lds` and `tensor_store_from_lds` to gather mode.
  
  ```mlir
    %base = amdgpu.make_gather_dma_base %global[%idx0, %idx1], %lds[%idx2, %idx3] : memref<64x64xi32>, memref<64x64xi32, #gpu.address_space<workgroup>> -> !amdgpu.tdm_gather_base<i32, i16>
    // %indices : i16
    %descriptor = amdgpu.make_gather_dma_descriptor %base[%indices] globalSize [2, 2] globalStride [2, 1] sharedSize [2, 2] : !amdgpu.tdm_gather_base<i32, i16>, i16 -> !amdgpu.tdm_descriptor
    amdgpu.tensor_load_to_lds %descriptor : !amdgpu.tdm_descriptor
  ```
  """

  OPERATION_NAME = "amdgpu.make_gather_dma_base"

  _ODS_OPERAND_SEGMENTS = [1,-1,1,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, base, global_, global_indices, lds, lds_indices, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(global_)
    operands.append(_get_op_results_or_values(global_indices))
    operands.append(lds)
    operands.append(_get_op_results_or_values(lds_indices))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(base)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def global_(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def global_indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def lds(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0]

  @builtins.property
  def lds_indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def base(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def make_gather_dma_base(base, global_, global_indices, lds, lds_indices, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return MakeGatherDmaBaseOp(base=base, global_=global_, global_indices=global_indices, lds=lds, lds_indices=lds_indices, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class MakeGatherDmaDescriptorOp(_ods_ir.OpView):
  OPERATION_NAME = "amdgpu.make_gather_dma_descriptor"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,-1,-1,0,0,0,0,0,-1,0,0,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, base, indices, global_dynamic_sizes, global_static_sizes, global_dynamic_strides, global_static_strides, shared_dynamic_sizes, shared_static_sizes, atomic_barrier_indices, *, workgroup_mask=None, early_timeout=None, pad_amount=None, pad_interval=None, atomic_barrier_address=None, global_increment=None, lds_increment=None, iteration_count=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(base)
    operands.append(indices)
    operands.append(_get_op_results_or_values(global_dynamic_sizes))
    operands.append(_get_op_results_or_values(global_dynamic_strides))
    operands.append(_get_op_results_or_values(shared_dynamic_sizes))
    operands.append(workgroup_mask)
    operands.append(early_timeout)
    operands.append(pad_amount)
    operands.append(pad_interval)
    operands.append(atomic_barrier_address)
    operands.append(_get_op_results_or_values(atomic_barrier_indices))
    operands.append(global_increment)
    operands.append(lds_increment)
    operands.append(iteration_count)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["global_static_sizes"] = (global_static_sizes if (
    isinstance(global_static_sizes, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(global_static_sizes, context=_ods_context))
    attributes["global_static_strides"] = (global_static_strides if (
    isinstance(global_static_strides, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(global_static_strides, context=_ods_context))
    attributes["shared_static_sizes"] = (shared_static_sizes if (
    isinstance(shared_static_sizes, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('DenseI64ArrayAttr')) else
      _ods_ir.AttrBuilder.get('DenseI64ArrayAttr')(shared_static_sizes, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def base(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def global_dynamic_sizes(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def global_dynamic_strides(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def shared_dynamic_sizes(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 4)
    return operand_range

  @builtins.property
  def workgroup_mask(self) -> _Optional[_ods_ir.Value[_ods_ir.VectorType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 5)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def early_timeout(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 6)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def pad_amount(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 7)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def pad_interval(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 8)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def atomic_barrier_address(self) -> _Optional[_ods_ir.Value[_ods_ir.MemRefType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 9)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def atomic_barrier_indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 10)
    return operand_range

  @builtins.property
  def global_increment(self) -> _Optional[_ods_ir.Value[_ods_ir.IndexType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 11)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def lds_increment(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 12)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def iteration_count(self) -> _Optional[_ods_ir.Value[_ods_ir.IndexType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 13)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def global_static_sizes(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["global_static_sizes"]

  @global_static_sizes.setter
  def global_static_sizes(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["global_static_sizes"] = value

  @builtins.property
  def global_static_strides(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["global_static_strides"]

  @global_static_strides.setter
  def global_static_strides(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["global_static_strides"] = value

  @builtins.property
  def shared_static_sizes(self) -> _ods_ir.DenseI64ArrayAttr:
    return self.operation.attributes["shared_static_sizes"]

  @shared_static_sizes.setter
  def shared_static_sizes(self, value: _ods_ir.DenseI64ArrayAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["shared_static_sizes"] = value

  @builtins.property
  def desc(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def make_gather_dma_descriptor(base, indices, global_dynamic_sizes, global_static_sizes, global_dynamic_strides, global_static_strides, shared_dynamic_sizes, shared_static_sizes, atomic_barrier_indices, *, workgroup_mask=None, early_timeout=None, pad_amount=None, pad_interval=None, atomic_barrier_address=None, global_increment=None, lds_increment=None, iteration_count=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return MakeGatherDmaDescriptorOp(base=base, indices=indices, global_dynamic_sizes=global_dynamic_sizes, global_static_sizes=global_static_sizes, global_dynamic_strides=global_dynamic_strides, global_static_strides=global_static_strides, shared_dynamic_sizes=shared_dynamic_sizes, shared_static_sizes=shared_static_sizes, atomic_barrier_indices=atomic_barrier_indices, workgroup_mask=workgroup_mask, early_timeout=early_timeout, pad_amount=pad_amount, pad_interval=pad_interval, atomic_barrier_address=atomic_barrier_address, global_increment=global_increment, lds_increment=lds_increment, iteration_count=iteration_count, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class MemoryCounterWaitOp(_ods_ir.OpView):
  r"""
  Wait for the specified counters to be less-than or equal-to the provided
  values before continuing.
  
  Counters can lower to different instructions on different architectires,
  including clamping to the some HW supported max value or combining multiple
  counters into one.
  """

  OPERATION_NAME = "amdgpu.memory_counter_wait"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, load=None, store=None, ds=None, exp=None, tensor=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    if load is not None: attributes["load"] = (load if (
        isinstance(load, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(load, context=_ods_context))
    if store is not None: attributes["store"] = (store if (
        isinstance(store, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(store, context=_ods_context))
    if ds is not None: attributes["ds"] = (ds if (
        isinstance(ds, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(ds, context=_ods_context))
    if exp is not None: attributes["exp"] = (exp if (
        isinstance(exp, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(exp, context=_ods_context))
    if tensor is not None: attributes["tensor"] = (tensor if (
        isinstance(tensor, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(tensor, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def load(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "load" not in self.operation.attributes:
      return None
    return self.operation.attributes["load"]

  @load.setter
  def load(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["load"] = value
    elif "load" in self.operation.attributes:
      del self.operation.attributes["load"]

  @load.deleter
  def load(self):
    del self.operation.attributes["load"]

  @builtins.property
  def store(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "store" not in self.operation.attributes:
      return None
    return self.operation.attributes["store"]

  @store.setter
  def store(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["store"] = value
    elif "store" in self.operation.attributes:
      del self.operation.attributes["store"]

  @store.deleter
  def store(self):
    del self.operation.attributes["store"]

  @builtins.property
  def ds(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "ds" not in self.operation.attributes:
      return None
    return self.operation.attributes["ds"]

  @ds.setter
  def ds(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["ds"] = value
    elif "ds" in self.operation.attributes:
      del self.operation.attributes["ds"]

  @ds.deleter
  def ds(self):
    del self.operation.attributes["ds"]

  @builtins.property
  def exp(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "exp" not in self.operation.attributes:
      return None
    return self.operation.attributes["exp"]

  @exp.setter
  def exp(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["exp"] = value
    elif "exp" in self.operation.attributes:
      del self.operation.attributes["exp"]

  @exp.deleter
  def exp(self):
    del self.operation.attributes["exp"]

  @builtins.property
  def tensor(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "tensor" not in self.operation.attributes:
      return None
    return self.operation.attributes["tensor"]

  @tensor.setter
  def tensor(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["tensor"] = value
    elif "tensor" in self.operation.attributes:
      del self.operation.attributes["tensor"]

  @tensor.deleter
  def tensor(self):
    del self.operation.attributes["tensor"]

def memory_counter_wait(*, load=None, store=None, ds=None, exp=None, tensor=None, loc=None, ip=None) -> MemoryCounterWaitOp:
  return MemoryCounterWaitOp(load=load, store=store, ds=ds, exp=exp, tensor=tensor, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class PackedScaledTruncOp(_ods_ir.OpView):
  r"""
  Scale and round the inputs `source` (which is undefined if not
  specified) into the low or high word (bottom two or top two) elements
  of the returned vector, keeping the other two elements of `existing`
  unchanged if present (or undefined if it was not passed in).
  
  The reason for this odd signature is that AMD GPUs cannot easily work with
  sub-registers, and so the conversion intrinsics take 32-bit wide
  packed vectors of float values.
  """

  OPERATION_NAME = "amdgpu.packed_scaled_trunc"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, scale, index, *, existing=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(scale)
    if existing is not None: operands.append(existing)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["index"] = (index if (
    isinstance(index, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(index, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value[_ods_ir.VectorType]:
    return self.operation.operands[0]

  @builtins.property
  def scale(self) -> _ods_ir.Value[_ods_ir.FloatType]:
    return self.operation.operands[1]

  @builtins.property
  def existing(self) -> _Optional[_ods_ir.Value]:
    return None if len(self.operation.operands) < 3 else self.operation.operands[2]

  @builtins.property
  def index(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["index"]

  @index.setter
  def index(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["index"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def packed_scaled_trunc(res, source, scale, index, *, existing=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PackedScaledTruncOp(res=res, source=source, scale=scale, index=index, existing=existing, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class PackedStochRoundFp8Op(_ods_ir.OpView):
  r"""
  Round the input `source`, adding in `stochiasticParam`, and place it into
  the `storeIndex`th element of `res`.
  
  If `existing` is passed in, elements of `res` other than the one at `storeIndex`
  are copied from `existing`.
  
  The reason for this odd signature is that AMD GPUs cannot easily work with
  sub-registers, and so the conversion intrinsics (which are currently the
  only way to work with 8-bit float types) take packed vectors of 4 8-bit
  values.
  """

  OPERATION_NAME = "amdgpu.packed_stoch_round_fp8"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, stochiasticParam, storeIndex, *, existing=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(stochiasticParam)
    if existing is not None: operands.append(existing)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["storeIndex"] = (storeIndex if (
    isinstance(storeIndex, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(storeIndex, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value[_ods_ir.FloatType]:
    return self.operation.operands[0]

  @builtins.property
  def stochiasticParam(self) -> _ods_ir.Value[_ods_ir.IntegerType]:
    return self.operation.operands[1]

  @builtins.property
  def existing(self) -> _Optional[_ods_ir.Value[_ods_ir.VectorType]]:
    return None if len(self.operation.operands) < 3 else self.operation.operands[2]

  @builtins.property
  def storeIndex(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["storeIndex"]

  @storeIndex.setter
  def storeIndex(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["storeIndex"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult[_ods_ir.VectorType]:
    return self.operation.results[0]

def packed_stoch_round_fp8(res, source, stochiastic_param, store_index, *, existing=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PackedStochRoundFp8Op(res=res, source=source, stochiasticParam=stochiastic_param, storeIndex=store_index, existing=existing, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class PackedTrunc2xFp8Op(_ods_ir.OpView):
  r"""
  Round the inputs `sourceA` and `sourceB` (which is undefined if not
  specified) into the low or high word (bottom two or top two) elements
  of the returned vector, keeping the other two elements of `existing`
  unchanged if present (or undefined if it was not passed in).
  
  The reason for this odd signature is that AMD GPUs cannot easily work with
  sub-registers, and so the conversion intrinsics (which are currently the
  only way to work with 8-bit float types) take packed vectors of 4 8-bit
  values.
  """

  OPERATION_NAME = "amdgpu.packed_trunc_2xfp8"

  _ODS_OPERAND_SEGMENTS = [1,0,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, res, sourceA, wordIndex, *, sourceB=None, existing=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(existing)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["wordIndex"] = (wordIndex if (
    isinstance(wordIndex, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(wordIndex, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value[_ods_ir.FloatType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def sourceB(self) -> _Optional[_ods_ir.Value[_ods_ir.FloatType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def existing(self) -> _Optional[_ods_ir.Value[_ods_ir.VectorType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def wordIndex(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["wordIndex"]

  @wordIndex.setter
  def wordIndex(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["wordIndex"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult[_ods_ir.VectorType]:
    return self.operation.results[0]

def packed_trunc_2xfp8(res, source_a, word_index, *, source_b=None, existing=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PackedTrunc2xFp8Op(res=res, sourceA=source_a, wordIndex=word_index, sourceB=source_b, existing=existing, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class PermlaneSwapOp(_ods_ir.OpView):
  r"""
  High-level wrapper on `rocdl.permlane{16,32}.swap` variants for permutations
  on rows of lanes in a subgroup.
  
  Supports arbitrary int/float/vector types, which will be repacked to i32 and
  one or more `rocdl.permlane_swap` ops during lowering.
  Supported lane permutations:
  - Swap the data between odd and even rows of 16 lanes
  - Swap the data between the first 32 lanes and the last 32 lanes
  
  Example:
  ```mlir
  %0 = amdgpu.permlane_swap %src 16 : f16
  %1 = amdgpu.permlane_swap %src 32 { fetch_inactive = true, bound_ctrl = true } : f16
  ```
  
  Operands:
  * `$src`: Vector register to permute across lanes of the subgroup.
  * `$row_length`: The length of a row to permute in number of lanes (valid values are 16 and 32).
  * `$fetch_inactive`: Optional. Used to dertermine behavior of a fetch from a disabled lane.
    `fetch_inactive = false`: If the source lane is disabled, use `bound_ctrl` to determine the source value.
    `fetch_inactive = true`: If the source lane is disabled, fetch the source value anyway (ignoring `bound_ctrl`).
  * `$bound_ctrl`: Optional. Used to determine what a thread should do if its source operand is from
    a disabled lane: use the value zero, or disable the write.
    `bound_ctrl = false`: Do not write when source is from a disabled lane
    `bound_ctrl = true`: Use zero as input if source is from a disabled lane
  
  Note: Lowering is only supported on gfx950 and up.
  """

  OPERATION_NAME = "amdgpu.permlane_swap"

  _ODS_REGIONS = (0, True)

  def __init__(self, src, row_length, *, fetch_inactive=None, bound_ctrl=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["row_length"] = (row_length if (
    isinstance(row_length, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(row_length, context=_ods_context))
    if fetch_inactive is not None: attributes["fetch_inactive"] = (fetch_inactive if (
        isinstance(fetch_inactive, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(fetch_inactive, context=_ods_context))
    if bound_ctrl is not None: attributes["bound_ctrl"] = (bound_ctrl if (
        isinstance(bound_ctrl, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(bound_ctrl, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def row_length(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["row_length"]

  @row_length.setter
  def row_length(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["row_length"] = value

  @builtins.property
  def fetch_inactive(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["fetch_inactive"]

  @fetch_inactive.setter
  def fetch_inactive(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["fetch_inactive"] = value

  @builtins.property
  def bound_ctrl(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["bound_ctrl"]

  @bound_ctrl.setter
  def bound_ctrl(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["bound_ctrl"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def permlane_swap(src, row_length, *, fetch_inactive=None, bound_ctrl=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PermlaneSwapOp(src=src, row_length=row_length, fetch_inactive=fetch_inactive, bound_ctrl=bound_ctrl, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicCmpswapOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_cmpswap` op is a wrapper around the
  buffer-based atomic compare-and-swap min available on AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_cmpswap"

  _ODS_OPERAND_SEGMENTS = [1,1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, src, cmp, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    operands.append(cmp)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def cmp(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 4)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

  @builtins.property
  def value(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def raw_buffer_atomic_cmpswap(src, cmp, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return RawBufferAtomicCmpswapOp(src=src, cmp=cmp, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicFaddOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_fadd` op is a wrapper around the
  buffer-based atomic floating point addition available on the MI-* series
  of AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_fadd"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_fadd(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicFaddOp:
  return RawBufferAtomicFaddOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicFmaxOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_fmax` op is a wrapper around the
  buffer-based atomic floating point max available on AMD GPUs (except GFX9).
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_fmax"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_fmax(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicFmaxOp:
  return RawBufferAtomicFmaxOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicSmaxOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_smax` op is a wrapper around the
  buffer-based atomic signed integer max available on AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_smax"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value[_ods_ir.IntegerType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_smax(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicSmaxOp:
  return RawBufferAtomicSmaxOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicUminOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_umin` op is a wrapper around the
  buffer-based atomic signed integer min available on AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_umin"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value[_ods_ir.IntegerType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_umin(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicUminOp:
  return RawBufferAtomicUminOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferLoadOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_load` op is a wrapper around the buffer load intrinsics
  available on AMD GPUs, including extensions in newer GPUs.
  
  The index into the buffer is computed as for `memref.load` with the additon
  of `indexOffset` and `sgprOffset` (which **may or may not** be considered
  in bounds checks and includes any offset present on the memref type if it's
  non-zero).
  
  All indices and offsets are in units of the memref's data type and are
  converted to bytes during lowering.
  
  When a load is out of bounds, the instruction returns zero.
  Partially-out of bounds have chipset-dependent behavior: whether reading
  2 elements starting at index 7 of a `memref<8xf32>` returns the last element
  in the first vector component depends on the architecture.
  
  The memref struct is converted into a buffer resource (a V#) and the arguments
  are translated to intrinsic arguments as follows:
  - The base address of the buffer is the base address of the memref
  - The stride is 0 to enable raw mode
  - The number of records is the size of the memref, in bytes
    In the case of dynamically-shaped memrefs, this is computed at runtime
    as max_d (size(d) * stride(d)) * sizeof(elementType(memref))
  - The offset enable bit is 1, the index enable bit is 0.
  - The thread ID addition bit is off
  - If `boundsCheck` is false and the target chipset is RDNA, OOB_SELECT is set
    to 2 to disable bounds checks, otherwise it is 3
  - The cache coherency bits are off
  """

  OPERATION_NAME = "amdgpu.raw_buffer_load"

  _ODS_OPERAND_SEGMENTS = [1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    results.append(value)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def memref(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

  @builtins.property
  def value(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def raw_buffer_load(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return RawBufferLoadOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class RawBufferStoreOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_store` op is a wrapper around the buffer store
  intrinsics available on AMD GPUs, including extensions in newer GPUs.
  
  The store index is computed as in `memref.store` with the addition of
  `indexOffset` (which is included for uniformity with atomics and may be useful
  when writing vectorized code) and `sgprOffset` (which is added after bounds
  checks and implicitly includes the offset of the memref type if non-zero).
  All index components are in terms of the elements of the memref, not bytes,
  and are scaled up appropriately.
  
  Out of bounds stores are ignored in hardware.
  Wthether a vector write that includes some in-bounds and soeme out-of-bounds
  components is partically completed is chipset-dependent.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_store"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value[_ods_ir.IntegerType]]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_store(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferStoreOp:
  return RawBufferStoreOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ScaledExtPackedMatrixOp(_ods_ir.OpView):
  r"""
  Extend matrix of microfloats (8 or 16 elements per lane) using a set of scales
  that may be stored on other lanes.
  
  The scales applied to the input microfloats are stored in bytes which
  come from the `scales` input provided in a *half* of the wave identified
  by `firstScaleLane`. The bytes used is selected by `firstScaleByte` and depends
  on the type of `source`. The 16 vectors in consecutive lanes starting from
  `firstScaleLane` (which we'll call the scale vectors) will be used by both
  halves of the wave (with lane L reading from L % 16'th scale vector).
  
  When `source` is either F4E2M1FN, F6E2M3FN, or F6E3M2FN each half of the
  wave will use a different byte. The first one being `firstScaleByte` and
  the second one being `firstScaleByte` + 1. When the block size is 32,
  `firstScaleByte` can be either 0 or 2, selecting halves of the scale vectors.
  Lanes 0-15 will read from `firstScaleByte` and lanes 16-31 will read
  from `firstScaleByte` + 1.
  
  
  For example:
  ```mlir
  // Input: 8-element vector of F8E4M3FN, converting to F32
  // Lanes 0-15 read from byte 0, lanes 16-31 read from byte 1
  %result = amdgpu.scaled_ext_packed_matrix %source scale(%scales)
    blockSize(32) firstScaleLane(0) firstScaleByte(0)
    : vector<8xf8E4M3FN>, vector<4xf8E8M0FNU> -> vector<8xf32>
  
  // Input: 16-element vector of F6E2M3FN, converting to F16
  // Lanes 0-15 read from byte 2, lanes 16-31 read from byte 3
  %result = amdgpu.scaled_ext_packed_matrix %source scale(%scales)
    blockSize(32) firstScaleLane(16) firstScaleByte(2)
    : vector<16xf6E2M3FN>, vector<4xf8E8M0FNU> -> vector<16xf16>
  ```
  
  When `source` is either F4E2M1FN, F6E2M3FN, or F6E3M2FN and
  the block size is 16, `firstScaleByte` can be 0 or 1.
  Lanes 0-15 read from the `firstScaleByte`th element of the scale vectors,
  while lanes 16-31 read from `firstScaleByte` + 2.
  For example:
  ```mlir
  // Input: 8-element vector of F8E5M2, converting to BF16
  // Lanes 0-15 read from byte 0, lanes 16-31 read from byte 2 (0+2)
  %result = amdgpu.scaled_ext_packed_matrix %source scale(%scales)
    blockSize(16) firstScaleLane(0) firstScaleByte(0)
    : vector<8xf8E5M2>, vector<4xf8E8M0FNU> -> vector<8xbf16>
  
  // Input: 16-element vector of F6E3M2FN, converting to F32
  // Lanes 0-15 read from byte 1, lanes 16-31 read from byte 3 (1+2)
  %result = amdgpu.scaled_ext_packed_matrix %source scale(%scales)
    blockSize(16) firstScaleLane(16) firstScaleByte(1)
    : vector<16xf6E3M2FN>, vector<4xf8E8M0FNU> -> vector<16xf32>
  ```
  
  Note: the layout for the scales generally mirrors how the WMMA
  instructions use for matrix scales. These selection operands allows
  one to choose portions of the matrix to convert.
  
  When `source` is either F8E4M3FN or F8E5M2 and `blockSize` is 32,
  then the same byte will be used by both halves of the wave.
  In this case, `firstScaleByte` can be any value from 0 to 3.
  
  When `source` is either F8E4M3FN or F8E5M2 and `blockSize` is 16,
  following combinations are allowed:
  * `firstScaleLane(0), firstScaleByte(0)`
  * `firstScaleLane(16), firstScaleByte(2)`
  all other combinations are reserved.
  
  Available on gfx1250+.
  """

  OPERATION_NAME = "amdgpu.scaled_ext_packed_matrix"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, scale, blockSize, firstScaleLane, firstScaleByte, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(scale)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["blockSize"] = (blockSize if (
    isinstance(blockSize, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(blockSize, context=_ods_context))
    attributes["firstScaleLane"] = (firstScaleLane if (
    isinstance(firstScaleLane, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(firstScaleLane, context=_ods_context))
    attributes["firstScaleByte"] = (firstScaleByte if (
    isinstance(firstScaleByte, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(firstScaleByte, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def scale(self) -> _ods_ir.Value[_ods_ir.VectorType]:
    return self.operation.operands[1]

  @builtins.property
  def blockSize(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["blockSize"]

  @blockSize.setter
  def blockSize(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["blockSize"] = value

  @builtins.property
  def firstScaleLane(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["firstScaleLane"]

  @firstScaleLane.setter
  def firstScaleLane(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["firstScaleLane"] = value

  @builtins.property
  def firstScaleByte(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["firstScaleByte"]

  @firstScaleByte.setter
  def firstScaleByte(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["firstScaleByte"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def scaled_ext_packed_matrix(res, source, scale, block_size, first_scale_lane, first_scale_byte, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return ScaledExtPackedMatrixOp(res=res, source=source, scale=scale, blockSize=block_size, firstScaleLane=first_scale_lane, firstScaleByte=first_scale_byte, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ScaledExtPackedOp(_ods_ir.OpView):
  r"""
  Extend and scale two packed floats in `source[index]` to two floats and
  return them.
  
  This rather unusual signature arises from the fact that AMD GPUs cannot
  easily work with sub 32-bit quantities, so the compiler intrinsics for
  extending 8-bit floats (which are, currently, the only way to work with
  this operation) take packed vectors of 2 such floats.
  
  If the passed-in vector has fewer than two elements, or the input is scalar,
  the remaining values in the <2 x i8> will be filled with
  undefined values as needed.
  """

  OPERATION_NAME = "amdgpu.scaled_ext_packed"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, scale, index, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(scale)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["index"] = (index if (
    isinstance(index, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(index, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def scale(self) -> _ods_ir.Value[_ods_ir.FloatType]:
    return self.operation.operands[1]

  @builtins.property
  def index(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["index"]

  @index.setter
  def index(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["index"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def scaled_ext_packed(res, source, scale, index, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return ScaledExtPackedOp(res=res, source=source, scale=scale, index=index, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ScaledMFMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.scaled_mfma` op is an MLIR wrapper around intrinsics
  for various scaled versions of `mfma` instructions in the CDNA architecture, which
  perform multiple outer products in order to allow fast matrix multiplication.
  
  The wrapper will select an appropriate `mfma` instruction, if one is available,
  based on the provided `m`, `k`, `n`, and `nBlks` attributes, along with the
  types of the source and destination arguments.
  
  Note, this wrapper allows specifying `vector<4Kxi8>` arguments to MFMA
  intrinsics that take an integer type of width `4K`. For example,
  one can provide a `vector<4xi8>` as an argument to an MFMA instruction that
  logically takes 4 i8s but whose intrinsics are specified to take an i32.
  In these cases, the bytes in the vector will be concatenated in little-endian
  order (that is, v[0] will go to arg[7:0], v[1] to arg[15:8] and so on).
  
  This wrapper takes inspiration from `amdgpu.mfma`, but has some key differences:
  - `amdgpu.scaled_mfma` operates on fp4 (f4E2M1FN), fp6 (f6E2M3FN and f6E3M2FN) and
    fp8 (f8E4M3FN and f8E5M2) types using either M=N=16, K=128 or M=N=32, K=64 as
    their tile size.
  - `amdgpu.scaled_mfma` does not support broadcasting. So, `cbsz`, `abid`, and `blgp`
    are omitted from this wrapper.
  - The `negateA`, `negateB`, and `negateC` flags in `amdgpu.mfma` are only supported
    for double-precision operations on gfx94x and so are not included here.
  
  Example:
  ```mlir
    %0 = amdgpu.scaled_mfma 32x32x64 (%arg0[0] * %arg1) * (%arg0[1] * %arg1) + %arg2
      : vector<4xf8E8M0FNU>, vector<32xf6E2M3FN>, f8E8M0FNU, vector<32xf6E2M3FN>, vector<16xf32>
  ```
  """

  OPERATION_NAME = "amdgpu.scaled_mfma"

  _ODS_REGIONS = (0, True)

  def __init__(self, m, n, k, sourceA, sourceB, destC, scalesA, scalesB, scalesIdxA, scalesIdxB, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    operands.append(scalesA)
    operands.append(scalesB)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["m"] = (m if (
    isinstance(m, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(m, context=_ods_context))
    attributes["n"] = (n if (
    isinstance(n, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(n, context=_ods_context))
    attributes["k"] = (k if (
    isinstance(k, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(k, context=_ods_context))
    attributes["scalesIdxA"] = (scalesIdxA if (
    isinstance(scalesIdxA, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(scalesIdxA, context=_ods_context))
    attributes["scalesIdxB"] = (scalesIdxB if (
    isinstance(scalesIdxB, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(scalesIdxB, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def scalesA(self) -> _ods_ir.Value:
    return self.operation.operands[3]

  @builtins.property
  def scalesB(self) -> _ods_ir.Value:
    return self.operation.operands[4]

  @builtins.property
  def m(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["m"]

  @m.setter
  def m(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["m"] = value

  @builtins.property
  def n(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["n"]

  @n.setter
  def n(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["n"] = value

  @builtins.property
  def k(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["k"]

  @k.setter
  def k(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["k"] = value

  @builtins.property
  def scalesIdxA(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["scalesIdxA"]

  @scalesIdxA.setter
  def scalesIdxA(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["scalesIdxA"] = value

  @builtins.property
  def scalesIdxB(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["scalesIdxB"]

  @scalesIdxB.setter
  def scalesIdxB(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["scalesIdxB"] = value

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def scaled_mfma(m, n, k, source_a, source_b, dest_c, scales_a, scales_b, scales_idx_a, scales_idx_b, *, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return ScaledMFMAOp(m=m, n=n, k=k, sourceA=source_a, sourceB=source_b, destC=dest_c, scalesA=scales_a, scalesB=scales_b, scalesIdxA=scales_idx_a, scalesIdxB=scales_idx_b, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ScaledWMMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.scaled_wmma` op is an MLIR wrapper around intrinsics for scaled
  `wmma` instructions. These instructions perform matrix multiplication with
  per-block scaling of inputs, supporting fp4, fp6, and fp8 data formats.
  
  The scale instructions support a block size of 16 or 32 and two tile sizes:
  - 16x16x128 with mixed f8/f6/f4 formats (output: vector<8xf32>)
  - 32x16x128 with f4 format only (output: vector<16xf32>)
  
  Scale parameters (`scaleA`, `scaleB`) are small vectors of f8 scale values
  (either f8E8M0FNU, or f8E4M3FN) that are packed into i32/i64 values during
  lowering. Each lane can operate on 4 bytes (4 scale values), and the
  number of scales required for each matrix is determined by:
    num_scales_A = (M × K) / block_size
    num_scales_B = (N × K) / block_size
    
  The index attributes (`a_first_scale_lane`, `b_first_scale_lane`) select
  which lane to start reading scale values from (0 or 16):
  - For block size 32, 32 lanes across a single wave are used for the scale
  values. If the number of scales (num_scales_A or num_scales_B) can fit
  into half of the available lanes
  (i.e., num_scales / scales_per_lane == 16 (num_lanes)),
  then then first_scale_lane can be either 0 or 16. If all lanes are required
  for storing the scale values (num_scales / scales_per_lane == 32 (num_lanes)),
  then the first_scale_lane must be 0.
  - For block size 16, the same rules apply as above except that there are 64
  lanes across two waves that are used for the scale values. When
  num_scales / scales_per_lane == 32 (num lanes), then 16 lanes from each wave are used.
  first_scale_lane of 0 or 16 will decide which lanes are used for this. When
  num_scales / scales_per_lane == 64 (num_lanes), then first_scale_lane must
  be set to 0.
  
  Example:
  ```mlir
    // 16x16x128: fp8 inputs
    %0 = amdgpu.scaled_wmma 16x16x128 (%scaleVecA * %matA) * (%scaleVecB * %matB) + %matC
      {a_first_scale_lane = 0 : i32, b_first_scale_lane = 0 : i32}
      : vector<4xf8E8M0FNU>, vector<64xf8E4M3FN>,
      vector<4xf8E8M0FNU>, vector<64xf8E4M3FN>, vector<8xf32>
  
    // 32x16x128: fp4 inputs with different scale lanes
    %1 = amdgpu.scaled_wmma 32x16x128 (%scaleVecD * %matD) * (%scaleVecE * %matE) + %matF
      {a_first_scale_lane = 0 : i32, b_first_scale_lane = 16 : i32}
      : vector<8xf8E4M3FN>, vector<128xf4E2M1FN>,
      vector<8xf8E4M3FN>, vector<64xf4E2M1FN>, vector<16xf32>
  ```
  """

  OPERATION_NAME = "amdgpu.scaled_wmma"

  _ODS_REGIONS = (0, True)

  def __init__(self, m, n, k, sourceA, sourceB, destC, scaleA, a_first_scale_lane, scaleB, b_first_scale_lane, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    operands.append(scaleA)
    operands.append(scaleB)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["m"] = (m if (
    isinstance(m, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(m, context=_ods_context))
    attributes["n"] = (n if (
    isinstance(n, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(n, context=_ods_context))
    attributes["k"] = (k if (
    isinstance(k, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(k, context=_ods_context))
    attributes["a_first_scale_lane"] = (a_first_scale_lane if (
    isinstance(a_first_scale_lane, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(a_first_scale_lane, context=_ods_context))
    attributes["b_first_scale_lane"] = (b_first_scale_lane if (
    isinstance(b_first_scale_lane, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(b_first_scale_lane, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def scaleA(self) -> _ods_ir.Value[_ods_ir.VectorType]:
    return self.operation.operands[3]

  @builtins.property
  def scaleB(self) -> _ods_ir.Value[_ods_ir.VectorType]:
    return self.operation.operands[4]

  @builtins.property
  def m(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["m"]

  @m.setter
  def m(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["m"] = value

  @builtins.property
  def n(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["n"]

  @n.setter
  def n(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["n"] = value

  @builtins.property
  def k(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["k"]

  @k.setter
  def k(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["k"] = value

  @builtins.property
  def a_first_scale_lane(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["a_first_scale_lane"]

  @a_first_scale_lane.setter
  def a_first_scale_lane(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["a_first_scale_lane"] = value

  @builtins.property
  def b_first_scale_lane(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["b_first_scale_lane"]

  @b_first_scale_lane.setter
  def b_first_scale_lane(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["b_first_scale_lane"] = value

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def scaled_wmma(m, n, k, source_a, source_b, dest_c, scale_a, a_first_scale_lane, scale_b, b_first_scale_lane, *, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return ScaledWMMAOp(m=m, n=n, k=k, sourceA=source_a, sourceB=source_b, destC=dest_c, scaleA=scale_a, a_first_scale_lane=a_first_scale_lane, scaleB=scale_b, b_first_scale_lane=b_first_scale_lane, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class SchedBarrierOp(_ods_ir.OpView):
  r"""
  `amdgpu.sched_barrier` serves as a barrier that could be
  configured to restrict movements of instructions through it as
  defined by sched_barrier_opts.
  """

  OPERATION_NAME = "amdgpu.sched_barrier"

  _ODS_REGIONS = (0, True)

  def __init__(self, opts, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["opts"] = (opts if (
    isinstance(opts, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('AMDGPU_SchedBarrierOpOptAttr')) else
      _ods_ir.AttrBuilder.get('AMDGPU_SchedBarrierOpOptAttr')(opts, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def opts(self) -> _ods_ir.Attribute:
    return self.operation.attributes["opts"]

  @opts.setter
  def opts(self, value: _ods_ir.Attribute):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["opts"] = value

def sched_barrier(opts, *, loc=None, ip=None) -> SchedBarrierOp:
  return SchedBarrierOp(opts=opts, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class SparseMFMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.sparse_mfma` op is an MLIR wrapper around intrinsics for various
  `smfmac` instructions in the AMDGPU architecture, which perform matrix
  multiply-accumulate operations using 2:4 structured sparsity on matrix A
  with dense matrices B, C, and D.
  
  On gfx942, smfmac intrinsics support:
    - M=N=16, K=32 and M=N=32, K=16 for f16 and bf16 sources
    - M=N=16, K=64 and M=N=32, K=32 for i8 and fp8 sources
  
  On gfx950, smfmac intrinsics additionally support:
    - M=N=16, K=64 and M=N=32, K=32 for f16 and bf16 sources
    - M=N=16, K=128 and M=N=32, K=64 for i8 and fp8 sources
  
  The `sparseIdx` parameter contains packed indices identifying the positions
  of non-zero elements in the 2:4 sparse matrix A. For 16-bit source data,
  use `vector<4xi8>` (four 8-bit indices). For 8-bit source data, use
  `vector<2xi16>` (two 16-bit indices).
  
  The `cbsz` and `abid` parameters are repurposed to select the index set.
  If `cbsz == 0`, then `abid[1:0]` selects which index set to use. 
  If `cbsz != 0`, then the very first is selected.
  
  Example:
  ```mlir
    %0 = amdgpu.sparse_mfma 16x16x32 %matA * %matB + %matC sparse(%idx : vector<4xi8>)
      : vector<4xf16>, vector<8xf16>, vector<4xf32>
  
    %1 = amdgpu.sparse_mfma 16x16x64 %matA * %matB + %matC sparse(%idx : vector<2xi16>)
      : vector<8xi8>, vector<16xi8>, vector<4xi32>
  
    %2 = amdgpu.sparse_mfma 16x16x64 %matA * %matB + %matC sparse(%idx : vector<2xi16>)
      { cbsz = 0 : i32, abid = 1 : i32 }
      : vector<8xf8E4M3FNUZ>, vector<16xf8E4M3FNUZ>, vector<4xf32>
  ```
  """

  OPERATION_NAME = "amdgpu.sparse_mfma"

  _ODS_REGIONS = (0, True)

  def __init__(self, m, n, k, sourceA, sourceB, destC, sparseIdx, *, cbsz=None, abid=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    operands.append(sparseIdx)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["m"] = (m if (
    isinstance(m, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(m, context=_ods_context))
    attributes["n"] = (n if (
    isinstance(n, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(n, context=_ods_context))
    attributes["k"] = (k if (
    isinstance(k, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(k, context=_ods_context))
    if cbsz is not None: attributes["cbsz"] = (cbsz if (
        isinstance(cbsz, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(cbsz, context=_ods_context))
    if abid is not None: attributes["abid"] = (abid if (
        isinstance(abid, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(abid, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def sparseIdx(self) -> _ods_ir.Value:
    return self.operation.operands[3]

  @builtins.property
  def m(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["m"]

  @m.setter
  def m(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["m"] = value

  @builtins.property
  def n(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["n"]

  @n.setter
  def n(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["n"] = value

  @builtins.property
  def k(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["k"]

  @k.setter
  def k(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["k"] = value

  @builtins.property
  def cbsz(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["cbsz"]

  @cbsz.setter
  def cbsz(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["cbsz"] = value

  @builtins.property
  def abid(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["abid"]

  @abid.setter
  def abid(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["abid"] = value

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def sparse_mfma(m, n, k, source_a, source_b, dest_c, sparse_idx, *, cbsz=None, abid=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return SparseMFMAOp(m=m, n=n, k=k, sourceA=source_a, sourceB=source_b, destC=dest_c, sparseIdx=sparse_idx, cbsz=cbsz, abid=abid, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class SwizzleBitModeOp(_ods_ir.OpView):
  r"""
  High-level wrapper on bitmode `rocdl.ds_swizzle` op, masks are represented
  as separate fields so user won't need to do manual bitpacking.
  
  Supports arbitrary int/float/vector types, which will be repacked to i32 and
  one or more `rocdl.ds_swizzle` ops during lowering.
  """

  OPERATION_NAME = "amdgpu.swizzle_bitmode"

  _ODS_REGIONS = (0, True)

  def __init__(self, src, and_mask, or_mask, xor_mask, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["and_mask"] = (and_mask if (
    isinstance(and_mask, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(and_mask, context=_ods_context))
    attributes["or_mask"] = (or_mask if (
    isinstance(or_mask, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(or_mask, context=_ods_context))
    attributes["xor_mask"] = (xor_mask if (
    isinstance(xor_mask, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(xor_mask, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def and_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["and_mask"]

  @and_mask.setter
  def and_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["and_mask"] = value

  @builtins.property
  def or_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["or_mask"]

  @or_mask.setter
  def or_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["or_mask"] = value

  @builtins.property
  def xor_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["xor_mask"]

  @xor_mask.setter
  def xor_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["xor_mask"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def swizzle_bitmode(src, and_mask, or_mask, xor_mask, *, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return SwizzleBitModeOp(src=src, and_mask=and_mask, or_mask=or_mask, xor_mask=xor_mask, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class TensorLoadToLDSOp(_ods_ir.OpView):
  r"""
  Load tensors of up to five dimensions from global memory to LDS.
  
  This operation was introduced in gfx1250.
  """

  OPERATION_NAME = "amdgpu.tensor_load_to_lds"

  _ODS_REGIONS = (0, True)

  def __init__(self, desc, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(desc)
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def desc(self) -> _ods_ir.Value:
    return self.operation.operands[0]

def tensor_load_to_lds(desc, *, loc=None, ip=None) -> TensorLoadToLDSOp:
  return TensorLoadToLDSOp(desc=desc, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class TensorStoreFromLDSOp(_ods_ir.OpView):
  r"""
  Store tensors of up to five dimensions from LDS to global memory.
  
  This operation was introduced in gfx1250.
  """

  OPERATION_NAME = "amdgpu.tensor_store_from_lds"

  _ODS_REGIONS = (0, True)

  def __init__(self, desc, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(desc)
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def desc(self) -> _ods_ir.Value:
    return self.operation.operands[0]

def tensor_store_from_lds(desc, *, loc=None, ip=None) -> TensorStoreFromLDSOp:
  return TensorStoreFromLDSOp(desc=desc, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class TransposeLoadOp(_ods_ir.OpView):
  r"""
  The `amdgpu.transpose_load` op is a wrapper around the `ds_read_tr` instructions.
  The transpose load op represents a subgroup load from LDS memory,
  where the subgroup of threads collectively reads a matrix from the source
  memref, with each thread reading a vector of the matrix, and gets a transposed matrix
  in as the result. That is, each thread reads a vector of the col-major matrix at different
  indices, and the thread's read result is a vector of the corresponding row of the transposed
  matrix.
  
  This op is a direct wrapper around the ROCDL `ds_read_tr` family intrinsics. Please refer
  to the CDNA4 ISA documentation for more details about its exact semantics.
  
  Format example:
  ```
  %0 = amdgpu.transpose_load %src[%srcIndices] : memref<128x256xf16> -> vector<4xf16>
  ```
  Operands:
  * `$src`: LDS memref to read from.
  * `$srcIndices`: indices into `$src` to read from for this thread.
  * `$result`: target register this transpose load instruction will write to.
  
  Note: Lowering is only supported on gfx950 and up.
  """

  OPERATION_NAME = "amdgpu.transpose_load"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, src, srcIndices, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    operands.extend(_get_op_results_or_values(srcIndices))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value[_ods_ir.MemRefType]:
    return self.operation.operands[0]

  @builtins.property
  def srcIndices(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 2 + 1
    return self.operation.operands[1:1 + _ods_variadic_group_length]

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def transpose_load(result, src, src_indices, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return TransposeLoadOp(result=result, src=src, srcIndices=src_indices, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class WMMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.wmma` op is an MLIR wrapper around intrinsics for various `wmma`
  instructions in the AMDGPU architecture, which perform matrix multiplication.
  
  On gfx11/RDNA3, wmma intrinsics have M=N=K=16 dimensions.
  
  On gfx12/RDNA4, wmma intrinsics have M=N=16 dimensions and support K=16 for
  all element types, and K=32 for i4 sources.
  
  On gfx1250, wmma intrinsics have M=N=16 and K dimensions of 4, 32, 64, or 128,
  depending on the element types.
  
  On gfx11/RDNA3, emitting f16->f16 (or bf16->bf16) wmma the output is a 16xf16
  (or 16xbf16) vector containing only 8 valid values:
    - If `subwordOffset` is 0, then the output is stored at indices 0, 2, 4, ..., 14.
    - If `subwordOffset` is 1, then the output is stored at indices 1, 3, 5, ..., 15.
  On gfx12/RDNA4 and gfx1250, the result is instead returned as vector where all
  the values are valid and the `subwordOffset` must be `0`, as it cannot be used.
  
  `unsignedA` and `unsignedB` flag that the `int8` LLVM inputs are unsigned.
  
  The `clamp` flag is used to saturate the output of type T to `numeric_limits<T>::max()`
  in case of overflow.
  
  Example:
  ```mlir
    %0 = amdgpu.wmma 16x16x16 %matA * %matB + %matC : vector<8xf16>, vector<8xf16>, vector<8xf16>
  
    %1 = amdgpu.wmma 16x16x64 %matD * %matE + %matF : vector<32xi8>, vector<8xf32>, vector<8xf32>
  
    %2 = amdgpu.wmma 16x16x128 %matG * %matH + %matI : vector<64xf4E2M1FN>, vector<64xf4E2M1FN>, vector<8xf32>
  
    %3 = amdgpu.wmma 16x16x4 %matJ * %matK + %matL : vector<2xf32>, vector<2xf32>, vector<8xf32>
  ```
  """

  OPERATION_NAME = "amdgpu.wmma"

  _ODS_REGIONS = (0, True)

  def __init__(self, m, n, k, sourceA, sourceB, destC, *, subwordOffset=None, unsignedA=None, unsignedB=None, clamp=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["m"] = (m if (
    isinstance(m, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(m, context=_ods_context))
    attributes["n"] = (n if (
    isinstance(n, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(n, context=_ods_context))
    attributes["k"] = (k if (
    isinstance(k, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(k, context=_ods_context))
    if subwordOffset is not None: attributes["subwordOffset"] = (subwordOffset if (
        isinstance(subwordOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(subwordOffset, context=_ods_context))
    if bool(unsignedA): attributes["unsignedA"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(unsignedB): attributes["unsignedB"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(clamp): attributes["clamp"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def m(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["m"]

  @m.setter
  def m(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["m"] = value

  @builtins.property
  def n(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["n"]

  @n.setter
  def n(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["n"] = value

  @builtins.property
  def k(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["k"]

  @k.setter
  def k(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["k"] = value

  @builtins.property
  def subwordOffset(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["subwordOffset"]

  @subwordOffset.setter
  def subwordOffset(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["subwordOffset"] = value

  @builtins.property
  def unsignedA(self) -> bool:
    return "unsignedA" in self.operation.attributes

  @unsignedA.setter
  def unsignedA(self, value):
    if bool(value):
      self.operation.attributes["unsignedA"] = _ods_ir.UnitAttr.get()
    elif "unsignedA" in self.operation.attributes:
      del self.operation.attributes["unsignedA"]

  @unsignedA.deleter
  def unsignedA(self):
    del self.operation.attributes["unsignedA"]

  @builtins.property
  def unsignedB(self) -> bool:
    return "unsignedB" in self.operation.attributes

  @unsignedB.setter
  def unsignedB(self, value):
    if bool(value):
      self.operation.attributes["unsignedB"] = _ods_ir.UnitAttr.get()
    elif "unsignedB" in self.operation.attributes:
      del self.operation.attributes["unsignedB"]

  @unsignedB.deleter
  def unsignedB(self):
    del self.operation.attributes["unsignedB"]

  @builtins.property
  def clamp(self) -> bool:
    return "clamp" in self.operation.attributes

  @clamp.setter
  def clamp(self, value):
    if bool(value):
      self.operation.attributes["clamp"] = _ods_ir.UnitAttr.get()
    elif "clamp" in self.operation.attributes:
      del self.operation.attributes["clamp"]

  @clamp.deleter
  def clamp(self):
    del self.operation.attributes["clamp"]

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def wmma(m, n, k, source_a, source_b, dest_c, *, subword_offset=None, unsigned_a=None, unsigned_b=None, clamp=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return WMMAOp(m=m, n=n, k=k, sourceA=source_a, sourceB=source_b, destC=dest_c, subwordOffset=subword_offset, unsignedA=unsigned_a, unsignedB=unsigned_b, clamp=clamp, results=results, loc=loc, ip=ip).result
