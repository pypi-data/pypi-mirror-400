"""
æ•°æ®å¤„ç†æ¨¡å—

å¤„ç†å„ç§æ ¼å¼çš„è®­ç»ƒæ•°æ®
"""

import json
from pathlib import Path

from datasets import Dataset


def load_training_data(data_path: str | Path) -> list[dict]:
    """åŠ è½½è®­ç»ƒæ•°æ®

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ (.json æˆ– .jsonl)

    Returns:
        è®­ç»ƒæ ·æœ¬åˆ—è¡¨
    """
    data_path = Path(data_path)

    if not data_path.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

    if data_path.suffix == ".jsonl":
        # JSONL æ ¼å¼
        data = []
        with open(data_path) as f:
            for line in f:
                data.append(json.loads(line))
        return data
    elif data_path.suffix == ".json":
        # JSON æ ¼å¼
        with open(data_path) as f:
            return json.load(f)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_path.suffix}")


def format_alpaca_sample(sample: dict) -> dict:
    """æ ¼å¼åŒ– Alpaca æ ¼å¼çš„æ ·æœ¬

    Alpaca æ ¼å¼:
    {
        "instruction": "ä»»åŠ¡æè¿°",
        "input": "å¯é€‰çš„è¾“å…¥",
        "output": "æœŸæœ›çš„è¾“å‡º"
    }

    Args:
        sample: Alpaca æ ¼å¼æ ·æœ¬

    Returns:
        æ ¼å¼åŒ–åçš„æ ·æœ¬ {"text": "..."}
    """
    text = f"### Instruction:\n{sample['instruction']}\n\n"
    if sample.get("input"):
        text += f"### Input:\n{sample['input']}\n\n"
    text += f"### Response:\n{sample['output']}"
    return {"text": text}


def format_conversation_sample(sample: dict) -> dict:
    """æ ¼å¼åŒ–å¯¹è¯æ ¼å¼çš„æ ·æœ¬

    å¯¹è¯æ ¼å¼:
    {
        "conversations": [
            {"role": "user", "content": "..."},
            {"role": "assistant", "content": "..."},
            ...
        ]
    }

    Args:
        sample: å¯¹è¯æ ¼å¼æ ·æœ¬

    Returns:
        æ ¼å¼åŒ–åçš„æ ·æœ¬ {"text": "..."}
    """
    text = ""
    for msg in sample["conversations"]:
        role = msg["role"]
        content = msg["content"]
        if role == "user":
            text += f"### User:\n{content}\n\n"
        elif role == "assistant":
            text += f"### Assistant:\n{content}\n\n"
        elif role == "system":
            text += f"### System:\n{content}\n\n"
    return {"text": text.strip()}


def format_qa_sample(sample: dict) -> dict:
    """æ ¼å¼åŒ–é—®ç­”æ ¼å¼çš„æ ·æœ¬

    QA æ ¼å¼:
    {
        "question": "é—®é¢˜",
        "answer": "ç­”æ¡ˆ",
        "context": "å¯é€‰çš„ä¸Šä¸‹æ–‡"
    }

    Args:
        sample: QA æ ¼å¼æ ·æœ¬

    Returns:
        æ ¼å¼åŒ–åçš„æ ·æœ¬ {"text": "..."}
    """
    text = f"### Question:\n{sample['question']}\n\n"
    if sample.get("context"):
        text += f"### Context:\n{sample['context']}\n\n"
    text += f"### Answer:\n{sample['answer']}"
    return {"text": text}


def detect_data_format(sample: dict) -> str:
    """è‡ªåŠ¨æ£€æµ‹æ•°æ®æ ¼å¼

    Args:
        sample: æ ·æœ¬æ•°æ®

    Returns:
        æ ¼å¼ç±»å‹: "alpaca", "conversation", "qa", "text"
    """
    if "instruction" in sample and "output" in sample:
        return "alpaca"
    elif "conversations" in sample:
        return "conversation"
    elif "question" in sample and "answer" in sample:
        return "qa"
    elif "text" in sample:
        return "text"
    else:
        raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼: {sample.keys()}")


def prepare_dataset(
    data_path: str | Path,
    tokenizer,
    max_length: int = 1024,
    format_type: str | None = None,
) -> Dataset:
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›†

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        format_type: æ•°æ®æ ¼å¼ç±»å‹ï¼ˆNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰

    Returns:
        å¤„ç†åçš„ Dataset
    """
    # åŠ è½½æ•°æ®
    data = load_training_data(data_path)

    if len(data) == 0:
        raise ValueError("æ•°æ®é›†ä¸ºç©º")

    # æ£€æµ‹æ ¼å¼
    if format_type is None:
        format_type = detect_data_format(data[0])

    print(f"ğŸ“Š æ•°æ®æ ¼å¼: {format_type}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(data)}")

    # æ ¼å¼åŒ–æ•°æ®
    if format_type == "alpaca":
        formatted_data = [format_alpaca_sample(s) for s in data]
    elif format_type == "conversation":
        formatted_data = [format_conversation_sample(s) for s in data]
    elif format_type == "qa":
        formatted_data = [format_qa_sample(s) for s in data]
    elif format_type == "text":
        formatted_data = data
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹: {format_type}")

    # åˆ›å»º Dataset
    dataset = Dataset.from_list(formatted_data)

    # Tokenize
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length",
        )

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"],
        desc="Tokenizing",
    )

    print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
    return tokenized_dataset


def create_sample_data(output_path: str | Path, format_type: str = "alpaca", num_samples: int = 10):
    """åˆ›å»ºç¤ºä¾‹æ•°æ®

    Args:
        output_path: è¾“å‡ºè·¯å¾„
        format_type: æ•°æ®æ ¼å¼
        num_samples: æ ·æœ¬æ•°é‡
    """
    output_path = Path(output_path)

    if format_type == "alpaca":
        samples = [
            {
                "instruction": f"ç¤ºä¾‹ä»»åŠ¡ {i}",
                "input": f"ç¤ºä¾‹è¾“å…¥ {i}",
                "output": f"ç¤ºä¾‹è¾“å‡º {i}",
            }
            for i in range(num_samples)
        ]
    elif format_type == "qa":
        samples = [
            {
                "question": f"ç¤ºä¾‹é—®é¢˜ {i}",
                "answer": f"ç¤ºä¾‹ç­”æ¡ˆ {i}",
                "context": f"ç¤ºä¾‹ä¸Šä¸‹æ–‡ {i}",
            }
            for i in range(num_samples)
        ]
    elif format_type == "conversation":
        samples = [
            {
                "conversations": [
                    {"role": "user", "content": f"ç”¨æˆ·æ¶ˆæ¯ {i}"},
                    {"role": "assistant", "content": f"åŠ©æ‰‹å›å¤ {i}"},
                ]
            }
            for i in range(num_samples)
        ]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")

    with open(output_path, "w") as f:
        json.dump(samples, f, indent=2, ensure_ascii=False)

    print(f"âœ… åˆ›å»ºäº† {num_samples} ä¸ªç¤ºä¾‹æ ·æœ¬: {output_path}")
