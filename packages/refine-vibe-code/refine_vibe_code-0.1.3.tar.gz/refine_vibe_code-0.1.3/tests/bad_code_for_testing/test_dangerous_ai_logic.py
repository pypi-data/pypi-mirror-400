"""
Test file containing various dangerous AI logic patterns that should be detected
by the DangerousAILogicChecker.
"""

import torch
import tensorflow as tf
import openai
import pickle

# DANGEROUS: Hardcoded API key
OPENAI_API_KEY = "sk-1234567890abcdef1234567890abcdef1234567890"

def dangerous_model_loading():
    """Dangerous: Loading model from untrusted HTTP source."""
    # This should be flagged as dangerous
    model = torch.load("https://untrusted-source.com/malicious_model.pth")

    # Also dangerous: loading without weights_only parameter
    model2 = torch.load("model.pth")  # Missing weights_only=True

    return model, model2

def unsafe_pickle_usage():
    """Dangerous: Using pickle to load/save models."""
    # This is vulnerable to deserialization attacks
    with open("model.pkl", "rb") as f:
        model = pickle.load(f)

    # Also dangerous
    model = pickle.load(open("unsafe_model.pkl", "rb"))

    return model

def code_execution_from_ai_output():
    """CRITICAL: Executing code generated by AI models."""
    # This should be flagged as CRITICAL
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt="Generate Python code to calculate sum",
        max_tokens=100
    )

    # DANGEROUS: Direct execution of AI output
    code = response.choices[0].text.strip()
    result = eval(code)  # This is extremely dangerous!

    return result

def dangerous_training_loop():
    """Dangerous: Infinite training loop."""
    model = torch.nn.Linear(10, 1)

    # This could run forever and exhaust resources
    while True:
        model.fit(some_data)  # Should be flagged

def data_poisoning_vulnerable():
    """Dangerous: Training data from user input without validation."""
    def train_model():
        # CRITICAL: Direct user input into training data
        user_data = input("Enter training data: ")  # Dangerous!

        # Process and train
        model = torch.nn.Linear(10, 1)
        # ... training code ...

    return train_model

def unsafe_model_prediction():
    """Dangerous: No input validation before prediction."""
    model = torch.load("model.pth")

    # DANGEROUS: Direct prediction on user input without validation
    user_input = request.args.get('data')  # From web request
    prediction = model.predict(user_input)  # No validation!

    return prediction

def hardcoded_model_config():
    """Potentially dangerous: Hardcoded model parameters."""
    # These should be configurable, not hardcoded
    learning_rate = 0.001
    epochs = 100
    batch_size = 32

    model = torch.nn.Sequential(
        torch.nn.Linear(784, 128),
        torch.nn.ReLU(),
        torch.nn.Linear(128, 10)
    )

    # Training with hardcoded values
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    return model, optimizer

def model_inversion_risk():
    """Dangerous: Attempting to extract data from model."""
    model = torch.load("trained_model.pth")

    # POTENTIAL PRIVACY VIOLATION: Extracting parameters
    params = model.parameters()
    state_dict = model.state_dict()

    # This could be used for model inversion attacks
    def extract_training_data():
        # Code that attempts to recover training data from model
        pass

    return params, state_dict

def unsafe_serialization():
    """Dangerous: Unsafe model serialization."""
    model = torch.nn.Linear(10, 1)

    # DANGEROUS: Using pickle for model serialization
    with open("model.pkl", "wb") as f:
        pickle.dump(model, f)

    return model

def unvalidated_ai_input():
    """Dangerous: AI model input from untrusted sources."""
    import openai

    # User input directly to AI without validation
    user_query = request.form.get('query')  # No validation!

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": user_query}]
    )

    return response

def dangerous_tensorflow_loading():
    """Dangerous: Loading TensorFlow models unsafely."""
    # Loading from HTTP source
    model = tf.saved_model.load("https://malicious-site.com/model")

    # Also dangerous patterns
    model2 = tf.keras.models.load_model("http://unsafe.com/model.h5")

    return model, model2

def infinite_generation_loop():
    """Dangerous: Potential infinite generation loop."""
    while True:
        # This could generate forever
        response = openai.Completion.create(
            engine="text-davinci-003",
            prompt="Continue generating text",
            max_tokens=100
        )
        print(response.choices[0].text)

def unsafe_huggingface_usage():
    """Dangerous: Hardcoded HuggingFace token."""
    from transformers import pipeline

    # DANGEROUS: Hardcoded API token
    HF_TOKEN = "hf_abcdef1234567890abcdef1234567890abcdef"

    model = pipeline("text-generation", token=HF_TOKEN)

    return model

def no_error_handling_ai():
    """Dangerous: No error handling in AI pipeline."""
    model = torch.load("model.pth")

    # No try/catch, no validation
    result = model.predict(user_input)  # Could fail silently or dangerously

    return result

def bias_amplification_risk():
    """Potentially dangerous: No bias mitigation."""
    # Training on potentially biased data without checks
    model = torch.nn.Linear(100, 2)  # Binary classification

    # No fairness checks, no bias mitigation
    # No validation of training data diversity
    # No monitoring for biased predictions

    return model

# Additional dangerous patterns
def dangerous_exec_usage():
    """CRITICAL: Using exec with AI output."""
    ai_response = openai.Completion.create(
        engine="code-davinci-002",
        prompt="Write Python function",
        max_tokens=200
    )

    # EXTREMELY DANGEROUS
    code = ai_response.choices[0].text
    exec(code)  # Code injection vulnerability!

def unsafe_model_path():
    """Dangerous: Hardcoded model paths."""
    # These should be configurable
    MODEL_PATH = "/hardcoded/path/to/model.pkl"
    WEIGHTS_PATH = "hardcoded_weights.h5"
    CHECKPOINT_DIR = "/fixed/checkpoint/dir"

    model = torch.load(MODEL_PATH)
    weights = tf.keras.models.load_model(WEIGHTS_PATH)

    return model, weights

def unmonitored_model_deployment():
    """Dangerous: Model deployment without monitoring."""
    model = torch.load("production_model.pth")

    # No logging, no monitoring, no fallback
    def predict(input_data):
        return model.predict(input_data)

    # No health checks, no performance monitoring
    # No drift detection, no adversarial input detection

    return predict
