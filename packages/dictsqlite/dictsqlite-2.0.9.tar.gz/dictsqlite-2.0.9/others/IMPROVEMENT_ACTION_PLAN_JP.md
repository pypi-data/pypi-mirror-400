# DictSQLite v4.1 æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³

**ä½œæˆæ—¥**: 2025å¹´  
**å¯¾è±¡**: dictsqlite_v4.1 Rustç‰ˆ  
**ç›®çš„**: èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ãå…·ä½“çš„ãªæ”¹å–„æ‰‹é †

---

## Phase 1: ç·Šæ€¥å¯¾å¿œï¼ˆ1-2é€±é–“ï¼‰

### ã‚¿ã‚¹ã‚¯1.1: AsyncDictSQLite ã®æ°¸ç¶šåŒ–å®Ÿè£…

**æœŸé–“**: 5æ—¥é–“  
**å„ªå…ˆåº¦**: ğŸ”´ Critical  
**æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ«**: `src/async_ops.rs`

#### å®Ÿè£…å†…å®¹

```rust
use crate::storage::StorageEngine;
use crate::Config;

#[pyclass]
pub struct AsyncDictSQLite {
    /// Lock-free concurrent hashmap with shard-per-core
    cache: Arc<DashMap<String, Vec<u8>>>,
    
    /// Storage engine for persistence (NEW)
    storage: Arc<Mutex<Option<StorageEngine>>>,
    
    /// Configuration (NEW)
    config: Config,
    
    /// Capacity
    capacity: usize,
}

#[pymethods]
impl AsyncDictSQLite {
    #[new]
    #[pyo3(signature = (db_path, capacity=1_000_000, persist_mode="lazy"))]
    fn new(db_path: String, capacity: usize, persist_mode: &str) -> PyResult<Self> {
        let num_shards = num_cpus::get();
        let cache = Arc::new(DashMap::with_capacity_and_shard_amount(capacity, num_shards));
        
        // Create config
        let mut config = Config::default();
        config.persist_mode = PersistMode::from_str(persist_mode)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyValueError, _>(e))?;
        
        // Initialize storage engine
        let storage = if config.persist_mode == PersistMode::Memory {
            Arc::new(Mutex::new(None))
        } else {
            Arc::new(Mutex::new(Some(
                StorageEngine::new(&db_path, &config)
                    .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e.to_string()))?
            )))
        };
        
        Ok(AsyncDictSQLite {
            cache,
            storage,
            config,
            capacity,
        })
    }
    
    /// Async get with storage fallback
    fn get_async(&self, key: String, py: Python) -> PyResult<Option<PyObject>> {
        // Try cache first
        if let Some(value) = self.cache.get(&key) {
            return Ok(Some(PyBytes::new(py, &value).into()));
        }
        
        // Fallback to storage
        if self.config.persist_mode != PersistMode::Memory {
            let storage_guard = self.storage.lock().unwrap();
            if let Some(ref storage) = *storage_guard {
                if let Ok(Some(value)) = storage.get(&key) {
                    // Promote to cache
                    drop(storage_guard);
                    self.cache.insert(key, value.clone());
                    return Ok(Some(PyBytes::new(py, &value).into()));
                }
            }
        }
        
        Ok(None)
    }
    
    /// Flush cache to storage
    fn flush(&self) -> PyResult<()> {
        if self.config.persist_mode == PersistMode::Memory {
            return Ok(());
        }
        
        let mut storage_guard = self.storage.lock().unwrap();
        if let Some(ref mut storage) = *storage_guard {
            for entry in self.cache.iter() {
                storage.set(entry.key(), entry.value())
                    .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e.to_string()))?;
            }
        }
        Ok(())
    }
    
    /// Close and flush
    fn close(&self) -> PyResult<()> {
        self.flush()
    }
}
```

#### ãƒ†ã‚¹ãƒˆ

```python
# tests/test_async_persistence.py
def test_async_persistence():
    import tempfile
    import os
    
    db_path = tempfile.mktemp(suffix=".db")
    
    try:
        # Write data
        db1 = AsyncDictSQLite(db_path, persist_mode="lazy")
        db1.set_async("key1", b"value1")
        db1.set_async("key2", b"value2")
        db1.flush()
        db1.close()
        
        # Read data in new instance
        db2 = AsyncDictSQLite(db_path, persist_mode="lazy")
        assert db2.get_async("key1") == b"value1"
        assert db2.get_async("key2") == b"value2"
        db2.close()
        
        print("âœ… Async persistence test passed")
    finally:
        if os.path.exists(db_path):
            os.unlink(db_path)
```

---

### ã‚¿ã‚¹ã‚¯1.2: LRU ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè£…

**æœŸé–“**: 3æ—¥é–“  
**å„ªå…ˆåº¦**: ğŸ”´ Critical  
**æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ«**: `Cargo.toml`, `src/lib.rs`

#### ä¾å­˜é–¢ä¿‚ã®è¿½åŠ 

```toml
# Cargo.toml
[dependencies]
lru = "0.12"  # LRU cache implementation
```

#### å®Ÿè£…å†…å®¹

```rust
use lru::LruCache;
use std::num::NonZeroUsize;

#[pyclass]
pub struct DictSQLiteV4 {
    hot_tier: Arc<DashMap<String, Vec<u8>>>,
    access_tracker: Arc<Mutex<LruCache<String, ()>>>,  // NEW
    storage: Arc<Mutex<Option<StorageEngine>>>,
    config: Config,
    crypto: Option<Arc<CryptoEngine>>,
    safe_pickle: Option<Arc<SafePickleValidator>>,
}

impl DictSQLiteV4 {
    #[new]
    fn new(/* ... */) -> PyResult<Self> {
        // ... existing code ...
        
        let access_tracker = Arc::new(Mutex::new(
            LruCache::new(NonZeroUsize::new(config.hot_tier_capacity).unwrap())
        ));
        
        Ok(DictSQLiteV4 {
            hot_tier,
            access_tracker,
            storage,
            config,
            crypto,
            safe_pickle,
        })
    }
    
    fn get(&self, key: String, py: Python) -> PyResult<Option<PyObject>> {
        // Track access
        self.access_tracker.lock().unwrap().put(key.clone(), ());
        
        // ... existing get logic ...
    }
    
    fn set(&self, key: String, value: Vec<u8>) -> PyResult<()> {
        // ... existing validation and encryption ...
        
        self.hot_tier.insert(key.clone(), data.clone());
        
        // Track access
        self.access_tracker.lock().unwrap().put(key.clone(), ());
        
        // Evict if necessary
        if self.hot_tier.len() > self.config.hot_tier_capacity {
            self.evict_to_warm_tier()?;
        }
        
        // ... existing persist logic ...
        
        Ok(())
    }
    
    fn evict_to_warm_tier(&self) -> PyResult<()> {
        let mut tracker = self.access_tracker.lock().unwrap();
        
        // Find LRU entry
        if let Some((evict_key, _)) = tracker.pop_lru() {
            if let Some((_, value)) = self.hot_tier.remove(&evict_key) {
                // Write to storage (warm tier)
                let mut storage_guard = self.storage.lock().unwrap();
                if let Some(ref mut storage) = *storage_guard {
                    storage.set(&evict_key, &value)
                        .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e.to_string()))?;
                }
            }
        }
        
        Ok(())
    }
}
```

---

### ã‚¿ã‚¹ã‚¯1.3: READMEã®æ›´æ–°

**æœŸé–“**: 1æ—¥é–“  
**å„ªå…ˆåº¦**: ğŸŸ¡ High  
**æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ«**: `README_JP.md`, `README.md`

#### ä¿®æ­£å†…å®¹

1. **æœªå®Ÿè£…æ©Ÿèƒ½ã®å‰Šé™¤**

```markdown
# å‰Šé™¤ã™ã‚‹å†…å®¹
~~db.get("key", "default")~~
~~db.setdefault("key", b"value")~~
~~db.update({"k1": b"v1", "k2": b"v2"})~~
~~value = db.pop("key")~~
```

2. **å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½ã®æ˜è¨˜**

```markdown
## å®Ÿè£…æ¸ˆã¿API

### è¾æ›¸é¢¨ã‚¢ã‚¯ã‚»ã‚¹
- `db[key] = value` - å€¤ã®è¨­å®š
- `value = db[key]` - å€¤ã®å–å¾—ï¼ˆKeyErrorç™ºç”Ÿï¼‰
- `del db[key]` - ã‚­ãƒ¼ã®å‰Šé™¤
- `key in db` - ã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
- `len(db)` - ã‚¢ã‚¤ãƒ†ãƒ æ•°ã®å–å¾—

### ãƒ¡ã‚½ãƒƒãƒ‰
- `db.keys()` - å…¨ã‚­ãƒ¼ã®å–å¾—
- `db.bulk_insert(dict)` - ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
- `db.flush()` - ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
- `db.close()` - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ã‚º
- `db.stats()` - çµ±è¨ˆæƒ…å ±ã®å–å¾—

## ä»Šå¾Œå®Ÿè£…äºˆå®šã®API

- `db.get(key, default)` - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»˜ãå–å¾—
- `db.items()` - (key, value)ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿
- `db.values()` - å€¤ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿
- `db.setdefault(key, default)` - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»˜ãè¨­å®š
- `db.update(dict)` - è¾æ›¸æ›´æ–°
- `db.pop(key)` - ã‚­ãƒ¼ã®å‰Šé™¤ã¨å€¤ã®å–å¾—
```

3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°èª¬æ˜è¿½åŠ **

```markdown
## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

### DictSQLiteV4.__init__()

- `db_path` (str): ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
  - `":memory:"` ã§ç´”ç²‹ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰
  
- `hot_capacity` (int, default=1_000_000): ãƒ›ãƒƒãƒˆãƒ†ã‚£ã‚¢ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒªæ•°
  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ â‰ˆ hot_capacity Ã— å¹³å‡å€¤ã‚µã‚¤ã‚º
  
- `enable_async` (bool, default=True): ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰éåŒæœŸãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹åŒ–
  - `True`: å®šæœŸçš„ã«è‡ªå‹•ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼ˆæ¨å¥¨ï¼‰
  - `False`: flush()ã‚’æ˜ç¤ºçš„ã«å‘¼ã¶å¿…è¦ã‚ã‚Š
  
- `persist_mode` (str, default="writethrough"): æ°¸ç¶šåŒ–ãƒ¢ãƒ¼ãƒ‰
  - `"memory"`: ç´”ç²‹ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼ˆæ°¸ç¶šåŒ–ãªã—ã€æœ€é€Ÿï¼‰
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: 1.24M ops/sec
    - ç”¨é€”: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ä¸€æ™‚ãƒ‡ãƒ¼ã‚¿
  - `"lazy"`: é…å»¶æ°¸ç¶šåŒ–ï¼ˆflush()æ™‚ã«æ›¸ãè¾¼ã¿ã€é«˜é€Ÿï¼‰
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: 1.30M ops/sec
    - ç”¨é€”: é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ**æ¨å¥¨**ï¼‰
  - `"writethrough"`: å³åº§ã«æ°¸ç¶šåŒ–ï¼ˆå®‰å…¨ã€ä½é€Ÿï¼‰
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: 29.79K ops/sec
    - ç”¨é€”: é‡‘èå–å¼•ã€ç›£æŸ»ãƒ­ã‚°
    
- `encryption_password` (str|None): AES-256-GCMæš—å·åŒ–ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
  - æŒ‡å®šã™ã‚‹ã¨å…¨ãƒ‡ãƒ¼ã‚¿ãŒæš—å·åŒ–ã•ã‚Œã‚‹
  - ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯å®‰å…¨ã«ç®¡ç†ã™ã‚‹ã“ã¨
  
- `enable_safe_pickle` (bool, default=False): Safe Pickleæ¤œè¨¼ã®æœ‰åŠ¹åŒ–
  - `True`: å±é™ºãªpickle opcodeã‚’æ¤œå‡ºãƒ»æ‹’å¦
  - `False`: æ¤œè¨¼ãªã—ï¼ˆé«˜é€Ÿã ãŒå±é™ºï¼‰
  
- `safe_pickle_allowed_modules` (List[str]|None): è¨±å¯ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
  - ä¾‹: `["myapp.", "trusted."]`
  - `None`: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å®‰å…¨ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿è¨±å¯
```

---

## Phase 2: é‡è¦ãªæ©Ÿèƒ½è¿½åŠ ï¼ˆ2-3é€±é–“ï¼‰

### ã‚¿ã‚¹ã‚¯2.1: éåŒæœŸãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã®å®Ÿè£…

**æœŸé–“**: 5æ—¥é–“  
**å„ªå…ˆåº¦**: ğŸ”´ Critical  
**æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ«**: `src/async_ops.rs`

#### å®Ÿè£…å†…å®¹

```rust
use std::collections::HashMap;
use std::time::Duration;
use tokio::time::interval;

#[pyclass]
pub struct AsyncDictSQLite {
    cache: Arc<DashMap<String, Vec<u8>>>,
    storage: Arc<Mutex<Option<StorageEngine>>>,
    config: Config,
    capacity: usize,
    
    // Async buffering (NEW)
    write_buffer: Arc<Mutex<HashMap<String, Vec<u8>>>>,
    buffer_size: usize,
    buffer_interval_ms: u64,
}

#[pymethods]
impl AsyncDictSQLite {
    #[new]
    #[pyo3(signature = (db_path, capacity=1_000_000, persist_mode="lazy", 
                        buffer_size=100, buffer_interval_ms=5000))]
    fn new(
        db_path: String, 
        capacity: usize, 
        persist_mode: &str,
        buffer_size: usize,
        buffer_interval_ms: u64,
    ) -> PyResult<Self> {
        // ... existing initialization ...
        
        let write_buffer = Arc::new(Mutex::new(HashMap::new()));
        
        // Start background flush task
        if persist_mode != "memory" {
            let buffer_clone = write_buffer.clone();
            let storage_clone = storage.clone();
            let interval_ms = buffer_interval_ms;
            
            std::thread::spawn(move || {
                let rt = tokio::runtime::Runtime::new().unwrap();
                rt.block_on(async {
                    let mut ticker = interval(Duration::from_millis(interval_ms));
                    loop {
                        ticker.tick().await;
                        Self::flush_buffer_static(buffer_clone.clone(), storage_clone.clone());
                    }
                });
            });
        }
        
        Ok(AsyncDictSQLite {
            cache,
            storage,
            config,
            capacity,
            write_buffer,
            buffer_size,
            buffer_interval_ms,
        })
    }
    
    /// Async set with buffering
    fn set_async(&self, key: String, value: Vec<u8>) -> PyResult<()> {
        // Update cache immediately
        self.cache.insert(key.clone(), value.clone());
        
        // Add to buffer
        let mut buffer = self.write_buffer.lock().unwrap();
        buffer.insert(key, value);
        
        // Auto-flush if buffer is full
        if buffer.len() >= self.buffer_size {
            drop(buffer);
            self.flush_buffer()?;
        }
        
        Ok(())
    }
    
    fn flush_buffer(&self) -> PyResult<()> {
        let mut buffer = self.write_buffer.lock().unwrap();
        
        if buffer.is_empty() {
            return Ok(());
        }
        
        // Extract buffer contents
        let items: HashMap<String, Vec<u8>> = buffer.drain().collect();
        drop(buffer);
        
        // Bulk write to storage
        let mut storage_guard = self.storage.lock().unwrap();
        if let Some(ref mut storage) = *storage_guard {
            for (key, value) in items {
                storage.set(&key, &value)
                    .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e.to_string()))?;
            }
        }
        
        Ok(())
    }
    
    fn flush_buffer_static(
        buffer: Arc<Mutex<HashMap<String, Vec<u8>>>>,
        storage: Arc<Mutex<Option<StorageEngine>>>,
    ) {
        // Similar to flush_buffer but without PyResult
    }
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:
- 1000ä»¶ã®æ›¸ãè¾¼ã¿: 30ç§’ â†’ **0.1ç§’**ï¼ˆ**300å€é«˜é€ŸåŒ–**ï¼‰

---

### ã‚¿ã‚¹ã‚¯2.2: è¾æ›¸äº’æ›APIã®å®Ÿè£…

**æœŸé–“**: 3æ—¥é–“  
**å„ªå…ˆåº¦**: ğŸŸ¡ High  
**æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ«**: `src/lib.rs`

#### å®Ÿè£…å†…å®¹

```rust
#[pymethods]
impl DictSQLiteV4 {
    /// Get with default value (dict-compatible)
    #[pyo3(signature = (key, default=None))]
    fn get(&self, key: String, default: Option<Vec<u8>>, py: Python) -> PyResult<PyObject> {
        // Try hot tier first
        if let Some(value) = self.hot_tier.get(&key) {
            let data = if let Some(ref crypto) = self.crypto {
                crypto.decrypt(&value)
                    .map_err(|e| PyErr::new::<pyo3::exceptions::PyValueError, _>(e.to_string()))?
            } else {
                value.clone()
            };
            return Ok(PyBytes::new(py, &data).into());
        }
        
        // Try storage
        if self.config.persist_mode != PersistMode::Memory {
            let storage_guard = self.storage.lock().unwrap();
            if let Some(ref storage) = *storage_guard {
                if let Ok(Some(value)) = storage.get(&key) {
                    let data = if let Some(ref crypto) = self.crypto {
                        crypto.decrypt(&value)
                            .map_err(|e| PyErr::new::<pyo3::exceptions::PyValueError, _>(e.to_string()))?
                    } else {
                        value
                    };
                    return Ok(PyBytes::new(py, &data).into());
                }
            }
        }
        
        // Return default
        Ok(default.map(|v| PyBytes::new(py, &v).into())
            .unwrap_or_else(|| py.None()))
    }
    
    /// Setdefault (dict-compatible)
    fn setdefault(&self, key: String, default: Vec<u8>, py: Python) -> PyResult<PyObject> {
        if !self.hot_tier.contains_key(&key) {
            self.set(key.clone(), default.clone())?;
        }
        self.get(key, Some(default), py)
    }
    
    /// Update from dict (dict-compatible)
    fn update(&self, items: Bound<'_, PyDict>) -> PyResult<()> {
        self.bulk_insert(items)
    }
    
    /// Pop with optional default (dict-compatible)
    #[pyo3(signature = (key, default=None))]
    fn pop(&self, key: String, default: Option<Vec<u8>>, py: Python) -> PyResult<PyObject> {
        if let Some((_, value)) = self.hot_tier.remove(&key) {
            let data = if let Some(ref crypto) = self.crypto {
                crypto.decrypt(&value)
                    .map_err(|e| PyErr::new::<pyo3::exceptions::PyValueError, _>(e.to_string()))?
            } else {
                value
            };
            return Ok(PyBytes::new(py, &data).into());
        }
        
        Ok(default.map(|v| PyBytes::new(py, &v).into())
            .unwrap_or_else(|| py.None()))
    }
    
    /// Items iterator (dict-compatible)
    fn items(&self, py: Python) -> PyResult<Vec<(String, PyObject)>> {
        let items: Vec<(String, PyObject)> = self.hot_tier.iter()
            .map(|entry| {
                let value = if let Some(ref crypto) = self.crypto {
                    crypto.decrypt(entry.value()).unwrap_or_else(|_| entry.value().clone())
                } else {
                    entry.value().clone()
                };
                (entry.key().clone(), PyBytes::new(py, &value).into())
            })
            .collect();
        Ok(items)
    }
    
    /// Values iterator (dict-compatible)
    fn values(&self, py: Python) -> PyResult<Vec<PyObject>> {
        let values: Vec<PyObject> = self.hot_tier.iter()
            .map(|entry| {
                let value = if let Some(ref crypto) = self.crypto {
                    crypto.decrypt(entry.value()).unwrap_or_else(|_| entry.value().clone())
                } else {
                    entry.value().clone()
                };
                PyBytes::new(py, &value).into()
            })
            .collect();
        Ok(values)
    }
}
```

---

### ã‚¿ã‚¹ã‚¯2.3: ãƒãƒƒãƒæ›¸ãè¾¼ã¿æœ€é©åŒ–

**æœŸé–“**: 4æ—¥é–“  
**å„ªå…ˆåº¦**: ğŸŸ¡ High  
**æ‹…å½“ãƒ•ã‚¡ã‚¤ãƒ«**: `src/lib.rs`

#### å®Ÿè£…å†…å®¹

```rust
const WRITE_BATCH_SIZE: usize = 100;

#[pyclass]
pub struct DictSQLiteV4 {
    // ... existing fields ...
    
    // Write batching for WriteThrough mode
    write_batch: Arc<Mutex<Vec<(String, Vec<u8>)>>>,
}

impl DictSQLiteV4 {
    fn set(&self, key: String, value: Vec<u8>) -> PyResult<()> {
        // ... existing validation and encryption ...
        
        self.hot_tier.insert(key.clone(), data.clone());
        
        // Batch writes for WriteThrough mode
        if self.config.persist_mode == PersistMode::WriteThrough {
            let mut batch = self.write_batch.lock().unwrap();
            batch.push((key, data));
            
            if batch.len() >= WRITE_BATCH_SIZE {
                // Flush batch
                let items: Vec<(String, Vec<u8>)> = batch.drain(..).collect();
                drop(batch);
                
                let mut storage_guard = self.storage.lock().unwrap();
                if let Some(ref mut storage) = *storage_guard {
                    // Use transaction for batch write
                    storage.batch_set(&items)
                        .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e.to_string()))?;
                }
            }
        }
        
        Ok(())
    }
}
```

**storage.rs ã¸ã®è¿½åŠ **:

```rust
impl StorageEngine {
    pub fn batch_set(&mut self, items: &[(String, Vec<u8>)]) -> Result<()> {
        let tx = self.cold_conn.lock().unwrap().transaction()?;
        
        for (key, value) in items {
            tx.execute(
                "INSERT OR REPLACE INTO kv_store (key, value) VALUES (?1, ?2)",
                params![key, value],
            )?;
        }
        
        tx.commit()?;
        Ok(())
    }
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:
- WriteThrough ãƒ¢ãƒ¼ãƒ‰: 29.79K â†’ 1.30M ops/secï¼ˆ**43å€é«˜é€ŸåŒ–**ï¼‰

---

## Phase 3: é•·æœŸçš„æ”¹å–„ï¼ˆ1-3ãƒ¶æœˆï¼‰

### ã‚¿ã‚¹ã‚¯3.1: çœŸã®éåŒæœŸAPIã®å®Ÿè£…

**æœŸé–“**: 2é€±é–“  
**å„ªå…ˆåº¦**: ğŸŸ¢ Medium  
**ä¾å­˜é–¢ä¿‚**: `pyo3-asyncio` ã‚¯ãƒ¬ãƒ¼ãƒˆã®è¿½åŠ 

#### å®Ÿè£…æ¦‚è¦

```toml
# Cargo.toml
[dependencies]
pyo3-asyncio = { version = "0.20", features = ["tokio-runtime"] }
```

```rust
use pyo3_asyncio::tokio::future_into_py;

#[pymethods]
impl AsyncDictSQLite {
    #[pyo3(name = "aset")]
    fn aset_py<'p>(&self, py: Python<'p>, key: String, value: Vec<u8>) -> PyResult<&'p PyAny> {
        let cache = self.cache.clone();
        let write_buffer = self.write_buffer.clone();
        let buffer_size = self.buffer_size;
        
        future_into_py(py, async move {
            // Async buffering logic
            cache.insert(key.clone(), value.clone());
            
            let mut buffer = write_buffer.lock().unwrap();
            buffer.insert(key, value);
            
            if buffer.len() >= buffer_size {
                // Auto flush
            }
            
            Ok(Python::with_gil(|py| py.None()))
        })
    }
}
```

---

### ã‚¿ã‚¹ã‚¯3.2: SIMDæœ€é©åŒ–

**æœŸé–“**: 3é€±é–“  
**å„ªå…ˆåº¦**: ğŸŸ¢ Medium  
**æœŸå¾…åŠ¹æœ**: 10-20% ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š

---

## å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

```
Week 1-2: Phase 1 (ç·Šæ€¥å¯¾å¿œ)
â”œâ”€ Day 1-5:   AsyncDictSQLiteæ°¸ç¶šåŒ–
â”œâ”€ Day 6-8:   LRUã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³
â””â”€ Day 9-10:  READMEæ›´æ–°

Week 3-5: Phase 2 (é‡è¦æ©Ÿèƒ½)
â”œâ”€ Day 11-15: éåŒæœŸãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
â”œâ”€ Day 16-18: è¾æ›¸äº’æ›API
â””â”€ Day 19-22: ãƒãƒƒãƒæ›¸ãè¾¼ã¿æœ€é©åŒ–

Week 6-14: Phase 3 (é•·æœŸæ”¹å–„)
â”œâ”€ Week 6-7:  çœŸã®éåŒæœŸAPI
â””â”€ Week 8-10: SIMDæœ€é©åŒ–
```

---

## æˆåŠŸæŒ‡æ¨™

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™

| æŒ‡æ¨™ | ç¾åœ¨ | Phase 1 | Phase 2 | Phase 3 |
|------|------|---------|---------|---------|
| éåŒæœŸæ›¸ãè¾¼ã¿ (1000ä»¶) | 30s | 0.1s | 0.05s | 0.03s |
| WriteThrough æ›¸ãè¾¼ã¿ | 29.79K | 29.79K | 1.30M | 1.50M |
| é †æ¬¡èª­ã¿è¾¼ã¿ | 3.97M | 4.50M | 5.00M | 6.00M |
| ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | âˆ | åˆ¶é™ã‚ã‚Š | æœ€é©åŒ– | æœ€é©åŒ– |

### æ©Ÿèƒ½ç›®æ¨™

- âœ… Phase 1: æ°¸ç¶šåŒ–ã€LRUã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- âœ… Phase 2: éåŒæœŸãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã€è¾æ›¸äº’æ›APIã€ãƒãƒƒãƒæœ€é©åŒ–
- âœ… Phase 3: çœŸã®éåŒæœŸAPIã€SIMDæœ€é©åŒ–

### å“è³ªç›®æ¨™

- âœ… å…¨ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹ç‡: 100%
- âœ… ã‚«ãƒãƒ¬ãƒƒã‚¸: 80%ä»¥ä¸Š
- âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ­£ç¢ºæ€§: 100%
- âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§: 0ä»¶

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ã“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼**
2. **Phase 1 ã®å®Ÿè£…ã‚’é–‹å§‹**
3. **å„ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«ãƒ†ã‚¹ãƒˆã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ**
4. **å®šæœŸçš„ã«é€²æ—ã‚’å ±å‘Š**

---

**ä½œæˆè€…**: GitHub Copilot  
**æ‰¿èªå¾…ã¡**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚ªãƒ¼ãƒŠãƒ¼
