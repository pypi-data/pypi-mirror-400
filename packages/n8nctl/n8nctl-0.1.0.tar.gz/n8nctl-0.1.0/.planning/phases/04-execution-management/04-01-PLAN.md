---
phase: 04-execution-management
plan: 04-01
type: execute
---

<objective>
Build ExecutionsResource API class with list, get, and retry methods for workflow execution management.

Purpose: Establish the API foundation for execution commands, following the same hub-and-spoke pattern as WorkflowsResource from Phase 3.
Output: ExecutionsResource class with retry logic, type-safe Pydantic models, and 100% test coverage.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-execution-management/04-CONTEXT.md
@.planning/phases/03-workflow-management/03-01-SUMMARY.md
@src/n8n_cli/client/resources/workflows.py
@src/n8n_cli/client/models/execution.py
@src/n8n_cli/client/exceptions.py
@tests/test_workflows_resource.py

**Phase 3 patterns to follow:**
- Hub-and-spoke architecture (ExecutionsResource receives httpx.Client from APIClient)
- @retry decorator on all methods (httpx.HTTPError, ConnectionError, ServerError)
- Type-safe Pydantic model returns (Execution model already exists)
- 100% test coverage with pytest-httpx mocking
- TDD approach: RED → GREEN → REFACTOR cycle

**API endpoints (from REQUIREMENTS.md):**
- GET /executions?workflowId={id}&limit={n}&status={status}
- GET /executions/{id}?includeData={bool}
- POST /executions/{id}/retry

**Key differences from workflows:**
- list() has workflow_id filter, limit parameter, and status filter
- get() has include_data parameter for full execution data
- retry() is POST endpoint (not PATCH like activate/deactivate)
</context>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Create ExecutionsResource with list() and get() methods</name>
  <files>src/n8n_cli/client/resources/executions.py, tests/test_executions_resource.py, src/n8n_cli/client/resources/__init__.py</files>
  <test-first>
    Test: list() returns list of Execution objects with proper filtering
    Cases:
    - list() returns all executions (no filters)
    - list(workflow_id="123") filters by workflow
    - list(limit=10) limits results
    - list(status="error") filters by status
    - list() retries on ServerError/HTTPError/ConnectionError

    Test: get() returns single Execution with optional data
    Cases:
    - get("exec_id") returns Execution without full data
    - get("exec_id", include_data=True) includes full execution data
    - get() retries on transient errors
    - get() raises NotFoundError on 404
  </test-first>
  <action>
    Create ExecutionsResource class following WorkflowsResource pattern:

    1. __init__(self, client: httpx.Client) - store client reference

    2. list(workflow_id: str | None = None, limit: int = 20, status: str | None = None) method:
       - Build query params dict from non-None arguments (workflowId, limit, status)
       - GET /executions with params
       - Parse response as list[Execution] using Pydantic
       - Use @retry decorator (2s/4s/8s, max 10s, 3 attempts) on (httpx.HTTPError, ConnectionError, ServerError)

    3. get(execution_id: str, include_data: bool = False) method:
       - Build query params {"includeData": str(include_data).lower()} if include_data=True
       - GET /executions/{execution_id} with params
       - Call handle_response() for error handling (NotFoundError on 404)
       - Parse response as Execution using Pydantic
       - Use @retry decorator

    4. Export ExecutionsResource in resources/__init__.py

    Follow exact patterns from workflows.py:
    - Use handle_response() for error handling (raises NotFoundError, ServerError, etc.)
    - Return type-safe Pydantic models (not dicts)
    - Include ServerError in retry decorator (critical for 5xx handling)
    - Use type: ignore[valid-type] for list return type (Pydantic/mypy compatibility)
  </action>
  <verify>
    pytest tests/test_executions_resource.py -v passes all tests
    mypy src/n8n_cli/client/resources/executions.py passes with no errors
    ruff check src/n8n_cli/client/resources/executions.py passes
  </verify>
  <done>
    - ExecutionsResource has list() and get() methods with retry logic
    - list() supports workflow_id, limit, and status filtering
    - get() supports include_data parameter
    - All methods return type-safe Execution models
    - Tests verify filtering, retry behavior, and error handling
    - 100% coverage for implemented methods
  </done>
</task>

<task type="auto" tdd="true">
  <name>Task 2: Add retry() method to ExecutionsResource</name>
  <files>src/n8n_cli/client/resources/executions.py, tests/test_executions_resource.py</files>
  <test-first>
    Test: retry() retries a failed execution and returns new Execution
    Cases:
    - retry("exec_id") POSTs to /executions/{id}/retry, returns Execution
    - retry() with load_workflow=False passes loadWorkflow=false param
    - retry() retries on transient errors (ServerError, HTTPError, ConnectionError)
    - retry() raises NotFoundError on 404 (execution doesn't exist)
  </test-first>
  <action>
    Add retry(execution_id: str, load_workflow: bool = True) method to ExecutionsResource:

    1. Build query params: {"loadWorkflow": str(load_workflow).lower()}
    2. POST /executions/{execution_id}/retry with params
    3. Call handle_response() for error handling
    4. Parse response as Execution (returns the NEW execution created by retry)
    5. Use @retry decorator with same config as list/get

    Note: This creates a NEW execution (returns new Execution object), it doesn't modify the failed one.

    Add comprehensive tests:
    - Test retry creates new execution
    - Test load_workflow parameter (True and False)
    - Test retry behavior on errors
    - Test 404 handling (NotFoundError)
  </action>
  <verify>
    pytest tests/test_executions_resource.py::TestExecutionsResource::test_retry* -v passes
    All quality gates pass: pytest (all execution tests), mypy, ruff
  </verify>
  <done>
    - retry() method implemented with load_workflow parameter
    - Method returns new Execution object
    - Retry logic handles transient errors
    - Tests verify all retry scenarios
    - ExecutionsResource complete with list/get/retry methods
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest tests/test_executions_resource.py passes (all tests green)
- [ ] mypy src/n8n_cli/client/resources/ passes with no errors
- [ ] ruff check src/n8n_cli/ passes with no violations
- [ ] Test coverage for ExecutionsResource is 100%
- [ ] All methods follow WorkflowsResource patterns
</verification>

<success_criteria>

- ExecutionsResource class created in resources/executions.py
- All three methods implemented: list(), get(), retry()
- All methods have @retry decorators with ServerError handling
- All methods return type-safe Execution models
- Comprehensive test suite with 100% coverage
- All quality gates passing (pytest, mypy, ruff)
- Patterns match WorkflowsResource from Phase 3
  </success_criteria>

<output>
After completion, create `.planning/phases/04-execution-management/04-01-SUMMARY.md`:

# Phase 4 Plan 1: ExecutionsResource API Methods Summary

**[Substantive one-liner - what shipped, not "phase complete"]**

## Accomplishments

- ExecutionsResource class with list/get/retry methods
- Type-safe Pydantic Execution models
- Retry logic with exponential backoff
- 100% test coverage

## Files Created/Modified

- `src/n8n_cli/client/resources/executions.py` - ExecutionsResource implementation
- `tests/test_executions_resource.py` - Comprehensive test suite
- `src/n8n_cli/client/resources/__init__.py` - Export ExecutionsResource

## Decisions Made

[Document any technical decisions or deviations from plan]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 04-02-PLAN.md (List and View Commands)
</output>
