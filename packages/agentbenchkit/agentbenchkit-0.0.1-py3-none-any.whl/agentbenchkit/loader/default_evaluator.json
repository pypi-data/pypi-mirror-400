{
    "AI2D": {
        "evaluator":"exact_match",
        "Dataset": "lmms-lab/ai2d",
        "Url": "https://huggingface.co/datasets/lmms-lab/ai2d",
        "Type": "MCQ"
    },
    "Blink": {
        "evaluator":"exact_match",
        "Dataset": "BLINK-Benchmark/BLINK",
        "Url": "https://huggingface.co/datasets/BLINK-Benchmark/BLINK",
        "Type": "MCQ"
    },
    "COCO-Caption2017": {
        "evaluator":"nlg_score",
        "Dataset": "lmms-lab/coco-caption2017",
        "Url": "https://huggingface.co/datasets/lmms-lab/coco-caption2017",
        "Type": "NLG"
    },
    "CVBench": {
        "evaluator":"exact_match",
        "Dataset": "nyu-visionx/CV-Bench",
        "Url": "https://huggingface.co/datasets/nyu-visionx/CV-Bench",
        "Type": "MCQ"
    },
    "DocVQA": {
        "evaluator":"anls_score",
        "Dataset": "lmms-lab/DocVQA",
        "Url": "https://huggingface.co/datasets/lmms-lab/DocVQA",
        "Type": "Match"
    },
    "EmbSpatialBench": {
        "evaluator": "exact_match",
        "Dataset":"Phineas476/EmbSpatial-Bench",
        "Url": "https://huggingface.co/datasets/Phineas476/EmbSpatial-Bench",
        "Type": "MCQ"
    },
    "ERQA": {
        "evaluator":"exact_match",
        "Dataset": "FlagEval/ERQA",
        "Url": "https://huggingface.co/datasets/FlagEval/ERQA",
        "Type": "MCQ"
    },
    "HallusionBench": {
        "evaluator":"hallusionbench_eval",
        "Dataset": "lmms-lab/hallusionbench",
        "Url": "https://huggingface.co/datasets/lmms-lab/HallusionBench",
        "Type": "Judge"
    },
    "MathVision": {
        "evaluator":"exact_match",
        "Dataset": "MathLLMs/MathVision",
        "Url": "https://huggingface.co/datasets/MathLLMs/MathVision",
        "Type": "Match"
    },
    "MathVista": {
        "evaluator":"exact_match",
        "Dataset": "AI4Math/mathvista",
        "Url": "https://huggingface.co/datasets/AI4Math/mathvista",
        "Type": "MCQ"
    },
    "MMBench_CN": {
        "evaluator":"exact_match_2l",
        "Dataset": "lmms-lab/MMBench_CN",
        "Url": "https://huggingface.co/datasets/lmms-lab/MMBench_CN",
        "Type": "MCQ_2l"
    },
    "MMBench_EN": {
        "evaluator":"exact_match_2l",
        "Dataset": "lmms-lab/MMBench_EN",
        "Type": "MCQ_2l"
    },
    "MME": {
        "evaluator": "mme_score",
        "Dataset": "lmms-lab/mme",
        "Url": "https://huggingface.co/datasets/lmms-lab/MME",
        "Type": "MME"
    },
    "MMStar": {
        "evaluator":"exact_match",
        "Dataset": "Lin-Chen/mmstar",
        "Url": "https://huggingface.co/datasets/Lin-Chen/mmstar",
        "Type": "MCQ"
    },
     "MMTBenchmark": {
        "evaluator":"exact_match_2l",
        "Dataset": "lmms-lab/mmtbenchmark",
         "Url": "https://huggingface.co/datasets/lmms-lab/mmtbenchmark",
        "Type": "MCQ_2l"
    },
    "MUIR": {
        "evaluator":"exact_match",
        "Dataset": "MUIRBENCH/MUIRBENCH",
        "Url": "https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH",
        "Type": "MCQ"
    },
    "OCRBench": {
        "evaluator":"ocrbench_eval",
        "Dataset": "echo840/OCRBench",
        "Url": "https://huggingface.co/datasets/echo840/OCRBench",
        "Type": "OCR"
    },
    "POPE": {
        "evaluator":"hallucination_score",
        "Dataset": "lmms-lab/pope",
        "Url": "https://huggingface.co/datasets/lmms-lab/pope",
        "Type": "Judge"
    },
    "RealWorldQA": {
        "evaluator": "exact_match",
        "Dataset": "lmms-lab/realworldqa",
        "Url": "https://huggingface.co/datasets/lmms-lab/realworldqa",
        "Type": "PromptEmbed"
    },
    "RefSpatialBench": {
        "evaluator":"in_mask",
        "Dataset": "BAAI/RefSpatial-Bench",
        "Url": "https://huggingface.co/datasets/BAAI/RefSpatial-Bench",
        "Type": "PromptEmbed"
    },
    "ScienceQA_TEST": {
        "evaluator":"exact_match",
        "Dataset": "derek-thomas/ScienceQA",
        "Url": "https://huggingface.co/datasets/derek-thomas/ScienceQA",
        "Type": "MCQ"
    },
    "ScienceQA_VAL": {
        "evaluator":"exact_match",
        "Dataset": "derek-thomas/ScienceQA",
        "Url": "https://huggingface.co/datasets/derek-thomas/ScienceQA",
        "Type": "MCQ"
    },
    "SEED-Bench-2-Plus": {
        "evaluator":"exact_match",
        "Dataset": "doolayer/seed-bench-2-plus",
        "Url": "https://huggingface.co/datasets/doolayer/seed-bench-2-plus",
        "Type": "MCQ"
    },
    "Squad": {
        "evaluator":"exact_match",
        "Dataset": "rajpurkar/squad",
        "Url": "https://huggingface.co/datasets/rajpurkar/squad",
        "Type": "Match"
    }
}