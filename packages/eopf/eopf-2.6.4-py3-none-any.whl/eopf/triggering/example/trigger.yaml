# Triggering payload example
# Any tag with #Optional can be absent from the payload
#
---
# dotenv : will load these dotenv file into env vars
dotenv:
  - "$EOPF_CONFIG/.env_test"
# > general configuration will populate the EOConfiguration dictionary,
# The priority policy of parameters is this one :
# config_files > general_configuration > env variables starting with EOPF_
general_configuration: # Optional
  logging:
    level: DEBUG
  # setup a basic logging configuration
  triggering__use_basic_logging: true
  # Wait before stopping the dask cluster, useful for localCluster to access the dashboard before exit
  triggering__wait_before_exit: 90
  # Create a temp folder at startup, optional, default to true
  triggering__create_temporary: true
  # Ensure that the temporary is reachable from the workers, default to false
  triggering__temporary_shared: true
  # Validate the outputs of the processing units
  triggering__validate_run: true
  # Validation mode on product if validate_run is True
  triggering__validate_mode: STAC
  # Triggering error policy
  triggering__error_policy: FAIL_FAST
  # Temporary folder parameters, optional, folder default to gettempdir()
  temporary__folder: s3::${S3_OUTPUT_TEST_DATA_PATH}/temporary
  # S3 secret name to get in secrets files, optional, only if using S3
  temporary__folder_s3_secret: common
  # Create the temporary_folder if doesn't exist, optional, default to true
  temporary__folder_create_folder: true
  # Export the dask graph for variables at writing time
  dask__export_graphs: ./graphs/
# external modules allows to import needed modules in the global scope
external_modules: # Optional
  - name: math # Name of the module
    alias: m # Optional, alias for example import numpy as "np"
    nested: true # Optional, Import all sub attributes/functions from the lib from module import "*"
  - name: empty_test_store
    folder: ./tests/store # Optional, add this folder in sys.path to import
# > breakpoints: activate intermediate data dump
# see eopf.computing.breakpoint
breakpoints: # Optional
  all: true # Optional : Activate all breakpoints
  ids:
    - example_breakpoint_1 # activate breakpoint named example_breakpoint_1
  folder: ./breakpoints # define the breakpoint folder
  store_params:
    # storage options directly in the payload ( not recommended )
    storage_options:
      key: <key>
      secret: <secret>
      client_kwargs:
        endpoint_url: <url>
        region_name: <region>
# > worflow : define the processing unit orchestration workflow
# the workflow should describe an acyclic graph
workflow:
  - name: example_processor_1
    # Activate or not this unit, if not actived its outputs will not be available
    active: true # Optional
    # Validate or not the outputs when triggering__validate_run is activated
    validate: false
    # python module to import from
    module: eopf.computing.utils
    # python class to import from the module
    processing_unit: EORechunkingUnit
    # input products IDs
    inputs: # Optional
      # "processing_unit_input_name" : "I/O input id" | "source_processing_unit_name.outputname"
      l1a: OLCI
      # An input is considered optional and will not trigger exception either if not in the payload or if can't be read
      # if the processing unit doesn't declare it as mandatory in the pu.get_mandatory_input_list()
      tci: NotAvailable
    # Input auxiliary files
    adfs: # Optional
      # ADFS I/O Id
      dem: DEM
      # An ADF is considered optional and will not trigger exception either if not in the payload or doesn't exists
      # if the processing unit doesn't declare it as mandatory in the pu.get_mandatory_adf_list()
      pod: NotAvailable
    outputs: # Optional
      # "processing_unit_output_name" : "I/O output id", supports regex on the processor output name
      l1a: output_l1a_internal
      # "processing_unit_output_regex" : "I/O output id", supports regex on the processor output name
      l1b_.*: output_folder
      # special output to put the product in a temporary folder then reload from disc, will be cleanup at end
      # Only working if triggering__create_temporary=True
      l1b_intermediate: cache
    # Any parameters to pass to the **kwargs of the ProcessingUnit
    parameters: # Optional
      value: 0.5
      variable_path: "/measurements/stuff"
      variable_name: stiff
      shape:
        - 1500
  - name: example_processor_2
    active: false # Optional
    module: eopf.computing.utils
    processing_unit: FullProcessor
    inputs: # Optional
      # in this example we put in input the previous output of the other ProcessingUnit
      # This will do a write of l1a from example_processor_1 to disc than reload it to put in input
      internal_l1a: output_l1a_internal
      # This will automatically plug the output data in "l1b_intermediate" output of example_processor_1
      l1b: example_processor_1.l1b_intermediate
    adfs: # Optional
      dem: DEM
    # outputs : Optional , will write the designated outputs using the I/O output Id
    outputs: # Optional
      # "processing_unit_output_name" : "I/O output id", supports regex on the processor output name
      internal_l1a: output_l1a_internal
      # "processing_unit_output_regex" : "I/O output id", supports regex on the processor output name
      l1b_.*: output_folder
    # Any parameters to pass to the **kwargs of the ProcessingUnit
    parameters: # Optional
      value: 0.7
      variable_path: "/measurements/stiff"
      variable_name: stiff
      shape:
        - 500
        - 400
# > io section describe all I/O parameters, I/O kept for compatibility
io:
  # input products descriptions and access parameters
  input_products:
    - id: OLCI
      path: s3::zip::${TEST_DATA_FOLDER}/S3A_OL_1_EFR____20200101T101517_20200101T101817_20200102T141102_0179_053_179_2520_LN1_O_NT_002.zip
      store_type: safe
      store_params:
        # storage options directly in the payload ( not recommended )
        storage_options:
          key: <key>
          secret: <secret>
          client_kwargs:
            endpoint_url: <url>
            region_name: <region>
    - id: SLICES
      path: zip::${TEST_DATA_FOLDER}
      store_type: safe
      type: regex
      store_params:
        # glob regex to apply
        regex: "S3A_OL*"
        # storage options directly in the payload ( not recommended )
        storage_options:
          key: <key>
          secret: <secret>
          client_kwargs:
            endpoint_url: <url>
            region_name: <region>
  # Adf inputs, will not be opened by the triggering, the info are just passed to the ProcessingUnit
  adfs: # Optional
    - id: DEM
      path: zip::/mnt/1TERA/EOPF/ADFS/DEM.zarr
      store_params: { }
  # Outputs descriptors
  output_products:
    - id: output_l1a_internal
      path: internal_l1a.zarr
      # Type can be filename or folder, in case of folder the product naming will depends on the triggering__use_default_filename EOConfiguration
      type: filename
      # Opening mode, should be one of : CREATE CREATE_OVERWRITE or UPDATE(not supported by all stores)
      opening_mode: "CREATE_OVERWRITE"
      # The store type to use
      store_type: zarr
      # Any access parameters
      store_params: { }
    - id: output_folder
      path: "./finals/"
      opening_mode: "CREATE_OVERWRITE"
      type: folder
      store_type: zarr
      store_params:
        compressor: !ZarrCompressor
          cname: "zstd"
          clevel: 3
          shuffle: 2
# > dask_context : Dask context configuration, see eopf.dask_utils.dask_context_manager
dask_context: # Optional
  # on of ClusterType
  cluster_type: local
  # All these parameters will be directly given to the dask cluster constructor for example LocalCluster
  cluster_config:
    n_workers: 6
    threads_per_worker: 1
  # All these parameters will be directly given to the dask client constructor : https://distributed.dask.org/en/stable/api.html#distributed.Client
  client_config: { }
  # Optional : Activate the dask performance report file writing in this file
  performance_report_file: report.html
  # Optional : Set config on dask side, dict same as in dask.config.set
  dask_config: { }
# > logging : Any logging file(s) to load to the EOLogging facility
logging: # Optional
  - "$EOPF_ROOT/logging/conf/default.json"
# > config : Any config file(s) to load to the EOConfiguration facility, general_configuration and env variables will overload them
config: # Optional
  - "$EOPF_ROOT/config/default/eopf.toml"
# > secret : Any secret files to be loaded in the secrets dictionary of EOConfiguration
secret: # Optional
  - "$EOPF_CONFIG/secrets.json"
eoqc: # Optional
  config_folders:
    - "$EOPF_ROOT/qualitycontrol/config" # Optional
    - "./config" # Optional
  parameters: { } # Optional
  update_attrs: true # Optional
  report_path: "./reports/" # Optional
