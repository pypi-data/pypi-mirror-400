name: "orca_dpo"
description: "Intel Orca DPO Pairs - Question-answer pairs with chosen/rejected responses for Direct Preference Optimization"
version: "1.0"
source: "Intel/orca_dpo_pairs"
license: "Apache-2.0"
homepage: "https://huggingface.co/datasets/Intel/orca_dpo_pairs"

# Dataset type and characteristics
type: "preference_learning"
task: "alignment"
format: "huggingface"

# Data structure
features:
  - system: "System prompt/instruction"
  - question: "Input question"
  - chosen: "Preferred/better response"
  - rejected: "Non-preferred/worse response"

# Splits available
splits:
  - train

# Download method
download:
  method: "huggingface"
  auto: true
  requires:
    - datasets

# Usage information
use_cases:
  - "Direct Preference Optimization (DPO) training"
  - "Model alignment research"
  - "Preference learning experiments"
  - "RLHF (Reinforcement Learning from Human Feedback)"

# Citation
citation: |
  @article{orca,
    title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4},
    author={Mukherjee, Subhabrata and Mitra, Arindam and others},
    journal={arXiv preprint},
    year={2023}
  }
