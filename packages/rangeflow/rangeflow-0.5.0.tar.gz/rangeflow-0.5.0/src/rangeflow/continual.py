"""
RangeFlow Continual Learning Module
====================================
Solves catastrophic forgetting using interval weights.

Mathematical Principle:
----------------------
Instead of learning a single weight w=0.5, we learn a SAFE INTERVAL w∈[0.4, 0.6].
When learning a new task, we find a weight that lies in the INTERSECTION of
all previous task intervals → Zero forgetting with mathematical guarantees!

Key Innovation:
--------------
- RangeParameter: Weights that are intervals, not points
- Intersection Loss: Forces new weights into overlap with old safe zones
- Elastic Training: Automatically identifies which weights are "critical"
"""

import torch
import torch.nn as nn
import numpy as np
from .core import RangeTensor
from .layers import RangeModule
from .backend import get_backend

xp = get_backend()


class RangeParameter(nn.Module):
    """
    A learnable weight that is an Interval, not a Point.
    
    Used for Continual Learning to preserve solution spaces.
    
    Attributes:
        mu: Center of the weight (standard learnable param)
        rho: Allowable drift (how much we can change without breaking things)
            - We learn this too! If a weight is unimportant, rho grows large
        mode: 'full' (interval), 'mu_only' (point), or 'frozen' (fixed interval)
    
    Example:
        >>> # Create interval weight
        >>> param = RangeParameter((10, 5), init_std=0.01, mode='full')
        >>> 
        >>> # Get range representation
        >>> w_range = param.get_range()  # Returns RangeTensor [mu-|rho|, mu+|rho|]
        >>> 
        >>> # Standard point weight (for comparison)
        >>> param_point = RangeParameter((10, 5), mode='mu_only')
    """
    
    def __init__(self, shape, init_std=0.01, mode='full', device='cpu'):
        """
        Args:
            shape: Weight shape tuple
            init_std: Initialization standard deviation
            mode: 'full' (learn mu and rho), 'mu_only' (point weight), 'frozen'
            device: 'cpu' or 'cuda'
        """
        super().__init__()
        self.shape = shape
        self.mode = mode
        self.device = device
        
        # The center of the weight (Standard learnable param)
        self.mu = nn.Parameter(torch.randn(shape, device=device) * init_std)
        
        if mode == 'full':
            # The allowable "drift" (How much we can change without breaking things)
            # We learn this too! If a weight is unimportant, rho grows large.
            self.rho = nn.Parameter(torch.ones(shape, device=device) * init_std * 0.1)
        elif mode == 'frozen':
            # Fixed interval (not learnable)
            self.register_buffer('rho', torch.ones(shape, device=device) * init_std * 0.1)
        else:  # mu_only
            # No interval, just point weight
            self.register_buffer('rho', torch.zeros(shape, device=device))
    
    def get_range(self):
        """
        Returns [mu - |rho|, mu + |rho|]
        
        We use abs(rho) to ensure width is positive.
        
        Returns:
            RangeTensor representing the weight interval
        """
        if self.mode == 'mu_only':
            # Degenerate range (point weight)
            return RangeTensor.from_array(self.mu)
        
        drift = self.rho.abs()
        return RangeTensor.from_range(self.mu - drift, self.mu + drift)
    
    def get_point(self):
        """Get center point (for standard forward passes)"""
        return self.mu
    
    def snapshot(self):
        """
        Save current interval for later comparison.
        
        Used to check if new weights stay inside safe zone.
        
        Returns:
            Dict with mu and rho detached
        """
        return {
            'mu': self.mu.detach().clone(),
            'rho': self.rho.detach().clone() if isinstance(self.rho, nn.Parameter) else self.rho.clone(),
            'mode': self.mode
        }
    
    def load_snapshot(self, snapshot):
        """Restore from snapshot"""
        self.mu.data = snapshot['mu'].to(self.device)
        if isinstance(self.rho, nn.Parameter):
            self.rho.data = snapshot['rho'].to(self.device)
        else:
            self.rho = snapshot['rho'].to(self.device)
    
    def width(self):
        """Get average interval width"""
        return self.rho.abs().mean()


class ContinualLinear(RangeModule):
    """
    Linear layer that remembers previous tasks.
    
    Uses RangeParameter internally to maintain safe weight intervals.
    
    Args:
        in_features: Input dimension
        out_features: Output dimension
        bias: Whether to include bias
        mode: 'full' (interval weights), 'mu_only' (point weights), 'hybrid'
        hybrid_ratio: If mode='hybrid', fraction of interval weights (0-1)
    
    Example:
        >>> # Full interval weights
        >>> layer_full = ContinualLinear(128, 64, mode='full')
        >>> 
        >>> # Hybrid: 50% interval, 50% point
        >>> layer_hybrid = ContinualLinear(128, 64, mode='hybrid', hybrid_ratio=0.5)
        >>> 
        >>> # Standard point weights
        >>> layer_point = ContinualLinear(128, 64, mode='mu_only')
    """
    
    def __init__(self, in_features, out_features, bias=True, 
                 mode='full', hybrid_ratio=1.0, device='cpu'):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.mode = mode
        self.hybrid_ratio = hybrid_ratio
        self.device = device
        
        if mode == 'hybrid':
            # Hybrid mode: Some weights are intervals, some are points
            num_interval = int(out_features * hybrid_ratio)
            num_point = out_features - num_interval
            
            self.interval_weight = RangeParameter(
                (num_interval, in_features), mode='full', device=device
            ) if num_interval > 0 else None
            
            self.point_weight = nn.Parameter(
                torch.randn(num_point, in_features, device=device) * 0.05
            ) if num_point > 0 else None
            
            self.num_interval = num_interval
            self.num_point = num_point
        else:
            # Pure mode: All weights same type
            self.weight = RangeParameter(
                (out_features, in_features), mode=mode, device=device
            )
        
        if bias:
            self.bias = RangeParameter((out_features,), mode=mode, device=device)
        else:
            self.register_parameter('bias', None)
    
    def forward(self, x, use_range=True):
        """
        Forward pass with uncertainty propagation.
        
        Args:
            x: Input tensor or RangeTensor
            use_range: If True, propagate intervals; if False, use centers
        
        Returns:
            Output RangeTensor or tensor
        """
        # Wrap input if needed
        if not isinstance(x, RangeTensor):
            x = RangeTensor.from_array(x)
        
        if self.mode == 'hybrid':
            # Hybrid forward: concatenate interval and point outputs
            outputs = []
            
            if self.interval_weight is not None:
                w_range = self.interval_weight.get_range()
                out_interval = x @ w_range.transpose(-1, -2)
                outputs.append(out_interval)
            
            if self.point_weight is not None:
                w_point = RangeTensor.from_array(self.point_weight)
                out_point = x @ w_point.transpose(-1, -2)
                outputs.append(out_point)
            
            # Concatenate along output dimension
            if len(outputs) == 2:
                # Need to concatenate ranges
                min1, max1 = outputs[0].decay()
                min2, max2 = outputs[1].decay()
                out = RangeTensor.from_range(
                    torch.cat([min1, min2], dim=-1),
                    torch.cat([max1, max2], dim=-1)
                )
            else:
                out = outputs[0]
        else:
            # Standard forward
            w_range = self.weight.get_range() if use_range else RangeTensor.from_array(self.weight.mu)
            out = x @ w_range.transpose(-1, -2)
        
        if self.bias is not None:
            b_range = self.bias.get_range() if use_range else RangeTensor.from_array(self.bias.mu)
            out = out + b_range
        
        return out
    
    def snapshot_weights(self):
        """Save current weight intervals"""
        snapshot = {}
        if self.mode == 'hybrid':
            if self.interval_weight is not None:
                snapshot['interval_weight'] = self.interval_weight.snapshot()
            if self.point_weight is not None:
                snapshot['point_weight'] = self.point_weight.detach().clone()
        else:
            snapshot['weight'] = self.weight.snapshot()
        
        if self.bias is not None:
            snapshot['bias'] = self.bias.snapshot()
        
        return snapshot


def elastic_memory_loss(model, old_weights_snapshot, lambda_elastic=1.0):
    """
    Penalizes weights moving outside their previously certified safe zones.
    
    This is the CORE of continual learning - it forces new weights to
    stay inside the intersection of old task solution spaces.
    
    Mathematical Logic:
    ------------------
    For each weight w:
    - Old task said: "I'm safe if w ∈ [old_min, old_max]"
    - New weight is:  w ∈ [curr_min, curr_max]
    - Intersection:   [max(old_min, curr_min), min(old_max, curr_max)]
    
    If intersection is empty (curr_min > old_max), we have FORGETTING!
    We penalize the gap distance.
    
    Args:
        model: Neural network with RangeParameters
        old_weights_snapshot: Dict from previous task
        lambda_elastic: Strength of memory preservation (higher = stricter)
    
    Returns:
        Scalar loss (add to your main loss)
    
    Example:
        >>> # After Task A
        >>> snapshot_A = save_task_memory(model)
        >>> 
        >>> # Training Task B
        >>> for data, labels in task_B_loader:
        >>>     loss_B = cross_entropy(model(data), labels)
        >>>     loss_elastic = elastic_memory_loss(model, snapshot_A)
        >>>     total_loss = loss_B + loss_elastic
        >>>     total_loss.backward()
    """
    loss = torch.tensor(0.0, device=next(model.parameters()).device)
    
    for name, module in model.named_modules():
        if isinstance(module, RangeParameter):
            # Current interval
            curr_range = module.get_range()
            curr_min, curr_max = curr_range.decay()
            
            # Old interval (from Task A)
            if name not in old_weights_snapshot:
                continue
            
            old_snapshot = old_weights_snapshot[name]
            old_min = old_snapshot['mu'] - old_snapshot['rho'].abs()
            old_max = old_snapshot['mu'] + old_snapshot['rho'].abs()
            
            # Check Intersection: max(starts) <= min(ends)
            intersect_start = torch.max(curr_min, old_min)
            intersect_end = torch.min(curr_max, old_max)
            
            # If start > end, the intervals pulled apart (Forgetting happened!)
            # We penalize the distance
            gap = torch.nn.functional.relu(intersect_start - intersect_end)
            loss += torch.sum(gap)
    
    return lambda_elastic * loss


def save_task_memory(model, task_id=None):
    """
    Snapshot all RangeParameters after training on a task.
    
    This creates a "memory" of what weights are safe for this task.
    
    Args:
        model: Trained model
        task_id: Optional task identifier
    
    Returns:
        Dict of weight snapshots
    
    Example:
        >>> # Train on Task A
        >>> train_loop(model, task_A_data)
        >>> memory_A = save_task_memory(model, task_id='MNIST')
        >>> 
        >>> # Train on Task B while preserving A
        >>> train_loop(model, task_B_data, preserve=memory_A)
    """
    memory = {'task_id': task_id, 'weights': {}}
    
    for name, module in model.named_modules():
        if isinstance(module, ContinualLinear):
            memory['weights'][name] = module.snapshot_weights()
    
    return memory


def continual_train_step(model, optimizer, data, target, 
                        old_memories=None, lambda_elastic=1.0):
    """
    Single training step with continual learning.
    
    Args:
        model: Neural network
        optimizer: Optimizer
        data: Input data
        target: Labels
        old_memories: List of memory dicts from previous tasks
        lambda_elastic: Strength of memory preservation
    
    Returns:
        (total_loss, task_loss, elastic_loss)
    
    Example:
        >>> memories = []
        >>> 
        >>> # Task A
        >>> train_standard(model, task_A_data)
        >>> memories.append(save_task_memory(model, 'Task_A'))
        >>> 
        >>> # Task B (preserving A)
        >>> for data, target in task_B_data:
        >>>     loss, task_loss, elastic_loss = continual_train_step(
        >>>         model, optimizer, data, target, memories
        >>>     )
        >>>     loss.backward()
        >>>     optimizer.step()
    """
    optimizer.zero_grad()
    
    # Forward pass
    output = model(data)
    if isinstance(output, RangeTensor):
        output = output.avg()
    
    # Task loss
    task_loss = torch.nn.functional.cross_entropy(output, target)
    
    # Elastic loss (preserve old tasks)
    elastic_loss = torch.tensor(0.0, device=data.device)
    if old_memories is not None:
        for memory in old_memories:
            elastic_loss += elastic_memory_loss(model, memory['weights'], lambda_elastic)
    
    total_loss = task_loss + elastic_loss
    
    return total_loss, task_loss, elastic_loss


def compute_forgetting_score(model, old_memory, test_loader, device='cpu'):
    """
    Measure how much the model forgot a previous task.
    
    Args:
        model: Current model state
        old_memory: Memory dict from previous task
        test_loader: Test data for the old task
        device: 'cpu' or 'cuda'
    
    Returns:
        Forgetting score (0 = perfect memory, 1 = total forgetting)
    
    Example:
        >>> memory_mnist = save_task_memory(model, 'MNIST')
        >>> # ... train on other tasks ...
        >>> forgetting = compute_forgetting_score(model, memory_mnist, mnist_test_loader)
        >>> print(f"Forgot {forgetting*100:.1f}% of MNIST")
    """
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            
            if isinstance(output, RangeTensor):
                output = output.avg()
            
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
    
    current_accuracy = correct / total
    
    # Compare to accuracy when memory was created
    if 'accuracy' in old_memory:
        old_accuracy = old_memory['accuracy']
        forgetting = max(0, old_accuracy - current_accuracy)
    else:
        # No baseline - just return current accuracy as proxy
        forgetting = 1.0 - current_accuracy
    
    return forgetting


def visualize_weight_intervals(model, layer_name=None):
    """
    Visualize weight intervals to see which weights are "critical".
    
    Critical weights have tight intervals (small rho).
    Flexible weights have wide intervals (large rho).
    
    Args:
        model: Model with RangeParameters
        layer_name: Specific layer to visualize (None = all)
    
    Example:
        >>> import matplotlib.pyplot as plt
        >>> visualize_weight_intervals(model, layer_name='layer1')
        >>> plt.show()
    """
    import matplotlib.pyplot as plt
    
    widths = []
    names = []
    
    for name, module in model.named_modules():
        if isinstance(module, ContinualLinear):
            if layer_name is not None and layer_name not in name:
                continue
            
            if module.mode != 'mu_only':
                width = module.weight.width().item()
                widths.append(width)
                names.append(name)
    
    if not widths:
        print("No interval weights found")
        return
    
    plt.figure(figsize=(12, 6))
    plt.bar(range(len(widths)), widths)
    plt.xticks(range(len(widths)), names, rotation=45, ha='right')
    plt.ylabel('Average Interval Width')
    plt.title('Weight Flexibility (Larger = More Flexible for New Tasks)')
    plt.tight_layout()
    plt.grid(True, alpha=0.3, axis='y')
    
    return plt.gcf()


class HybridModelBuilder:
    """
    Helper to build models with mixed interval/point weights.
    
    Useful for memory-efficiency: only use intervals where needed.
    
    Example:
        >>> builder = HybridModelBuilder()
        >>> model = builder.build_mlp(
        >>>     [784, 256, 128, 10],
        >>>     interval_ratios=[0.5, 0.8, 1.0]  # 50%, 80%, 100% interval
        >>> )
    """
    
    @staticmethod
    def build_mlp(layer_sizes, interval_ratios=None, device='cpu'):
        """
        Build MLP with customizable interval ratios per layer.
        
        Args:
            layer_sizes: List of layer dimensions [input, hidden1, ..., output]
            interval_ratios: List of ratios (0-1) for each layer
                - 1.0 = full interval weights
                - 0.5 = half interval, half point
                - 0.0 = pure point weights
            device: 'cpu' or 'cuda'
        
        Returns:
            Sequential model with hybrid layers
        """
        if interval_ratios is None:
            interval_ratios = [1.0] * (len(layer_sizes) - 1)
        
        assert len(interval_ratios) == len(layer_sizes) - 1
        
        layers = []
        for i in range(len(layer_sizes) - 1):
            ratio = interval_ratios[i]
            
            if ratio == 0.0:
                mode = 'mu_only'
            elif ratio == 1.0:
                mode = 'full'
            else:
                mode = 'hybrid'
            
            layers.append(ContinualLinear(
                layer_sizes[i], layer_sizes[i+1],
                mode=mode, hybrid_ratio=ratio, device=device
            ))
            
            if i < len(layer_sizes) - 2:
                from .layers import RangeReLU
                layers.append(RangeReLU())
        
        return nn.Sequential(*layers)