# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import builtins
import enum
import typing

@typing.final
class Agent:
    r"""
    The Agent is the central orchestrator that connects the **language model**, **tools**, and **knowledge** components.
    It manages the entire reasoning and action loop, coordinating how each subsystem contributes to the final response.
    
    In essence, the Agent:
    
    - Understands user input
    - Interprets structured responses from the language model (such as tool calls)
    - Executes tools as needed
    - Retrieves and integrates contextual knowledge before or during inference
    
    # Public APIs
    - `run_delta`: Runs a user query and streams incremental deltas (partial outputs)
    - `run`: Runs a user query and returns a complete message once all deltas are accumulated
    
    ## Delta vs. Complete Message
    A *delta* represents a partial piece of model output, such as a text fragment or intermediate reasoning step.
    Deltas can be accumulated into a full message using the provided accumulation utilities.
    This allows real-time streaming while preserving the ability to reconstruct the final structured result.
    
    See `MessageDelta`.
    
    # Components
    - **Language Model**: Generates natural language and structured outputs. It interprets the conversation context and predicts the assistant’s next action.
    - **Tool**: Represents external functions or APIs that the model can dynamically invoke. The `Agent` detects tool calls and automatically executes them during the reasoning loop.
    - **Knowledge**: Provides retrieval-augmented reasoning by fetching relevant information from stored documents or databases. When available, the `Agent` enriches model input with these results before generating an answer.
    """
    @property
    def lm(self) -> LangModel: ...
    @property
    def tools(self) -> builtins.list[Tool]: ...
    def __new__(cls, lm: LangModel, tools: typing.Optional[typing.Sequence[Tool]] = None, knowledge: typing.Optional[Knowledge] = None) -> Agent: ...
    def __repr__(self) -> builtins.str: ...
    def add_tools(self, tools: typing.Sequence[Tool]) -> None: ...
    def add_tool(self, tool: Tool) -> None: ...
    def remove_tools(self, tool_names: typing.Sequence[builtins.str]) -> None: ...
    def remove_tool(self, tool_name: builtins.str) -> None: ...
    def clear_tools(self) -> None: ...
    def set_knowledge(self, knowledge: Knowledge) -> None: ...
    def remove_knowledge(self) -> None: ...
    def run_delta(self, messages: str | list[Message], config: typing.Optional[AgentConfig] = None) -> MessageDeltaOutputIterator: ...
    def run_delta_sync(self, messages: str | list[Message], config: typing.Optional[AgentConfig] = None) -> MessageDeltaOutputSyncIterator: ...
    def run(self, messages: str | list[Message], config: typing.Optional[AgentConfig] = None) -> MessageOutputIterator: ...
    def run_sync(self, messages: str | list[Message], config: typing.Optional[AgentConfig] = None) -> MessageOutputSyncIterator: ...

@typing.final
class AgentConfig:
    r"""
    Configuration for running the agent.
    
    See `LangModelInferConfig` and `KnowledgeConfig` for more details.
    """
    @property
    def inference(self) -> typing.Optional[LangModelInferConfig]: ...
    @inference.setter
    def inference(self, value: typing.Optional[LangModelInferConfig]) -> None: ...
    @property
    def knowledge(self) -> typing.Optional[KnowledgeConfig]: ...
    @knowledge.setter
    def knowledge(self, value: typing.Optional[KnowledgeConfig]) -> None: ...
    def __new__(cls, inference: typing.Optional[LangModelInferConfig] = None, knowledge: typing.Optional[KnowledgeConfig] = None) -> AgentConfig: ...
    @classmethod
    def from_dict(cls, config: dict) -> AgentConfig: ...

@typing.final
class CacheProgress:
    @property
    def comment(self) -> builtins.str: ...
    @property
    def current(self) -> builtins.int: ...
    @property
    def total(self) -> builtins.int: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class Document:
    @property
    def id(self) -> builtins.str: ...
    @property
    def title(self) -> typing.Optional[builtins.str]: ...
    @property
    def text(self) -> builtins.str: ...
    def __eq__(self, other: builtins.object) -> builtins.bool: ...
    def __new__(cls, id: builtins.str, text: builtins.str, title: typing.Optional[builtins.str] = None) -> Document: ...

@typing.final
class DocumentPolyfill:
    r"""
    Provides a polyfill for LLMs that do not natively support the Document feature.
    """
    @property
    def system_message_template(self) -> typing.Optional[builtins.str]: ...
    @system_message_template.setter
    def system_message_template(self, value: typing.Optional[builtins.str]) -> None: ...
    @property
    def query_message_template(self) -> typing.Optional[builtins.str]: ...
    @query_message_template.setter
    def query_message_template(self, value: typing.Optional[builtins.str]) -> None: ...
    def __new__(cls, system_message_template: typing.Optional[builtins.str] = None, query_message_template: typing.Optional[builtins.str] = None) -> DocumentPolyfill: ...
    @classmethod
    def get(cls, kind: typing.Literal["Qwen3"]) -> DocumentPolyfill: ...

@typing.final
class EmbeddingModel:
    @classmethod
    def new_local(cls, model_name: builtins.str, device_id: typing.Optional[builtins.int] = None, validate_checksum: typing.Optional[builtins.bool] = None, progress_callback: typing.Callable[[CacheProgress], None] = None) -> typing.Awaitable[EmbeddingModel]: ...
    @classmethod
    def new_local_sync(cls, model_name: builtins.str, device_id: typing.Optional[builtins.int] = None, validate_checksum: typing.Optional[builtins.bool] = None, progress_callback: typing.Callable[[CacheProgress], None] = None) -> EmbeddingModel: ...
    @classmethod
    def download(cls, model_name: builtins.str, progress_callback: typing.Callable[[CacheProgress], None] = None) -> None: ...
    @classmethod
    def remove(cls, model_name: builtins.str) -> None: ...
    async def infer(self, text: builtins.str) -> builtins.list[float]: ...
    def infer_sync(self, text: builtins.str) -> builtins.list[float]: ...

class Grammar:
    @typing.final
    class Plain(Grammar):
        __match_args__ = ()
        def __new__(cls) -> Grammar.Plain: ...
    
    @typing.final
    class JSON(Grammar):
        __match_args__ = ()
        def __new__(cls) -> Grammar.JSON: ...
    
    @typing.final
    class JSONSchema(Grammar):
        __match_args__ = ("schema",)
        @property
        def schema(self) -> builtins.str: ...
        def __new__(cls, schema: builtins.str) -> Grammar.JSONSchema: ...
    
    @typing.final
    class Regex(Grammar):
        __match_args__ = ("regex",)
        @property
        def regex(self) -> builtins.str: ...
        def __new__(cls, regex: builtins.str) -> Grammar.Regex: ...
    
    @typing.final
    class CFG(Grammar):
        __match_args__ = ("cfg",)
        @property
        def cfg(self) -> builtins.str: ...
        def __new__(cls, cfg: builtins.str) -> Grammar.CFG: ...
    
    ...

@typing.final
class KVCacheConfig:
    @property
    def context_window_size(self) -> typing.Optional[builtins.int]: ...
    @context_window_size.setter
    def context_window_size(self, value: typing.Optional[builtins.int]) -> None: ...
    @property
    def prefill_chunk_size(self) -> typing.Optional[builtins.int]: ...
    @prefill_chunk_size.setter
    def prefill_chunk_size(self, value: typing.Optional[builtins.int]) -> None: ...
    @property
    def sliding_window_size(self) -> typing.Optional[builtins.int]: ...
    @sliding_window_size.setter
    def sliding_window_size(self, value: typing.Optional[builtins.int]) -> None: ...
    def __new__(cls, context_window_size: typing.Optional[builtins.int], prefill_chunk_size: typing.Optional[builtins.int], sliding_window_size: typing.Optional[builtins.int]) -> KVCacheConfig: ...

@typing.final
class Knowledge:
    @classmethod
    def new_vector_store(cls, store: VectorStore, embedding_model: EmbeddingModel) -> Knowledge: ...
    async def retrieve(self, query: builtins.str, config: KnowledgeConfig) -> builtins.list[Document]: ...
    def as_tool(self) -> Tool: ...

@typing.final
class KnowledgeConfig:
    @property
    def top_k(self) -> typing.Optional[builtins.int]: ...
    @top_k.setter
    def top_k(self, value: typing.Optional[builtins.int]) -> None: ...
    def __new__(cls, top_k: typing.Optional[builtins.int] = None) -> KnowledgeConfig: ...
    @classmethod
    def from_dict(cls, config: dict) -> KnowledgeConfig: ...

@typing.final
class LangModel:
    @classmethod
    def new_local(cls, model_name: builtins.str, device_id: typing.Optional[builtins.int] = None, validate_checksum: typing.Optional[builtins.bool] = None, kv_cache: typing.Optional[KVCacheConfig] = None, progress_callback: typing.Callable[[CacheProgress], None] = None) -> typing.Awaitable[LangModel]: ...
    @classmethod
    def new_local_sync(cls, model_name: builtins.str, device_id: typing.Optional[builtins.int] = None, validate_checksum: typing.Optional[builtins.bool] = None, kv_cache: typing.Optional[KVCacheConfig] = None, progress_callback: typing.Callable[[CacheProgress], None] = None) -> LangModel: ...
    @classmethod
    def new_stream_api(cls, spec: typing.Literal["ChatCompletion", "OpenAI", "Gemini", "Claude", "Responses", "Grok"], model_name: builtins.str, api_key: builtins.str) -> LangModel: ...
    @classmethod
    def download(cls, model_name: builtins.str, progress_callback: typing.Callable[[CacheProgress], None] = None) -> None: ...
    @classmethod
    def remove(cls, model_name: builtins.str) -> None: ...
    def infer_delta(self, messages: str | list[Message], tools: typing.Optional[typing.Sequence[ToolDesc]] = None, documents: typing.Optional[typing.Sequence[Document]] = None, config: typing.Optional[LangModelInferConfig] = None) -> MessageDeltaOutputIterator: ...
    def infer_delta_sync(self, messages: str | list[Message], tools: typing.Optional[typing.Sequence[ToolDesc]] = None, documents: typing.Optional[typing.Sequence[Document]] = None, config: typing.Optional[LangModelInferConfig] = None) -> MessageDeltaOutputSyncIterator: ...
    def infer(self, messages: str | list[Message], tools: typing.Optional[typing.Sequence[ToolDesc]] = None, documents: typing.Optional[typing.Sequence[Document]] = None, config: typing.Optional[LangModelInferConfig] = None) -> typing.Awaitable[MessageOutput]: ...
    def infer_sync(self, messages: str | list[Message], tools: typing.Optional[typing.Sequence[ToolDesc]] = None, documents: typing.Optional[typing.Sequence[Document]] = None, config: typing.Optional[LangModelInferConfig] = None) -> MessageOutput: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class LangModelInferConfig:
    r"""
    Configuration parameters that control the behavior of language model inference.
    
    # Fields
    
    - **`document_polyfill`**: Configuration describing how retrieved documents are embedded into the model input.
      If `None`, it does not perform any polyfill, (ignoring documents).
    
    - **`think_effort`**: Controls the model's reasoning intensity.
      In local models, `low`, `medium`, `high` is ignored.
      In API models, it is up to it's API. See API parameters.
      Possible values: `disable`, `enable`, `low`, `medium`, `high`.
    
    - **`temperature`**: Sampling temperature controlling randomness of output.
      Lower values make output more deterministic; higher values increase diversity.
    
    - **`top_p`**: Nucleus sampling parameter (probability mass cutoff).
      Limits token sampling to a cumulative probability ≤ `top_p`.`
    
    - **`max_tokens`**: Maximum number of tokens to generate for a single inference.
    
    - **`grammar`**: Optional grammar constraint that restricts valid output forms.
      Supported types include:
      `Plain` (unconstrained text),
      `JSON` (ensures valid JSON output),
      `JSONSchema { schema }` (validates JSON against the given schema),
      `Regex { regex }` (constrains generation by a regular expression),
      `CFG { cfg }` (uses a context-free grammar definition).
    """
    @property
    def document_polyfill(self) -> typing.Optional[DocumentPolyfill]: ...
    @document_polyfill.setter
    def document_polyfill(self, value: typing.Optional[DocumentPolyfill]) -> None: ...
    @property
    def think_effort(self) -> typing.Optional[typing.Literal["disable", "enable", "low", "medium", "high"]]: ...
    @think_effort.setter
    def think_effort(self, value: typing.Optional[typing.Literal["disable", "enable", "low", "medium", "high"]]) -> None: ...
    @property
    def temperature(self) -> typing.Optional[builtins.float]: ...
    @temperature.setter
    def temperature(self, value: typing.Optional[builtins.float]) -> None: ...
    @property
    def top_p(self) -> typing.Optional[builtins.float]: ...
    @top_p.setter
    def top_p(self, value: typing.Optional[builtins.float]) -> None: ...
    @property
    def max_tokens(self) -> typing.Optional[builtins.int]: ...
    @max_tokens.setter
    def max_tokens(self, value: typing.Optional[builtins.int]) -> None: ...
    @property
    def grammar(self) -> typing.Optional[Grammar]: ...
    @grammar.setter
    def grammar(self, value: typing.Optional[Grammar]) -> None: ...
    def __new__(cls, document_polyfill: typing.Optional[DocumentPolyfill] = None, think_effort: typing.Optional[typing.Literal["disable", "enable", "low", "medium", "high"]] = None, temperature: typing.Optional[builtins.float] = None, top_p: typing.Optional[builtins.float] = None, max_tokens: typing.Optional[builtins.int] = None) -> LangModelInferConfig: ...
    @classmethod
    def from_dict(cls, config: dict) -> LangModelInferConfig: ...

@typing.final
class MCPClient:
    @property
    def tools(self) -> builtins.list[Tool]: ...
    def __repr__(self) -> builtins.str: ...
    @classmethod
    def from_stdio(cls, command: builtins.str, args: typing.Sequence[builtins.str]) -> typing.Awaitable[MCPClient]: ...
    @classmethod
    def from_streamable_http(cls, url: builtins.str) -> typing.Awaitable[MCPClient]: ...
    def get_tool(self, name: builtins.str) -> typing.Optional[Tool]: ...

@typing.final
class Message:
    r"""
    A chat message generated by a user, model, or tool.
    
    `Message` is the concrete, non-streaming container used by the application to store, transmit, or feed structured content into models or tools.
    It can represent various kinds of messages, including user input, assistant responses, tool-call outputs, or signed *thinking* metadata.
    
    Note that many different kinds of messages can be produced.
    For example, a language model may internally generate a `thinking` trace before emitting its final output, in order to improve reasoning accuracy.
    In other cases, a model may produce *function calls* — structured outputs that instruct external tools to perform specific actions.
    
    This struct is designed to handle all of these situations in a unified way.
    
    # Example
    
    ## Rust
    ```rust
    let msg = Message::new(Role::User).with_contents([Part::text("hello")]);
    assert_eq!(msg.role, Role::User);
    assert_eq!(msg.contents.len(), 1);
    ```
    """
    @property
    def role(self) -> typing.Literal["system", "user", "assistant", "tool"]:
        r"""
        Author of the message.
        """
    @role.setter
    def role(self, value: typing.Literal["system", "user", "assistant", "tool"]) -> None:
        r"""
        Author of the message.
        """
    @property
    def contents(self) -> builtins.list[Part]:
        r"""
        Primary parts of the message (e.g., text, image, value, or function).
        """
    @contents.setter
    def contents(self, value: typing.Optional[str | list[Part]]) -> None: ...
    @property
    def id(self) -> typing.Optional[builtins.str]:
        r"""
        Optional stable identifier for deduplication or threading.
        """
    @id.setter
    def id(self, value: typing.Optional[builtins.str]) -> None:
        r"""
        Optional stable identifier for deduplication or threading.
        """
    @property
    def thinking(self) -> typing.Optional[builtins.str]:
        r"""
        Internal “thinking” text used by some models before producing final output.
        """
    @thinking.setter
    def thinking(self, value: typing.Optional[builtins.str]) -> None:
        r"""
        Internal “thinking” text used by some models before producing final output.
        """
    @property
    def tool_calls(self) -> typing.Optional[builtins.list[Part]]:
        r"""
        Tool-call parts emitted alongside the main contents.
        """
    @tool_calls.setter
    def tool_calls(self, value: typing.Optional[builtins.list[Part]]) -> None:
        r"""
        Tool-call parts emitted alongside the main contents.
        """
    @property
    def signature(self) -> typing.Optional[builtins.str]:
        r"""
        Optional signature for the `thinking` field.
        
        This is only applicable to certain LLM APIs that require a signature as part of the `thinking` payload.
        """
    @signature.setter
    def signature(self, value: typing.Optional[builtins.str]) -> None:
        r"""
        Optional signature for the `thinking` field.
        
        This is only applicable to certain LLM APIs that require a signature as part of the `thinking` payload.
        """
    def __new__(cls, role: typing.Literal["system", "user", "assistant", "tool"], contents: typing.Optional[str | list[Part]] = None, id: typing.Optional[builtins.str] = None, thinking: typing.Optional[builtins.str] = None, tool_calls: typing.Optional[typing.Sequence[Part]] = None, signature: typing.Optional[builtins.str] = None) -> Message: ...
    def __repr__(self) -> builtins.str: ...
    def append_tool_call(self, part: Part) -> None: ...

@typing.final
class MessageDelta:
    r"""
    A streaming, incremental update to a [`Message`].
    
    `MessageDelta` accumulates partial outputs (text chunks, tool-call fragments, IDs, signatures, etc.) until they can be materialized as a full [`Message`].
    It implements [`Delta`] to support accumulation.
    
    # Accumulation Rules
    - `role`: merging two distinct roles fails.
    - `thinking`: concatenated in arrival order.
    - `contents`/`tool_calls`: last element is accumulated with the incoming delta when both are compatible (e.g., Text+Text, Function+Function with matching ID policy), otherwise appended as a new fragment.
    - `id`/`signature`: last-writer-wins.
    
    # Finalization
    - `finish()` converts the accumulated deltas into a fully-formed [`Message`].
      Fails if required fields (e.g., `role`) are missing or inner deltas cannot be finalized.
    
    # Examples
    ```rust
    let d1 = MessageDelta::new().with_role(Role::Assistant).with_contents([PartDelta::Text { text: "Hel".into() }]);
    let d2 = MessageDelta::new().with_contents([PartDelta::Text { text: "lo".into() }]);
    
    let merged = d1.accumulate(d2).unwrap();
    let msg = merged.finish().unwrap();
    assert_eq!(msg.contents[0].as_text().unwrap(), "Hello");
    ```
    """
    @property
    def role(self) -> typing.Optional[typing.Literal["system", "user", "assistant", "tool"]]: ...
    @role.setter
    def role(self, value: typing.Optional[typing.Literal["system", "user", "assistant", "tool"]]) -> None: ...
    @property
    def contents(self) -> builtins.list[PartDelta]: ...
    @contents.setter
    def contents(self, value: builtins.list[PartDelta]) -> None: ...
    @property
    def id(self) -> typing.Optional[builtins.str]: ...
    @id.setter
    def id(self, value: typing.Optional[builtins.str]) -> None: ...
    @property
    def thinking(self) -> typing.Optional[builtins.str]: ...
    @thinking.setter
    def thinking(self, value: typing.Optional[builtins.str]) -> None: ...
    @property
    def tool_calls(self) -> builtins.list[PartDelta]: ...
    @tool_calls.setter
    def tool_calls(self, value: builtins.list[PartDelta]) -> None: ...
    @property
    def signature(self) -> typing.Optional[builtins.str]: ...
    @signature.setter
    def signature(self, value: typing.Optional[builtins.str]) -> None: ...
    def __new__(cls, role: typing.Optional[typing.Literal["system", "user", "assistant", "tool"]] = None, contents: typing.Optional[typing.Sequence[PartDelta]] = None, id: typing.Optional[builtins.str] = None, thinking: typing.Optional[builtins.str] = None, tool_calls: typing.Optional[typing.Sequence[PartDelta]] = None, signature: typing.Optional[builtins.str] = None) -> MessageDelta: ...
    def __repr__(self) -> builtins.str: ...
    def __add__(self, other: MessageDelta) -> MessageDelta: ...
    def finish(self) -> Message: ...

@typing.final
class MessageDeltaOutput:
    r"""
    A container for a streamed message delta and its termination signal.
    
    During streaming, `delta` carries the incremental payload; once a terminal
    condition is reached, `finish_reason` may be populated to explain why.
    
    # Examples
    ```rust
    let mut out = MessageOutput::new();
    out.delta = MessageDelta::new().with_role(Role::Assistant).with_contents([PartDelta::Text { text: "Hi".into() }]);
    assert!(out.finish_reason.is_none());
    ```
    
    # Lifecycle
    - While streaming: `finish_reason` is typically `None`.
    - On completion: `finish_reason` is set; callers can then `finish()` the delta to obtain a concrete [`Message`].
    """
    @property
    def delta(self) -> MessageDelta: ...
    @delta.setter
    def delta(self, value: MessageDelta) -> None: ...
    @property
    def finish_reason(self) -> typing.Optional[FinishReason]: ...
    @finish_reason.setter
    def finish_reason(self, value: typing.Optional[FinishReason]) -> None: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class MessageDeltaOutputIterator:
    def __aiter__(self) -> MessageDeltaOutputIterator: ...
    def __anext__(self) -> typing.Awaitable[MessageDeltaOutput]: ...

@typing.final
class MessageDeltaOutputSyncIterator:
    def __iter__(self) -> MessageDeltaOutputSyncIterator: ...
    def __next__(self) -> MessageDeltaOutput: ...

@typing.final
class MessageOutput:
    @property
    def message(self) -> Message: ...
    @message.setter
    def message(self, value: Message) -> None: ...
    @property
    def finish_reason(self) -> FinishReason: ...
    @finish_reason.setter
    def finish_reason(self, value: FinishReason) -> None: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class MessageOutputIterator:
    def __aiter__(self) -> MessageOutputIterator: ...
    def __anext__(self) -> typing.Awaitable[MessageOutput]: ...

@typing.final
class MessageOutputSyncIterator:
    def __iter__(self) -> MessageOutputSyncIterator: ...
    def __next__(self) -> MessageOutput: ...

class Part:
    r"""
    Represents a semantically meaningful content unit exchanged between the model and the user.
    
    Conceptually, each `Part` encapsulates a piece of **data** that contributes
    to a chat message — such as text, a function invocation, or an image.  
    
    For example, a single message consisting of a sequence like  
    `(text..., image, text...)` is represented as a `Message` containing
    an array of three `Part` elements.
    
    Note that a `Part` does **not** carry "intent", such as "reasoning" or "tool call".
    These higher-level semantics are determined by the context of a [`Message`].
    
    # Example
    
    ## Rust
    ```rust
    let part = Part::text("Hello, world!");
    assert!(part.is_text());
    ```
    """
    @property
    def part_type(self) -> builtins.str: ...
    def __repr__(self) -> builtins.str: ...
    @classmethod
    def image_from_bytes(cls, data: bytes) -> Part: ...
    @classmethod
    def image_from_base64(cls, data: builtins.str) -> Part: ...
    @classmethod
    def image_from_url(cls, url: builtins.str) -> Part: ...
    @typing.final
    class Text(Part):
        r"""
        Plain utf-8 encoded text.
        """
        __match_args__ = ("text",)
        @property
        def text(self) -> builtins.str: ...
        def __new__(cls, text: builtins.str) -> Part.Text: ...
    
    @typing.final
    class Function(Part):
        r"""
        Represents a structured function call to an external tool.
        
        Many language models (LLMs) use a **function calling** mechanism to extend their capabilities.
        When an LLM decides to use external *tools*, it produces a structured output called a `function`.
        A function conventionally consists of two fields: a `name`, and an `arguments` field formatted as JSON.
        This is conceptually similar to making an HTTP POST request, where the request body carries a single JSON object.
        
        This struct models that convention, representing a function invocation request
        from an LLM to an external tool or API.
        
        # Examples
        ```rust
        let f = PartFunction {
            name: "translate".to_string(),
            arguments: Value::from_json(r#"{"source": "hello", "lang": "cn"}"#).unwrap(),
        };
        ```
        """
        __match_args__ = ("id", "function",)
        @property
        def id(self) -> typing.Optional[builtins.str]: ...
        @property
        def function(self) -> PartFunction: ...
        def __new__(cls, id: typing.Optional[builtins.str], function: PartFunction) -> Part.Function: ...
    
    @typing.final
    class Value(Part):
        r"""
        Holds a structured data value, typically considered as a JSON structure.
        """
        __match_args__ = ("value",)
        @property
        def value(self) -> typing.Any: ...
        def __new__(cls, value: typing.Any) -> Part.Value: ...
    
    @typing.final
    class Image(Part):
        r"""
        Contains an image payload or reference used within a message part.
        The image may be provided as raw binary data or an encoded format (e.g., PNG, JPEG),
        or as a reference via a URL. Optional metadata can be included alongside the image.
        """
        __match_args__ = ("image",)
        @property
        def image(self) -> PartImage: ...
        def __new__(cls, image: PartImage) -> Part.Image: ...
    

class PartDelta:
    r"""
    Represents a partial or incremental update (delta) of a [`Part`].
    
    This type enables composable, streaming updates to message parts.
    For example, text may be produced token-by-token, or a function call
    may be emitted gradually as its arguments stream in.
    
    # Example
    
    ## Rust
    ```rust
    let d1 = PartDelta::Text { text: "Hel".into() };
    let d2 = PartDelta::Text { text: "lo".into() };
    let merged = d1.accumulate(d2).unwrap();
    assert_eq!(merged.to_text().unwrap(), "Hello");
    ```
    
    # Error Handling
    Accumulation or finalization may return an error if incompatible deltas
    (e.g. mismatched function IDs) are combined or invalid JSON arguments are given.
    """
    @property
    def part_type(self) -> builtins.str: ...
    def __repr__(self) -> builtins.str: ...
    @typing.final
    class Text(PartDelta):
        r"""
        Incremental text fragment.
        """
        __match_args__ = ("text",)
        @property
        def text(self) -> builtins.str: ...
        def __new__(cls, text: builtins.str) -> PartDelta.Text: ...
    
    @typing.final
    class Function(PartDelta):
        r"""
        Incremental function call fragment.
        """
        __match_args__ = ("id", "function",)
        @property
        def id(self) -> typing.Optional[builtins.str]: ...
        @property
        def function(self) -> PartDeltaFunction: ...
        def __new__(cls, id: typing.Optional[builtins.str], function: PartDeltaFunction) -> PartDelta.Function: ...
    
    @typing.final
    class Value(PartDelta):
        r"""
        JSON-like value update.
        """
        __match_args__ = ("value",)
        @property
        def value(self) -> typing.Any: ...
        def __new__(cls, value: typing.Any) -> PartDelta.Value: ...
    
    @typing.final
    class Null(PartDelta):
        r"""
        Placeholder representing no data yet.
        """
        __match_args__ = ()
        def __new__(cls) -> PartDelta.Null: ...
    

class PartDeltaFunction:
    r"""
    Represents an incremental update (delta) of a function part.
    
    This type is used during streaming or partial message generation, when function calls are being streamed as text chunks or partial JSON fragments.
    
    # Variants
    * `Verbatim(String)` — Raw text content, typically a partial JSON fragment.
    * `WithStringArgs { name, arguments }` — Function name and its serialized arguments as strings.
    * `WithParsedArgs { name, arguments }` — Function name and parsed arguments as a `Value`.
    
    # Use Case
    When the model streams out a function call response (e.g., `"function_call":{"name":...}`),
    the incremental deltas can be accumulated until the full function payload is formed.
    
    # Example
    ```rust
    let delta = PartDeltaFunction::WithStringArgs {
        name: "translate".into(),
        arguments: r#"{"text":"hi"}"#.into(),
    };
    ```
    """
    @typing.final
    class Verbatim(PartDeltaFunction):
        __match_args__ = ("text",)
        @property
        def text(self) -> builtins.str: ...
        def __new__(cls, text: builtins.str) -> PartDeltaFunction.Verbatim: ...
    
    @typing.final
    class WithStringArgs(PartDeltaFunction):
        __match_args__ = ("name", "arguments",)
        @property
        def name(self) -> builtins.str: ...
        @property
        def arguments(self) -> builtins.str: ...
        def __new__(cls, name: builtins.str, arguments: builtins.str) -> PartDeltaFunction.WithStringArgs: ...
    
    @typing.final
    class WithParsedArgs(PartDeltaFunction):
        __match_args__ = ("name", "arguments",)
        @property
        def name(self) -> builtins.str: ...
        @property
        def arguments(self) -> typing.Any: ...
        def __new__(cls, name: builtins.str, arguments: typing.Any) -> PartDeltaFunction.WithParsedArgs: ...
    
    ...

@typing.final
class PartFunction:
    r"""
    Represents a function call contained within a message part.
    """
    @property
    def name(self) -> builtins.str: ...
    @property
    def arguments(self) -> dict[str, typing.Any]: ...
    def __eq__(self, other: builtins.object) -> builtins.bool: ...
    def __new__(cls, name: builtins.str, arguments: typing.Any) -> PartFunction: ...
    def __repr__(self) -> builtins.str: ...

class PartImage:
    r"""
    Represents the image data contained in a [`Part`].
    
    `PartImage` provides structured access to image data.
    Currently, it only implments "binary" types.
    
    # Example
    ```rust
    let part = Part::image_binary(640, 480, "rgb", (0..640*480*3).map(|i| (i % 255) as u8)).unwrap();
    
    if let Some(img) = part.as_image() {
        assert_eq!(img.height(), 640);
        assert_eq!(img.width(), 480);
    }
    ```
    """
    @typing.final
    class Binary(PartImage):
        __match_args__ = ("height", "width", "colorspace", "data",)
        @property
        def height(self) -> builtins.int: ...
        @property
        def width(self) -> builtins.int: ...
        @property
        def colorspace(self) -> typing.Literal["grayscale", "rgb", "rgba"]: ...
        @property
        def data(self) -> typing.Any: ...
        def __new__(cls, height: builtins.int, width: builtins.int, colorspace: typing.Literal["grayscale", "rgb", "rgba"], data: typing.Any) -> PartImage.Binary: ...
    
    @typing.final
    class Url(PartImage):
        __match_args__ = ("url",)
        @property
        def url(self) -> builtins.str: ...
        def __new__(cls, url: builtins.str) -> PartImage.Url: ...
    
    ...

class Tool:
    @classmethod
    def new_builtin(cls, kind: typing.Literal["terminal", "web_search_duckduckgo", "web_fetch"], **kwargs: typing.Any) -> Tool: ...
    @classmethod
    def new_py_function(cls, func: typing.Any, desc: typing.Optional[ToolDesc] = None) -> Tool: ...
    def __repr__(self) -> builtins.str: ...
    def get_description(self) -> ToolDesc: ...
    def __call__(self, **kwargs: typing.Any) -> typing.Awaitable[typing.Any]: ...
    def call(self, **kwargs: typing.Any) -> typing.Awaitable[typing.Any]: ...
    def call_sync(self, **kwargs: typing.Any) -> typing.Any: ...

@typing.final
class ToolDesc:
    r"""
    Describes a **tool** (or function) that a language model can invoke.
    
    `ToolDesc` defines the schema, behavior, and input/output specification of a callable
    external function, allowing an LLM to understand how to use it.
    
    The primary role of this struct is to describe to the LLM what a *tool* does,
    how it can be invoked, and what input (`parameters`) and output (`returns`) schemas it expects.
    
    The format follows the same **schema conventions** used by Hugging Face’s
    `transformers` library, as well as APIs such as *OpenAI* and *Anthropic*.
    The `parameters` and `returns` fields are typically defined using **JSON Schema**.
    
    We provide a builder [`ToolDescBuilder`] helper for convenient and fluent construction.
    Please refer to [`ToolDescBuilder`].
    
    # Example
    ```rust
    use crate::value::{ToolDescBuilder, to_value};
    
    let desc = ToolDescBuilder::new("temperature")
        .description("Get the current temperature for a given city")
        .parameters(to_value!({
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city name"
                },
                "unit": {
                    "type": "string",
                    "description": "Temperature unit (default: Celsius)",
                    "enum": ["Celsius", "Fahrenheit"]
                }
            },
            "required": ["location"]
        }))
        .returns(to_value!({
            "type": "number"
        }))
        .build();
    
    assert_eq!(desc.name, "temperature");
    ```
    """
    @property
    def name(self) -> builtins.str: ...
    @property
    def description(self) -> typing.Optional[builtins.str]: ...
    @property
    def parameters(self) -> dict: ...
    @property
    def returns(self) -> typing.Optional[dict]: ...
    def __new__(cls, name: builtins.str, description: typing.Optional[builtins.str], parameters: dict, *, returns: typing.Optional[dict] = None) -> ToolDesc: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class VectorStore:
    @classmethod
    def new_faiss(cls, dim: builtins.int) -> VectorStore: ...
    @classmethod
    def new_chroma(cls, url: builtins.str, collection_name: typing.Optional[builtins.str]) -> VectorStore: ...
    def add_vector(self, input: VectorStoreAddInput) -> builtins.str: ...
    def add_vectors(self, inputs: typing.Sequence[VectorStoreAddInput]) -> builtins.list[builtins.str]: ...
    def get_by_id(self, id: builtins.str) -> typing.Optional[VectorStoreGetResult]: ...
    def get_by_ids(self, ids: typing.Sequence[builtins.str]) -> builtins.list[VectorStoreGetResult]: ...
    def retrieve(self, query_embedding: builtins.list[float], top_k: builtins.int) -> builtins.list[VectorStoreRetrieveResult]: ...
    def batch_retrieve(self, query_embeddings: typing.Sequence[builtins.list[float]], top_k: builtins.int) -> builtins.list[builtins.list[VectorStoreRetrieveResult]]: ...
    def remove_vector(self, id: builtins.str) -> None: ...
    def remove_vectors(self, ids: typing.Sequence[builtins.str]) -> None: ...
    def clear(self) -> None: ...
    def count(self) -> builtins.int: ...

@typing.final
class VectorStoreAddInput:
    @property
    def embedding(self) -> builtins.list[float]: ...
    @embedding.setter
    def embedding(self, value: builtins.list[float]) -> None: ...
    @property
    def document(self) -> builtins.str: ...
    @document.setter
    def document(self, value: builtins.str) -> None: ...
    @property
    def metadata(self) -> typing.Optional[builtins.dict[builtins.str, typing.Any]]: ...
    @metadata.setter
    def metadata(self, value: typing.Optional[builtins.dict[builtins.str, typing.Any]]) -> None: ...
    def __new__(cls, embedding: builtins.list[float], document: builtins.str, metadata: typing.Optional[typing.Mapping[builtins.str, typing.Any]] = None) -> VectorStoreAddInput: ...

@typing.final
class VectorStoreGetResult:
    @property
    def id(self) -> builtins.str: ...
    @id.setter
    def id(self, value: builtins.str) -> None: ...
    @property
    def document(self) -> builtins.str: ...
    @document.setter
    def document(self, value: builtins.str) -> None: ...
    @property
    def metadata(self) -> typing.Optional[builtins.dict[builtins.str, typing.Any]]: ...
    @metadata.setter
    def metadata(self, value: typing.Optional[builtins.dict[builtins.str, typing.Any]]) -> None: ...
    @property
    def embedding(self) -> builtins.list[float]: ...
    @embedding.setter
    def embedding(self, value: builtins.list[float]) -> None: ...

@typing.final
class VectorStoreRetrieveResult:
    @property
    def id(self) -> builtins.str: ...
    @id.setter
    def id(self, value: builtins.str) -> None: ...
    @property
    def document(self) -> builtins.str: ...
    @document.setter
    def document(self, value: builtins.str) -> None: ...
    @property
    def metadata(self) -> typing.Optional[builtins.dict[builtins.str, typing.Any]]: ...
    @metadata.setter
    def metadata(self, value: typing.Optional[builtins.dict[builtins.str, typing.Any]]) -> None: ...
    @property
    def distance(self) -> builtins.float: ...
    @distance.setter
    def distance(self, value: builtins.float) -> None: ...

@typing.final
class FinishReason(enum.Enum):
    r"""
    Explains why a language model's streamed generation finished.
    """
    Stop = ...
    r"""
    The model stopped naturally (e.g., EOS token or stop sequence).
    """
    Length = ...
    r"""
    Hit the maximum token/length limit.
    """
    ToolCall = ...
    r"""
    Stopped because a tool call was produced, waiting for it's execution.
    """
    Refusal = ...
    r"""
    Content was refused/filtered; string provides reason.
    """

    def __repr__(self) -> builtins.str: ...

def ailoy_model_cli() -> None: ...

