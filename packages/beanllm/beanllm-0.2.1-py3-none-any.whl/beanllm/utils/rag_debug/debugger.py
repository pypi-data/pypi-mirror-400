"""
RAG Debug Utils - RAG íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹… ë° ê²€ì¦ ë„êµ¬
ì¤‘ê°„ ê³¼ì •ì„ í™•ì¸í•˜ê³  ë¬¸ì œë¥¼ ì°¾ëŠ” ë° ë„ì›€
"""

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

try:
    import numpy as np

    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

    # numpy ëŒ€ì²´ í•¨ìˆ˜ë“¤
    class np:
        @staticmethod
        def array(x):
            return x

        @staticmethod
        def dot(a, b):
            return sum(x * y for x, y in zip(a, b))

        @staticmethod
        def linalg_norm(x):
            return sum(v**2 for v in x) ** 0.5

        class linalg:
            @staticmethod
            def norm(x):
                return sum(v**2 for v in x) ** 0.5


# ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ TYPE_CHECKING ì‚¬ìš©
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from beanllm.domain.loaders import Document
else:
    # ëŸ°íƒ€ì„ì—ë§Œ import (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
    # DocumentëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œë§Œ import
    Document = Any  # type: ignore


@dataclass
class EmbeddingInfo:
    """ì„ë² ë”© ì •ë³´"""

    text: str
    vector: List[float]
    dimension: int
    norm: float  # ë²¡í„° í¬ê¸°
    preview: List[float]  # ì• 10ê°œ ê°’


@dataclass
class SimilarityInfo:
    """ìœ ì‚¬ë„ ì •ë³´"""

    text1: str
    text2: str
    cosine_similarity: float
    euclidean_distance: float
    interpretation: str  # í•´ì„


class RAGDebugger:
    """
    RAG íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹… ë„êµ¬

    Example:
        debugger = RAGDebugger()

        # ì„ë² ë”© í™•ì¸
        debugger.inspect_embedding(text, vector)

        # ìœ ì‚¬ë„ í™•ì¸
        debugger.compare_texts(text1, text2, embedding_function)

        # Vector Store í™•ì¸
        debugger.inspect_vector_store(store, sample_queries)
    """

    def __init__(self, verbose: bool = True):
        """
        Args:
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.verbose = verbose

    def _print(self, *args, **kwargs):
        """Verbose ëª¨ë“œì¼ ë•Œë§Œ ì¶œë ¥"""
        if self.verbose:
            print(*args, **kwargs)

    # ==================== ì„ë² ë”© ê²€ì¦ ====================

    def inspect_embedding(
        self, text: str, vector: List[float], show_preview: int = 10
    ) -> EmbeddingInfo:
        """
        ë‹¨ì¼ ì„ë² ë”© ê²€ì‚¬

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            vector: ì„ë² ë”© ë²¡í„°
            show_preview: ë¯¸ë¦¬ë³´ê¸° ê°œìˆ˜

        Returns:
            EmbeddingInfo
        """
        dimension = len(vector)
        norm = float(np.linalg.norm(vector))
        preview = vector[:show_preview]

        info = EmbeddingInfo(
            text=text, vector=vector, dimension=dimension, norm=norm, preview=preview
        )

        self._print(f"\n{'=' * 60}")
        self._print("ğŸ“Š Embedding ì •ë³´")
        self._print(f"{'=' * 60}")
        self._print(f"í…ìŠ¤íŠ¸: {text[:100]}...")
        self._print(f"ì°¨ì›: {dimension}")
        self._print(f"ë²¡í„° í¬ê¸° (norm): {norm:.4f}")
        self._print(f"ë¯¸ë¦¬ë³´ê¸° ({show_preview}ê°œ):")
        self._print(f"  {preview}")
        self._print(f"{'=' * 60}\n")

        return info

    def compare_embeddings(self, embeddings: List[Tuple[str, List[float]]]) -> None:
        """
        ì—¬ëŸ¬ ì„ë² ë”© ë¹„êµ

        Args:
            embeddings: [(text, vector), ...] ë¦¬ìŠ¤íŠ¸

        Example:
            debugger.compare_embeddings([
                ("ê°•ì•„ì§€", vec1),
                ("ê°œ", vec2),
                ("ìë™ì°¨", vec3)
            ])
        """
        self._print(f"\n{'=' * 60}")
        self._print("ğŸ“Š Embeddings ë¹„êµ")
        self._print(f"{'=' * 60}")

        # ê° ì„ë² ë”© ê¸°ë³¸ ì •ë³´
        for text, vector in embeddings:
            norm = float(np.linalg.norm(vector))
            self._print(f"\n{text}:")
            self._print(f"  ì°¨ì›: {len(vector)}")
            self._print(f"  Norm: {norm:.4f}")
            self._print(f"  ì• 5ê°œ: {vector[:5]}")

        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤
        self._print(f"\n{'=' * 60}")
        self._print("ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ (Cosine Similarity):")
        self._print(f"{'=' * 60}")

        texts = [t for t, _ in embeddings]
        vectors = [v for _, v in embeddings]

        # í—¤ë”
        header = f"{'':15}"
        for text in texts:
            header += f"{text[:12]:>15}"
        self._print(header)
        self._print("-" * (15 + 15 * len(texts)))

        # ê° í–‰
        for i, text1 in enumerate(texts):
            row = f"{text1[:12]:15}"
            for j, text2 in enumerate(texts):
                sim = self._cosine_similarity(vectors[i], vectors[j])
                row += f"{sim:>15.3f}"
            self._print(row)

        self._print(f"{'=' * 60}\n")

    # ==================== ìœ ì‚¬ë„ ê³„ì‚° ====================

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        a_arr = np.array(a)
        b_arr = np.array(b)
        return float(np.dot(a_arr, b_arr) / (np.linalg.norm(a_arr) * np.linalg.norm(b_arr)))

    def _euclidean_distance(self, a: List[float], b: List[float]) -> float:
        """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
        if HAS_NUMPY:
            import numpy as real_np

            a_arr = real_np.array(a)
            b_arr = real_np.array(b)
            return float(real_np.linalg.norm(a_arr - b_arr))
        else:
            # numpy ì—†ì´ ê³„ì‚°
            return sum((x - y) ** 2 for x, y in zip(a, b)) ** 0.5

    def _interpret_similarity(self, cosine_sim: float) -> str:
        """ìœ ì‚¬ë„ í•´ì„"""
        if cosine_sim >= 0.9:
            return "ë§¤ìš° ìœ ì‚¬ (ê±°ì˜ ê°™ì€ ì˜ë¯¸)"
        elif cosine_sim >= 0.7:
            return "ìœ ì‚¬ (ê´€ë ¨ìˆëŠ” ë‚´ìš©)"
        elif cosine_sim >= 0.5:
            return "ì–´ëŠì •ë„ ê´€ë ¨ (ì•½í•œ ì—°ê´€ì„±)"
        elif cosine_sim >= 0.3:
            return "ì•½ê°„ ê´€ë ¨ (ê±°ì˜ ë¬´ê´€)"
        else:
            return "ë¬´ê´€ (ì „í˜€ ë‹¤ë¥¸ ì˜ë¯¸)"

    def compare_texts(self, text1: str, text2: str, embedding_function) -> SimilarityInfo:
        """
        ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            embedding_function: ì„ë² ë”© í•¨ìˆ˜

        Returns:
            SimilarityInfo
        """
        # ì„ë² ë”© ìƒì„±
        vectors = embedding_function([text1, text2])
        vec1, vec2 = vectors[0], vectors[1]

        # ìœ ì‚¬ë„ ê³„ì‚°
        cosine_sim = self._cosine_similarity(vec1, vec2)
        euclidean_dist = self._euclidean_distance(vec1, vec2)
        interpretation = self._interpret_similarity(cosine_sim)

        info = SimilarityInfo(
            text1=text1,
            text2=text2,
            cosine_similarity=cosine_sim,
            euclidean_distance=euclidean_dist,
            interpretation=interpretation,
        )

        self._print(f"\n{'=' * 60}")
        self._print("ğŸ“Š í…ìŠ¤íŠ¸ ìœ ì‚¬ë„")
        self._print(f"{'=' * 60}")
        self._print(f"í…ìŠ¤íŠ¸ 1: {text1[:50]}...")
        self._print(f"í…ìŠ¤íŠ¸ 2: {text2[:50]}...")
        self._print(f"\nì½”ì‚¬ì¸ ìœ ì‚¬ë„: {cosine_sim:.4f}")
        self._print(f"ìœ í´ë¦¬ë“œ ê±°ë¦¬: {euclidean_dist:.4f}")
        self._print(f"í•´ì„: {interpretation}")
        self._print(f"{'=' * 60}\n")

        return info

    # ==================== ì²­í¬ ê²€ì¦ ====================

    def inspect_chunks(self, chunks: List[Any], show_samples: int = 3) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ ê²€ì‚¬

        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            show_samples: ìƒ˜í”Œ ê°œìˆ˜

        Returns:
            ì²­í¬ í†µê³„
        """
        if not chunks:
            self._print("âš ï¸  ì²­í¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            return {}

        # í†µê³„
        total_chunks = len(chunks)
        chunk_lengths = [len(chunk.content) for chunk in chunks]
        avg_length = sum(chunk_lengths) / len(chunk_lengths)
        min_length = min(chunk_lengths)
        max_length = max(chunk_lengths)

        stats = {
            "total_chunks": total_chunks,
            "avg_length": avg_length,
            "min_length": min_length,
            "max_length": max_length,
            "chunk_lengths": chunk_lengths,
        }

        self._print(f"\n{'=' * 60}")
        self._print("ğŸ“„ ì²­í¬ ì •ë³´")
        self._print(f"{'=' * 60}")
        self._print(f"ì´ ì²­í¬ ìˆ˜: {total_chunks}")
        self._print(f"í‰ê·  ê¸¸ì´: {avg_length:.1f} ë¬¸ì")
        self._print(f"ìµœì†Œ ê¸¸ì´: {min_length} ë¬¸ì")
        self._print(f"ìµœëŒ€ ê¸¸ì´: {max_length} ë¬¸ì")

        # ìƒ˜í”Œ ì¶œë ¥
        self._print(f"\nìƒ˜í”Œ ì²­í¬ (ì²˜ìŒ {show_samples}ê°œ):")
        for i, chunk in enumerate(chunks[:show_samples], 1):
            self._print(f"\n[Chunk {i}] ({len(chunk.content)} ë¬¸ì)")
            self._print(f"  ë‚´ìš©: {chunk.content[:100]}...")
            if chunk.metadata:
                self._print(f"  ë©”íƒ€: {chunk.metadata}")

        self._print(f"{'=' * 60}\n")

        return stats

    # ==================== Vector Store ê²€ì¦ ====================

    def inspect_vector_store(self, store, sample_queries: List[str], k: int = 3) -> Dict[str, Any]:
        """
        Vector Store ê²€ì‚¬

        Args:
            store: VectorStore ì¸ìŠ¤í„´ìŠ¤
            sample_queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        self._print(f"\n{'=' * 60}")
        self._print("ğŸ” Vector Store ê²€ì‚¬")
        self._print(f"{'=' * 60}")

        results = {}

        for query in sample_queries:
            self._print(f'\nì¿¼ë¦¬: "{query}"')
            self._print("-" * 60)

            try:
                search_results = store.similarity_search(query, k=k)

                if not search_results:
                    self._print("  âš ï¸  ê²°ê³¼ ì—†ìŒ")
                    results[query] = []
                    continue

                results[query] = search_results

                for i, result in enumerate(search_results, 1):
                    score = result.score
                    content = result.document.content
                    metadata = result.document.metadata

                    self._print(f"\n  [{i}] Score: {score:.4f}")
                    self._print(f"      Content: {content[:100]}...")
                    if metadata:
                        self._print(f"      Metadata: {metadata}")

                    # ì ìˆ˜ í•´ì„
                    interpretation = self._interpret_similarity(score)
                    self._print(f"      í•´ì„: {interpretation}")

            except Exception as e:
                self._print(f"  âŒ ì—ëŸ¬: {e}")
                results[query] = None

        self._print(f"\n{'=' * 60}\n")

        return results

    # ==================== ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦ ====================

    def validate_rag_pipeline(
        self,
        documents: List[Any],
        chunks: List[Any],
        embedding_function,
        store,
        test_queries: List[str],
    ) -> Dict[str, Any]:
        """
        ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ê²€ì¦

        Args:
            documents: ì›ë³¸ ë¬¸ì„œ
            chunks: ë¶„í• ëœ ì²­í¬
            embedding_function: ì„ë² ë”© í•¨ìˆ˜
            store: VectorStore
            test_queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬

        Returns:
            ì „ì²´ ê²€ì¦ ê²°ê³¼
        """
        self._print(f"\n{'#' * 60}")
        self._print("# RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ê²€ì¦")
        self._print(f"{'#' * 60}\n")

        report = {}

        # 1. ë¬¸ì„œ í™•ì¸
        self._print("1ï¸âƒ£  ì›ë³¸ ë¬¸ì„œ í™•ì¸")
        report["documents"] = {
            "count": len(documents),
            "total_length": sum(len(doc.content) for doc in documents),
        }
        self._print(
            f"   âœ“ {len(documents)}ê°œ ë¬¸ì„œ, ì´ {report['documents']['total_length']} ë¬¸ì\n"
        )

        # 2. ì²­í¬ í™•ì¸
        self._print("2ï¸âƒ£  ì²­í¬ í™•ì¸")
        chunk_stats = self.inspect_chunks(chunks, show_samples=2)
        report["chunks"] = chunk_stats

        # 3. ì„ë² ë”© í…ŒìŠ¤íŠ¸
        self._print("3ï¸âƒ£  ì„ë² ë”© í…ŒìŠ¤íŠ¸")
        test_text = chunks[0].content[:100] if chunks else "Test"
        test_vector = embedding_function([test_text])[0]
        self.inspect_embedding(test_text, test_vector, show_preview=5)
        report["embedding_dim"] = len(test_vector)

        # 4. Vector Store í…ŒìŠ¤íŠ¸
        self._print("4ï¸âƒ£  Vector Store í…ŒìŠ¤íŠ¸")
        search_results = self.inspect_vector_store(store, test_queries, k=3)
        report["search_results"] = search_results

        # 5. ì¢…í•© í‰ê°€
        self._print(f"\n{'=' * 60}")
        self._print("ğŸ“Š ì¢…í•© í‰ê°€")
        self._print(f"{'=' * 60}")

        issues = []

        # ì²­í¬ í¬ê¸° í™•ì¸
        if chunk_stats.get("avg_length", 0) < 50:
            issues.append("âš ï¸  ì²­í¬ê°€ ë„ˆë¬´ ì‘ìŒ (í‰ê·  < 50)")
        elif chunk_stats.get("avg_length", 0) > 2000:
            issues.append("âš ï¸  ì²­í¬ê°€ ë„ˆë¬´ í¼ (í‰ê·  > 2000)")

        # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        empty_results = sum(1 for r in search_results.values() if not r)
        if empty_results > 0:
            issues.append(f"âš ï¸  {empty_results}ê°œ ì¿¼ë¦¬ì—ì„œ ê²°ê³¼ ì—†ìŒ")

        # ë‚®ì€ ì ìˆ˜ í™•ì¸
        low_scores = []
        for query, results in search_results.items():
            if results and results[0].score < 0.5:
                low_scores.append((query, results[0].score))

        if low_scores:
            issues.append(f"âš ï¸  {len(low_scores)}ê°œ ì¿¼ë¦¬ì—ì„œ ë‚®ì€ ì ìˆ˜ (< 0.5)")

        if not issues:
            self._print("âœ… ë¬¸ì œ ì—†ìŒ - íŒŒì´í”„ë¼ì¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        else:
            self._print("ë°œê²¬ëœ ë¬¸ì œ:")
            for issue in issues:
                self._print(f"  {issue}")

        self._print(f"{'=' * 60}\n")

        report["issues"] = issues

        return report


# ==================== í¸ì˜ í•¨ìˆ˜ ====================


def inspect_embedding(text: str, embedding_function, show_preview: int = 10) -> EmbeddingInfo:
    """
    ì„ë² ë”© ê²€ì‚¬ (ê°„ë‹¨í•œ ë²„ì „)

    Example:
        from beanllm import Embedding, inspect_embedding

        embed_func = Embedding.openai().embed_sync
        info = inspect_embedding("Hello world", embed_func)
    """
    vector = embedding_function([text])[0]
    debugger = RAGDebugger(verbose=True)
    return debugger.inspect_embedding(text, vector, show_preview)


def compare_texts(text1: str, text2: str, embedding_function) -> SimilarityInfo:
    """
    ë‘ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ (ê°„ë‹¨í•œ ë²„ì „)

    Example:
        from beanllm import Embedding, compare_texts

        embed_func = Embedding.openai().embed_sync
        info = compare_texts("ê°•ì•„ì§€", "ê°œ", embed_func)
    """
    debugger = RAGDebugger(verbose=True)
    return debugger.compare_texts(text1, text2, embedding_function)


def validate_pipeline(
    documents: List[Any],
    chunks: List[Any],
    embedding_function,
    store,
    test_queries: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """
    ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ê²€ì¦ (ê°„ë‹¨í•œ ë²„ì „)

    Example:
        from beanllm import validate_pipeline

        report = validate_pipeline(
            documents=docs,
            chunks=chunks,
            embedding_function=embed_func,
            store=store,
            test_queries=["What is AI?", "How does ML work?"]
        )
    """
    if test_queries is None:
        # ê¸°ë³¸ ì¿¼ë¦¬
        test_queries = [chunks[0].content[:50]] if chunks else ["test"]

    debugger = RAGDebugger(verbose=True)
    return debugger.validate_rag_pipeline(
        documents, chunks, embedding_function, store, test_queries
    )


# ==================== ì‹œê°í™” ìœ í‹¸ë¦¬í‹° ====================


def visualize_embeddings_2d(texts: List[str], embedding_function, save_path: Optional[str] = None):
    """
    ì„ë² ë”©ì„ 2Dë¡œ ì‹œê°í™” (ê¸°ì¡´ í•¨ìˆ˜ - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)

    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        embedding_function: ì„ë² ë”© í•¨ìˆ˜
        save_path: ì €ì¥ ê²½ë¡œ (ì„ íƒ)

    Example:
        from beanllm import Embedding, visualize_embeddings_2d

        texts = ["ê°•ì•„ì§€", "ê°œ", "ê³ ì–‘ì´", "ìë™ì°¨", "ë¹„í–‰ê¸°"]
        embed_func = Embedding.openai().embed_sync
        visualize_embeddings_2d(texts, embed_func)
    """
    # ìƒˆë¡œìš´ í•¨ìˆ˜ë¡œ ìœ„ì„
    visualize_embeddings(
        texts,
        embedding_function,
        method="tsne",
        dimensions=2,
        save_path=save_path,
        interactive=False,
    )


def visualize_embeddings(
    texts: List[str],
    embedding_function,
    method: str = "tsne",  # "tsne" ë˜ëŠ” "pca"
    dimensions: int = 2,  # 2 ë˜ëŠ” 3
    save_path: Optional[str] = None,
    interactive: bool = False,  # plotly ì‚¬ìš©
):
    """
    ì„ë² ë”©ì„ 2D/3Dë¡œ ì‹œê°í™” (í™•ì¥ëœ í•¨ìˆ˜)

    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        embedding_function: ì„ë² ë”© í•¨ìˆ˜
        method: ì°¨ì› ì¶•ì†Œ ë°©ë²• ("tsne" ë˜ëŠ” "pca")
        dimensions: ì°¨ì› ìˆ˜ (2 ë˜ëŠ” 3)
        save_path: ì €ì¥ ê²½ë¡œ
        interactive: ì¸í„°ë™í‹°ë¸Œ í”Œë¡¯ (plotly)

    Example:
        from beanllm import Embedding, visualize_embeddings

        texts = ["AI", "ML", "DL", "ê°•ì•„ì§€", "ê³ ì–‘ì´"]
        embed_func = Embedding.openai().embed_sync

        # 2D ì‹œê°í™”
        visualize_embeddings(texts, embed_func, method="tsne", dimensions=2)

        # 3D ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”
        visualize_embeddings(texts, embed_func, method="pca", dimensions=3, interactive=True)
    """
    # ì„ë² ë”© ìƒì„±
    vectors = embedding_function(texts)
    vectors_array = np.array(vectors)

    # ì°¨ì› ì¶•ì†Œ
    if method == "tsne":
        try:
            from sklearn.manifold import TSNE

            reducer = TSNE(
                n_components=dimensions,
                random_state=42,
                perplexity=min(30, len(texts) - 1),
            )
        except ImportError:
            print("âš ï¸  scikit-learn í•„ìš”:")
            print("   pip install scikit-learn")
            return
    elif method == "pca":
        try:
            from sklearn.decomposition import PCA

            reducer = PCA(n_components=dimensions, random_state=42)
        except ImportError:
            print("âš ï¸  scikit-learn í•„ìš”:")
            print("   pip install scikit-learn")
            return
    else:
        raise ValueError(f"Unknown method: {method}")

    vectors_reduced = reducer.fit_transform(vectors_array)

    # ì‹œê°í™”
    if interactive:
        # Plotly ì‚¬ìš© (3D ì§€ì›, ì¸í„°ë™í‹°ë¸Œ)
        try:
            import plotly.graph_objects as go

            if dimensions == 3:
                fig = go.Figure(
                    data=go.Scatter3d(
                        x=vectors_reduced[:, 0],
                        y=vectors_reduced[:, 1],
                        z=vectors_reduced[:, 2],
                        mode="markers+text",
                        text=texts,
                        marker=dict(size=8, color=vectors_reduced[:, 0]),
                    )
                )
            else:
                fig = go.Figure(
                    data=go.Scatter(
                        x=vectors_reduced[:, 0],
                        y=vectors_reduced[:, 1],
                        mode="markers+text",
                        text=texts,
                        marker=dict(size=12),
                    )
                )

            fig.update_layout(title=f"Embeddings ì‹œê°í™” ({method.upper()}, {dimensions}D)")
            fig.show()

            if save_path:
                fig.write_html(save_path.replace(".png", ".html"))

        except ImportError:
            # plotly ì—†ìœ¼ë©´ matplotlib ì‚¬ìš©
            interactive = False

    if not interactive:
        # Matplotlib ì‚¬ìš©
        try:
            import matplotlib.pyplot as plt

            if dimensions == 3:
                fig = plt.figure(figsize=(12, 8))
                ax = fig.add_subplot(111, projection="3d")
                ax.scatter(
                    vectors_reduced[:, 0],
                    vectors_reduced[:, 1],
                    vectors_reduced[:, 2],
                    s=200,
                    alpha=0.6,
                )
                for i, text in enumerate(texts):
                    ax.text(
                        vectors_reduced[i, 0],
                        vectors_reduced[i, 1],
                        vectors_reduced[i, 2],
                        text,
                        fontsize=10,
                    )
            else:
                plt.figure(figsize=(12, 8))
                plt.scatter(
                    vectors_reduced[:, 0],
                    vectors_reduced[:, 1],
                    s=200,
                    alpha=0.6,
                )
                for i, text in enumerate(texts):
                    plt.annotate(
                        text,
                        (vectors_reduced[i, 0], vectors_reduced[i, 1]),
                        fontsize=12,
                    )

            plt.title(f"Embeddings ì‹œê°í™” ({method.upper()}, {dimensions}D)", fontsize=16)
            if dimensions == 2:
                plt.xlabel("Dimension 1")
                plt.ylabel("Dimension 2")
            plt.grid(True, alpha=0.3)

            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches="tight")
                print(f"âœ“ ì €ì¥: {save_path}")

            plt.show()

        except ImportError:
            print("âš ï¸  matplotlib í•„ìš”:")
            print("   pip install matplotlib")
            return


def similarity_heatmap(
    texts: List[str],
    embedding_function,
    save_path: Optional[str] = None,
    cluster: bool = True,  # í´ëŸ¬ìŠ¤í„°ë§ ì ìš©
    method: str = "ward",  # í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•
):
    """
    ìœ ì‚¬ë„ íˆíŠ¸ë§µ ìƒì„± (í™•ì¥ëœ í•¨ìˆ˜)

    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        embedding_function: ì„ë² ë”© í•¨ìˆ˜
        save_path: ì €ì¥ ê²½ë¡œ (ì„ íƒ)
        cluster: í´ëŸ¬ìŠ¤í„°ë§ ì ìš© ì—¬ë¶€
        method: í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ("ward", "complete", "average")

    Example:
        from beanllm import Embedding, similarity_heatmap

        texts = ["AI", "ML", "DL", "NLP", "CV"]
        embed_func = Embedding.openai().embed_sync
        similarity_heatmap(texts, embed_func, cluster=True)
    """
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        from sklearn.metrics.pairwise import cosine_similarity
    except ImportError:
        print("âš ï¸  matplotlib, seaborn, scikit-learn í•„ìš”:")
        print("   pip install matplotlib seaborn scikit-learn")
        return

    # ì„ë² ë”© ìƒì„±
    vectors = embedding_function(texts)
    vectors_array = np.array(vectors)

    # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    similarity_matrix = cosine_similarity(vectors_array)

    # í´ëŸ¬ìŠ¤í„°ë§ ì ìš©
    if cluster:
        try:
            from scipy.cluster.hierarchy import leaves_list, linkage

            # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§
            linkage_matrix = linkage(vectors_array, method=method)

            # í´ëŸ¬ìŠ¤í„° ìˆœì„œë¡œ ì¬ì •ë ¬
            order = leaves_list(linkage_matrix)
            similarity_matrix = similarity_matrix[order][:, order]
            texts_ordered = [texts[i] for i in order]
        except ImportError:
            print("âš ï¸  scipy í•„ìš” (í´ëŸ¬ìŠ¤í„°ë§ìš©):")
            print("   pip install scipy")
            cluster = False
            texts_ordered = texts
    else:
        texts_ordered = texts

    # íˆíŠ¸ë§µ ìƒì„±
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        similarity_matrix,
        xticklabels=texts_ordered,
        yticklabels=texts_ordered,
        annot=True,
        fmt=".2f",
        cmap="RdYlGn",
        center=0.5,
        square=True,
        linewidths=0.5,
        vmin=0,
        vmax=1,
    )
    plt.title("ìœ ì‚¬ë„ íˆíŠ¸ë§µ", fontsize=16)
    plt.xticks(rotation=45, ha="right")
    plt.yticks(rotation=0)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"âœ“ ì €ì¥: {save_path}")

    plt.show()
