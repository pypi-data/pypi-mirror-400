{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aeaf81-607b-4d55-9e77-be267ac08479",
   "metadata": {},
   "source": [
    "# L-GATr-slim Quickstart\n",
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/heidelberg-hepml/lgatr/blob/main/examples/demo_lgatr_slim.ipynb)\n",
    "\n",
    "In this tutorial, we give a quick introduction for how to use L-GATr-slim. L-GATr-slim is a Lorentz-equivariant transformer for applications in high-energy physics and other domains where Lorentz symmetry is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aeaa9b-a47d-4467-825e-b1731964bd17",
   "metadata": {},
   "source": [
    "`LGATrSlim` is build on a mix of scalar and vector representations. The design is inspired by the original `LGATr` network and closely follows it, but deviates in a few critical points: \n",
    "\n",
    "The main difference is that `LGATrSlim` uses only scalar and vector representations, whereas `LGATr` uses the unifying multivector representations, that contain also bivector, axialvector, and pseudoscalar representations. However, we find that these representations are not required in most practical high-energy physics tasks, and link the axialvector to vector and pseudoscalar to scalar representations to reduce the network size, effectively breaking $O(1,3)$-equivariance to $SO^+(1,3)$-equivariance.\n",
    "\n",
    "In principle, `LGATrSlim` could be implemented to be completely equivalent to `LGATr` where the higher-order representations are set to zero. Our implementation mostly follows this approach, but deviates in a few aspects:\n",
    "- `LGATrSlim`'s `Linear` simply multiplies scalars and vectors with one common weight, whereas `LGATr`'s `EquiLinear` creates a 16x16 matrix weight for multivectors for all possible interactions. Most entries in this matrix are zero and therefore significantly increase the FLOPs, but the `opt_einsum` package allows for an efficient implementation. The two implementations yield equivalent results, but the `LGATrSlim` linear layer avoids unnecessary operations, making it more efficient.\n",
    "- `LGATrSlim`'s `GatedLinearUnit` replaces `LGATr`'s mix of `GeometricProduct`, `EquiLinear` and `ScalarGatedNonlinearity` in the MLP. First, this means that `LGATrSlim` can not express outer products. We find that this restriction does not affect the network performance in the tasks that we tested, but significantly speeds up the network. Second, the `GatedLinearUnit` uses gated nonlinearities also for the scalar channels, and constructs the vector gates from an inner product instead of using scalar channels directly.\n",
    "- `LGATrSlim`'s `RMSNorm` computes one unified norm for scalars and vectors, where `LGATr`'s `EquiLayerNorm` normalizes multivectors and scalars seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58232c6b-85e1-4e48-857b-24995368ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the lgatr package\n",
    "%pip install lgatr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94a506-afb2-47ad-9c44-dbdf7f1081f1",
   "metadata": {},
   "source": [
    "After importing the required modules, we construct a `LGATrSlim` encoder module. The user can specify the number of input, output, and hidden channels for scalars and vectors, as well as the number of heads and transformer blocks. More hyperparameters can be found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffec1e2-d9a0-4f9f-9821-a4d228f7e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lgatr import LGATrSlim\n",
    "\n",
    "lgatrslim = LGATrSlim(\n",
    "    in_v_channels=1,\n",
    "    out_v_channels=1,\n",
    "    hidden_v_channels=8,\n",
    "    in_s_channels=1,\n",
    "    out_s_channels=1,\n",
    "    hidden_s_channels=16,\n",
    "    num_blocks=2,\n",
    "    num_heads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437a87b-26a2-4cba-bbee-5c50e777cbbb",
   "metadata": {},
   "source": [
    "We now test `LGATrSlim` on toy data, e.g. a bunch of LHC events. We create particles with fixed mass and gaussian noise as momentum. The resulting four-momenta have shape `p.shape = (128, 20, 1, 4)`; for batch size 128, 20 particles per jet, 1 four-momentum per particle, and 4 numbers for the four-momentum. We also generated random particle types `pid` with `pid.shape = (128, 20, 1)`. More generally, `LGATrSlim` operates on vectors of shape `(batch_size, num_particles, num_v_channels, 4)` and scalars of shape `(batch_size, num_particles, num_s_channels)`, while normal transformers operate on `(batch_size, num_particles, num_channels)`, without the extra 'vector' dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739008ef-6a14-4f8e-a1d8-6e1e79edb4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 1, 4])\n",
      "torch.Size([128, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "# generate toy data\n",
    "import torch\n",
    "\n",
    "p3 = torch.randn(128, 20, 1, 3)\n",
    "mass = 1\n",
    "E = (mass**2 + (p3**2).sum(dim=-1, keepdim=True)) ** 0.5\n",
    "p = torch.cat((E, p3), dim=-1)\n",
    "pid = torch.randint(high=3, size=p3.shape[:-1]).float()\n",
    "print(p.shape)  # torch.Size([128, 20, 1, 4])\n",
    "print(pid.shape)  # torch.Size([128, 20, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05b73c-69ca-4adc-aec4-45f79cf0a17f",
   "metadata": {},
   "source": [
    "We can now process the vector with the `LGATrSlim` architecture! It returns new vectors and scalars, from which we can extract the component that we want -- for instance the scalar component for a jet tagging or amplitude regression application, or the vector component for flow matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e61ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 1, 4])\n",
      "torch.Size([128, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "vectors = p\n",
    "scalars = pid\n",
    "output_v, output_s = lgatrslim(vectors=vectors, scalars=scalars)\n",
    "print(output_v.shape)  # torch.Size([128, 20, 1, 4])\n",
    "print(output_s.shape)  # torch.Size([128, 20, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fe9d9-032b-4e65-afaf-48f21ed28a9b",
   "metadata": {},
   "source": [
    "The design choice of using only scalar and vector channels in `LGATrSlim` allows a particularly efficient implementation. We add a `compile` option, defaulting to `compile=False`, that triggers an internal `self.__class__ = torch.compile(self.__class__, dynamic=True)` call in the `LGATrSlim` constructor. `torch.compile` automatically selects the most efficient kernels and fuses operations. We find significant speed gains and use `compile=True` in all our production runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb7cf0f-fae4-48b7-a0ea-9d153d835663",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgatrslim = LGATrSlim(\n",
    "    in_v_channels=1,\n",
    "    out_v_channels=1,\n",
    "    hidden_v_channels=8,\n",
    "    in_s_channels=1,\n",
    "    out_s_channels=1,\n",
    "    hidden_s_channels=16,\n",
    "    num_blocks=2,\n",
    "    num_heads=1,\n",
    "    compile=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aac801-f657-494e-9018-a65f02e3cbe5",
   "metadata": {},
   "source": [
    "The price of `torch.compile` is the initial compilation that happens on the first forward pass, and typically takes between 10 seconds and 2 minutes. The code does not have to be re-compiled later on when the network is called with different shapes, thanks to the option `dynamic=True` in `torch.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b895e83a-8454-4072-a63a-ad4fabcee0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 1, 4])\n",
      "torch.Size([128, 20, 1])\n",
      "First iteration (including compilation) takes 7.4s\n",
      "Second iteration takes 0.0042s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "output_v, output_s = lgatrslim(vectors=vectors, scalars=scalars)\n",
    "dt = time.time() - t0\n",
    "print(output_v.shape)  # torch.Size([128, 20, 1, 4])\n",
    "print(output_s.shape)  # torch.Size([128, 20, 1])\n",
    "print(f\"First iteration (including compilation) takes {dt:.2}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "output_v, output_s = lgatrslim(vectors=vectors, scalars=scalars)\n",
    "dt = time.time() - t0\n",
    "print(f\"Second iteration takes {dt:.2}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53356edb-8a63-4008-91cc-781afbd74e0c",
   "metadata": {},
   "source": [
    "Thats it, now you're ready to build your own `LGATrSlim` model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0f4d8-4523-4a34-825b-ffd4a2db77bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
