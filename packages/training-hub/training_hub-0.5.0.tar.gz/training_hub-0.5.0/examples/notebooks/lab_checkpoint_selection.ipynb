{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5f4fa6",
   "metadata": {},
   "source": [
    "# Checkpoint Selection with LAB SFT\n",
    "\n",
    "This notebook demonstrates how checkpoint selection can be done when running the LAB training methodology through training hub. \n",
    "\n",
    "Checkpoint selection is an optional step at the end of an LLM training run, often used to maximize how much you get out of your training run.\n",
    "When customizing a model using the LAB methodology, the resulting checkpoints may not differ greatly on their domain-knowledge, but may vary in their performance on general domains.\n",
    "\n",
    "This notebook shows how you can use a complex benchmark such as OpenLLM Leaderboard v2 to select the most optimal checkpoint out of your training run.\n",
    "\n",
    "> Note: Although this notebook showcases the LAB technique, it is focused on checkpoint evaluation.\n",
    "> \n",
    "> For a comprehensive tutorial on the LAB multiphase training technique, see `lab_multiphase_training_tutorial.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e379d1",
   "metadata": {},
   "source": [
    "## Installing modules for evaluation\n",
    "\n",
    "\n",
    "In addition to the dependencies from training-hub, we'll need to install the required dependencies.\n",
    "\n",
    "For this notebook, we'll be using [Open LLM Leaderboard v2](https://huggingface.co/docs/leaderboards/open_llm_leaderboard/about) as our heuristic benchmark; although any other benchmark may be swapped in.\n",
    "\n",
    "We'll be using leaderboard through the `instructlab-eval` package, however you can use any other eval provider such as [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) or [OpenCompass](https://github.com/open-compass/opencompass).\n",
    "\n",
    "To get started, please install via one of the following options:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5312ce7",
   "metadata": {},
   "source": [
    "### Option 1: Install with standard pip (most vanilla environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aab6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conventional python environments (vanilla, venv, conda, etc.)\n",
    "!pip install instructlab-eval && pip install instructlab-eval[cuda] --no-build-isolation && pip install instructlab-eval[leaderboard] --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0c551",
   "metadata": {},
   "source": [
    "### Option 2: Install with UV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using UV, run with this command:\n",
    "!uv pip install instructlab-eval && uv pip install instructlab-eval[cuda] --no-build-isolation && uv pip install instructlab-eval[leaderboard] --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a789cc5",
   "metadata": {},
   "source": [
    "## Important: Run this before anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Fix for CUDA multiprocessing issue - must be run first!\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "print(\"‚úÖ Set multiprocessing start method to 'spawn' for CUDA compatibility\")\n",
    "\n",
    "# Ensure CUDA is available for vLLM\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Set CUDA device visibility if not already set\n",
    "if 'CUDA_VISIBLE_DEVICES' not in os.environ:\n",
    "    # Make all GPUs visible\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(i) for i in range(torch.cuda.device_count()))\n",
    "    print(f\"‚úÖ Set CUDA_VISIBLE_DEVICES to: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "\n",
    "# Verify CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA is available with {torch.cuda.device_count()} GPU(s)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: CUDA is not available! vLLM may fall back to CPU backend.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fddd86",
   "metadata": {},
   "source": [
    "## Configure your hardware setup\n",
    "\n",
    "You may configure your system here as-needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HARDWARE AND TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Training hyperparameters\n",
    "max_tokens_per_gpu = 25_000  # Memory limit per GPU (reduce if hitting OOM errors)\n",
    "max_seq_len = 20_000         # Maximum sequence length\n",
    "\n",
    "# LAB-specific configurations\n",
    "phase07_effective_batch_size = 128   # Smaller batch size for knowledge dataset\n",
    "phase10_effective_batch_size = 3840  # Larger batch size for skills + replay dataset\n",
    "learning_rate = 2e-5                 # Learning rate for both phases\n",
    "num_epochs = 7                       # Number of epochs per phase\n",
    "warmup_steps = 0                     # Number of warmup steps\n",
    "\n",
    "# Single-node distributed training setup\n",
    "nproc_per_node = 8   # Number of GPUs per node (adjust based on your hardware)\n",
    "nnodes = 1           # Number of nodes (single-node setup)\n",
    "node_rank = 0        # This node's rank (always 0 for single-node)\n",
    "rdzv_id = 47         # Rendezvous ID for distributed training\n",
    "rdzv_endpoint = \"127.0.0.1:12345\"  # Local endpoint for single-node\n",
    "\n",
    "# Calculate total resources\n",
    "total_gpus = nproc_per_node * nnodes\n",
    "\n",
    "print(\"üñ•Ô∏è  Hardware Configuration:\")\n",
    "print(f\"  GPUs per node: {nproc_per_node}\")\n",
    "print(f\"  Total GPUs: {total_gpus}\")\n",
    "print(f\"  Max tokens per GPU: {max_tokens_per_gpu:,}\")\n",
    "print(f\"  Max sequence length: {max_seq_len:,}\")\n",
    "print()\n",
    "print(\"üìä LAB Training Configuration:\")\n",
    "print(f\"  Phase07 batch size: {phase07_effective_batch_size} (knowledge tuning)\")\n",
    "print(f\"  Phase10 batch size: {phase10_effective_batch_size} (skills + replay)\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs per phase: {num_epochs}\")\n",
    "print()\n",
    "print(\"üí° Note: If you encounter OOM (Out of Memory) errors, reduce max_tokens_per_gpu\")\n",
    "print(\"   For fewer GPUs, adjust nproc_per_node to match your available hardware\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7168e72",
   "metadata": {},
   "source": [
    "### Setup utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports for logging management\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "# Configure logging to prevent notebook crashes from excessive output\n",
    "# while still showing essential progress and error information\n",
    "\n",
    "def setup_training_logging():\n",
    "    \"\"\"Set up logging configuration optimized for notebook environments.\"\"\"\n",
    "    # Reduce logging level for common noisy loggers\n",
    "    logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"accelerate\").setLevel(logging.WARNING)\n",
    "    \n",
    "    # Set up a custom logger that shows progress without overwhelming the notebook\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    print(\"‚úÖ Logging configured for notebook environment\")\n",
    "\n",
    "def run_training_with_managed_output(training_func, description=\"Training\"):\n",
    "    \"\"\"\n",
    "    Run training with balanced output showing progress without overwhelming the notebook.\n",
    "    Shows essential progress, errors, and key milestones while filtering excessive logs.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting {description}...\")\n",
    "    print(\"üìù Showing essential progress and key training milestones\")\n",
    "    print(\"‚è≥ This may take a while. Training progress will appear below:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run training with minimal output redirection to allow subprocess logs\n",
    "        # but reduce verbosity of the most chatty components\n",
    "        import os\n",
    "        \n",
    "        # Set environment variables to reduce some verbose output\n",
    "        old_env = {}\n",
    "        env_settings = {\n",
    "            'TRANSFORMERS_VERBOSITY': 'warning',\n",
    "            'TOKENIZERS_PARALLELISM': 'false',  # Reduces tokenizer warnings\n",
    "        }\n",
    "        \n",
    "        for key, value in env_settings.items():\n",
    "            old_env[key] = os.environ.get(key)\n",
    "            os.environ[key] = value\n",
    "        \n",
    "        try:\n",
    "            result = training_func()\n",
    "        finally:\n",
    "            # Restore environment\n",
    "            for key, old_value in old_env.items():\n",
    "                if old_value is None:\n",
    "                    os.environ.pop(key, None)\n",
    "                else:\n",
    "                    os.environ[key] = old_value\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"‚úÖ {description} completed successfully!\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {duration/3600:.2f} hours\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"‚ùå {description} failed after {duration/60:.1f} minutes\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nüí° The error occurred in the distributed training subprocess.\")\n",
    "        print(\"   Check the training logs above for more context about the failure.\")\n",
    "        print(\"   Common issues include: data path problems, memory issues, or model loading errors.\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "def find_latest_checkpoint(ckpt_output_dir: str) -> str | None:\n",
    "    \"\"\"Find the most recently created checkpoint in the hf_format directory.\n",
    "    \n",
    "    Args:\n",
    "        ckpt_output_dir: Base checkpoint output directory\n",
    "        \n",
    "    Returns:\n",
    "        str or None: Path to the latest checkpoint directory, or None if no checkpoints found\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    hf_format_dir = os.path.join(ckpt_output_dir, \"hf_format\")\n",
    "    if not os.path.exists(hf_format_dir):\n",
    "        return None\n",
    "    \n",
    "    # List all items in hf_format directory and pick the most recent\n",
    "    items = os.listdir(hf_format_dir)\n",
    "    if not items:\n",
    "        return None\n",
    "    \n",
    "    # Get full paths and find the most recently created item\n",
    "    item_paths = [os.path.join(hf_format_dir, item) for item in items]\n",
    "    latest_checkpoint = max(item_paths, key=os.path.getctime)\n",
    "    \n",
    "    return latest_checkpoint\n",
    "\n",
    "# Set up logging\n",
    "setup_training_logging()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5d187",
   "metadata": {},
   "source": [
    "## Train your model with LAB (or any other method)\n",
    "\n",
    "The following section showcases running the LAB method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_hub import sft\n",
    "import os\n",
    "\n",
    "# Model and data paths - Update these to your actual paths\n",
    "base_model_path = \"/path/to/your/model\"  # e.g., granite-3.1-8b-starter-v2.1\n",
    "phase07_knowledge_data = \"/path/to/your/knowledge/data.jsonl\"  # Knowledge data for Phase07\n",
    "phase10_skills_replay_data = \"/path/to/your/skills_plus_replay/data.jsonl\"  # Skills + replay data for Phase10\n",
    "\n",
    "# Configure your output directories\n",
    "ckpt_output_base_dir = \"/path/to/checkpoint-save-dir\"\n",
    "data_output_dir = \"/dev/shm\"  # A good default on most systems, but you can change as-needed\n",
    "\n",
    "\n",
    "# Phase-specific output directories\n",
    "phase07_ckpt_output_dir = os.path.join(ckpt_output_base_dir, \"_phase07\")\n",
    "phase10_ckpt_output_dir = os.path.join(ckpt_output_base_dir, \"_phase10\")\n",
    "\n",
    "print(f\"üìÇ Model and data configuration:\")\n",
    "print(f\"   Base model: {base_model_path}\")\n",
    "print(f\"   Phase07 knowledge data: {phase07_knowledge_data}\")\n",
    "print(f\"   Phase10 skills + replay data: {phase10_skills_replay_data}\")\n",
    "print(f\"   Phase07 output: {phase07_ckpt_output_dir}\")\n",
    "print(f\"   Phase10 output: {phase10_ckpt_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853cd64",
   "metadata": {},
   "source": [
    "## Run LAB Multiphase Training\n",
    "\n",
    "In this example, we provide a template for training a model using the LAB SFT approach, since it's a particularly useful to have checkpoint selection here.\n",
    "\n",
    "However; this technique applies to any training regime, so feel free to swap this out for your own techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f407aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Knowledge Tuning (Phase07)\n",
    "print(\"üìö Phase 1: Knowledge Tuning (Phase07)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def phase07_training():\n",
    "    \"\"\"Execute Phase07 training with all parameters.\"\"\"\n",
    "    sft(\n",
    "        # Required parameters\n",
    "        model_path=base_model_path,\n",
    "        data_path=phase07_knowledge_data,\n",
    "        ckpt_output_dir=phase07_ckpt_output_dir,\n",
    "        \n",
    "        # Core training parameters\n",
    "        num_epochs=num_epochs,\n",
    "        effective_batch_size=phase07_effective_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "        \n",
    "        # Data and checkpointing parameters\n",
    "        data_output_dir=data_output_dir,\n",
    "        warmup_steps=warmup_steps,\n",
    "        save_samples=0,\n",
    "        checkpoint_at_epoch=True,\n",
    "        accelerate_full_state_at_epoch=False,  # Save space for intermediate phase\n",
    "        \n",
    "        # Distributed training parameters\n",
    "        nproc_per_node=nproc_per_node,\n",
    "        nnodes=nnodes,\n",
    "        node_rank=node_rank,\n",
    "        rdzv_id=rdzv_id,\n",
    "        rdzv_endpoint=rdzv_endpoint,\n",
    "    )\n",
    "    \n",
    "\n",
    "# Execute Phase07 training\n",
    "try:\n",
    "    run_training_with_managed_output(phase07_training, \"Phase07 (Knowledge Tuning)\")\n",
    "    print(\"üéØ Phase07 training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"üí• Phase07 training failed: {e}\")\n",
    "    print(\"üîç Check the error details above for troubleshooting\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff85957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé§ Phase 2: Skills + Knowledge Tuning (Phase10)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def phase10_training():\n",
    "    \"\"\"Execute Phase10 training with all parameters.\"\"\"\n",
    "\n",
    "    # Next, find and load the last checkpoint from the knowledge training phase\n",
    "    latest_phase07_checkpoint = find_latest_checkpoint(phase07_ckpt_output_dir)\n",
    "    print(f\"üîç Found latest checkpoint: {latest_phase07_checkpoint}\")\n",
    "    print(f\"üèã Loading checkpoint into Phase10 training...\")\n",
    "\n",
    "    sft(\n",
    "        # Required parameters\n",
    "        model_path=latest_phase07_checkpoint,\n",
    "        data_path=phase10_skills_replay_data,\n",
    "        ckpt_output_dir=phase10_ckpt_output_dir,\n",
    "        \n",
    "        # Core training parameters\n",
    "        checkpoint_at_epoch=True,\n",
    "        num_epochs=num_epochs,\n",
    "        effective_batch_size=phase10_effective_batch_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "        learning_rate=learning_rate,\n",
    "\n",
    "        # Data and checkpointing parameters\n",
    "        data_output_dir=data_output_dir,\n",
    "        warmup_steps=warmup_steps,\n",
    "        save_samples=0,\n",
    "        accelerate_full_state_at_epoch=False,\n",
    "\n",
    "        # Distributed training parameters\n",
    "        nproc_per_node=nproc_per_node,\n",
    "        nnodes=nnodes,\n",
    "        node_rank=node_rank,\n",
    "        rdzv_id=rdzv_id,\n",
    "        rdzv_endpoint=rdzv_endpoint,\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    run_training_with_managed_output(phase10_training, \"Phase10 (Skills + Knowledge Tuning)\")\n",
    "    print(\"üéØ Phase10 training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"üí• Phase10 training failed: {e}\")\n",
    "    print(\"üîç Check the error details above for troubleshooting\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bd71f",
   "metadata": {},
   "source": [
    "## Evaluating checkpoints\n",
    "\n",
    "### Why Evaluation Matters for Custom Instruction-Tuned Models\n",
    "\n",
    "When fine-tuning instruction-following models with the LAB methodology, we mix the base model's original instruction data with our custom domain-specific data. This data mixing creates an important challenge: while we want the model to excel at our specific tasks, we also need to ensure it maintains its general-purpose capabilities as an intelligent chatbot.\n",
    "\n",
    "The training process can create subtle trade-offs:\n",
    "- Early checkpoints might retain more general capabilities but have less domain expertise\n",
    "- Later checkpoints might excel at your specific tasks but potentially degrade on general tasks\n",
    "- Different checkpoints may balance these capabilities differently\n",
    "\n",
    "This is why evaluating checkpoints across diverse, complex domains is crucial - we need to find the checkpoint that best balances domain-specific performance with general intelligence.\n",
    "\n",
    "### OpenLLM Leaderboard v2: An Ideal Evaluation Tool\n",
    "\n",
    "The OpenLLM Leaderboard v2 is particularly well-suited for checkpoint selection because:\n",
    "\n",
    "1. **Comprehensive Coverage**: It tests multiple facets of intelligence including reasoning (GPQA, MuSR), math (MATH), instruction following (IFEval), and general knowledge (MMLU-Pro)\n",
    "\n",
    "2. **Challenging Tasks**: These benchmarks are designed to be difficult, helping distinguish between checkpoints that might perform similarly on easier tasks\n",
    "\n",
    "3. **Real-World Relevance**: The tasks mirror the diverse queries a chatbot encounters in production, from technical questions to creative problem-solving\n",
    "\n",
    "4. **Standardized Metrics**: Provides consistent, comparable scores across checkpoints, making selection objective rather than subjective\n",
    "\n",
    "By evaluating all checkpoints from your training run on this benchmark, you can identify which checkpoint best preserves the model's versatility while incorporating your custom knowledge - ensuring you deploy the most capable version of your fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109309e4",
   "metadata": {},
   "source": [
    "### Running Checkpoint Selection with Leaderboard\n",
    "\n",
    "In this notebook, we leverage the `instructlab-eval` library which wraps around lm-eval-harness to provide a seamless API in Python.\n",
    "\n",
    "Although it's possible to run this same script directly through via lm-eval-harness, it comes with the following problems:\n",
    "\n",
    "1. **Complex output**: The leaderboard scores returned from `lm-eval` are complex and require post-processing to obtain a score that's easy to read.\n",
    "2. **Difficult to optimize**: Without the correct configuration, leaderboard can take an hour and a half to run on an **8xH100** machine. But obtaining the correct configuration requires an engineering effort to work properly. \n",
    "\n",
    "We solve this challenge in `instructlab-eval` by packaging it up so it's\n",
    "\n",
    "1. Easy to call from a script like this üìû\n",
    "2. Configures it for speed (üê¢ 90 minutes --> üèéÔ∏è 15 minutes) üî•\n",
    "3. Provides the scores in a simple, readable format üìú --> üìÑ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4220cc",
   "metadata": {},
   "source": [
    "### Evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33481a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluator\n",
    "print(\"üì¶ Importing LeaderboardV2Evaluator...\")\n",
    "from instructlab.eval.leaderboard import LeaderboardV2Evaluator\n",
    "print(\"‚úÖ Import complete\")\n",
    "\n",
    "# Configuration for evaluation\n",
    "eval_num_gpus = 8  # Number of GPUs to use for evaluation (adjust based on your hardware)\n",
    "eval_config = {\"batch_size\": \"auto\", \"max_batch_size\": 64}  # Optimized evaluation config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928d9ad",
   "metadata": {},
   "source": [
    "### Step 1: Checkpoint Discovery\n",
    "\n",
    "Now we'll find all Phase10 checkpoints that need to be evaluated. Phase10 represents the final training phase with skills and comprehensive replay, so these are the checkpoints we want to evaluate for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ae133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Phase10 checkpoints to evaluate\n",
    "print(\"üîç Finding Phase10 checkpoints to evaluate...\")\n",
    "\n",
    "phase10_hf_dir = os.path.join(phase10_ckpt_output_dir, \"hf_format\")\n",
    "checkpoints_to_evaluate = []\n",
    "\n",
    "if os.path.exists(phase10_hf_dir):\n",
    "    # List all checkpoint directories\n",
    "    phase10_checkpoints = [\n",
    "        os.path.join(phase10_hf_dir, d) \n",
    "        for d in os.listdir(phase10_hf_dir) \n",
    "        if os.path.isdir(os.path.join(phase10_hf_dir, d))\n",
    "    ]\n",
    "    \n",
    "    # Sort by name to ensure consistent ordering\n",
    "    phase10_checkpoints.sort()\n",
    "    \n",
    "    checkpoints_to_evaluate = phase10_checkpoints\n",
    "    print(f\"‚úÖ Found {len(checkpoints_to_evaluate)} Phase10 checkpoint(s):\")\n",
    "    for ckpt in checkpoints_to_evaluate:\n",
    "        print(f\"   - {os.path.basename(ckpt)}\")\n",
    "else:\n",
    "    print(f\"‚ùå No Phase10 checkpoints found at {phase10_hf_dir}\")\n",
    "    print(\"   Please ensure Phase10 training completed successfully.\")\n",
    "\n",
    "print(f\"\\nüìä Total checkpoints to evaluate: {len(checkpoints_to_evaluate)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8255a",
   "metadata": {},
   "source": [
    "### Step 2: Initialize Evaluation\n",
    "\n",
    "Before we start the evaluation loop, let's set up our results storage and verify we have checkpoints to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b269a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation tracking\n",
    "if not checkpoints_to_evaluate:\n",
    "    print(\"‚ùå No checkpoints found to evaluate. Exiting evaluation process.\")\n",
    "else:\n",
    "    print(\"‚úÖ Ready to evaluate checkpoints\")\n",
    "    print(f\"‚è±Ô∏è  Total estimated time: ~{len(checkpoints_to_evaluate) * 15}-{len(checkpoints_to_evaluate) * 20} minutes\")\n",
    "\n",
    "checkpoint_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87845fef",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate Each Checkpoint\n",
    "\n",
    "Now we'll evaluate each checkpoint using the OpenLLM Leaderboard v2. This will test each checkpoint across multiple dimensions including reasoning, math, instruction following, and general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93945fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fe719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "\n",
    "for i, checkpoint_path in enumerate(checkpoints_to_evaluate, 1):\n",
    "    checkpoint_name = os.path.basename(checkpoint_path)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìç Evaluating checkpoint {i}/{len(checkpoints_to_evaluate)}: {checkpoint_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Create evaluator for this checkpoint\n",
    "        print(\"  üîß Creating evaluator...\")\n",
    "        evaluator = LeaderboardV2Evaluator(\n",
    "            model_path=checkpoint_path,\n",
    "            num_gpus=eval_num_gpus,\n",
    "            eval_config=eval_config\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(\"  üöÄ Running leaderboard evaluation...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = evaluator.run()\n",
    "        \n",
    "        eval_time = time.time() - start_time\n",
    "        print(f\"  ‚úÖ Evaluation complete in {eval_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Store results\n",
    "        checkpoint_results.append({\n",
    "            \"checkpoint_path\": checkpoint_path,\n",
    "            \"checkpoint_name\": checkpoint_name,\n",
    "            \"results\": result,\n",
    "            \"overall_score\": result[\"overall_score\"],\n",
    "            \"eval_time_minutes\": eval_time/60\n",
    "        })\n",
    "        \n",
    "        # Print scores for this checkpoint\n",
    "        print(f\"\\n  üìä Scores for {checkpoint_name}:\")\n",
    "        print(f\"    Overall: {result['overall_score'] * 100:.2f}%\")\n",
    "        if \"leaderboard_ifeval\" in result:\n",
    "            print(f\"    IFEval: {result['leaderboard_ifeval']['score'] * 100:.2f}%\")\n",
    "        if \"leaderboard_mmlu_pro\" in result:\n",
    "            print(f\"    MMLU-Pro: {result['leaderboard_mmlu_pro']['score'] * 100:.2f}%\")\n",
    "        if \"leaderboard_math_hard\" in result:\n",
    "            print(f\"    MATH-Hard: {result['leaderboard_math_hard']['score'] * 100:.2f}%\")\n",
    "        if \"leaderboard_gpqa\" in result:\n",
    "            print(f\"    GPQA: {result['leaderboard_gpqa']['score'] * 100:.2f}%\")\n",
    "        if \"leaderboard_musr\" in result:\n",
    "            print(f\"    MUSR: {result['leaderboard_musr']['score'] * 100:.2f}%\")\n",
    "        if \"leaderboard_bbh\" in result:\n",
    "            print(f\"    BBH: {result['leaderboard_bbh']['score'] * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error evaluating checkpoint: {e}\")\n",
    "        checkpoint_results.append({\n",
    "            \"checkpoint_path\": checkpoint_path,\n",
    "            \"checkpoint_name\": checkpoint_name,\n",
    "            \"results\": None,\n",
    "            \"overall_score\": -1,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation loop completed. Evaluated {len(checkpoint_results)} checkpoints.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca3e30",
   "metadata": {},
   "source": [
    "### Step 4: Analyze Results\n",
    "\n",
    "Let's sort and analyze all the evaluation results to understand how each checkpoint performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by overall score\n",
    "if checkpoint_results:\n",
    "    print(\"üìä Analyzing evaluation results...\\n\")\n",
    "    \n",
    "    # Sort by overall score (descending)\n",
    "    sorted_results = sorted(\n",
    "        checkpoint_results, \n",
    "        key=lambda x: x[\"overall_score\"], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Display all results ranked\n",
    "    print(\"üèÜ CHECKPOINT RANKINGS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Rank':<6} {'Checkpoint':<30} {'Overall Score':<15} {'Status'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        score_str = f\"{result['overall_score'] * 100:.2f}%\" if result['overall_score'] >= 0 else \"ERROR\"\n",
    "        status = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else \"‚úì\"\n",
    "        print(f\"{i:<6} {result['checkpoint_name']:<30} {score_str:<15} {status}\")\n",
    "    \n",
    "    # Show evaluation time statistics\n",
    "    successful_evals = [r for r in checkpoint_results if r[\"overall_score\"] >= 0]\n",
    "    if successful_evals:\n",
    "        avg_time = sum(r.get(\"eval_time_minutes\", 0) for r in successful_evals) / len(successful_evals)\n",
    "        print(f\"\\n‚è±Ô∏è  Average evaluation time: {avg_time:.1f} minutes per checkpoint\")\n",
    "else:\n",
    "    print(\"‚ùå No results to analyze.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677db5ee",
   "metadata": {},
   "source": [
    "### Step 5: Identify Best Checkpoint\n",
    "\n",
    "Finally, let's identify and highlight the best-performing checkpoint based on the overall leaderboard score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and display the best checkpoint\n",
    "if checkpoint_results and sorted_results and sorted_results[0][\"overall_score\"] >= 0:\n",
    "    best_checkpoint = sorted_results[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ú® BEST CHECKPOINT FOUND ‚ú®\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nüìç Checkpoint: {best_checkpoint['checkpoint_name']}\")\n",
    "    print(f\"üìÇ Full Path: {best_checkpoint['checkpoint_path']}\")\n",
    "    print(f\"üèÜ Overall Score: {best_checkpoint['overall_score'] * 100:.2f}%\")\n",
    "    \n",
    "    if best_checkpoint['results']:\n",
    "        print(\"\\nüìä Detailed Performance Breakdown:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Define the order we want to display tasks\n",
    "        task_order = [\n",
    "            (\"leaderboard_ifeval\", \"IFEval (Instruction Following)\"),\n",
    "            (\"leaderboard_mmlu_pro\", \"MMLU-Pro (General Knowledge)\"),\n",
    "            (\"leaderboard_math_hard\", \"MATH-Hard (Mathematics)\"),\n",
    "            (\"leaderboard_gpqa\", \"GPQA (Graduate-level QA)\"),\n",
    "            (\"leaderboard_musr\", \"MUSR (Multi-step Reasoning)\"),\n",
    "            (\"leaderboard_bbh\", \"BBH (Big-Bench Hard)\")\n",
    "        ]\n",
    "        \n",
    "        for task_key, task_name in task_order:\n",
    "            if task_key in best_checkpoint['results'] and isinstance(best_checkpoint['results'][task_key], dict):\n",
    "                score = best_checkpoint['results'][task_key].get('score', 0)\n",
    "                print(f\"  {task_name:<35} {score * 100:>6.2f}%\")\n",
    "    \n",
    "    print(\"\\nüí° This checkpoint achieved the best balance across all evaluation tasks.\")\n",
    "    print(\"   It is recommended for deployment unless domain-specific requirements suggest otherwise.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No valid checkpoint could be identified as best.\")\n",
    "    print(\"   Please check the evaluation errors above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab6dd5",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we've demonstrated a complete LAB training and checkpoint selection workflow:\n",
    "\n",
    "1. **LAB Training**: Executed two-phase training with knowledge tuning (Phase07) followed by skills + replay training (Phase10)\n",
    "2. **Checkpoint Collection**: Gathered all checkpoints from both training phases\n",
    "3. **Comprehensive Evaluation**: Used OpenLLM Leaderboard v2 to evaluate each checkpoint across multiple intelligence dimensions\n",
    "4. **Optimal Selection**: Identified the best-performing checkpoint based on overall scores\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Trade-offs Matter**: Different checkpoints excel at different tasks - the best checkpoint balances general intelligence with your domain expertise\n",
    "- **Evaluation is Essential**: Without proper evaluation, you might deploy a suboptimal checkpoint that underperforms on important capabilities\n",
    "- **Leaderboard v2 Efficiency**: With proper configuration, evaluation takes only ~15 minutes per checkpoint instead of 90+ minutes\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy Your Best Model**: Use the identified best checkpoint for inference or further fine-tuning\n",
    "2. **Domain-Specific Testing**: Additionally test the best checkpoint on your specific use cases\n",
    "3. **Consider Task-Specific Selection**: If certain tasks are more important for your application, you might weight those scores higher in selection\n",
    "4. **Monitor in Production**: Continue evaluating model performance on real-world tasks after deployment\n",
    "\n",
    "### Alternative Evaluation Options\n",
    "\n",
    "While this notebook uses OpenLLM Leaderboard v2, you can adapt the same workflow with other evaluation frameworks:\n",
    "- **lm-evaluation-harness**: For custom task suites\n",
    "- **OpenCompass**: For comprehensive Chinese language evaluation\n",
    "- **Your Custom Benchmarks**: For domain-specific evaluation needs\n",
    "\n",
    "The key is to maintain a consistent evaluation protocol across all checkpoints to ensure fair comparison.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
