//! Batch iterator for streaming Redis data as Arrow RecordBatches.
//!
//! This module provides the core iteration logic that powers the Polars IO plugin.
//! It handles SCAN iteration, batching, and conversion to Arrow format.

use arrow::array::RecordBatch;
use redis::aio::ConnectionManager;
use tokio::runtime::Runtime;

use super::convert::hashes_to_record_batch;
use super::reader::{HashData, fetch_hashes};
use crate::connection::RedisConnection;
use crate::error::{Error, Result};
use crate::options::{
    ParallelStrategy, ScanOptions, get_default_batch_size, get_default_count_hint,
};
use crate::schema::HashSchema;

/// Configuration for batch iteration.
///
/// This struct configures how keys are scanned and batched when reading from Redis.
/// It supports environment variable defaults:
/// - `POLARS_REDIS_BATCH_SIZE`: Default batch size (default: 1000)
/// - `POLARS_REDIS_COUNT_HINT`: Default SCAN COUNT hint (default: 100)
///
/// # Example
///
/// ```ignore
/// use polars_redis::BatchConfig;
///
/// let config = BatchConfig::new("user:*")
///     .with_batch_size(500)
///     .with_count_hint(200)
///     .with_max_rows(10000);
/// ```
#[derive(Debug, Clone)]
pub struct BatchConfig {
    /// Key pattern to match (e.g., "user:*").
    pub pattern: String,
    /// Number of keys to process per batch.
    pub batch_size: usize,
    /// SCAN COUNT hint for Redis.
    pub count_hint: usize,
    /// Maximum total rows to return (None for unlimited).
    pub max_rows: Option<usize>,
    /// Parallel processing strategy.
    pub parallel: ParallelStrategy,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            pattern: "*".to_string(),
            batch_size: get_default_batch_size(),
            count_hint: get_default_count_hint(),
            max_rows: None,
            parallel: ParallelStrategy::None,
        }
    }
}

impl BatchConfig {
    /// Create a new BatchConfig with the given pattern.
    pub fn new(pattern: impl Into<String>) -> Self {
        Self {
            pattern: pattern.into(),
            ..Default::default()
        }
    }

    /// Set the batch size.
    pub fn with_batch_size(mut self, size: usize) -> Self {
        self.batch_size = size;
        self
    }

    /// Set the COUNT hint for SCAN.
    pub fn with_count_hint(mut self, count: usize) -> Self {
        self.count_hint = count;
        self
    }

    /// Set the maximum number of rows to return.
    pub fn with_max_rows(mut self, max: usize) -> Self {
        self.max_rows = Some(max);
        self
    }

    /// Set the parallel processing strategy.
    pub fn with_parallel(mut self, strategy: ParallelStrategy) -> Self {
        self.parallel = strategy;
        self
    }
}

impl From<ScanOptions> for BatchConfig {
    fn from(opts: ScanOptions) -> Self {
        Self {
            pattern: opts.pattern,
            batch_size: opts.batch_size,
            count_hint: opts.count_hint,
            max_rows: opts.n_rows,
            parallel: opts.parallel,
        }
    }
}

impl From<BatchConfig> for ScanOptions {
    fn from(config: BatchConfig) -> Self {
        Self {
            pattern: config.pattern,
            batch_size: config.batch_size,
            count_hint: config.count_hint,
            n_rows: config.max_rows,
            parallel: config.parallel,
        }
    }
}

/// Iterator for scanning Redis hashes in batches as Arrow RecordBatches.
///
/// This iterator fetches hash keys matching a pattern and retrieves their
/// field-value pairs, converting them to Arrow RecordBatches for use with Polars.
///
/// The iterator uses Redis SCAN for memory-efficient key iteration and pipelines
/// HGETALL/HMGET commands for efficient data retrieval.
///
/// # Example
///
/// ```ignore
/// use polars_redis::{HashBatchIterator, HashSchema, BatchConfig, RedisType};
///
/// let schema = HashSchema::new(vec![
///     ("name".to_string(), RedisType::Utf8),
///     ("age".to_string(), RedisType::Int64),
/// ]).with_key(true);
///
/// let config = BatchConfig::new("user:*").with_batch_size(1000);
///
/// let mut iterator = HashBatchIterator::new(url, schema, config, None)?;
///
/// while let Some(batch) = iterator.next_batch()? {
///     println!("Got {} rows", batch.num_rows());
/// }
/// ```
///
/// # Performance
///
/// - Uses `SCAN` with configurable `COUNT` hint for non-blocking iteration
/// - Pipelines multiple `HGETALL`/`HMGET` commands per batch
/// - Supports projection pushdown (only fetch requested fields)
/// - Memory-efficient streaming (processes one batch at a time)
pub struct HashBatchIterator {
    /// Tokio runtime for async operations.
    runtime: Runtime,
    /// Redis connection manager (cheap to clone, auto-reconnects).
    conn: ConnectionManager,
    /// Schema for the hash data.
    schema: HashSchema,
    /// Batch configuration.
    config: BatchConfig,
    /// Fields to fetch (None = all fields via HGETALL).
    projection: Option<Vec<String>>,
    /// Current SCAN cursor.
    cursor: u64,
    /// Whether we've started scanning (cursor has moved at least once).
    scan_started: bool,
    /// Whether we've completed the SCAN.
    done: bool,
    /// Total rows yielded so far.
    rows_yielded: usize,
    /// Buffer of keys waiting to be fetched.
    key_buffer: Vec<String>,
    /// Current row offset for row index column.
    row_offset: u64,
}

impl HashBatchIterator {
    /// Create a new HashBatchIterator.
    pub fn new(
        url: &str,
        schema: HashSchema,
        config: BatchConfig,
        projection: Option<Vec<String>>,
    ) -> Result<Self> {
        let runtime = Runtime::new()
            .map_err(|e| Error::Runtime(format!("Failed to create runtime: {}", e)))?;
        let connection = RedisConnection::new(url)?;
        let conn = runtime.block_on(connection.get_connection_manager())?;

        Ok(Self {
            runtime,
            conn,
            schema,
            config,
            projection,
            cursor: 0,
            scan_started: false,
            done: false,
            rows_yielded: 0,
            key_buffer: Vec::new(),
            row_offset: 0,
        })
    }

    /// Get the next batch of data as a RecordBatch.
    ///
    /// Returns None when iteration is complete.
    pub fn next_batch(&mut self) -> Result<Option<RecordBatch>> {
        if self.done {
            return Ok(None);
        }

        // Check if we've hit the max rows limit
        if let Some(max) = self.config.max_rows
            && self.rows_yielded >= max
        {
            self.done = true;
            return Ok(None);
        }

        // Accumulate keys until we have enough for a batch
        while self.key_buffer.len() < self.config.batch_size && !self.scan_complete() {
            self.scan_more_keys()?;
        }

        if self.key_buffer.is_empty() {
            self.done = true;
            return Ok(None);
        }

        // Take up to batch_size keys
        let keys_to_fetch: Vec<String> = self
            .key_buffer
            .drain(..self.key_buffer.len().min(self.config.batch_size))
            .collect();

        // Fetch hash data
        let hash_data = self.fetch_batch(&keys_to_fetch)?;

        if hash_data.is_empty() {
            // All keys were deleted between SCAN and fetch, try again
            return self.next_batch();
        }

        // Apply projection to schema if needed
        let effective_schema = match &self.projection {
            Some(cols) => self.schema.project(cols),
            None => self.schema.clone(),
        };

        // Convert to RecordBatch with current row offset
        let mut batch = hashes_to_record_batch(&hash_data, &effective_schema, self.row_offset)?;

        // Update row offset for next batch
        self.row_offset += batch.num_rows() as u64;

        // Apply max rows limit
        if let Some(max) = self.config.max_rows {
            let remaining = max - self.rows_yielded;
            if batch.num_rows() > remaining {
                batch = batch.slice(0, remaining);
                self.done = true;
            }
        }

        self.rows_yielded += batch.num_rows();

        // Check if we're done
        if self.key_buffer.is_empty() && self.scan_complete() {
            self.done = true;
        }

        Ok(Some(batch))
    }

    /// Check if the SCAN is complete.
    fn scan_complete(&self) -> bool {
        // cursor == 0 after at least one iteration means we've completed the full scan
        (self.cursor == 0 && self.scan_started) || self.done
    }

    /// Scan more keys from Redis.
    fn scan_more_keys(&mut self) -> Result<()> {
        let mut conn = self.conn.clone();
        let (new_cursor, keys) = self.runtime.block_on(scan_keys_batch(
            &mut conn,
            self.cursor,
            &self.config.pattern,
            self.config.count_hint,
        ))?;

        self.key_buffer.extend(keys);
        self.cursor = new_cursor;
        self.scan_started = true;

        Ok(())
    }

    /// Fetch hash data for a batch of keys.
    fn fetch_batch(&mut self, keys: &[String]) -> Result<Vec<HashData>> {
        let include_ttl = self.schema.include_ttl();
        let worker_count = self.config.parallel.worker_count();

        // Use parallel fetching if configured
        if worker_count > 1 && keys.len() > worker_count {
            self.fetch_batch_parallel(keys, include_ttl, worker_count)
        } else {
            let projection = self.projection.as_deref();
            let mut conn = self.conn.clone();
            self.runtime
                .block_on(fetch_hashes(&mut conn, keys, projection, include_ttl))
        }
    }

    /// Fetch hash data in parallel using multiple workers.
    fn fetch_batch_parallel(
        &mut self,
        keys: &[String],
        include_ttl: bool,
        worker_count: usize,
    ) -> Result<Vec<HashData>> {
        // Split keys into chunks for parallel processing
        let chunk_size = keys.len().div_ceil(worker_count);
        let chunks: Vec<Vec<String>> = keys.chunks(chunk_size).map(|c| c.to_vec()).collect();

        // Clone what we need before the async block
        let conn = self.conn.clone();
        let projection_owned: Option<Vec<String>> = self.projection.clone();

        self.runtime.block_on(async {
            let mut handles = Vec::with_capacity(chunks.len());

            for chunk in chunks {
                let mut conn = conn.clone();
                let proj = projection_owned.clone();

                let handle = tokio::spawn(async move {
                    fetch_hashes(&mut conn, &chunk, proj.as_deref(), include_ttl).await
                });
                handles.push(handle);
            }

            // Collect results in order
            let mut all_data = Vec::with_capacity(keys.len());
            for handle in handles {
                match handle.await {
                    Ok(Ok(data)) => all_data.extend(data),
                    Ok(Err(e)) => return Err(e),
                    Err(e) => {
                        return Err(Error::Runtime(format!("Task join error: {}", e)));
                    }
                }
            }

            Ok(all_data)
        })
    }

    /// Check if iteration is complete.
    pub fn is_done(&self) -> bool {
        self.done
    }

    /// Get the number of rows yielded so far.
    pub fn rows_yielded(&self) -> usize {
        self.rows_yielded
    }
}

/// Scan a batch of keys from Redis.
async fn scan_keys_batch(
    conn: &mut ConnectionManager,
    cursor: u64,
    pattern: &str,
    count: usize,
) -> Result<(u64, Vec<String>)> {
    let result: (u64, Vec<String>) = redis::cmd("SCAN")
        .arg(cursor)
        .arg("MATCH")
        .arg(pattern)
        .arg("COUNT")
        .arg(count)
        .query_async(conn)
        .await?;

    Ok(result)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::schema::{HashSchema, RedisType};

    #[test]
    fn test_batch_config_default() {
        let config = BatchConfig::default();
        assert_eq!(config.pattern, "*");
        assert_eq!(config.batch_size, get_default_batch_size());
        assert_eq!(config.count_hint, get_default_count_hint());
        assert!(config.max_rows.is_none());
    }

    #[test]
    fn test_batch_config_from_scan_options() {
        use crate::options::ScanOptions;

        let scan_opts = ScanOptions::new("user:*")
            .with_batch_size(500)
            .with_count_hint(50)
            .with_n_rows(1000);

        let config: BatchConfig = scan_opts.into();
        assert_eq!(config.pattern, "user:*");
        assert_eq!(config.batch_size, 500);
        assert_eq!(config.count_hint, 50);
        assert_eq!(config.max_rows, Some(1000));
    }

    #[test]
    fn test_scan_options_from_batch_config() {
        use crate::options::ScanOptions;

        let config = BatchConfig::new("session:*")
            .with_batch_size(250)
            .with_count_hint(25)
            .with_max_rows(500);

        let scan_opts: ScanOptions = config.into();
        assert_eq!(scan_opts.pattern, "session:*");
        assert_eq!(scan_opts.batch_size, 250);
        assert_eq!(scan_opts.count_hint, 25);
        assert_eq!(scan_opts.n_rows, Some(500));
    }

    #[test]
    fn test_batch_config_builder() {
        let config = BatchConfig::new("user:*")
            .with_batch_size(500)
            .with_count_hint(200)
            .with_max_rows(10000);

        assert_eq!(config.pattern, "user:*");
        assert_eq!(config.batch_size, 500);
        assert_eq!(config.count_hint, 200);
        assert_eq!(config.max_rows, Some(10000));
    }

    #[test]
    #[ignore] // Requires running Redis instance
    fn test_hash_batch_iterator_creation() {
        let schema = HashSchema::new(vec![
            ("name".to_string(), RedisType::Utf8),
            ("age".to_string(), RedisType::Int64),
        ]);
        let config = BatchConfig::new("test:*");

        let result = HashBatchIterator::new("redis://localhost:6379", schema, config, None);
        assert!(result.is_ok());
    }
}
