#!/usr/bin/env python3
"""SAGE Chat CLI - Embedded programming assistant backed by SageVDB."""

from __future__ import annotations

import json
import os
import re
import shutil
import tempfile
import textwrap
import urllib.request
import zipfile
from collections.abc import Sequence
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import typer
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

# å»¶è¿Ÿå¯¼å…¥ sage_db ä»¥å…è®¸ CLI åœ¨æ²¡æœ‰ C++ æ‰©å±•çš„æƒ…å†µä¸‹å¯åŠ¨
# from sage.middleware.components.sage_db.python.sage_db import SageDB, SageDBException
from sage.cli.commands.apps import pipeline as pipeline_builder
from sage.cli.commands.apps.pipeline_domain import load_domain_contexts
from sage.cli.commands.apps.pipeline_knowledge import get_default_knowledge_base
from sage.common.components.sage_embedding import get_embedding_model
from sage.common.components.sage_embedding.embedding_model import EmbeddingModel
from sage.common.config.output_paths import find_sage_project_root
from sage.common.config.ports import SagePorts

# Import document processing utilities from sage-common (L1)
from sage.common.utils.document_processing import (
    iter_markdown_files,
    parse_markdown_sections,
    slugify,
)

console = Console()

# sage_db éœ€è¦ C++ æ‰©å±•ï¼Œä½¿ç”¨å»¶è¿Ÿå¯¼å…¥
SAGE_DB_AVAILABLE = False
SAGE_DB_IMPORT_ERROR: Exception | None = None
SageDB = None  # type: ignore
SageDBException = Exception  # type: ignore


def _lazy_import_sage_db():
    """å»¶è¿Ÿå¯¼å…¥ sage_dbï¼Œåªåœ¨éœ€è¦æ—¶å¯¼å…¥"""
    global SageDB, SageDBException, SAGE_DB_AVAILABLE, SAGE_DB_IMPORT_ERROR

    if SAGE_DB_AVAILABLE:
        return  # å·²ç»æˆåŠŸå¯¼å…¥

    try:
        from sage.middleware.components.sage_db.python.sage_db import SageDB as _SageDB
        from sage.middleware.components.sage_db.python.sage_db import (
            SageDBException as _SageDBException,
        )

        SageDB = _SageDB
        SageDBException = _SageDBException
        SAGE_DB_AVAILABLE = True
        SAGE_DB_IMPORT_ERROR = None
    except (ImportError, ModuleNotFoundError) as e:
        SAGE_DB_AVAILABLE = False
        SAGE_DB_IMPORT_ERROR = e


DEFAULT_INDEX_NAME = "docs-public"
DEFAULT_CHUNK_SIZE = 800
DEFAULT_CHUNK_OVERLAP = 160
DEFAULT_TOP_K = 4
DEFAULT_BACKEND = "auto"  # è‡ªåŠ¨æ£€æµ‹æœ¬åœ° LLM æœåŠ¡ï¼Œè‹¥æ— åˆ™å›é€€ mock
# é»˜è®¤ä½¿ç”¨æœ¬åœ° embedding serverï¼ˆä¸ sage-gateway ç»Ÿä¸€ï¼‰
DEFAULT_EMBEDDING_METHOD = "openai"  # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è¿æ¥æœ¬åœ° embedding server
DEFAULT_EMBEDDING_MODEL = "BAAI/bge-m3"  # é»˜è®¤ embedding æ¨¡å‹
DEFAULT_FIXED_DIM = 384  # ä»…ç”¨äº hash æ–¹æ³•çš„å›é€€
DEFAULT_FINETUNE_MODEL = "sage_code_expert"
DEFAULT_FINETUNE_PORT = SagePorts.GATEWAY_DEFAULT

# Note: SUPPORTED_MARKDOWN_SUFFIXES now imported from sage.common.utils.document_processing

METHODS_REQUIRE_MODEL = {
    "hf",
    "openai",
    "jina",
    "cohere",
    "zhipu",
    "bedrock",
    "ollama",
    "siliconcloud",
    "nvidia_openai",
    "lollms",
}

GITHUB_DOCS_ZIP_URL = "https://github.com/intellistream/SAGE-Pub/archive/refs/heads/main.zip"

app = typer.Typer(
    help="ğŸ§­ åµŒå…¥å¼ SAGE ç¼–ç¨‹åŠ©æ‰‹ (Docs + SageDB + LLM)",
    invoke_without_command=True,
)


@dataclass
class ChatManifest:
    """Metadata describing a built knowledge index."""

    index_name: str
    db_path: Path
    created_at: str
    source_dir: str
    embedding: dict[str, object]
    chunk_size: int
    chunk_overlap: int
    num_documents: int
    num_chunks: int

    @property
    def embed_config(self) -> dict[str, object]:
        return self.embedding


def ensure_sage_db() -> None:
    """ç¡®ä¿ SageDB æ‰©å±•å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™æå‰é€€å‡ºã€‚"""
    # å°è¯•å»¶è¿Ÿå¯¼å…¥
    _lazy_import_sage_db()

    if SAGE_DB_AVAILABLE:
        return

    message = (
        "[red]SageDB C++ æ‰©å±•ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨ `sage chat`ã€‚[/red]\n"
        "è¯·å…ˆé€šè¿‡å‘½ä»¤ `sage extensions install sage_db` æ„å»º SageDB ç»„ä»¶ï¼ˆå¦‚éœ€é‡æ–°å®‰è£…å¯åŠ ä¸Š --forceï¼‰ã€‚"
    )
    if SAGE_DB_IMPORT_ERROR:
        message += f"\nåŸå§‹é”™è¯¯: {SAGE_DB_IMPORT_ERROR}"
    console.print(message)
    raise typer.Exit(code=1)


def resolve_index_root(index_root: str | None) -> Path:
    """è§£æç´¢å¼•å­˜å‚¨æ ¹ç›®å½•ã€‚

    ç”¨æˆ·æ•°æ®ç¼“å­˜ï¼ˆèŠå¤©ç´¢å¼•ï¼‰å§‹ç»ˆä½¿ç”¨ç”¨æˆ·ç›®å½• ~/.sage/cache/chat/ï¼Œ
    ç¡®ä¿ sage-chat å’Œ sage-gateway ä½¿ç”¨åŒä¸€ä»½ç´¢å¼•ã€‚
    """
    if index_root:
        root = Path(index_root).expanduser().resolve()
        root.mkdir(parents=True, exist_ok=True)
        return root
    # å§‹ç»ˆä½¿ç”¨ç”¨æˆ·ç›®å½•ï¼Œç¡®ä¿ä¸ sage-gateway å…±äº«åŒä¸€ä»½ç´¢å¼•
    cache_dir = Path.home() / ".sage" / "cache" / "chat"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def default_source_dir() -> Path:
    project_root = find_sage_project_root()
    if not project_root:
        project_root = Path.cwd()
    candidate = project_root / "docs-public" / "docs_src"
    return candidate


def manifest_path(index_root: Path, index_name: str) -> Path:
    # ä½¿ç”¨ä¸‹åˆ’çº¿æ ¼å¼ï¼Œä¸ sage-gateway ä¿æŒä¸€è‡´
    return index_root / f"{index_name}_manifest.json"


def db_file_path(index_root: Path, index_name: str) -> Path:
    return index_root / f"{index_name}.sagedb"


def load_manifest(index_root: Path, index_name: str) -> ChatManifest:
    path = manifest_path(index_root, index_name)
    if not path.exists():
        # å…¼å®¹æ—§æ ¼å¼ï¼šå°è¯•è¯»å– .manifest.json
        old_path = index_root / f"{index_name}.manifest.json"
        if old_path.exists():
            path = old_path
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç´¢å¼• manifest: {path}. è¯·å…ˆè¿è¡Œ `sage chat ingest`.")
    payload = json.loads(path.read_text(encoding="utf-8"))
    manifest = ChatManifest(
        index_name=index_name,
        db_path=Path(payload["db_path"]),
        created_at=payload["created_at"],
        source_dir=payload["source_dir"],
        embedding=payload["embedding"],
        chunk_size=payload["chunk_size"],
        chunk_overlap=payload["chunk_overlap"],
        num_documents=payload.get("num_documents", 0),
        num_chunks=payload.get("num_chunks", 0),
    )
    return manifest


def save_manifest(
    index_root: Path,
    index_name: str,
    manifest: ChatManifest,
) -> None:
    path = manifest_path(index_root, index_name)
    payload = {
        "index_name": manifest.index_name,
        "db_path": str(manifest.db_path),
        "created_at": manifest.created_at,
        "source_dir": manifest.source_dir,
        "embedding": manifest.embedding,
        "chunk_size": manifest.chunk_size,
        "chunk_overlap": manifest.chunk_overlap,
        "num_documents": manifest.num_documents,
        "num_chunks": manifest.num_chunks,
    }
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def build_embedder(config: dict[str, object]) -> Any:
    """æ„å»º embedder å®ä¾‹ï¼ˆä½¿ç”¨æ–°çš„ç»Ÿä¸€æ¥å£ï¼‰

    Args:
        config: embedding é…ç½®
            - method: æ–¹æ³•å (hash, hf, openai, mockembedder, ...)
            - params: æ–¹æ³•ç‰¹å®šå‚æ•°

    Returns:
        BaseEmbedding å®ä¾‹

    Examples:
        >>> config = {"method": "hash", "params": {"dim": 384}}
        >>> emb = build_embedder(config)
        >>> vec = emb.embed("test")
    """
    method = str(config.get("method", DEFAULT_EMBEDDING_METHOD))
    params_raw = config.get("params", {})
    params = dict(params_raw) if isinstance(params_raw, dict) else {}  # type: ignore[arg-type]

    # ç»Ÿä¸€ä½¿ç”¨æ–°æ¥å£ï¼Œä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼
    return get_embedding_model(method, **params)


# Note: Document processing functions now imported from sage.common.utils.document_processing
# - iter_markdown_files
# - parse_markdown_sections
# - chunk_text
# - slugify
# - truncate_text
# - sanitize_metadata_value


def ensure_docs_corpus(index_root: Path) -> Path:
    """Ensure we have a docs-public/docs_src directory available."""

    local_source = default_source_dir()
    if local_source.exists():
        return local_source

    cache_root = index_root / "remote_docs"
    docs_path = cache_root / "docs_src"
    if docs_path.exists():
        return docs_path

    cache_root.mkdir(parents=True, exist_ok=True)
    console.print(
        "ğŸŒ æœªæ£€æµ‹åˆ°æœ¬åœ° docs-public/docs_srcï¼Œæ­£åœ¨ä¸‹è½½å®˜æ–¹æ–‡æ¡£åŒ…...",
        style="cyan",
    )

    fd, tmp_path = tempfile.mkstemp(prefix="sage_docs_", suffix=".zip")
    os.close(fd)
    tmp_file = Path(tmp_path)
    try:
        urllib.request.urlretrieve(GITHUB_DOCS_ZIP_URL, tmp_file)
        with zipfile.ZipFile(tmp_file, "r") as zf:
            zf.extractall(cache_root)
    except Exception as exc:
        if tmp_file.exists():
            tmp_file.unlink()
        raise RuntimeError(f"ä¸‹è½½ docs-public æ–‡æ¡£å¤±è´¥: {exc}") from exc

    if tmp_file.exists():
        tmp_file.unlink()

    extracted_docs: Path | None = None
    for candidate in cache_root.glob("**/docs_src"):
        if candidate.is_dir():
            extracted_docs = candidate
            break

    if extracted_docs is None:
        raise RuntimeError("ä¸‹è½½çš„æ–‡æ¡£åŒ…ä¸­æœªæ‰¾åˆ° docs_src ç›®å½•")

    if docs_path.exists() and docs_path == extracted_docs:
        return docs_path

    if not docs_path.exists():
        docs_path.mkdir(parents=True, exist_ok=True)
        for item in extracted_docs.iterdir():
            shutil.move(str(item), docs_path / item.name)
    return docs_path


def bootstrap_default_index(index_root: Path, index_name: str) -> ChatManifest | None:
    try:
        source_dir = ensure_docs_corpus(index_root)
    except Exception as exc:
        console.print(f"[red]æ— æ³•å‡†å¤‡æ–‡æ¡£è¯­æ–™: {exc}[/red]")
        return None

    # æ£€æµ‹æœ¬åœ° embedding server æ˜¯å¦å¯ç”¨
    from sage.common.config.ports import SagePorts

    embedding_port = SagePorts.EMBEDDING_DEFAULT
    embedding_available = False

    try:
        import requests

        # ä½¿ç”¨ /v1/models ç«¯ç‚¹æ£€æµ‹ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰
        response = requests.get(f"http://localhost:{embedding_port}/v1/models", timeout=2)
        embedding_available = response.status_code == 200
    except Exception:
        pass

    if embedding_available:
        console.print(f"[green]âœ… æ£€æµ‹åˆ°æœ¬åœ° Embedding æœåŠ¡ (ç«¯å£ {embedding_port})[/green]")
        embedding_config: dict[str, object] = {
            "method": "openai",
            "params": {
                "model": DEFAULT_EMBEDDING_MODEL,
                "base_url": f"http://localhost:{embedding_port}/v1",
                "api_key": "local",  # æœ¬åœ°æœåŠ¡ä¸éœ€è¦çœŸå® key  # pragma: allowlist secret
            },
        }
    else:
        console.print(
            f"[yellow]âš ï¸  æœªæ£€æµ‹åˆ°æœ¬åœ° Embedding æœåŠ¡ (ç«¯å£ {embedding_port})ï¼Œä½¿ç”¨ hash æ–¹æ³•[/yellow]\n"
            "[dim]æç¤º: è¿è¡Œ `sage llm serve` å¯åŠ¨ Embedding æœåŠ¡ä»¥è·å¾—æ›´å¥½çš„æ£€ç´¢æ•ˆæœ[/dim]"
        )
        embedding_config = {
            "method": "hash",
            "params": {"dim": DEFAULT_FIXED_DIM},
        }

    console.print(
        f"ğŸš€ æ­£åœ¨å¯¼å…¥ [cyan]{source_dir}[/cyan] ä»¥åˆå§‹åŒ– `{index_name}` ç´¢å¼•...",
        style="green",
    )
    manifest = ingest_source(
        source_dir=source_dir,
        index_root=index_root,
        index_name=index_name,
        chunk_size=DEFAULT_CHUNK_SIZE,
        chunk_overlap=DEFAULT_CHUNK_OVERLAP,
        embedding_config=embedding_config,
        max_files=None,
    )
    return manifest


def load_or_bootstrap_manifest(index_root: Path, index_name: str) -> ChatManifest:
    try:
        return load_manifest(index_root, index_name)
    except FileNotFoundError:
        console.print(
            "ğŸ” æ£€æµ‹åˆ°å°šæœªä¸º `sage chat` åˆå§‹åŒ–ç´¢å¼•ã€‚",
            style="yellow",
        )
        if not typer.confirm("æ˜¯å¦ç«‹å³å¯¼å…¥ docs-public æ–‡æ¡£ï¼Ÿ", default=True):
            console.print(
                "ğŸ’¡ å¯ä½¿ç”¨ `sage chat ingest` æ‰‹åŠ¨å¯¼å…¥åå†é‡è¯•ã€‚",
                style="cyan",
            )
            raise typer.Exit(code=1)

        manifest = bootstrap_default_index(index_root, index_name)
        if manifest is None:
            raise typer.Exit(code=1)
        return manifest


def _create_markdown_processor(
    source_dir: Path, max_files: int | None = None, show_progress: bool = True
):
    """Create a custom document processor for Markdown files.

    This processor handles SAGE-specific Markdown processing with:
    - Section splitting by headings
    - Metadata extraction (doc_path, title, heading, anchor)
    - Text preview generation

    Note: Progress display is handled by IndexBuilder's Rich progress bar.
    This processor shows live progress during document processing.
    """

    def process_markdown(src: Path) -> list[dict[str, Any]]:
        chunks = []
        total_docs = 0
        skipped_docs = []  # Track skipped documents

        # Count total files first for progress display
        all_files = list(iter_markdown_files(src))
        total_files = len(all_files) if max_files is None else min(len(all_files), max_files)

        if show_progress:
            from rich.live import Live
            from rich.text import Text

            # Use Rich Live for real-time progress updates
            with Live(
                Text(f"ğŸ“„ å¤„ç†æ–‡æ¡£ 0/{total_files}, å·²ç”Ÿæˆ 0 ä¸ªç‰‡æ®µ", style="cyan"),
                refresh_per_second=10,
                transient=True,
            ) as live:
                for idx, file_path in enumerate(all_files, start=1):
                    if max_files is not None and idx > max_files:
                        break

                    rel_path = file_path.relative_to(src)
                    text = file_path.read_text(encoding="utf-8", errors="ignore")
                    sections = parse_markdown_sections(text)

                    if not sections:
                        skipped_docs.append(
                            (rel_path, "æ— æ³•è§£æå‡ºæœ‰æ•ˆç« èŠ‚ï¼ˆå¯èƒ½ä¸ºç©ºæˆ–æ ¼å¼ä¸æ”¯æŒï¼‰")
                        )
                        continue

                    doc_title = sections[0]["heading"] if sections else file_path.stem

                    for section in sections:
                        chunks.append(
                            {
                                "content": section["content"],
                                "metadata": {
                                    "doc_path": str(rel_path),
                                    "title": doc_title,
                                    "heading": section["heading"],
                                    "anchor": slugify(section["heading"]),
                                },
                            }
                        )

                    total_docs += 1
                    # Update live progress after each document
                    live.update(
                        Text(
                            f"ğŸ“„ å¤„ç†æ–‡æ¡£ {total_docs}/{total_files}, å·²ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ",
                            style="cyan",
                        )
                    )

            # Print final summary
            console.print(
                f"[green]âœ“ æ–‡æ¡£å¤„ç†å®Œæˆ: {total_docs}/{total_files} ä¸ªæ–‡æ¡£, {len(chunks)} ä¸ªç‰‡æ®µ[/green]"
            )

            # Report skipped documents
            if skipped_docs:
                console.print(f"[yellow]âš  è·³è¿‡ {len(skipped_docs)} ä¸ªæ–‡æ¡£:[/yellow]")
                for doc_path, reason in skipped_docs:
                    console.print(f"[dim]  - {doc_path}: {reason}[/dim]")
        else:
            # Quiet mode - no progress display
            for idx, file_path in enumerate(all_files, start=1):
                if max_files is not None and idx > max_files:
                    break

                rel_path = file_path.relative_to(src)
                text = file_path.read_text(encoding="utf-8", errors="ignore")
                sections = parse_markdown_sections(text)

                if not sections:
                    skipped_docs.append((rel_path, "æ— æ³•è§£æå‡ºæœ‰æ•ˆç« èŠ‚"))
                    continue

                doc_title = sections[0]["heading"] if sections else file_path.stem

                for section in sections:
                    chunks.append(
                        {
                            "content": section["content"],
                            "metadata": {
                                "doc_path": str(rel_path),
                                "title": doc_title,
                                "heading": section["heading"],
                                "anchor": slugify(section["heading"]),
                            },
                        }
                    )

                total_docs += 1

        return chunks

    return process_markdown


def ingest_source(
    source_dir: Path,
    index_root: Path,
    index_name: str,
    chunk_size: int,
    chunk_overlap: int,
    embedding_config: dict[str, object],
    max_files: int | None = None,
    show_progress: bool = True,
) -> ChatManifest:
    """Build RAG index from source documents using IndexBuilder.

    This function now uses the unified IndexBuilder from sage-middleware,
    allowing code sharing with sage-gateway and other components.
    """
    ensure_sage_db()

    if not source_dir.exists():
        raise FileNotFoundError(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {source_dir}")

    # Build embedder
    embedder = build_embedder(embedding_config)

    # Prepare database path
    db_path = db_file_path(index_root, index_name)
    if db_path.exists():
        db_path.unlink()

    # Import IndexBuilder and SageDBBackend
    try:
        from sage.middleware.components.sage_db.backend import SageDBBackend
        from sage.middleware.operators.rag.index_builder import IndexBuilder
    except ImportError as e:
        raise RuntimeError(
            f"Failed to import IndexBuilder or SageDBBackend: {e}\n"
            "Ensure sage-middleware is installed with --dev option"
        ) from e

    # Create backend factory for SageDB
    def backend_factory(persist_path: Path, dim: int):
        return SageDBBackend(persist_path, dim)

    # Create document processor for Markdown
    document_processor = _create_markdown_processor(source_dir, max_files, show_progress)

    # Build index using IndexBuilder
    if show_progress:
        console.print("ğŸ”¨ Building index using IndexBuilder...", style="cyan")
    builder = IndexBuilder(backend_factory=backend_factory)

    index_manifest = builder.build_from_docs(
        source_dir=source_dir,
        persist_path=db_path,
        embedding_model=embedder,
        index_name=index_name,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        document_processor=document_processor,
        show_progress=show_progress,
    )

    # Convert IndexManifest to ChatManifest for compatibility
    manifest = ChatManifest(
        index_name=index_name,
        db_path=db_path,
        created_at=index_manifest.created_at,
        source_dir=str(source_dir),
        embedding=embedding_config,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        num_documents=index_manifest.num_documents,
        num_chunks=index_manifest.num_chunks,
    )

    save_manifest(index_root, index_name, manifest)
    if show_progress:
        console.print(Panel.fit(f"âœ… ç´¢å¼•å·²æ›´æ–° -> {db_path}", title="INGEST", style="green"))
        console.print(
            f"ğŸ“Š Documents: {manifest.num_documents}, Chunks: {manifest.num_chunks}", style="green"
        )

    return manifest


def open_database(manifest: ChatManifest) -> Any:
    ensure_sage_db()
    if not manifest.db_path.exists():
        prefix = manifest.db_path
        siblings = list(prefix.parent.glob(prefix.name + "*"))
        if not siblings:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶ {manifest.db_path}ã€‚è¯·é‡æ–°è¿è¡Œ `sage chat ingest`."
            )
    embedder = build_embedder(manifest.embed_config)
    if not SageDB:
        raise RuntimeError("SageDB not available - sage-middleware not installed")
    db = SageDB(embedder.get_dim())
    db.load(str(manifest.db_path))
    return db


def build_prompt(question: str, contexts: Sequence[str]) -> list[dict[str, str]]:
    """æ„å»ºå¯¹è¯ promptã€‚

    é’ˆå¯¹å°æ¨¡å‹ä¼˜åŒ–ï¼š
    1. ç®€çŸ­æ¸…æ™°çš„æŒ‡ä»¤
    2. æ˜ç¡®åŒºåˆ†é—²èŠå’ŒæŠ€æœ¯é—®é¢˜
    3. åªåœ¨çœŸæ­£ç›¸å…³æ—¶ä½¿ç”¨ä¸Šä¸‹æ–‡
    """
    # æ£€æµ‹æ˜¯å¦æ˜¯ç®€å•é—²èŠï¼ˆä¸éœ€è¦æ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
    casual_patterns = [
        "ä½ å¥½",
        "hi",
        "hello",
        "å—¨",
        "hey",
        "è°¢è°¢",
        "thanks",
        "thank you",
        "å†è§",
        "bye",
        "æ‹œæ‹œ",
        "test",
        "æµ‹è¯•",
        "è¯•è¯•",
        "ok",
        "å¥½çš„",
        "å—¯",
    ]
    question_lower = question.strip().lower()
    is_casual = (
        any(
            question_lower == p
            or question_lower.startswith(p + " ")
            or question_lower.endswith(" " + p)
            for p in casual_patterns
        )
        and len(question.strip()) < 20
    )

    if is_casual:
        # ç®€å•é—²èŠï¼šä¸ä½¿ç”¨æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œé¿å…å¹²æ‰°
        system_instructions = (
            "ä½ æ˜¯ SAGE æ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·åœ¨å’Œä½ æ‰“æ‹›å‘¼æˆ–é—²èŠï¼Œè¯·è‡ªç„¶å‹å¥½åœ°å›åº”ï¼Œä¸è¦è¾“å‡ºä»£ç ã€‚"
        )
        return [
            {"role": "system", "content": system_instructions},
            {"role": "user", "content": question.strip()},
        ]

    # æŠ€æœ¯é—®é¢˜ï¼šä½¿ç”¨æ£€ç´¢ä¸Šä¸‹æ–‡
    context_block = "\n\n".join(
        f"[{idx}] {textwrap.dedent(ctx).strip()}"
        for idx, ctx in enumerate(contexts, start=1)
        if ctx
    )
    system_instructions = textwrap.dedent(
        """
        ä½ æ˜¯ SAGE æ™ºèƒ½åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œæä¾›çš„æ–‡æ¡£ä¸Šä¸‹æ–‡å›ç­”ã€‚

        è§„åˆ™ï¼š
        - ä¾æ®ä¸Šä¸‹æ–‡å›ç­”ï¼Œå¼•ç”¨æ—¶ç”¨ [ç¼–å·] æ ‡æ³¨
        - å¯ä»¥ç»™å‡ºç¤ºä¾‹ä»£ç 
        - å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œå¦è¯šè¯´æ˜
        """
    ).strip()

    if context_block:
        system_instructions += f"\n\nå‚è€ƒæ–‡æ¡£:\n{context_block}"

    return [
        {"role": "system", "content": system_instructions},
        {"role": "user", "content": question.strip()},
    ]


class ResponseGenerator:
    def __init__(
        self,
        backend: str,
        model: str,
        base_url: str | None,
        api_key: str | None,
        temperature: float = 0.2,
        finetune_model: str | None = None,
        finetune_port: int = DEFAULT_FINETUNE_PORT,
    ) -> None:
        self.backend = backend.lower()
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.temperature = temperature
        self.finetune_model = finetune_model
        self.finetune_port = finetune_port
        self._llm_server = None  # ç”¨äºè¿½è¸ª sageLLM æœåŠ¡

        if self.backend == "mock":
            self.client = None
        elif self.backend == "auto":
            # è‡ªåŠ¨æ£€æµ‹æœ¬åœ° LLM æœåŠ¡
            self._setup_auto_backend()
        elif self.backend == "finetune":
            # ä½¿ç”¨å¾®è°ƒæ¨¡å‹
            self._setup_finetune_backend()
        else:
            try:
                from sage.llm import UnifiedInferenceClient

                if base_url and api_key:
                    # Explicit configuration â€” use factory and inject API key via env
                    old_key = os.environ.get("SAGE_UNIFIED_API_KEY")
                    try:
                        os.environ["SAGE_UNIFIED_API_KEY"] = api_key
                        self.client = UnifiedInferenceClient.create(
                            control_plane_url=base_url,
                            default_llm_model=model,
                        )
                    finally:
                        if old_key is None:
                            os.environ.pop("SAGE_UNIFIED_API_KEY", None)
                        else:
                            os.environ["SAGE_UNIFIED_API_KEY"] = old_key
                elif base_url:
                    # Only base_url provided
                    self.client = UnifiedInferenceClient.create(
                        control_plane_url=base_url,
                        default_llm_model=model,
                    )
                else:
                    # Auto-detection mode
                    self.client = UnifiedInferenceClient.create()
            except Exception as exc:  # pragma: no cover - runtime check
                raise RuntimeError(f"æ— æ³•åˆå§‹åŒ– UnifiedInferenceClient: {exc}") from exc

    def _setup_auto_backend(self) -> None:
        """è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®æœ€ä½³å¯ç”¨çš„ LLM åç«¯ã€‚

        ä¼˜å…ˆçº§: æœ¬åœ° LLM æœåŠ¡ â†’ äº‘ç«¯ API â†’ mock å›é€€
        """
        import os

        import requests

        from sage.common.config.ports import SagePorts

        # 1. å°è¯•æœ¬åœ° LLM æœåŠ¡
        local_ports = [SagePorts.BENCHMARK_LLM, SagePorts.LLM_DEFAULT]
        for port in local_ports:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=2)
                if response.status_code == 200:
                    # è·å–æœ¬åœ°æœåŠ¡çš„å®é™…æ¨¡å‹å
                    models_response = requests.get(f"http://localhost:{port}/v1/models", timeout=2)
                    local_model = self.model
                    if models_response.status_code == 200:
                        models_data = models_response.json()
                        if models_data.get("data"):
                            local_model = models_data["data"][0].get("id", self.model)

                    console.print(
                        f"[green]âœ… æ£€æµ‹åˆ°æœ¬åœ° LLM æœåŠ¡ (ç«¯å£ {port}, æ¨¡å‹: {local_model})[/green]"
                    )
                    from sage.llm import UnifiedInferenceClient

                    self.client = UnifiedInferenceClient.create(
                        control_plane_url=f"http://localhost:{port}/v1",
                        default_llm_model=local_model,
                    )
                    self.model = local_model
                    self.backend = "local"
                    return
            except Exception:  # noqa: S110
                pass

        # 2. å°è¯•äº‘ç«¯ APIï¼ˆæ£€æŸ¥ç¯å¢ƒå˜é‡ï¼‰
        api_key = os.getenv("SAGE_CHAT_API_KEY") or os.getenv("OPENAI_API_KEY")
        if api_key:
            try:
                from sage.llm import UnifiedInferenceClient

                # ä½¿ç”¨ create ä¼šè‡ªåŠ¨æ£€æµ‹é…ç½®
                self.client = UnifiedInferenceClient.create()
                status = self.client.get_status()
                if status.get("llm_available"):
                    console.print("[green]âœ… ä½¿ç”¨äº‘ç«¯ API æœåŠ¡[/green]")
                    self.backend = "api"
                    return
            except Exception as e:
                console.print(f"[yellow]âš ï¸  äº‘ç«¯ API åˆå§‹åŒ–å¤±è´¥: {e}[/yellow]")

        # 3. å›é€€åˆ° mock æ¨¡å¼
        console.print(
            "[yellow]âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨çš„ LLM æœåŠ¡ï¼Œä½¿ç”¨ mock æ¨¡å¼[/yellow]\n"
            "[dim]æç¤º: å¯åŠ¨æœ¬åœ°æœåŠ¡ `sage llm serve` æˆ–é…ç½® SAGE_CHAT_API_KEY/OPENAI_API_KEY ç¯å¢ƒå˜é‡[/dim]"
        )
        self.client = None
        self.backend = "mock"

    def _setup_finetune_backend(self) -> None:
        """è®¾ç½®å¾®è°ƒæ¨¡å‹ backendï¼ˆé€šè¿‡ sageLLM LLMAPIServerï¼‰"""
        from pathlib import Path

        import requests

        model_name = self.finetune_model or DEFAULT_FINETUNE_MODEL
        port = self.finetune_port

        # ä½¿ç”¨ SAGE é…ç½®ç›®å½•
        sage_config_dir = Path.home() / ".sage"
        finetune_output_dir = sage_config_dir / "finetune_output"

        # æ£€æŸ¥å¾®è°ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨
        finetune_dir = finetune_output_dir / model_name
        merged_path = finetune_dir / "merged_model"
        checkpoint_path = finetune_dir / "checkpoints"

        if not finetune_dir.exists():
            raise FileNotFoundError(
                f"å¾®è°ƒæ¨¡å‹ä¸å­˜åœ¨: {model_name}\n"
                f"è·¯å¾„: {finetune_dir}\n"
                f"è¯·å…ˆè¿è¡Œå¾®è°ƒæˆ–æŒ‡å®šå…¶ä»–æ¨¡å‹:\n"
                f"  sage finetune quickstart {model_name}"
            )

        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²è¿è¡Œ
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=1)
            service_running = response.status_code == 200
        except Exception:  # noqa: S110
            service_running = False

        if service_running:
            console.print(f"[green]âœ… LLM æœåŠ¡å·²åœ¨ç«¯å£ {port} è¿è¡Œ[/green]")
            model_to_use = self.model if self.model != "qwen-max" else None
        else:
            console.print("[yellow]â³ æ­£åœ¨å¯åŠ¨å¾®è°ƒæ¨¡å‹æœåŠ¡ï¼ˆé€šè¿‡ sageLLMï¼‰...[/yellow]")

            # æ£€æŸ¥æ˜¯å¦æœ‰åˆå¹¶æ¨¡å‹
            if not merged_path.exists():
                console.print("[yellow]âš ï¸  æœªæ‰¾åˆ°åˆå¹¶æ¨¡å‹ï¼Œæ­£åœ¨è‡ªåŠ¨åˆå¹¶ LoRA æƒé‡...[/yellow]")

                # æ£€æŸ¥æ˜¯å¦æœ‰ checkpoint
                if not checkpoint_path.exists():
                    raise FileNotFoundError(
                        f"æœªæ‰¾åˆ°æ¨¡å‹æˆ– checkpoint: {model_name}\n"
                        f"è¯·ç¡®ä¿å·²å®Œæˆè®­ç»ƒæˆ–è¿è¡Œ: sage finetune merge {model_name}"
                    )

                # å°è¯•è‡ªåŠ¨åˆå¹¶
                try:
                    console.print("[cyan]æ­£åœ¨åˆå¹¶ LoRA æƒé‡...[/cyan]")
                    from sage.libs.finetune.service import merge_lora_weights

                    # è¯»å– meta è·å–åŸºç¡€æ¨¡å‹
                    meta_file = finetune_dir / "finetune_meta.json"
                    if meta_file.exists():
                        import json

                        with open(meta_file) as f:
                            meta = json.load(f)
                        base_model = meta.get("model", "")
                    else:
                        raise RuntimeError("æœªæ‰¾åˆ° meta ä¿¡æ¯æ–‡ä»¶")

                    # æ‰¾åˆ°æœ€æ–°çš„ checkpoint
                    checkpoints = sorted(checkpoint_path.glob("checkpoint-*"))
                    if not checkpoints:
                        raise RuntimeError("æœªæ‰¾åˆ° checkpoint")

                    latest_checkpoint = checkpoints[-1]
                    merge_lora_weights(latest_checkpoint, base_model, merged_path)
                    console.print("[green]âœ… æƒé‡åˆå¹¶å®Œæˆ[/green]")
                except Exception as merge_exc:
                    raise RuntimeError(
                        f"è‡ªåŠ¨åˆå¹¶å¤±è´¥: {merge_exc}\nè¯·æ‰‹åŠ¨è¿è¡Œ: sage finetune merge {model_name}"
                    ) from merge_exc

            # ä½¿ç”¨ sageLLM LLMAPIServer å¯åŠ¨æœåŠ¡
            try:
                from sage.llm import LLMAPIServer, LLMServerConfig

                config = LLMServerConfig(
                    model=str(merged_path),
                    backend="vllm",
                    host="0.0.0.0",
                    port=port,
                    gpu_memory_utilization=0.9,
                )

                self._llm_server = LLMAPIServer(config)
                success = self._llm_server.start(background=True)

                if not success:
                    raise RuntimeError("LLM æœåŠ¡å¯åŠ¨å¤±è´¥")

                console.print("[green]âœ… LLM æœåŠ¡å¯åŠ¨æˆåŠŸï¼ˆsageLLMï¼‰[/green]\n")

            except ImportError:
                raise RuntimeError("sageLLM ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿å·²å®‰è£… sage-common")
            except Exception as exc:
                raise RuntimeError(f"å¯åŠ¨ LLM æœåŠ¡å¤±è´¥: {exc}") from exc

            # è¯»å–å®é™…çš„æ¨¡å‹åç§°
            meta_file = finetune_dir / "finetune_meta.json"
            if meta_file.exists():
                import json

                with open(meta_file) as f:
                    meta = json.load(f)
                model_to_use = meta.get("model", str(merged_path))
            else:
                model_to_use = str(merged_path)

        # è®¾ç½® LLM å®¢æˆ·ç«¯è¿æ¥åˆ°æœ¬åœ°æœåŠ¡
        try:
            from sage.llm import UnifiedInferenceClient

            # Connect to local finetune LLM via factory
            self.client = UnifiedInferenceClient.create(
                control_plane_url=f"http://localhost:{port}/v1",
                default_llm_model=model_to_use or str(merged_path),
            )
            self.model = model_to_use or str(merged_path)
            console.print(f"[green]âœ… å·²è¿æ¥åˆ°å¾®è°ƒæ¨¡å‹: {model_name}[/green]\n")
        except Exception as exc:
            if hasattr(self, "_llm_server") and self._llm_server:
                self._llm_server.stop()
            raise RuntimeError(f"æ— æ³•è¿æ¥åˆ° LLM æœåŠ¡: {exc}") from exc

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æºï¼ˆå¦‚æœå¯åŠ¨äº† LLM æœåŠ¡ï¼‰"""
        if hasattr(self, "_llm_server") and self._llm_server:
            try:
                console.print("\n[yellow]â³ æ­£åœ¨å…³é—­ LLM æœåŠ¡...[/yellow]")
                self._llm_server.stop()
                console.print("[green]âœ… LLM æœåŠ¡å·²å…³é—­[/green]")
            except Exception:  # noqa: S110
                pass
            self._llm_server = None

    def answer(
        self,
        question: str,
        contexts: Sequence[str],
        references: Sequence[dict[str, str]],
        stream: bool = False,
    ) -> str:
        if self.backend == "mock":
            return self._mock_answer(question, contexts, references)

        if not self.client:
            raise RuntimeError("Client not initialized")
        messages = build_prompt(question, contexts)
        try:
            response = self.client.chat(
                messages,
                max_tokens=768,
                temperature=self.temperature,
                stream=stream,
            )
            if isinstance(response, str):
                return response
            # Non-streaming ensures string; streaming not yet supported.
            return str(response)
        except Exception as exc:
            raise RuntimeError(f"è°ƒç”¨è¯­è¨€æ¨¡å‹å¤±è´¥: {exc}") from exc

    @staticmethod
    def _mock_answer(
        question: str,
        contexts: Sequence[str],
        references: Sequence[dict[str, str]],
    ) -> str:
        if not contexts:
            return (
                "æš‚æ—¶æ²¡æœ‰ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°ç­”æ¡ˆã€‚è¯·å°è¯•æ”¹å†™æé—®ï¼Œæˆ–è¿è¡Œ `sage chat ingest` æ›´æ–°ç´¢å¼•ã€‚"
            )
        top_ref = references[0] if references else {"title": "èµ„æ–™", "heading": ""}
        snippet = contexts[0].strip().replace("\n", " ")
        citation = top_ref.get("label", top_ref.get("title", "Docs"))
        return (
            f"æ ¹æ® {citation} çš„è¯´æ˜ï¼š{snippet[:280]}...\n\n"
            "[Mock æ¨¡å¼] å¯åŠ¨æœ¬åœ°æœåŠ¡ `sage llm serve` æˆ–é…ç½® SAGE_CHAT_API_KEY ä»¥è·å¾—å®Œæ•´å›ç­”ã€‚"
        )


PIPELINE_TRIGGER_VERBS = (
    "æ„å»º",
    "ç”Ÿæˆ",
    "æ­å»º",
    "åˆ›å»º",
    "è®¾è®¡",
    "build",
    "create",
    "design",
    "orchestrate",
)

PIPELINE_TRIGGER_TERMS = (
    "pipeline",
    "å·¥ä½œæµ",
    "æµç¨‹",
    "å›¾è°±",
    "å¤§æ¨¡å‹åº”ç”¨",
    "åº”ç”¨",
    "app",
    "application",
    "workflow",
    "agent",
    "agents",
)

# å¸¸è§åœºæ™¯æ¨¡æ¿
COMMON_SCENARIOS = {
    "qa": {
        "name": "é—®ç­”åŠ©æ‰‹",
        "goal": "æ„å»ºåŸºäºæ–‡æ¡£çš„é—®ç­”ç³»ç»Ÿ",
        "data_sources": ["æ–‡æ¡£çŸ¥è¯†åº“"],
        "latency_budget": "å®æ—¶å“åº”ä¼˜å…ˆ",
        "constraints": "",
    },
    "rag": {
        "name": "RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ",
        "goal": "ç»“åˆå‘é‡æ£€ç´¢å’Œå¤§æ¨¡å‹ç”Ÿæˆçš„æ™ºèƒ½é—®ç­”",
        "data_sources": ["å‘é‡æ•°æ®åº“", "æ–‡æ¡£åº“"],
        "latency_budget": "å®æ—¶å“åº”ä¼˜å…ˆ",
        "constraints": "",
    },
    "chat": {
        "name": "å¯¹è¯æœºå™¨äºº",
        "goal": "æ”¯æŒå¤šè½®å¯¹è¯çš„æ™ºèƒ½åŠ©æ‰‹",
        "data_sources": ["ç”¨æˆ·è¾“å…¥", "å†å²å¯¹è¯"],
        "latency_budget": "å®æ—¶å“åº”ä¼˜å…ˆ",
        "constraints": "æ”¯æŒæµå¼è¾“å‡º",
    },
    "batch": {
        "name": "æ‰¹é‡å¤„ç†",
        "goal": "æ‰¹é‡å¤„ç†æ–‡æ¡£æˆ–æ•°æ®",
        "data_sources": ["æ–‡ä»¶ç³»ç»Ÿ", "æ•°æ®åº“"],
        "latency_budget": "æ‰¹å¤„ç†å¯æ¥å—",
        "constraints": "",
    },
    "agent": {
        "name": "æ™ºèƒ½ä»£ç†",
        "goal": "å…·æœ‰å·¥å…·è°ƒç”¨èƒ½åŠ›çš„AIä»£ç†",
        "data_sources": ["å·¥å…·API", "çŸ¥è¯†åº“"],
        "latency_budget": "å®æ—¶å“åº”ä¼˜å…ˆ",
        "constraints": "æ”¯æŒå‡½æ•°è°ƒç”¨",
    },
}


def _get_scenario_template(scenario_key: str) -> dict[str, str] | None:
    """è·å–åœºæ™¯æ¨¡æ¿"""
    return COMMON_SCENARIOS.get(scenario_key.lower())


def _show_scenario_templates() -> None:
    """æ˜¾ç¤ºå¯ç”¨çš„åœºæ™¯æ¨¡æ¿"""
    console.print("\n[bold cyan]ğŸ“š å¯ç”¨åœºæ™¯æ¨¡æ¿ï¼š[/bold cyan]\n")
    for key, template in COMMON_SCENARIOS.items():
        console.print(f"  [yellow]{key:10}[/yellow] - {template['name']}: {template['goal']}")
    console.print()


def _looks_like_pipeline_request(message: str) -> bool:
    text = message.strip()
    if not text:
        return False

    lowered = text.lower()
    if lowered.startswith("pipeline:") or lowered.startswith("workflow:"):
        return True

    has_verb = any(token in text for token in PIPELINE_TRIGGER_VERBS)
    has_term = any(token in text for token in PIPELINE_TRIGGER_TERMS)
    if has_verb and has_term:
        return True

    if "llm" in lowered and "build" in lowered and ("app" in lowered or "application" in lowered):
        return True

    return False


def _default_pipeline_name(seed: str) -> str:
    headline = seed.strip().splitlines()[0] if seed.strip() else "LLM åº”ç”¨"
    headline = re.sub(r"[ã€‚ï¼ï¼Ÿ.!?]", " ", headline)
    tokens = [tok for tok in re.split(r"\s+", headline) if tok]
    if not tokens:
        return "LLM åº”ç”¨"
    if len(tokens) == 1:
        word = tokens[0]
        return f"{word} åº”ç”¨" if len(word) <= 10 else word[:10]
    trimmed = " ".join(tokens[:4])
    return trimmed[:32] if len(trimmed) > 32 else trimmed


def _normalize_list_field(raw_value: str) -> list[str]:
    if not raw_value.strip():
        return []
    return [item.strip() for item in re.split(r"[,ï¼Œ/ï¼›;]", raw_value) if item.strip()]


def _validate_pipeline_config(plan: dict[str, Any]) -> tuple[bool, list[str]]:
    """
    éªŒè¯ç”Ÿæˆçš„ Pipeline é…ç½®æ˜¯å¦åˆæ³•

    Returns:
        (is_valid, error_messages)
    """
    errors = []

    # æ£€æŸ¥å¿…éœ€çš„é¡¶å±‚å­—æ®µ
    if "pipeline" not in plan:
        errors.append("ç¼ºå°‘ 'pipeline' å­—æ®µ")
    else:
        pipeline = plan["pipeline"]
        if not isinstance(pipeline, dict):
            errors.append("'pipeline' å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
        else:
            for required in ["name", "type"]:
                if required not in pipeline:
                    errors.append(f"'pipeline' ç¼ºå°‘å¿…éœ€å­—æ®µ '{required}'")

    # æ£€æŸ¥ source
    if "source" not in plan:
        errors.append("ç¼ºå°‘ 'source' å­—æ®µ")
    elif not isinstance(plan["source"], dict):
        errors.append("'source' å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
    elif "class" not in plan["source"]:
        errors.append("'source' ç¼ºå°‘ 'class' å­—æ®µ")

    # æ£€æŸ¥ sink
    if "sink" not in plan:
        errors.append("ç¼ºå°‘ 'sink' å­—æ®µ")
    elif not isinstance(plan["sink"], dict):
        errors.append("'sink' å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
    elif "class" not in plan["sink"]:
        errors.append("'sink' ç¼ºå°‘ 'class' å­—æ®µ")

    # æ£€æŸ¥ stagesï¼ˆå¯é€‰ä½†å¦‚æœå­˜åœ¨éœ€è¦æ˜¯åˆ—è¡¨ï¼‰
    if "stages" in plan:
        stages = plan["stages"]
        if not isinstance(stages, list):
            errors.append("'stages' å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")
        else:
            for idx, stage in enumerate(stages):
                if not isinstance(stage, dict):
                    errors.append(f"stages[{idx}] å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
                else:
                    for required in ["id", "kind", "class"]:
                        if required not in stage:
                            errors.append(f"stages[{idx}] ç¼ºå°‘å¿…éœ€å­—æ®µ '{required}'")

    return (len(errors) == 0, errors)


def _check_class_imports(plan: dict[str, Any]) -> list[str]:
    """
    æ£€æŸ¥é…ç½®ä¸­çš„ç±»æ˜¯å¦å¯ä»¥å¯¼å…¥

    Returns:
        List of import warnings
    """
    warnings = []
    classes_to_check = []

    # æ”¶é›†æ‰€æœ‰ç±»å
    if "source" in plan and isinstance(plan["source"], dict):
        if "class" in plan["source"]:
            classes_to_check.append(("source", plan["source"]["class"]))

    if "sink" in plan and isinstance(plan["sink"], dict):
        if "class" in plan["sink"]:
            classes_to_check.append(("sink", plan["sink"]["class"]))

    if "stages" in plan and isinstance(plan["stages"], list):
        for idx, stage in enumerate(plan["stages"]):
            if isinstance(stage, dict) and "class" in stage:
                classes_to_check.append((f"stages[{idx}]", stage["class"]))

    # å°è¯•å¯¼å…¥æ¯ä¸ªç±»
    for location, class_path in classes_to_check:
        if not class_path or not isinstance(class_path, str):
            continue

        try:
            # åˆ†å‰²æ¨¡å—è·¯å¾„å’Œç±»å
            parts = class_path.rsplit(".", 1)
            if len(parts) != 2:
                warnings.append(f"{location}: ç±»è·¯å¾„æ ¼å¼ä¸æ­£ç¡® '{class_path}'")
                continue

            module_path, class_name = parts

            # å°è¯•å¯¼å…¥ï¼ˆä½†ä¸å®é™…æ‰§è¡Œï¼Œåªæ˜¯æ£€æŸ¥è¯­æ³•ï¼‰
            # æ³¨æ„ï¼šè¿™é‡ŒåªåšåŸºæœ¬æ£€æŸ¥ï¼Œä¸æ‰§è¡ŒçœŸå®å¯¼å…¥ä»¥é¿å…å‰¯ä½œç”¨
            if not module_path or not class_name:
                warnings.append(f"{location}: ç±»è·¯å¾„ '{class_path}' æ— æ•ˆ")

        except Exception as exc:
            warnings.append(f"{location}: ç±» '{class_path}' å¯èƒ½æ— æ³•å¯¼å…¥ - {exc}")

    return warnings


class PipelineChatCoordinator:
    def __init__(
        self,
        backend: str,
        model: str,
        base_url: str | None,
        api_key: str | None,
    ) -> None:
        self.backend = backend
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.knowledge_top_k = 5
        self.show_knowledge = False
        self._domain_contexts: tuple[str, ...] | None = None
        self._knowledge_base: Any | None = None

    def detect(self, message: str) -> bool:
        return _looks_like_pipeline_request(message)

    def _ensure_api_key(self) -> bool:
        if self.backend == "mock":
            return True
        if self.api_key:
            return True

        console.print("[yellow]å½“å‰ä½¿ç”¨çœŸå®æ¨¡å‹åç«¯ï¼Œéœ€è¦æä¾› API Key æ‰èƒ½ç”Ÿæˆ pipelineã€‚[/yellow]")
        provided = typer.prompt("è¯·è¾“å…¥ LLM API Key (ç•™ç©ºå–æ¶ˆ)", default="")
        if not provided.strip():
            console.print("å·²å–æ¶ˆ pipeline æ„å»ºæµç¨‹ã€‚", style="yellow")
            return False

        self.api_key = provided.strip()
        return True

    def _ensure_contexts(self) -> tuple[str, ...]:
        if self._domain_contexts is None:
            try:
                self._domain_contexts = tuple(load_domain_contexts(limit=4))
            except Exception as exc:  # pragma: no cover - defensive
                console.print(f"[yellow]åŠ è½½é»˜è®¤ä¸Šä¸‹æ–‡å¤±è´¥: {exc}[/yellow]")
                self._domain_contexts = ()
        return self._domain_contexts

    def _ensure_knowledge_base(self) -> Any | None:
        if self._knowledge_base is None:
            try:
                self._knowledge_base = get_default_knowledge_base()
            except Exception as exc:  # pragma: no cover - defensive
                console.print(f"[yellow]åˆå§‹åŒ– pipeline çŸ¥è¯†åº“å¤±è´¥ï¼Œå°†è·³è¿‡æ£€ç´¢: {exc}[/yellow]")
                self._knowledge_base = None
        return self._knowledge_base

    def _collect_requirements(self, initial_request: str) -> dict[str, Any]:
        console.print("\n[bold cyan]ğŸ“‹ éœ€æ±‚æ”¶é›†[/bold cyan]", style="bold")
        console.print("è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ä»¥ç”Ÿæˆæœ€é€‚åˆçš„ Pipeline é…ç½®ï¼š\n", style="dim")

        # è¯¢é—®æ˜¯å¦ä½¿ç”¨æ¨¡æ¿
        use_template = typer.confirm("æ˜¯å¦ä½¿ç”¨é¢„è®¾åœºæ™¯æ¨¡æ¿ï¼Ÿ", default=False)
        template_data = None

        if use_template:
            _show_scenario_templates()
            template_key = (
                typer.prompt("é€‰æ‹©åœºæ™¯æ¨¡æ¿ (è¾“å…¥å…³é”®å­—ï¼Œå¦‚ 'qa', 'rag', 'chat' ç­‰)", default="qa")
                .strip()
                .lower()
            )
            template_data = _get_scenario_template(template_key)
            if template_data:
                console.print(f"\nâœ… å·²åŠ è½½ [green]{template_data['name']}[/green] æ¨¡æ¿\n")
            else:
                console.print(
                    f"\nâš ï¸  æœªæ‰¾åˆ°æ¨¡æ¿ '{template_key}'ï¼Œå°†ä½¿ç”¨è‡ªå®šä¹‰é…ç½®\n",
                    style="yellow",
                )

        default_name = _default_pipeline_name(initial_request)

        # æç¤ºç”¨æˆ·å¯ä»¥ç®€åŒ–è¾“å…¥
        console.print("ğŸ’¡ [dim]æç¤ºï¼šç›´æ¥å›è½¦å°†ä½¿ç”¨é»˜è®¤å€¼ï¼ˆåŸºäºæ‚¨çš„æè¿°è‡ªåŠ¨æ¨æ–­ï¼‰[/dim]\n")

        # å¦‚æœæœ‰æ¨¡æ¿ï¼Œä½¿ç”¨æ¨¡æ¿çš„é»˜è®¤å€¼
        if template_data:
            name = typer.prompt("ğŸ“› Pipeline åç§°", default=template_data["name"])
            goal = typer.prompt("ğŸ¯ Pipeline ç›®æ ‡æè¿°", default=template_data["goal"])
            default_sources = ", ".join(template_data["data_sources"])
            default_latency = template_data["latency_budget"]
            default_constraints = template_data["constraints"]
        else:
            name = typer.prompt("ğŸ“› Pipeline åç§°", default=default_name)
            goal = typer.prompt(
                "ğŸ¯ Pipeline ç›®æ ‡æè¿°", default=initial_request.strip() or default_name
            )
            default_sources = "æ–‡æ¡£çŸ¥è¯†åº“"
            default_latency = "å®æ—¶å“åº”ä¼˜å…ˆ"
            default_constraints = ""

        # æä¾›æ›´è¯¦ç»†çš„è¯´æ˜
        console.print("\n[dim]æ•°æ®æ¥æºç¤ºä¾‹ï¼šæ–‡æ¡£çŸ¥è¯†åº“ã€ç”¨æˆ·è¾“å…¥ã€æ•°æ®åº“ã€API ç­‰[/dim]")
        data_sources = typer.prompt("ğŸ“¦ ä¸»è¦æ•°æ®æ¥æº (é€—å·åˆ†éš”)", default=default_sources)

        console.print("\n[dim]å»¶è¿Ÿéœ€æ±‚ç¤ºä¾‹ï¼šå®æ—¶å“åº”ä¼˜å…ˆã€æ‰¹å¤„ç†å¯æ¥å—ã€é«˜ååé‡ä¼˜å…ˆ[/dim]")
        latency = typer.prompt("âš¡ å»¶è¿Ÿ/ååéœ€æ±‚", default=default_latency)

        console.print("\n[dim]çº¦æŸæ¡ä»¶ç¤ºä¾‹ï¼šä»…ä½¿ç”¨æœ¬åœ°æ¨¡å‹ã€å†…å­˜é™åˆ¶ 4GBã€å¿…é¡»æ”¯æŒæµå¼è¾“å‡º[/dim]")
        constraints = typer.prompt("âš™ï¸  ç‰¹æ®Šçº¦æŸ (å¯ç•™ç©º)", default=default_constraints)

        requirements: dict[str, Any] = {
            "name": name.strip() or default_name,
            "goal": goal.strip() or initial_request.strip() or default_name,
            "data_sources": _normalize_list_field(data_sources),
            "latency_budget": latency.strip(),
            "constraints": constraints.strip(),
            "initial_prompt": initial_request.strip(),
        }

        # æ˜¾ç¤ºæ”¶é›†åˆ°çš„éœ€æ±‚æ‘˜è¦
        console.print("\n[bold green]âœ… éœ€æ±‚æ”¶é›†å®Œæˆ[/bold green]")
        summary_table = Table(show_header=False, box=None, padding=(0, 2))
        summary_table.add_row("åç§°:", f"[cyan]{requirements['name']}[/cyan]")
        summary_table.add_row("ç›®æ ‡:", f"[yellow]{requirements['goal']}[/yellow]")
        summary_table.add_row(
            "æ•°æ®æº:", f"[magenta]{', '.join(requirements['data_sources'])}[/magenta]"
        )
        if requirements["latency_budget"]:
            summary_table.add_row("å»¶è¿Ÿéœ€æ±‚:", requirements["latency_budget"])
        if requirements["constraints"]:
            summary_table.add_row("çº¦æŸæ¡ä»¶:", requirements["constraints"])
        console.print(summary_table)
        console.print()

        return requirements

    def _build_config(self) -> pipeline_builder.BuilderConfig:
        domain_contexts = self._ensure_contexts() or ()
        knowledge_base = self._ensure_knowledge_base()
        return pipeline_builder.BuilderConfig(
            backend=self.backend,
            model=self.model,
            base_url=self.base_url,
            api_key=self.api_key,
            domain_contexts=domain_contexts,
            knowledge_base=knowledge_base,
            knowledge_top_k=self.knowledge_top_k,
            show_knowledge=self.show_knowledge,
        )

    def _generate_plan(self, requirements: dict[str, Any]) -> dict[str, Any] | None:
        console.print("\n[bold magenta]ğŸ¤– æ­£åœ¨ç”Ÿæˆ Pipeline é…ç½®...[/bold magenta]\n")

        config = self._build_config()
        generator = pipeline_builder.PipelinePlanGenerator(config)

        plan: dict[str, Any] | None = None
        feedback: str | None = None

        for round_num in range(1, 7):  # æœ€å¤š 6 è½®
            console.print(f"[dim]>>> ç¬¬ {round_num} è½®ç”Ÿæˆ...[/dim]")

            try:
                plan = generator.generate(requirements, plan, feedback)
                console.print("[green]âœ“[/green] ç”ŸæˆæˆåŠŸ\n")
            except pipeline_builder.PipelineBuilderError as exc:
                console.print(f"[red]âœ— ç”Ÿæˆå¤±è´¥: {exc}[/red]\n")

                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†å»ºè®®
                if "API" in str(exc) or "key" in str(exc).lower():
                    console.print("[yellow]ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®é…ç½®[/yellow]")
                elif "timeout" in str(exc).lower():
                    console.print("[yellow]ğŸ’¡ å»ºè®®ï¼šç½‘ç»œå¯èƒ½ä¸ç¨³å®šï¼Œå¯ä»¥é‡è¯•[/yellow]")
                elif "JSON" in str(exc) or "parse" in str(exc).lower():
                    console.print("[yellow]ğŸ’¡ å»ºè®®ï¼šæ¨¡å‹è¾“å‡ºæ ¼å¼å¼‚å¸¸ï¼Œå°è¯•ç®€åŒ–éœ€æ±‚æè¿°[/yellow]")

                if not typer.confirm("\næ˜¯å¦å°è¯•é‡æ–°ç”Ÿæˆï¼Ÿ", default=True):
                    return None

                feedback = typer.prompt(
                    "è¯·æä¾›æ›´å¤šéœ€æ±‚æˆ–ä¿®æ”¹å»ºè®®ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨åŸéœ€æ±‚é‡è¯•ï¼‰", default=""
                )
                if not feedback or not feedback.strip():
                    feedback = None
                continue

            # æ˜¾ç¤ºç”Ÿæˆçš„é…ç½®
            console.print("[bold cyan]ğŸ“„ ç”Ÿæˆçš„ Pipeline é…ç½®ï¼š[/bold cyan]")
            pipeline_builder.render_pipeline_plan(plan)
            pipeline_builder.preview_pipeline_plan(plan)

            # éªŒè¯é…ç½®
            console.print("\n[dim]>>> æ­£åœ¨éªŒè¯é…ç½®...[/dim]")
            is_valid, errors = _validate_pipeline_config(plan)

            if not is_valid:
                console.print("[red]âš ï¸  é…ç½®éªŒè¯å‘ç°é—®é¢˜ï¼š[/red]")
                for error in errors:
                    console.print(f"  â€¢ [red]{error}[/red]")
                console.print("\n[yellow]å»ºè®®ï¼šåœ¨åé¦ˆä¸­è¯´æ˜è¿™äº›é—®é¢˜ï¼Œè®©æ¨¡å‹é‡æ–°ç”Ÿæˆ[/yellow]")

                if not typer.confirm("æ˜¯å¦ç»§ç»­ä½¿ç”¨æ­¤é…ç½®ï¼ˆå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œï¼‰ï¼Ÿ", default=False):
                    feedback = typer.prompt(
                        "è¯·æè¿°éœ€è¦ä¿®æ­£çš„é—®é¢˜æˆ–æä¾›é¢å¤–è¦æ±‚",
                        default="è¯·ä¿®å¤é…ç½®éªŒè¯ä¸­å‘ç°çš„é—®é¢˜",
                    )
                    continue
            else:
                console.print("[green]âœ“[/green] é…ç½®éªŒè¯é€šè¿‡")

                # æ£€æŸ¥ç±»å¯¼å…¥ï¼ˆè­¦å‘Šçº§åˆ«ï¼‰
                import_warnings = _check_class_imports(plan)
                if import_warnings:
                    console.print("\n[yellow]âš ï¸  æ£€æµ‹åˆ°ä»¥ä¸‹æ½œåœ¨é—®é¢˜ï¼š[/yellow]")
                    for warning in import_warnings[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                        console.print(f"  â€¢ [yellow]{warning}[/yellow]")
                    if len(import_warnings) > 5:
                        console.print(f"  [dim]... è¿˜æœ‰ {len(import_warnings) - 5} ä¸ªè­¦å‘Š[/dim]")
                    console.print("[dim]æç¤ºï¼šè¿™äº›è­¦å‘Šä¸ä¸€å®šå¯¼è‡´è¿è¡Œå¤±è´¥ï¼Œä½†å»ºè®®æ£€æŸ¥[/dim]")

            if typer.confirm("\nâœ¨ å¯¹è¯¥é…ç½®æ»¡æ„å—ï¼Ÿ", default=True):
                console.print("\n[bold green]ğŸ‰ Pipeline é…ç½®å·²ç¡®è®¤ï¼[/bold green]\n")
                return plan

            console.print("\n[yellow]âš™ï¸  è¿›å…¥ä¼˜åŒ–æ¨¡å¼...[/yellow]")
            feedback = typer.prompt(
                "è¯·è¾“å…¥éœ€è¦è°ƒæ•´çš„åœ°æ–¹ï¼ˆä¾‹å¦‚ï¼šä½¿ç”¨æµå¼è¾“å‡ºã€æ”¹ç”¨æœ¬åœ°æ¨¡å‹ã€æ·»åŠ ç›‘æ§ï¼‰\nç•™ç©ºç»“æŸ",
                default="",
            )
            if not feedback or not feedback.strip():
                console.print("[yellow]æœªæä¾›è°ƒæ•´æ„è§ï¼Œä¿æŒå½“å‰ç‰ˆæœ¬ã€‚[/yellow]")
                return plan

        # è¾¾åˆ°æœ€å¤§è½®æ•°
        console.print("\n[yellow]âš ï¸  å·²è¾¾åˆ°æœ€å¤§ä¼˜åŒ–è½®æ•°ï¼ˆ6 è½®ï¼‰[/yellow]")
        if plan and typer.confirm("æ˜¯å¦ä½¿ç”¨æœ€åä¸€æ¬¡ç”Ÿæˆçš„é…ç½®ï¼Ÿ", default=True):
            return plan

        return plan

    def handle(self, message: str) -> bool:
        if not self.detect(message):
            return False

        if not self._ensure_api_key():
            return True

        # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯å’ŒåŠŸèƒ½è¯´æ˜
        console.print("\n" + "=" * 70)
        console.print("[bold magenta]ğŸš€ SAGE Pipeline Builder - æ™ºèƒ½ç¼–æ’åŠ©æ‰‹[/bold magenta]")
        console.print("=" * 70)
        console.print(
            """
[dim]åŠŸèƒ½è¯´æ˜ï¼š
  â€¢ åŸºäºæ‚¨çš„æè¿°è‡ªåŠ¨ç”Ÿæˆ SAGE Pipeline é…ç½®
  â€¢ ä½¿ç”¨ RAG æŠ€æœ¯æ£€ç´¢ç›¸å…³æ–‡æ¡£å’Œç¤ºä¾‹
  â€¢ æ”¯æŒå¤šè½®å¯¹è¯ä¼˜åŒ–é…ç½®
  â€¢ å¯ç›´æ¥è¿è¡Œç”Ÿæˆçš„ Pipeline
[/dim]
        """
        )

        console.print("ğŸ¯ [cyan]æ£€æµ‹åˆ° Pipeline æ„å»ºè¯·æ±‚[/cyan]")
        console.print(f"ğŸ“ æ‚¨çš„éœ€æ±‚: [yellow]{message}[/yellow]\n")

        # æ”¶é›†éœ€æ±‚
        requirements = self._collect_requirements(message)

        # ç”Ÿæˆé…ç½®
        plan = self._generate_plan(requirements)
        if plan is None:
            console.print("[yellow]âš ï¸  æœªç”Ÿæˆå¯ç”¨çš„ Pipeline é…ç½®ã€‚[/yellow]")
            return True

        # ä¿å­˜é…ç½®
        console.print("[bold cyan]ğŸ’¾ ä¿å­˜é…ç½®æ–‡ä»¶[/bold cyan]")
        destination = typer.prompt("ä¿å­˜åˆ°æ–‡ä»¶ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•)", default="").strip()
        output_path: Path | None = Path(destination).expanduser() if destination else None
        overwrite = False
        if output_path and output_path.exists():
            overwrite = typer.confirm("âš ï¸  ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ", default=False)
            if not overwrite:
                console.print("[yellow]å·²å–æ¶ˆä¿å­˜ï¼Œç»“æŸæœ¬æ¬¡æ„å»ºã€‚[/yellow]")
                return True

        saved_path = pipeline_builder.save_pipeline_plan(plan, output_path, overwrite)
        console.print(f"\nâœ… Pipeline é…ç½®å·²ä¿å­˜è‡³ [green]{saved_path}[/green]\n")

        # è¯¢é—®æ˜¯å¦è¿è¡Œ
        if not typer.confirm("â–¶ï¸  æ˜¯å¦ç«‹å³è¿è¡Œè¯¥ Pipelineï¼Ÿ", default=True):
            console.print("\n[dim]æç¤ºï¼šç¨åå¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œï¼š[/dim]")
            console.print(f"[cyan]  sage pipeline run {saved_path}[/cyan]\n")
            return True

        # é…ç½®è¿è¡Œå‚æ•°
        console.print("\n[bold cyan]âš™ï¸  é…ç½®è¿è¡Œå‚æ•°[/bold cyan]")
        autostop = typer.confirm("æäº¤åç­‰å¾…æ‰§è¡Œå®Œæˆ (autostop)?", default=True)

        pipeline_type = (plan.get("pipeline", {}).get("type") or "local").lower()
        host: str | None = None
        port_value: int | None = None

        if pipeline_type == "remote":
            console.print("\n[yellow]æ£€æµ‹åˆ°è¿œç¨‹ Pipelineï¼Œéœ€è¦é…ç½® JobManager è¿æ¥ä¿¡æ¯[/yellow]")
            host = typer.prompt("è¿œç¨‹ JobManager host", default="127.0.0.1").strip() or None
            port_text = typer.prompt("è¿œç¨‹ JobManager ç«¯å£", default="19001").strip()
            try:
                port_value = int(port_text)
            except ValueError:
                console.print("[yellow]ç«¯å£æ ¼å¼ä¸åˆæ³•ï¼Œä½¿ç”¨é»˜è®¤ 19001ã€‚[/yellow]")
                port_value = 19001

        # è¿è¡Œ Pipeline
        console.print("\n[bold green]ğŸš€ æ­£åœ¨å¯åŠ¨ Pipeline...[/bold green]\n")
        try:
            job_id = pipeline_builder.execute_pipeline_plan(
                plan,
                autostop=autostop,
                host=host,
                port=port_value,
                console_override=console,
            )
            if job_id:
                console.print(f"\n[bold green]âœ… Pipeline å·²æäº¤ï¼ŒJob ID: {job_id}[/bold green]")
            else:
                console.print("\n[bold green]âœ… Pipeline æ‰§è¡Œå®Œæˆ[/bold green]")
        except Exception as exc:
            console.print(f"\n[red]âŒ Pipeline æ‰§è¡Œå¤±è´¥: {exc}[/red]")
            console.print("\n[dim]æç¤ºï¼šè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œä¾èµ–é¡¹æ˜¯å¦æ­£ç¡®[/dim]")

        console.print("\n" + "=" * 70 + "\n")
        return True


def render_references(references: Sequence[dict[str, str]]) -> Table:
    table = Table(title="çŸ¥è¯†å¼•ç”¨", show_header=True, header_style="bold cyan")
    table.add_column("#", justify="right", width=3)
    table.add_column("æ–‡æ¡£")
    table.add_column("èŠ‚")
    table.add_column("å¾—åˆ†", justify="right", width=7)

    for idx, ref in enumerate(references, start=1):
        # score å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        score = ref.get("score", 0.0)
        try:
            score_float = float(score)
        except (ValueError, TypeError):
            score_float = 0.0
        table.add_row(
            str(idx),
            ref.get("title", "æœªçŸ¥"),
            ref.get("heading", "-"),
            f"{score_float:.4f}",
        )
    return table


def retrieve_context(
    db: Any,
    embedder: EmbeddingModel,
    question: str,
    top_k: int,
    show_progress: bool = True,
) -> dict[str, object]:
    """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€‚

    Args:
        db: SageDB æ•°æ®åº“å®ä¾‹
        embedder: Embedding æ¨¡å‹
        question: ç”¨æˆ·é—®é¢˜
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
    """
    from rich.progress import Progress, SpinnerColumn, TextColumn

    if show_progress:
        with Progress(
            SpinnerColumn(),
            TextColumn("[cyan]æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...[/cyan]"),
            transient=True,
        ) as progress:
            progress.add_task("embedding", total=None)
            query_vector = embedder.embed(question)
            results = db.search(query_vector, top_k, True)
    else:
        query_vector = embedder.embed(question)
        results = db.search(query_vector, top_k, True)

    contexts: list[str] = []
    references: list[dict[str, str]] = []
    for item in results:
        metadata = dict(item.metadata) if hasattr(item, "metadata") else {}
        contexts.append(metadata.get("text", ""))
        references.append(
            {
                "title": metadata.get("title", metadata.get("doc_path", "æœªçŸ¥")),
                "heading": metadata.get("heading", ""),
                "path": metadata.get("doc_path", ""),
                "anchor": metadata.get("anchor", ""),
                "score": str(float(getattr(item, "score", 0.0))),
                "label": f"[{metadata.get('doc_path', '?')}]",
            }
        )
    return {"contexts": contexts, "references": references}


def interactive_chat(
    manifest: ChatManifest,
    index_root: Path,
    top_k: int,
    backend: str,
    model: str,
    base_url: str | None,
    api_key: str | None,
    ask: str | None,
    stream: bool,
    finetune_model: str | None = None,
    finetune_port: int = DEFAULT_FINETUNE_PORT,
) -> None:
    embedder: Any | None = None
    db: Any | None = None
    generator = ResponseGenerator(
        backend,
        model,
        base_url,
        api_key,
        finetune_model=finetune_model,
        finetune_port=finetune_port,
    )
    pipeline_coordinator = PipelineChatCoordinator(backend, model, base_url, api_key)

    console.print(
        Panel(
            f"ç´¢å¼•: [cyan]{manifest.index_name}[/cyan]\n"
            f"æ¥æº: [green]{manifest.source_dir}[/green]\n"
            f"æ–‡æ¡£æ•°: {manifest.num_documents}  Chunkæ•°: {manifest.num_chunks}\n"
            f"Embedding: {manifest.embed_config}",
            title="SAGE Chat å‡†å¤‡å°±ç»ª",
        )
    )

    def ensure_retriever() -> tuple[Any, Any]:
        nonlocal embedder, db
        if embedder is None:
            embedder = build_embedder(manifest.embed_config)
        if db is None:
            db = open_database(manifest)
        return db, embedder

    def answer_once(query: str) -> None:
        # æ£€æµ‹æ˜¯å¦æ˜¯ç®€å•é—²èŠï¼ˆä¸éœ€è¦æ£€ç´¢ï¼‰
        casual_patterns = [
            "ä½ å¥½",
            "hi",
            "hello",
            "å—¨",
            "hey",
            "è°¢è°¢",
            "thanks",
            "thank you",
            "å†è§",
            "bye",
            "æ‹œæ‹œ",
            "test",
            "æµ‹è¯•",
            "è¯•è¯•",
            "ok",
            "å¥½çš„",
            "å—¯",
        ]
        query_lower = query.strip().lower()
        is_casual = (
            any(
                query_lower == p or query_lower.startswith(p + " ") or query_lower.endswith(" " + p)
                for p in casual_patterns
            )
            and len(query.strip()) < 20
        )

        if is_casual:
            # é—²èŠæ¨¡å¼ï¼šè·³è¿‡æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆå›ç­”
            contexts: Sequence[str] = []
            references: Sequence[dict[str, str]] = []
        else:
            # æŠ€æœ¯é—®é¢˜ï¼šæ‰§è¡Œæ£€ç´¢
            current_db, current_embedder = ensure_retriever()
            payload = retrieve_context(current_db, current_embedder, query, top_k)
            contexts = payload["contexts"]  # type: ignore[assignment]
            references = payload["references"]  # type: ignore[assignment]

        try:
            reply = generator.answer(query, contexts, references, stream=stream)
        except Exception as exc:
            console.print(f"[red]ç”Ÿæˆå›ç­”å¤±è´¥: {exc}[/red]")
            return

        # åªåœ¨æœ‰å¼•ç”¨æ—¶æ˜¾ç¤ºå¼•ç”¨è¡¨
        if references:
            context_table = render_references(references)
            console.print(context_table)

        if stream:
            text = Text()
            with Live(Panel(text, title="å›ç­”"), auto_refresh=False) as live:
                text.append(reply)
                live.refresh()
        else:
            console.print(Panel(Markdown(reply), title="å›ç­”", style="bold green"))

    if ask:
        if pipeline_coordinator.handle(ask):
            return
        answer_once(ask)
        return

    # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    console.print("\n[bold cyan]ğŸ§­ SAGE Chat ä½¿ç”¨æŒ‡å—[/bold cyan]")

    # æ˜¾ç¤ºå½“å‰é…ç½®
    if backend == "finetune":
        console.print(f"[green]âœ… ä½¿ç”¨å¾®è°ƒæ¨¡å‹: {finetune_model or DEFAULT_FINETUNE_MODEL}[/green]")
        console.print(f"[dim]ç«¯å£: {finetune_port}[/dim]\n")

    console.print(
        """
[dim]åŸºæœ¬å‘½ä»¤ï¼š
  â€¢ ç›´æ¥è¾“å…¥é—®é¢˜è·å–æ–‡æ¡£ç›¸å…³å›ç­”
  â€¢ è¾“å…¥åŒ…å« 'pipeline'ã€'æ„å»ºåº”ç”¨' ç­‰å…³é”®è¯è§¦å‘ Pipeline æ„å»ºæ¨¡å¼
  â€¢ è¾“å…¥ 'help' æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
  â€¢ è¾“å…¥ 'templates' æŸ¥çœ‹å¯ç”¨åœºæ™¯æ¨¡æ¿
  â€¢ è¾“å…¥ 'exit'ã€'quit' æˆ– 'q' é€€å‡ºå¯¹è¯
[/dim]
    """
    )

    try:
        while True:
            try:
                question = typer.prompt("ğŸ¤– ä½ çš„é—®é¢˜")
            except (EOFError, KeyboardInterrupt):
                console.print("\nå†è§ ğŸ‘‹", style="cyan")
                break
            if not question.strip():
                continue

            question_lower = question.lower().strip()

            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            if question_lower in {"exit", "quit", "q"}:
                console.print("å†è§ ğŸ‘‹", style="cyan")
                break
            elif question_lower in {"help", "å¸®åŠ©", "?"}:
                console.print("\n[bold cyan]ğŸ“š å¸®åŠ©ä¿¡æ¯[/bold cyan]")
                console.print(
                    """
[dim]åŠŸèƒ½è¯´æ˜ï¼š
  1. æ–‡æ¡£é—®ç­”ï¼šç›´æ¥æé—®å…³äº SAGE çš„é—®é¢˜
  2. Pipeline æ„å»ºï¼šæè¿°ä½ æƒ³è¦çš„åº”ç”¨åœºæ™¯

ç¤ºä¾‹é—®é¢˜ï¼š
  â€¢ "SAGE å¦‚ä½•é…ç½® RAGï¼Ÿ"
  â€¢ "è¯·å¸®æˆ‘æ„å»ºä¸€ä¸ªé—®ç­”åº”ç”¨"
  â€¢ "å¦‚ä½•ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼Ÿ"
  â€¢ "build a chat pipeline with streaming"

ç‰¹æ®Šå‘½ä»¤ï¼š
  â€¢ help      - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  â€¢ templates - æŸ¥çœ‹å¯ç”¨åœºæ™¯æ¨¡æ¿
  â€¢ exit/quit - é€€å‡ºå¯¹è¯
[/dim]
                """
                )
                continue
            elif question_lower in {"templates", "æ¨¡æ¿"}:
                _show_scenario_templates()
                console.print("[dim]æç¤ºï¼šåœ¨ Pipeline æ„å»ºæµç¨‹ä¸­å¯ä»¥é€‰æ‹©ä½¿ç”¨è¿™äº›æ¨¡æ¿[/dim]\n")
                continue

            # å¤„ç† Pipeline æ„å»ºè¯·æ±‚
            if pipeline_coordinator.handle(question):
                continue

            # å¤„ç†æ™®é€šé—®ç­”
            answer_once(question)
    finally:
        # æ¸…ç†èµ„æº
        generator.cleanup()


@app.callback()
def main(
    ctx: typer.Context,
    index_name: str = typer.Option(
        DEFAULT_INDEX_NAME,
        "--index",
        "-i",
        help="ç´¢å¼•åç§°ï¼Œç”¨äºè¯»å– manifest å’Œ SageDB æ–‡ä»¶",
    ),
    ask: str | None = typer.Option(
        None,
        "--ask",
        "-q",
        help="ç›´æ¥æé—®å¹¶é€€å‡ºï¼Œè€Œä¸æ˜¯è¿›å…¥äº¤äº’æ¨¡å¼",
    ),
    top_k: int = typer.Option(
        DEFAULT_TOP_K,
        "--top-k",
        "-k",
        min=1,
        max=20,
        help="æ£€ç´¢æ—¶è¿”å›çš„å‚è€ƒæ–‡æ¡£æ•°é‡",
    ),
    backend: str = typer.Option(
        DEFAULT_BACKEND,
        "--backend",
        help="å›ç­”ç”Ÿæˆåç«¯: mock / openai / compatible / finetune / vllm / ollama",
    ),
    model: str = typer.Option(
        "qwen-max",
        "--model",
        help="å›ç­”ç”Ÿæˆæ¨¡å‹åç§°ï¼ˆfinetune backend ä¼šè‡ªåŠ¨ä»é…ç½®è¯»å–ï¼‰",
    ),
    base_url: str | None = typer.Option(
        None,
        "--base-url",
        help="LLM API base_url (ä¾‹å¦‚ vLLM æˆ–å…¼å®¹ OpenAI çš„æ¥å£)",
    ),
    api_key: str | None = typer.Option(
        lambda: os.environ.get("TEMP_GENERATOR_API_KEY"),
        "--api-key",
        help="LLM API Key (é»˜è®¤è¯»å–ç¯å¢ƒå˜é‡ TEMP_GENERATOR_API_KEY)",
    ),
    finetune_model: str | None = typer.Option(
        DEFAULT_FINETUNE_MODEL,
        "--finetune-model",
        help="ä½¿ç”¨ finetune backend æ—¶çš„å¾®è°ƒæ¨¡å‹åç§°ï¼ˆ~/.sage/finetune_output/ ä¸‹çš„ç›®å½•åï¼‰",
    ),
    finetune_port: int = typer.Option(
        DEFAULT_FINETUNE_PORT,
        "--finetune-port",
        help="finetune backend ä½¿ç”¨çš„ vLLM æœåŠ¡ç«¯å£",
    ),
    index_root: str | None = typer.Option(
        None,
        "--index-root",
        help="ç´¢å¼•è¾“å‡ºç›®å½• (æœªæä¾›åˆ™ä½¿ç”¨ ~/.sage/cache/chat)",
    ),
    stream: bool = typer.Option(False, "--stream", help="å¯ç”¨æµå¼è¾“å‡º (ä»…å½“åç«¯æ”¯æŒ)"),
) -> None:
    if ctx.invoked_subcommand is not None:
        return

    ensure_sage_db()
    root = resolve_index_root(index_root)
    manifest = load_or_bootstrap_manifest(root, index_name)

    interactive_chat(
        manifest=manifest,
        index_root=root,
        top_k=top_k,
        backend=backend,
        model=model,
        base_url=base_url,
        api_key=api_key,
        ask=ask,
        stream=stream,
        finetune_model=finetune_model,
        finetune_port=finetune_port,
    )


@app.command("ingest")
def ingest(
    source_dir: Path | None = typer.Option(
        None,
        "--source",
        "-s",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
        help="æ–‡æ¡£æ¥æºç›®å½• (é»˜è®¤ docs-public/docs_src)",
    ),
    index_name: str = typer.Option(DEFAULT_INDEX_NAME, "--index", "-i", help="ç´¢å¼•åç§°"),
    chunk_size: int = typer.Option(
        DEFAULT_CHUNK_SIZE,
        "--chunk-size",
        help="chunk å­—ç¬¦é•¿åº¦",
        min=128,
        max=4096,
    ),
    chunk_overlap: int = typer.Option(
        DEFAULT_CHUNK_OVERLAP,
        "--chunk-overlap",
        help="chunk ä¹‹é—´çš„é‡å å­—ç¬¦æ•°",
        min=0,
        max=1024,
    ),
    embedding_method: str = typer.Option(
        DEFAULT_EMBEDDING_METHOD,
        "--embedding-method",
        help="Embedding æ–¹æ³• (mockembedder/hf/openai/...)",
    ),
    embedding_model: str | None = typer.Option(
        None,
        "--embedding-model",
        help="Embedding æ¨¡å‹åç§° (æ–¹æ³•éœ€è¦æ—¶æä¾›)",
    ),
    fixed_dim: int = typer.Option(
        DEFAULT_FIXED_DIM,
        "--fixed-dim",
        help="mockembedder ä½¿ç”¨çš„ç»´åº¦",
        min=64,
        max=2048,
    ),
    max_files: int | None = typer.Option(
        None,
        "--max-files",
        help="ä»…å¤„ç†æŒ‡å®šæ•°é‡çš„æ–‡ä»¶ (æµ‹è¯•/è°ƒè¯•ç”¨)",
    ),
    index_root: str | None = typer.Option(
        None,
        "--index-root",
        help="ç´¢å¼•è¾“å‡ºç›®å½• (æœªæä¾›åˆ™ä½¿ç”¨ ~/.sage/cache/chat)",
    ),
    embedding_base_url: str | None = typer.Option(
        None,
        "--embedding-base-url",
        help="Embedding æœåŠ¡ API ç«¯ç‚¹ (ç”¨äºè¿æ¥æœ¬åœ° embedding serverï¼Œå¦‚ http://localhost:8090/v1)",
    ),
    quiet: bool = typer.Option(
        False,
        "--quiet",
        "-q",
        help="é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºè¿›åº¦æ¡ä¸æ‰“å°è¯¦ç»†æ—¥å¿—",
    ),
) -> None:
    ensure_sage_db()
    root = resolve_index_root(index_root)
    target_source = source_dir or default_source_dir()

    needs_model = embedding_method in METHODS_REQUIRE_MODEL
    if needs_model and not embedding_model:
        raise typer.BadParameter(f"{embedding_method} æ–¹æ³•éœ€è¦æŒ‡å®š --embedding-model")

    # æ„å»º embedding é…ç½®ï¼ˆæ–°æ¥å£ä¼šè‡ªåŠ¨å¤„ç†é»˜è®¤å€¼ï¼‰
    embedding_config: dict[str, Any] = {"method": embedding_method, "params": {}}

    # è®¾ç½®æ–¹æ³•ç‰¹å®šå‚æ•°
    if embedding_method == "mockembedder":
        embedding_config["params"]["fixed_dim"] = fixed_dim
    elif embedding_method == "hash":
        embedding_config["params"]["dim"] = fixed_dim

    # è®¾ç½®æ¨¡å‹åç§°ï¼ˆå¦‚æœæä¾›ï¼‰
    if embedding_model:
        embedding_config["params"]["model"] = embedding_model

    # è®¾ç½® base_urlï¼ˆå¦‚æœæä¾›ï¼Œç”¨äºè¿æ¥æœ¬åœ° embedding æœåŠ¡ï¼‰
    if embedding_base_url:
        embedding_config["params"]["base_url"] = embedding_base_url
        # æœ¬åœ°æœåŠ¡ä¸éœ€è¦ API keyï¼Œè®¾ç½®ä¸€ä¸ªå ä½ç¬¦
        if "api_key" not in embedding_config["params"]:
            embedding_config["params"]["api_key"] = "local"  # pragma: allowlist secret

    if not quiet:
        console.print(
            Panel(
                f"ç´¢å¼•åç§°: [cyan]{index_name}[/cyan]\n"
                f"æ–‡æ¡£ç›®å½•: [green]{target_source}[/green]\n"
                f"ç´¢å¼•ç›®å½•: [magenta]{root}[/magenta]\n"
                f"Embedding: {embedding_config}",
                title="SAGE Chat Ingest",
            )
        )

    ingest_source(
        source_dir=target_source,
        index_root=root,
        index_name=index_name,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        embedding_config=embedding_config,
        max_files=max_files,
        show_progress=not quiet,
    )


@app.command("show")
def show_manifest(
    index_name: str = typer.Option(DEFAULT_INDEX_NAME, "--index", "-i"),
    index_root: str | None = typer.Option(None, "--index-root", help="ç´¢å¼•æ‰€åœ¨ç›®å½•"),
) -> None:
    ensure_sage_db()
    root = resolve_index_root(index_root)
    try:
        manifest = load_manifest(root, index_name)
    except FileNotFoundError as exc:
        console.print(f"[red]{exc}[/red]")
        raise typer.Exit(code=1)

    table = Table(title=f"SAGE Chat ç´¢å¼•: {index_name}")
    table.add_column("å±æ€§", style="cyan")
    table.add_column("å€¼", style="green")
    table.add_row("ç´¢å¼•è·¯å¾„", str(manifest.db_path))
    table.add_row("åˆ›å»ºæ—¶é—´", manifest.created_at)
    table.add_row("æ–‡æ¡£ç›®å½•", manifest.source_dir)
    table.add_row("æ–‡æ¡£æ•°é‡", str(manifest.num_documents))
    table.add_row("Chunk æ•°é‡", str(manifest.num_chunks))
    table.add_row("Embedding", json.dumps(manifest.embedding, ensure_ascii=False))
    table.add_row("Chunk é…ç½®", f"size={manifest.chunk_size}, overlap={manifest.chunk_overlap}")
    console.print(table)


__all__ = ["app"]
