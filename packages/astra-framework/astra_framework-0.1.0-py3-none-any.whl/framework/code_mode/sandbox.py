"""
Sandbox Executor for Code Execution Mode.

This module provides a sandbox environment for executing LLM-generated Python code in an isolated subprocess. The sandbox can call tools (Python @tool functions and MCP tools) via a runtime bridge that communicates with the parent Agent process.

Architecture:
    Agent Process:
       SandboxExecutor.execute(code)
        - Generates astra_runtime.py (injected into sandbox)
        - Spawns Python subprocess
        - Monitor stdout for tool calls
        - Handles tool execution via ToolRegistry

    Sandbox Process:
      Generated Code (from LLM)
       - Imports astra_api (generated by VirtualAPIGenerator)
       - Calls: crm.get_user(123)
       - Internally calls: call_tool("crm.get_user", {....})
       - Sends JSON to stdout -> Agent receives -> Executes -> Returns result

Components:
  1. SandboxExecutor: Main executor class
  2. SandboxResult: Result dataclass
  3. _generate_runtime_code() - Generates astra_runtime.py content
  4. _handle_tool_call() - Executes tools from sandbox requests
  5. _execute_in_subprocess() - Subprocess execution logic
  6. _monitor_tool_call() - Monitors stdout for tool call requests
"""

from __future__ import annotations

import asyncio
from dataclasses import dataclass
import json
from pathlib import Path
import sys
import time
from typing import TYPE_CHECKING, Any

from framework.agents.execution import ExecutionContext


if TYPE_CHECKING:
    from framework.agents.agent import Agent
    from framework.code_mode.tool_registry import ToolRegistry
else:
    # Runtime imports to avoid circular dependency
    from framework.code_mode.tool_registry import ToolRegistry


@dataclass
class SandboxResult:
    """Result from sandbox code execution.

    Attributes:
        stdout: Standard output from code execution (print statements)
        stderr: Standard error output
        success: Whether execution completed successfully
        tool_calls: List of tool calls made during execution
        exit_code: Process exit code (0 = success)
        execution_time: Time taken to execute (seconds)
    """

    stdout: str
    stderr: str
    success: bool
    tool_calls: list[dict[str, Any]]
    exit_code: int
    execution_time: float

    def __repr__(self) -> str:
        return (
            f"SandboxResult(success={self.success}, "
            f"exit_code={self.exit_code}, "
            f"execution_time={self.execution_time:.2f}s)"
        )


runtime_code = '''"""
Astra Runtime Bridge - Auto-generated for sandbox execution.

This module provides call_tool() and call_mcp_tool() functions that allow sandbox code to execute tools from the parent Agent process.
"""

import sys
import json

def call_tool(tool_name, args: dict) -> dict:
    """Call a Python @tool function from sandbox.

    Args:
        tool_name: tool name (e.g., "crm.get_user")
        args: Tool arguments

    Returns:
        Tool execution result
    """

    # Send request to parent process via stdout
    request = {
        "type": "call_tool",
        "name": tool_name,
        "args": args
    }

    print(json.dumps(request), flush=True)

    # Read response from parent via stdin
    response_line = sys.stdin.readline()
    if not response_line:
        raise RuntimeError("No response from parent process")

    response = json.loads(response_line)

    if response.get("type") == "error":
        raise RuntimeError(response.get("message", "Tool execution failed"))

    return response.get("data", {})

def call_mcp_tool(name: str, args: dict) -> dict:
    """Call an MCP tool from sandbox.

    Args:
        name: tool_name (e.g., "filesystem.read_file")
        args: Tool arguments

    Returns:
        Tool execution result
    """

    # Send request to parent process via stdout
    request = {
       "type": "call_mcp_tool",
       "name": name,
       "args": args
    }
    print(json.dumps(request), flush=True)

    # Read response from parent via stdin
    response_line = sys.stdin.readline()
    if not response_line:
        raise RuntimeError("No response from parent process")

    response = json.loads(response_line)

    if response.get("type") == "error":
        raise RuntimeError(response.get("message", "MCP tool execution failed"))

    return response.get("data", {})
'''


class SandboxExecutor:
    """Executes Python code in isolated subprocess with tool access.

    The sandbox provides a secure execution environment where LLM-generated code can run with controlled access to agent tools.
    Communication happens via JSON over stdin/stdout.

    This class handles:
    - Generating the runtime bridge code (astra_runtime.py)
    - Spawning and managing subprocess execution
    - Monitoring stdout for tool call requests
    - Executing tools via the Agent's ToolRegistry
    - Returning execution results

    Example:
       >>> executor = SandboxExecutor(tool_registry)
       >>> result = executor.execute(code)
       >>> print(result)
       ... from astra_api import crm
       ...
       ... user = crm.get_user(123)
       ... print(f"User: {user}")
       >>> print(result.stdout)  # "User: John Doe"
    """

    def __init__(self, agent: Agent):
        self.agent = agent
        self.tool_registry: ToolRegistry = agent.tool_registry

    def _generate_runtime_code(self) -> str:
        """Generates astra_runtime.py code to inject into sandbox.

        This runtime code provides a bridge between the sandbox and the parent process. It includes call_tool() and call_mcp_tool() functions that communicates with the parent Agent process via stdin/stdout.

        Communication Protocol:
          Request (sandbox -> Agent):
            {"type": "call_tool", "name": "crm.get_user", "args": {"user_id": 123}}

          Response (Agent -> sandbox):
            {"type": "result", "data": {"id": 123, "name": "John"}}
            OR
            {"type": "error", "message": "Tool not found}
        """
        return runtime_code

    async def _handle_tool_call(self, request: dict[str, Any]) -> dict[str, Any]:
        """Handle tool call request from sandbox.

        This method:
        1. Validate the request format
        2. Looks up the tool in the registry
        3. Executes the tool (Python or MCP)
        4. Returns result or error

        Args:
            request: Tool call request dict with keys:
                - type: "call_tool" or "call_mcp_tool"
                - name: tool name (e.g., "crm.get_user")
                - args: tool arguments

        Returns:
           Response dict with keys:
              - type: "result" or "error"
              - data: Result data (if type="result")
              - message: Error message (if type="error")
        """

        tool_name = request.get("name")
        args = request.get("args", {})

        if not tool_name:
            return {"type": "error", "message": "Missing tool name"}

        # Lookup tool in registry
        # Try fully qualified name first (e.g., "crm.get_user")
        tool_spec = self.tool_registry.get(tool_name)

        # If not found and name contains a dot, try just the tool name part
        if not tool_spec and "." in tool_name:
            simple_name = tool_name.split(".")[-1]
            tool_spec = self.tool_registry.get(simple_name)

        if not tool_spec:
            return {"type": "error", "message": f"Tool '{tool_name}' not found in registry"}

        # Execute tool
        try:
            # ToolSpec.invoke is callable (Tool object or wrapper function)
            # For Python tools: invoke is the Tool object (which wraps the actual function)
            # For MCP tools: invoke is a wrapper function that calls MCP client

            # Try to call the tool and check if result is a coroutine
            call_result = tool_spec.invoke(**args)

            # Check if result is a coroutine (for async tools wrapped in Tool)
            if asyncio.iscoroutine(call_result):
                result = await call_result
            elif asyncio.iscoroutinefunction(tool_spec.invoke):
                # Direct async function (MCP tools)
                result = await tool_spec.invoke(**args)
            else:
                # Sync function - run in thread pool to avoid blocking
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, lambda: tool_spec.invoke(**args))

            return {"type": "result", "data": result}

        except Exception as e:
            # Log error if observability is available
            if self.agent._context and self.agent._context.observability:
                self.agent._context.observability.logger.error(
                    f"Tool {tool_name} execution failed: {e}"
                )

            return {"type": "error", "message": str(e)}

    async def _monitor_tool_calls(
        self, stdout: asyncio.StreamReader, stdin: asyncio.StreamWriter
    ) -> tuple[str, list]:
        """Monitor sandbox stdout for call requests.

        This method:
        1. Reads lines from sandbox stdout
        2. Detects tool call requests (JSON messages)
        3. Routes to _handle_tool_call()
        4. Sends responses back via stdin
        5. Collects regular stdout output (print statements)

        Args:
          stdout: Sandbox stdout stream reader
          stdin: Sandbox stdin stream writer

        Returns:
          Tuple of (stdout_text, tool_calls_list)
        """

        stdout_lines: list[str] = []
        tool_calls: list[dict[str, Any]] = []

        while True:
            try:
                # Read line from sandbox stdout
                line_bytes = await asyncio.wait_for(stdout.readline(), timeout=0.1)
                if not line_bytes:
                    break

                line = line_bytes.decode("utf-8", errors="replace").rstrip()

                # Check if this is a tool call request (JSON)
                if line.startswith("{") and line.endswith("}"):
                    try:
                        request = json.loads(line)
                        if request.get("type") in ("call_tool", "call_mcp_tool"):
                            # This is a tool call request
                            tool_calls.append(request)

                            # Handle tool call
                            response = await self._handle_tool_call(request)

                            # Send response back via stdin
                            response_json = json.dumps(response) + "\n"
                            stdin.write(response_json.encode())
                            await stdin.drain()

                            # Don't add tool call JSON to stdout
                            continue
                    except json.JSONDecodeError:
                        # Not a valid JSON, treat as regular output
                        pass

                # Regular stdout output (print statements)
                stdout_lines.append(line)

            except asyncio.TimeoutError:
                # No data available, continue monitoring
                continue
            except Exception as e:
                # Log error but continue
                if self.agent._context and self.agent._context.observability:
                    self.agent._context.observability.logger.error(
                        f"Error monitoring sandbox output: {e}"
                    )
                break

        return "\n".join(stdout_lines), tool_calls

    async def execute(
        self,
        code: str,
        timeout: float = 30.0,
        working_dir: Path | None = None,
    ) -> SandboxResult:
        """Execute Python code in isolated subprocess.

        This method:
        1. Generates astra_runtime.py code
        2. Combines with user code
        3. Spawns python subprocess
        4. Monitors stdout for tool calls
        5. Handles tool execution
        6. Returns result

        Args:
          code: Python code to execute (from LLM)
          timeout: Maximum execution time in seconds (default: 30)
          working_dir: Working directory for subprocess (default: temp dir)

        Returns:
          SandboxResult with stdout, stderr, success status, and tool calls

        Raises:
          TimeoutError: If execution exceeds timeout
          RuntimeError: If subprocess fails to start
        """

        start_time = time.time()

        # Generate runtime code
        runtime_code = self._generate_runtime_code()

        # Get astra_api.py path (generated by VirtualAPIGenerator)
        api_file_path = Path(".astra/generated/astra_api.py")
        if not api_file_path.exists():
            return SandboxResult(
                success=False,
                stdout="",
                stderr=f"Error: astra_api.py not found at {api_file_path}",
                tool_calls=[],
                exit_code=1,
                execution_time=0.0,
            )

        # Combine code: runtime + llm generated code + generated file code
        # We need to inject astra_runtime into the sandbox's sys.modules
        # and make astra_api importable
        # Note: No indentation in the template - Python code must start at column 0
        full_code = f"""# Injected runtime
import sys
import json
from pathlib import Path

# Inject astra_runtime module
{runtime_code}

# Add astra_runtime to sys.modules
import types
astra_runtime_module = types.ModuleType('astra_runtime')
exec({runtime_code!r}, astra_runtime_module.__dict__)
sys.modules['astra_runtime'] = astra_runtime_module

# Import astra_api (generated file)
import importlib.util
spec = importlib.util.spec_from_file_location("astra_api", {str(api_file_path.resolve())!r})
astra_api = importlib.util.module_from_spec(spec)
spec.loader.exec_module(astra_api)
sys.modules['astra_api'] = astra_api

# User code
{code}
"""

        # Create subprocess
        process = await asyncio.create_subprocess_exec(
            sys.executable,
            "-c",
            full_code,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=working_dir or Path.cwd(),
            limit=1024 * 1024,  # 1MB buffer limit
        )

        if not process.stdout or not process.stdin:
            return SandboxResult(
                stdout="",
                stderr="Error: subprocess failed to start",
                success=False,
                tool_calls=[],
                exit_code=1,
                execution_time=time.time() - start_time,
            )

        # Monitor stdout for tool calls (in background)
        stdout_task = asyncio.create_task(self._monitor_tool_calls(process.stdout, process.stdin))

        # Wait for process completion with timeout
        # Note: Don't use process.communicate() since stdout is already being consumed
        # by the monitoring task. Instead, wait for process and read stderr separately.
        try:
            # Wait for process to finish
            returncode = await asyncio.wait_for(process.wait(), timeout=timeout)

            # Read stderr separately (stdout is handled by monitoring task)
            stderr_bytes = b""
            if process.stderr:
                try:
                    stderr_bytes = await process.stderr.read()
                except Exception:
                    pass
        except asyncio.TimeoutError:
            # Process exceeded timeout
            process.kill()
            await process.wait()
            stdout_task.cancel()
            return SandboxResult(
                stdout="",
                stderr=f"Execution timeout after {timeout}s",
                success=False,
                tool_calls=[],
                exit_code=124,  # Standard timeout exit code
                execution_time=time.time() - start_time,
            )

        # Get stdout and tool calls from monitoring task
        try:
            stdout_text, tool_calls = await stdout_task
        except Exception as e:
            stdout_text = ""
            tool_calls = []
            if self.agent._context and self.agent._context.observability:
                self.agent._context.observability.logger.error(f"Error reading stdout: {e}")

        stderr_text = stderr_bytes.decode("utf-8", errors="replace") if stderr_bytes else ""

        # Determine success
        success = returncode == 0 and not stderr_text

        execution_time = time.time() - start_time

        return SandboxResult(
            stdout=stdout_text,
            stderr=stderr_text,
            success=success,
            tool_calls=tool_calls,
            exit_code=returncode or 0,
            execution_time=execution_time,
        )


async def synthesize_response(
    agent: Agent,
    user_query: str,
    execution_result: SandboxResult,
    context: ExecutionContext,
) -> str:
    """
    Synthesize execution results into a meaningful user-facing response.

    This function takes the raw output from code execution and transforms it into
    a response that aligns with the agent's persona and instructions. It makes a
    second LLM call with:
    - Agent's original instructions (for persona alignment)
    - User's original query (for context)
    - Execution results (stdout, tool calls, metadata)
    - No tool access (pure interpretation)
    - No code generation (pure synthesis)

    Args:
        agent: The agent instance (for accessing model and instructions)
        user_query: The original user query
        execution_result: The SandboxResult from code execution
        context: Execution context for model invocation

    Returns:
        Synthesized response string that explains the execution results in a
        meaningful way aligned with the agent's persona.

    Example:
        After code execution returns "Product details retrieved successfully",
        this function will synthesize it into a detailed market research report
        with tables, insights, and recommendations based on the agent's
        instructions.
    """
    # Build the synthesis prompt that includes:
    # 1. Agent instructions (for persona and formatting guidelines)
    # 2. User query (for context)
    # 3. Execution results (stdout, tool calls, success status)
    # 4. Clear instructions on what to do (synthesize, don't execute)

    # Format execution results for the prompt
    # Include stdout (the main output from code execution)
    stdout_section = execution_result.stdout if execution_result.stdout else "(No output)"

    # Include tool calls information if available
    tool_calls_section = ""
    if execution_result.tool_calls:
        tool_calls_section = f"\n\nTool Calls Made: {len(execution_result.tool_calls)}"
        # Include first few tool calls as examples
        for tool_call in execution_result.tool_calls[:5]:
            tool_name = tool_call.get("name", "unknown")
            tool_calls_section += f"\n  - {tool_name}"
        if len(execution_result.tool_calls) > 5:
            tool_calls_section += f"\n  ... and {len(execution_result.tool_calls) - 5} more"

    # Include error information if execution failed
    error_section = ""
    if not execution_result.success:
        error_section = f"\n\nExecution Error:\n{execution_result.stderr or 'Unknown error'}"

    # Build the system prompt for synthesis
    # This tells the LLM to synthesize the results according to agent persona
    system_prompt = f"""{agent.instructions}

You are synthesizing the execution results from a code execution into a user-facing response.

The code has already been executed and the results are below. Your task is to:
1. Transform the raw execution output into a meaningful response
2. Align the response with your agent persona and instructions above
3. Format the response according to your response format guidelines
4. Answer the user's query using the execution results

Important constraints:
- Do NOT access any tools (execution is complete)
- Do NOT generate code (execution is complete)
- Do NOT re-execute anything
- Focus on explaining what happened and what it means
- Use the execution results to answer the user's query"""

    # Build the user message with query and execution results
    user_message = f"""User Query:
{user_query}

Execution Results:
{stdout_section}{tool_calls_section}{error_section}

Execution Status: {"Success" if execution_result.success else "Failed"}
Execution Time: {execution_result.execution_time:.2f}s

Now synthesize these execution results into a meaningful response that:
- Answers the user's query
- Aligns with your agent persona
- Follows your response format guidelines
- Explains what happened and what it means"""

    # Prepare messages for the synthesis LLM call
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message},
    ]

    # Invoke the model to synthesize the response
    # Use the same model as the agent for consistency
    # No tools needed - this is pure text generation
    try:
        response = await agent.model.invoke(
            messages=messages,
            tools=None,  # No tools for synthesis
            temperature=context.temperature,
            max_tokens=context.max_tokens or 4096,
        )

        # Extract and return the synthesized response
        synthesized = response.content or ""

        # If synthesis failed or returned empty, fallback to formatted execution output
        if not synthesized.strip():
            if execution_result.success:
                return execution_result.stdout or "Execution completed successfully."
            else:
                return f"Execution failed: {execution_result.stderr or 'Unknown error'}"

        return synthesized.strip()

    except Exception as e:
        # If synthesis fails, fallback to execution output with explanation
        # This ensures we always return something meaningful
        if agent._context and agent._context.observability:
            agent._context.observability.logger.error(
                f"Response synthesis failed: {e}, falling back to execution output"
            )

        # Return the execution output as fallback
        if execution_result.success:
            return execution_result.stdout or "Execution completed successfully."
        else:
            return f"Execution failed: {execution_result.stderr or 'Unknown error'}"
