{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2568a7c7",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This guide shows how to setup basic inference with open answers for an LLM to predict opinion of personas towards the Democratic and Republican party."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d940fa",
   "metadata": {},
   "source": [
    "## General Setup\n",
    "\n",
    "### 1. Importing Questionnaire\n",
    "\n",
    "The simplest way to setup inference with QSTN is to define our questionnaire in a ```pd.Dataframe``` or a ```.csv``` file. For this tutorial we create the Dataframe dynamically. If you have a csv file with the name \"parties.csv\" you can also just specify the path to your file.\n",
    "\n",
    "|questionnaire_item_id|question_content               |\n",
    "|-----------------|-------------------------------|\n",
    "|1                |The Democratic Party?          |\n",
    "|2                |The Republican Party?          |\n",
    "\n",
    "We can then either use ```pandas``` to read the file or give the path directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11701059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "questionnaire = [\n",
    "    {\"questionnaire_item_id\": 1, \"question_content\": \"The Democratic Party?\"},\n",
    "    {\"questionnaire_item_id\": 2, \"question_content\": \"The Republican Party?\"},\n",
    "]\n",
    "\n",
    "\n",
    "party_questionnaire = pd.DataFrame(questionnaire)\n",
    "# party_questionnaire = \"parties.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e0422",
   "metadata": {},
   "source": [
    "### 2. Creating the Questionnaire Object\n",
    "\n",
    "We can create the system prompt and prompt now. It is important to specify where exactly in the prompt (or system prompt) the questions should be asked. We can do so by specifying placeholders in our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6966c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.utilities import placeholder\n",
    "\n",
    "\n",
    "system_prompt = \"Act as if you were a black middle aged man from New York! Answer in a single short sentence!\"\n",
    "prompt = f\"Please tell us how you feel about the following parties:\\n{placeholder.PROMPT_QUESTIONS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929dfc4",
   "metadata": {},
   "source": [
    "For every experiment we want to conduct we need two key modules. The first is the [LLMPrompt]() class. This class is the main class for defining how your prompt looks like and can be further modified to design our Response Generation Methods or how to ask the questions. For this quickstart we will stick to the most basic approach and only define the system prompt and prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import LLMPrompt\n",
    "\n",
    "questionnaire = LLMPrompt(\n",
    "    questionnaire_name=\"political_parties\",\n",
    "    questionnaire_source=party_questionnaire,\n",
    "    system_prompt=system_prompt,\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc11a1",
   "metadata": {},
   "source": [
    "### 3. Setting up Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddc6c0",
   "metadata": {},
   "source": [
    "That's it! We can now just specify the model we want to use and run inference either locally or remotely. For both options, the code changes only slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb02cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d042ea",
   "metadata": {},
   "source": [
    "#### Local Inference\n",
    "\n",
    "We use [vllm](https://docs.vllm.ai/en/latest/) for local inference so we generate our model just like how we would with vllm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e64e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 11-26 15:41:23 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "chat_generator = LLM(model_id, max_model_len=5000, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09084923",
   "metadata": {},
   "source": [
    "#### Remote Inference\n",
    "\n",
    "For remote inference we use the [OpenAi Framework](https://github.com/openai/openai-python), specifically AsyncOpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f36c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "# For this tutorial we use a local vLLM API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "chat_generator = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4759516",
   "metadata": {},
   "source": [
    "### 4. Generating and Saving Output\n",
    "\n",
    "Now that we have generated the model or specified the client we can use the same code to run inference with the model.\n",
    "\n",
    "For inferencing we make use of the second main component of qstn: The [survey_manager](https://qstn.readthedocs.io/en/latest/api/qstn.html#module-qstn.survey_manager) module.\n",
    "\n",
    "Finally let's generate our answers. Already for this very simple example, we can make use of qstn to use different ways of prompting the questionnaire.\n",
    "\n",
    "First, let's ask each question in a new context by creating a new request for each single item in our questionnaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import survey_manager\n",
    "\n",
    "results = survey_manager.conduct_survey_single_item(\n",
    "    chat_generator,\n",
    "    questionnaire,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    # We can use the same inference arguments for inference, as we would for vllm or OpenAI\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98d65b",
   "metadata": {},
   "source": [
    "This gives us two conversations as output on our command line:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af73da49",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-- System Message --\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "-- User Message ---\n",
    "Please tell us how you feel about the following parties:\n",
    "The Democratic Party?\n",
    "-- Generated Message --\n",
    "Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?\n",
    "\n",
    "-- System Message --\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "-- User Message ---\n",
    "Please tell us how you feel about the following parties:\n",
    "The Republican Party?\n",
    "-- Generated Message --\n",
    "Da Republican Party? Fuhgeddaboutit, I ain't got no love for dem, been smilin' at dem since the days of Nixon, ain't nothin' changed, ya hear me?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a246e",
   "metadata": {},
   "source": [
    "QSTN can easily convert the output into a ``pd.Dataframe``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import parser\n",
    "\n",
    "parsed_results = parser.raw_responses(results)\n",
    "df = parsed_results[questionnaire]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c76db4",
   "metadata": {},
   "source": [
    "Which gives us the following:\n",
    "\n",
    "|questionnaire_item_id|question|llm_response|logprobs|reasoning|\n",
    "|-----------------|--------|------------|--------|---------|\n",
    "|1                |The Democratic Party?|Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?|        |         |\n",
    "|2                |The Republican Party?|Da Republican Party? Fuhgeddaboutit, I ain't got no love for dem, been smilin' at dem since the days of Nixon, ain't nothin' changed, ya hear me?|        |         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294239eb",
   "metadata": {},
   "source": [
    "We can also prompt the model to keep the previous questions and answers in a sequential manner, so that all questions are kept in the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d79c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = survey_manager.conduct_survey_sequential(\n",
    "    chat_generator,\n",
    "    questionnaire,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fc2226b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "System Prompt:\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "User Message:\n",
    "Please tell us how you feel about the following parties:\n",
    "The Democratic Party?\n",
    "Assistant Message:\n",
    "Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?\n",
    "User Message:\n",
    "The Republican Party?\n",
    "Generated Answer:\n",
    "Da Republican Party? Fuhgeddaboutit, dey ain't got nothin' but hate for da people, and dat's not somethin' I can get behind, know what I mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb462a7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Or ask all questions as a battery, which means all questions are presented in one prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = survey_manager.conduct_survey_battery(\n",
    "    chat_generator,\n",
    "    questionnaire,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6db582e0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-- System Message --\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "-- User Message ---\n",
    "Please tell us how you feel about the following parties:\n",
    "The Democratic Party?\n",
    "The Republican Party?\n",
    "-- Generated Message --\n",
    "Da Democratic Party's where it's at, ya hear me?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81637772",
   "metadata": {},
   "source": [
    "For all variations the we can use the same method to parse the output. \n",
    "\n",
    "## Multiple Prompts/Personas\n",
    "\n",
    "If we want to more personas or different prompts with efficient batching we simply have to add a new questionnaire as a list, the rest of the code stays the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e186365",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_texas = \"Act as if you were a white middle aged man from Texas! Answer in a single short sentence!\"\n",
    "\n",
    "texas_questionnaire = LLMPrompt(\n",
    "    questionnaire_name=\"Texas\",\n",
    "    questionnaire_source=party_questionnaire,\n",
    "    system_prompt=system_prompt_texas,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "both_questionnaires = [questionnaire, texas_questionnaire]\n",
    "\n",
    "results = survey_manager.conduct_survey_single_item(\n",
    "    chat_generator,\n",
    "    both_questionnaires,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")\n",
    "\n",
    "parsed_results = parser.raw_responses(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543cd5c",
   "metadata": {},
   "source": [
    "We can get all results in one dataframe with a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "df_both = create_one_dataframe(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904c110",
   "metadata": {},
   "source": [
    "|questionnaire_name|questionnaire_item_id|question|llm_response|logprobs|reasoning|\n",
    "|-----------|-----------------|--------|------------|--------|---------|\n",
    "|political_parties|1                |The Democratic Party?|Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?|        |         |\n",
    "|political_parties|2                |The Republican Party?|Da Republican Party? Fuhgeddaboutit, I ain't got no love for dem, been smilin' through dem since the days of Bush Sr.!|        |         |\n",
    "|Texas      |1                |The Democratic Party?|Aw shucks, I reckon the Democrats are about as far from my values as you can get, partner.|        |         |\n",
    "|Texas      |2                |The Republican Party?|I reckon I'm a proud Republican, y'all!|        |         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qstn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
