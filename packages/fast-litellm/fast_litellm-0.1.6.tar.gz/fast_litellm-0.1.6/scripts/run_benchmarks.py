#!/usr/bin/env python3
"""
Unified benchmark runner for Fast LiteLLM.

This script runs all benchmarks and generates a BENCHMARK.md report file.
It combines:
- Token counting benchmarks (Python vs Rust, shimmed vs direct)
- Comprehensive shimmed function benchmarks
- Before/after shimming comparison

Usage:
    python scripts/run_benchmarks.py
    python scripts/run_benchmarks.py --iterations 50 --output benchmark_results.json
"""

import argparse
import concurrent.futures
import gc
import json
import os
import platform
import statistics
import subprocess
import sys
import threading
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

try:
    import tracemalloc

    HAS_TRACEMALLOC = True
except ImportError:
    HAS_TRACEMALLOC = False

try:
    import psutil

    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False


@dataclass
class MemoryMetrics:
    """Memory usage metrics"""

    peak_memory_mb: float = 0.0
    current_memory_mb: float = 0.0
    memory_diff_mb: float = 0.0


@dataclass
class BenchmarkResult:
    """Represents the result of a single benchmark run"""

    name: str
    function: str
    iterations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    std_dev: float
    throughput: float
    config: Dict[str, Any]
    memory: Optional[MemoryMetrics] = None


def get_memory_usage_mb() -> float:
    """Get current memory usage in MB"""
    if HAS_PSUTIL:
        process = psutil.Process()
        return process.memory_info().rss / (1024 * 1024)
    return 0.0


def start_memory_tracking():
    """Start memory tracking"""
    gc.collect()
    if HAS_TRACEMALLOC:
        tracemalloc.start()
    return get_memory_usage_mb()


def stop_memory_tracking(start_memory: float) -> MemoryMetrics:
    """Stop memory tracking and return metrics"""
    current_memory = get_memory_usage_mb()
    peak_memory = current_memory

    if HAS_TRACEMALLOC:
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        peak_memory = peak / (1024 * 1024)

    return MemoryMetrics(
        peak_memory_mb=round(peak_memory, 2),
        current_memory_mb=round(current_memory, 2),
        memory_diff_mb=round(current_memory - start_memory, 2),
    )


@dataclass
class ComparisonResult:
    """Represents a comparison between baseline and accelerated results"""

    function: str
    baseline_avg: float
    accelerated_avg: float
    speedup: float
    improvement_percent: float
    throughput_improvement: float


def get_system_info() -> Dict[str, Any]:
    """Collect system information for the benchmark report"""
    info = {
        "platform": platform.system(),
        "platform_version": platform.version(),
        "architecture": platform.machine(),
        "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
        "processor": platform.processor(),
        "memory_tracking": HAS_PSUTIL or HAS_TRACEMALLOC,
    }

    # Get memory info if available
    if HAS_PSUTIL:
        mem = psutil.virtual_memory()
        info["total_memory_gb"] = round(mem.total / (1024**3), 2)

    # Get Fast LiteLLM info
    try:
        import fast_litellm

        info["fast_litellm_version"] = fast_litellm.__version__
        info["rust_available"] = fast_litellm.RUST_ACCELERATION_AVAILABLE
    except ImportError:
        info["fast_litellm_version"] = "not installed"
        info["rust_available"] = False

    # Get LiteLLM info
    try:
        from importlib.metadata import version

        info["litellm_version"] = version("litellm")
    except Exception:
        info["litellm_version"] = "not installed"

    return info


def create_test_data(size: str = "medium") -> Dict[str, Any]:
    """Create comprehensive test data for all benchmarks

    Args:
        size: One of "small", "medium", "large", "xlarge" to control workload size
    """
    # Base texts of varying sizes
    tiny_text = "Hello, world!"
    small_text = "This is a longer text with more tokens to count. " * 5
    medium_text = (
        "Large language models like GPT-4 and Claude are powerful tools for natural language processing. They can understand context, generate coherent responses, and assist with complex tasks. "
        * 20
    )
    large_text = "The quick brown fox jumps over the lazy dog. " * 200  # ~1000 words
    xlarge_text = (
        "Artificial intelligence is transforming every industry. " * 1000
    )  # ~5000 words

    # Size configurations
    size_configs = {
        "small": {
            "texts": [tiny_text, small_text],
            "num_models": 3,
            "num_deployments": 5,
            "rate_limit_keys": 10,
            "connection_endpoints": 5,
        },
        "medium": {
            "texts": [tiny_text, small_text, medium_text],
            "num_models": 5,
            "num_deployments": 20,
            "rate_limit_keys": 50,
            "connection_endpoints": 10,
        },
        "large": {
            "texts": [small_text, medium_text, large_text],
            "num_models": 10,
            "num_deployments": 50,
            "rate_limit_keys": 100,
            "connection_endpoints": 25,
        },
        "xlarge": {
            "texts": [medium_text, large_text, xlarge_text],
            "num_models": 20,
            "num_deployments": 100,
            "rate_limit_keys": 500,
            "connection_endpoints": 50,
        },
    }

    config = size_configs.get(size, size_configs["medium"])

    # Generate model names
    base_models = [
        "gpt-3.5-turbo",
        "gpt-4",
        "gpt-4-turbo",
        "claude-3-sonnet",
        "claude-3-opus",
        "claude-3-haiku",
        "gemini-pro",
        "llama-2-70b",
        "mistral-7b",
        "mixtral-8x7b",
    ]
    models = base_models[: config["num_models"]]

    # Generate model list with multiple deployments per model
    model_list = []
    for i in range(config["num_deployments"]):
        model_name = models[i % len(models)]
        model_list.append(
            {
                "model_name": model_name,
                "litellm_params": {
                    "model": model_name,
                    "api_base": f"https://api-{i % 5}.example.com",
                },
                "model_info": {
                    "id": f"deployment-{i}",
                    "max_tokens": 4096 + (i * 100),
                },
            }
        )

    return {
        "texts": config["texts"],
        "models": models,
        "model_list": model_list,
        "text_model_pairs": [(t, m) for t in config["texts"] for m in models],
        "rate_limit_keys": config["rate_limit_keys"],
        "connection_endpoints": config["connection_endpoints"],
        "size": size,
        "config": config,
    }


def run_python_baseline_subprocess(
    pairs: List[Tuple[str, str]], iterations: int
) -> Dict[str, Any]:
    """Run Python baseline token counting in a subprocess to avoid shimming contamination."""
    # Create a temporary script
    script = f"""
import time
import json
import statistics

pairs = {pairs!r}
iterations = {iterations}
times = []

import litellm

# Warmup
for text, model in pairs[:3]:
    try:
        litellm.token_counter(model=model, messages=[{{"role": "user", "content": text}}])
    except Exception:
        len(text.split())

# Benchmark
for _ in range(iterations):
    start = time.perf_counter()
    for text, model in pairs:
        try:
            litellm.token_counter(model=model, messages=[{{"role": "user", "content": text}}])
        except Exception:
            len(text.split())
    times.append(time.perf_counter() - start)

result = {{
    "avg_time": statistics.mean(times),
    "min_time": min(times),
    "max_time": max(times),
    "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
    "total_time": sum(times),
}}
print(json.dumps(result))
"""
    try:
        result = subprocess.run(
            [sys.executable, "-c", script],
            capture_output=True,
            text=True,
            timeout=300,
        )
        if result.returncode == 0:
            return json.loads(result.stdout.strip())
        else:
            return {"error": result.stderr}
    except Exception as e:
        return {"error": str(e)}


def run_token_counting_benchmark(
    test_data: Dict[str, Any], iterations: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark token counting performance"""
    times = []
    pairs = test_data["text_model_pairs"]
    memory_metrics = None

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            # Create a token counter instance
            counter = fast_litellm.SimpleTokenCounter(4096)

            # Warmup
            for text, model in pairs[:3]:
                try:
                    counter.count_tokens(text, model)
                except Exception:
                    len(text.split())

            # Start memory tracking
            start_mem = start_memory_tracking()

            # Benchmark
            for _ in range(iterations):
                start = time.perf_counter()
                for text, model in pairs:
                    try:
                        counter.count_tokens(text, model)
                    except Exception:
                        len(text.split())
                times.append(time.perf_counter() - start)

            # Stop memory tracking
            memory_metrics = stop_memory_tracking(start_mem)

            name = "Rust Token Counting"
            config = {"implementation": "rust", "rust_available": True}
        except Exception as e:
            return BenchmarkResult(
                name="Rust Token Counting",
                function="count_tokens",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        # Run Python baseline in subprocess to avoid shimming contamination
        result = run_python_baseline_subprocess(pairs, iterations)

        if "error" in result:
            return BenchmarkResult(
                name="Python Token Counting",
                function="token_counter",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": result["error"]},
            )

        total_time = result["total_time"]
        avg_time = result["avg_time"]
        times = [avg_time] * iterations  # Approximate for std_dev calc
        memory_metrics = None  # Can't track memory in subprocess easily

        name = "Python Token Counting"
        config = {"implementation": "python"}

        return BenchmarkResult(
            name=name,
            function="token_counter",
            iterations=iterations,
            total_time=total_time,
            avg_time=avg_time,
            min_time=result["min_time"],
            max_time=result["max_time"],
            std_dev=result["std_dev"],
            throughput=iterations / total_time if total_time > 0 else 0,
            config=config,
            memory=memory_metrics,
        )

    total_time = sum(times)
    avg_time = statistics.mean(times)

    return BenchmarkResult(
        name=name,
        function="token_counter",
        iterations=iterations,
        total_time=total_time,
        avg_time=avg_time,
        min_time=min(times),
        max_time=max(times),
        std_dev=statistics.stdev(times) if len(times) > 1 else 0,
        throughput=iterations / total_time if total_time > 0 else 0,
        config=config,
        memory=memory_metrics,
    )


def run_shimmed_benchmark(
    test_data: Dict[str, Any], iterations: int
) -> BenchmarkResult:
    """Benchmark shimmed LiteLLM functions (actual user experience)"""
    times = []
    pairs = test_data["text_model_pairs"]
    memory_metrics = None

    try:
        # Import fast_litellm first to apply shims
        import litellm

        import fast_litellm

        # Warmup
        for text, model in pairs[:3]:
            try:
                litellm.token_counter(
                    model=model, messages=[{"role": "user", "content": text}]
                )
            except Exception:
                len(text.split())

        # Start memory tracking
        start_mem = start_memory_tracking()

        # Benchmark
        for _ in range(iterations):
            start = time.perf_counter()
            for text, model in pairs:
                try:
                    litellm.token_counter(
                        model=model, messages=[{"role": "user", "content": text}]
                    )
                except Exception:
                    len(text.split())
            times.append(time.perf_counter() - start)

        # Stop memory tracking
        memory_metrics = stop_memory_tracking(start_mem)

        total_time = sum(times)
        avg_time = statistics.mean(times)

        return BenchmarkResult(
            name="Shimmed Token Counting",
            function="token_counter",
            iterations=iterations,
            total_time=total_time,
            avg_time=avg_time,
            min_time=min(times),
            max_time=max(times),
            std_dev=statistics.stdev(times) if len(times) > 1 else 0,
            throughput=iterations / total_time if total_time > 0 else 0,
            config={
                "implementation": "shimmed",
                "rust_available": fast_litellm.RUST_ACCELERATION_AVAILABLE,
            },
            memory=memory_metrics,
        )
    except Exception as e:
        return BenchmarkResult(
            name="Shimmed Token Counting",
            function="token_counter",
            iterations=0,
            total_time=0,
            avg_time=0,
            min_time=0,
            max_time=0,
            std_dev=0,
            throughput=0,
            config={"error": str(e)},
        )


def run_rate_limiter_benchmark(
    test_data: Dict[str, Any], iterations: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark rate limiting operations"""
    times = []
    memory_metrics = None
    num_keys = test_data.get("rate_limit_keys", 10)

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            # Warmup
            for _ in range(3):
                try:
                    fast_litellm.check_rate_limit("benchmark_key")
                except Exception:
                    pass

            # Start memory tracking
            start_mem = start_memory_tracking()

            # Benchmark - check rate limits across many keys
            for _ in range(iterations):
                start = time.perf_counter()
                for i in range(num_keys):
                    try:
                        fast_litellm.check_rate_limit(f"user_{i}_api_key")
                        fast_litellm.check_rate_limit(f"org_{i % 10}_quota")
                        fast_litellm.check_rate_limit(f"model_{i % 5}_limit")
                    except Exception:
                        pass
                times.append(time.perf_counter() - start)

            # Stop memory tracking
            memory_metrics = stop_memory_tracking(start_mem)

            name = "Rust Rate Limiter"
            config = {
                "implementation": "rust",
                "rust_available": True,
                "num_keys": num_keys,
            }
        except Exception as e:
            return BenchmarkResult(
                name="Rust Rate Limiter",
                function="check_rate_limit",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        # Python baseline - production-grade thread-safe rate limiter
        import threading
        import time as time_module
        from collections import defaultdict

        # Thread-safe rate limiter with token bucket + sliding window
        class PythonRateLimiter:
            def __init__(self, requests_per_second=10, burst_size=20):
                self.requests_per_second = requests_per_second
                self.burst_size = burst_size
                self.buckets = {}
                self.sliding_windows = defaultdict(list)
                self.lock = threading.Lock()

            def check(self, key: str) -> dict:
                now = time_module.time()
                with self.lock:
                    # Token bucket check
                    if key not in self.buckets:
                        self.buckets[key] = {
                            "tokens": self.burst_size,
                            "last_update": now,
                        }

                    bucket = self.buckets[key]
                    # Refill tokens
                    elapsed = now - bucket["last_update"]
                    bucket["tokens"] = min(
                        self.burst_size,
                        bucket["tokens"] + elapsed * self.requests_per_second,
                    )
                    bucket["last_update"] = now

                    # Sliding window check (per-minute)
                    window = self.sliding_windows[key]
                    window_start = now - 60.0
                    # Remove old entries
                    self.sliding_windows[key] = [t for t in window if t > window_start]
                    window = self.sliding_windows[key]

                    # Check limits
                    if (
                        bucket["tokens"] < 1
                        or len(window) >= self.requests_per_second * 60
                    ):
                        return {
                            "allowed": False,
                            "reason": "Rate limit exceeded",
                            "remaining": 0,
                        }

                    # Consume token
                    bucket["tokens"] -= 1
                    window.append(now)

                    return {
                        "allowed": True,
                        "reason": "Request allowed",
                        "remaining": int(bucket["tokens"]),
                    }

        limiter = PythonRateLimiter(requests_per_second=10, burst_size=20)

        def python_check_rate_limit(key: str) -> bool:
            """Thread-safe Python rate limiter with token bucket + sliding window"""
            return limiter.check(key)["allowed"]

        # Warmup
        for _ in range(3):
            python_check_rate_limit("benchmark_key")

        # Start memory tracking
        start_mem = start_memory_tracking()

        for _ in range(iterations):
            start = time.perf_counter()
            for i in range(num_keys):
                python_check_rate_limit(f"user_{i}_api_key")
                python_check_rate_limit(f"org_{i % 10}_quota")
                python_check_rate_limit(f"model_{i % 5}_limit")
            times.append(time.perf_counter() - start)

        # Stop memory tracking
        memory_metrics = stop_memory_tracking(start_mem)

        name = "Python Rate Limiter"
        config = {"implementation": "python", "num_keys": num_keys}

    total_time = sum(times)
    avg_time = statistics.mean(times) if times else 0

    return BenchmarkResult(
        name=name,
        function="check_rate_limit",
        iterations=iterations,
        total_time=total_time,
        avg_time=avg_time,
        min_time=min(times) if times else 0,
        max_time=max(times) if times else 0,
        std_dev=statistics.stdev(times) if len(times) > 1 else 0,
        throughput=iterations / total_time if total_time > 0 else 0,
        config=config,
        memory=memory_metrics,
    )


def run_connection_pool_benchmark(
    test_data: Dict[str, Any], iterations: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark connection pool operations"""
    times = []
    memory_metrics = None
    num_endpoints = test_data.get("connection_endpoints", 5)

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            # Warmup
            for _ in range(3):
                try:
                    conn_id = fast_litellm.get_connection("https://api.example.com")
                    if conn_id:
                        fast_litellm.return_connection(conn_id)
                except Exception:
                    pass

            # Start memory tracking
            start_mem = start_memory_tracking()

            # Benchmark - simulate realistic connection pool usage
            for _ in range(iterations):
                start = time.perf_counter()
                connections = []
                # Get connections from multiple endpoints
                for i in range(num_endpoints):
                    try:
                        endpoint = f"https://api{i}.openai.com/v1"
                        conn_id = fast_litellm.get_connection(endpoint)
                        if conn_id:
                            connections.append((endpoint, conn_id))
                    except Exception:
                        pass
                # Return all connections
                for endpoint, conn_id in connections:
                    try:
                        fast_litellm.return_connection(conn_id)
                    except Exception:
                        pass
                times.append(time.perf_counter() - start)

            # Stop memory tracking
            memory_metrics = stop_memory_tracking(start_mem)

            name = "Rust Connection Pool"
            config = {
                "implementation": "rust",
                "rust_available": True,
                "num_endpoints": num_endpoints,
            }
        except Exception as e:
            return BenchmarkResult(
                name="Rust Connection Pool",
                function="connection_pool",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        # Python baseline - production-grade thread-safe connection pool
        import threading
        import time as time_module
        from collections import defaultdict

        class PythonConnectionPool:
            """Thread-safe connection pool with health tracking and lifecycle management."""

            def __init__(self, max_connections_per_endpoint=10, max_idle_time=60.0):
                self.max_connections = max_connections_per_endpoint
                self.max_idle_time = max_idle_time
                self.pools = defaultdict(
                    list
                )  # endpoint -> [(conn_id, created_at, last_used)]
                self.active_connections = {}  # conn_id -> (endpoint, acquired_at)
                self.conn_counter = 0
                self.lock = threading.Lock()
                self.health_status = defaultdict(
                    lambda: {"healthy": True, "last_check": 0}
                )

            def get_connection(self, endpoint: str) -> Optional[str]:
                now = time_module.time()
                with self.lock:
                    # Clean up expired idle connections
                    if endpoint in self.pools:
                        self.pools[endpoint] = [
                            (cid, created, last_used)
                            for cid, created, last_used in self.pools[endpoint]
                            if now - last_used < self.max_idle_time
                        ]

                    # Try to get from pool
                    if self.pools[endpoint]:
                        conn_id, created_at, _ = self.pools[endpoint].pop()
                        self.active_connections[conn_id] = (endpoint, now)
                        return conn_id

                    # Check if we can create a new connection
                    active_count = sum(
                        1
                        for ep, _ in self.active_connections.values()
                        if ep == endpoint
                    )
                    if active_count < self.max_connections:
                        self.conn_counter += 1
                        conn_id = f"conn_{self.conn_counter}"
                        self.active_connections[conn_id] = (endpoint, now)
                        return conn_id

                    return None  # Pool exhausted

            def return_connection(self, conn_id: str):
                now = time_module.time()
                with self.lock:
                    if conn_id in self.active_connections:
                        endpoint, acquired_at = self.active_connections.pop(conn_id)
                        # Return to pool with updated timestamp
                        self.pools[endpoint].append((conn_id, acquired_at, now))

            def health_check(self, endpoint: str) -> bool:
                now = time_module.time()
                with self.lock:
                    status = self.health_status[endpoint]
                    # Only check every 5 seconds
                    if now - status["last_check"] > 5.0:
                        status["last_check"] = now
                        # Simulate health check (always healthy for benchmark)
                        status["healthy"] = True
                    return status["healthy"]

        pool = PythonConnectionPool(max_connections_per_endpoint=10, max_idle_time=60.0)

        def python_get_connection(endpoint: str) -> Optional[str]:
            # Check health first (like Rust does)
            pool.health_check(endpoint)
            return pool.get_connection(endpoint)

        def python_return_connection(endpoint: str, conn_id: str):
            pool.return_connection(conn_id)

        # Warmup
        for _ in range(3):
            conn = python_get_connection("https://api.example.com")
            if conn:
                python_return_connection("https://api.example.com", conn)

        # Start memory tracking
        start_mem = start_memory_tracking()

        for _ in range(iterations):
            start = time.perf_counter()
            connections = []
            # Get connections from multiple endpoints
            for i in range(num_endpoints):
                endpoint = f"https://api{i}.openai.com/v1"
                conn = python_get_connection(endpoint)
                if conn:
                    connections.append((endpoint, conn))
            # Return all connections
            for endpoint, conn in connections:
                python_return_connection(endpoint, conn)
            times.append(time.perf_counter() - start)

        # Stop memory tracking
        memory_metrics = stop_memory_tracking(start_mem)

        name = "Python Connection Pool"
        config = {"implementation": "python", "num_endpoints": num_endpoints}

    total_time = sum(times)
    avg_time = statistics.mean(times) if times else 0

    return BenchmarkResult(
        name=name,
        function="connection_pool",
        iterations=iterations,
        total_time=total_time,
        avg_time=avg_time,
        min_time=min(times) if times else 0,
        max_time=max(times) if times else 0,
        std_dev=statistics.stdev(times) if len(times) > 1 else 0,
        throughput=iterations / total_time if total_time > 0 else 0,
        config=config,
        memory=memory_metrics,
    )


def run_routing_benchmark(
    test_data: Dict[str, Any], iterations: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark routing operations"""
    times = []
    model_list = test_data["model_list"]
    memory_metrics = None

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            # Warmup
            for _ in range(3):
                try:
                    fast_litellm.get_available_deployment(
                        model_list, "gpt-3.5-turbo", [], None, {}
                    )
                except Exception:
                    pass

            # Start memory tracking
            start_mem = start_memory_tracking()

            # Benchmark
            for _ in range(iterations):
                start = time.perf_counter()
                for _ in range(100):  # Multiple calls per iteration
                    try:
                        fast_litellm.get_available_deployment(
                            model_list, "gpt-3.5-turbo", [], None, {}
                        )
                    except Exception:
                        pass
                times.append(time.perf_counter() - start)

            # Stop memory tracking
            memory_metrics = stop_memory_tracking(start_mem)

            name = "Rust Routing"
            config = {"implementation": "rust", "rust_available": True}
        except Exception as e:
            return BenchmarkResult(
                name="Rust Routing",
                function="get_available_deployment",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        # Python baseline - production-grade thread-safe routing with metrics
        import random
        import threading
        import time as time_module
        from collections import defaultdict

        class PythonRouter:
            """Thread-safe router with multiple strategies and metrics tracking."""

            def __init__(self, strategy: str = "simple_shuffle"):
                self.strategy = strategy
                self.metrics = defaultdict(
                    lambda: {
                        "requests": 0,
                        "total_latency": 0.0,
                        "avg_latency": 0.0,
                        "last_used": 0.0,
                        "failures": 0,
                    }
                )
                self.lock = threading.Lock()

            def route(
                self, models, target_model, blocked=None, context=None, settings=None
            ):
                """Route to a deployment using the configured strategy."""
                blocked = blocked or []
                with self.lock:
                    # Filter by model name
                    available = [m for m in models if m["model_name"] == target_model]

                    # Filter out blocked deployments
                    if blocked:
                        available = [m for m in available if m not in blocked]

                    # Fall back to all models if none match
                    if not available:
                        available = models

                    if not available:
                        return None

                    # Select based on strategy
                    if self.strategy == "simple_shuffle":
                        selected = random.choice(available)
                    elif self.strategy == "least_busy":
                        # Select deployment with fewest active requests
                        selected = min(
                            available,
                            key=lambda m: self.metrics[
                                m.get("model_info", {}).get("id", "unknown")
                            ]["requests"],
                        )
                    elif self.strategy == "latency_based":
                        # Select deployment with lowest average latency
                        selected = min(
                            available,
                            key=lambda m: self.metrics[
                                m.get("model_info", {}).get("id", "unknown")
                            ]["avg_latency"],
                        )
                    elif self.strategy == "cost_based":
                        # Select deployment with lowest cost (simulated by max_tokens)
                        selected = min(
                            available,
                            key=lambda m: m.get("model_info", {}).get(
                                "max_tokens", 4096
                            ),
                        )
                    else:
                        selected = random.choice(available)

                    # Update metrics
                    if selected:
                        key = selected.get("model_info", {}).get("id", "unknown")
                        self.metrics[key]["requests"] += 1
                        self.metrics[key]["last_used"] = time_module.time()

                    return selected

            def record_latency(self, deployment_id: str, latency: float):
                """Record latency for a deployment."""
                with self.lock:
                    metrics = self.metrics[deployment_id]
                    metrics["total_latency"] += latency
                    if metrics["requests"] > 0:
                        metrics["avg_latency"] = (
                            metrics["total_latency"] / metrics["requests"]
                        )

        router = PythonRouter(strategy="simple_shuffle")

        def python_route(models, target_model, blocked, context, settings):
            return router.route(models, target_model, blocked, context, settings)

        # Warmup
        for _ in range(3):
            python_route(model_list, "gpt-3.5-turbo", [], None, {})

        # Start memory tracking
        start_mem = start_memory_tracking()

        for _ in range(iterations):
            start = time.perf_counter()
            for _ in range(100):
                python_route(model_list, "gpt-3.5-turbo", [], None, {})
            times.append(time.perf_counter() - start)

        # Stop memory tracking
        memory_metrics = stop_memory_tracking(start_mem)

        name = "Python Routing"
        config = {"implementation": "python"}

    total_time = sum(times)
    avg_time = statistics.mean(times) if times else 0

    return BenchmarkResult(
        name=name,
        function="get_available_deployment",
        iterations=iterations,
        total_time=total_time,
        avg_time=avg_time,
        min_time=min(times) if times else 0,
        max_time=max(times) if times else 0,
        std_dev=statistics.stdev(times) if len(times) > 1 else 0,
        throughput=iterations / total_time if total_time > 0 else 0,
        config=config,
        memory=memory_metrics,
    )


def run_concurrent_rate_limiter_benchmark(
    test_data: Dict[str, Any], iterations: int, num_threads: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark rate limiting under concurrent access"""
    num_keys = test_data.get("rate_limit_keys", 10)
    ops_per_thread = iterations // num_threads

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            def rust_worker(thread_id: int) -> float:
                start = time.perf_counter()
                for i in range(ops_per_thread):
                    for j in range(10):  # 10 checks per iteration
                        try:
                            fast_litellm.check_rate_limit(
                                f"user_{(thread_id * 100 + i) % num_keys}"
                            )
                        except Exception:
                            pass
                return time.perf_counter() - start

            # Warmup
            for _ in range(3):
                fast_litellm.check_rate_limit("warmup_key")

            # Start memory tracking
            start_mem = start_memory_tracking()

            # Run concurrent benchmark
            start_total = time.perf_counter()
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=num_threads
            ) as executor:
                futures = [executor.submit(rust_worker, i) for i in range(num_threads)]
                thread_times = [
                    f.result() for f in concurrent.futures.as_completed(futures)
                ]
            total_time = time.perf_counter() - start_total

            memory_metrics = stop_memory_tracking(start_mem)

            name = f"Rust Rate Limiter ({num_threads} threads)"
            config = {
                "implementation": "rust",
                "threads": num_threads,
                "num_keys": num_keys,
            }

        except Exception as e:
            return BenchmarkResult(
                name=f"Rust Rate Limiter ({num_threads} threads)",
                function="check_rate_limit_concurrent",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        import time as time_module

        rate_limits = {}
        lock = threading.Lock()

        def python_check_rate_limit(key: str) -> bool:
            now = time_module.time()
            with lock:
                if key not in rate_limits:
                    rate_limits[key] = {"count": 0, "window_start": now}
                window = rate_limits[key]
                if now - window["window_start"] > 1.0:
                    window["count"] = 0
                    window["window_start"] = now
                window["count"] += 1
                return window["count"] <= 10

        def python_worker(thread_id: int) -> float:
            start = time.perf_counter()
            for i in range(ops_per_thread):
                for j in range(10):
                    python_check_rate_limit(f"user_{(thread_id * 100 + i) % num_keys}")
            return time.perf_counter() - start

        # Warmup
        for _ in range(3):
            python_check_rate_limit("warmup_key")

        start_mem = start_memory_tracking()

        start_total = time.perf_counter()
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(python_worker, i) for i in range(num_threads)]
            thread_times = [
                f.result() for f in concurrent.futures.as_completed(futures)
            ]
        total_time = time.perf_counter() - start_total

        memory_metrics = stop_memory_tracking(start_mem)

        name = f"Python Rate Limiter ({num_threads} threads)"
        config = {
            "implementation": "python",
            "threads": num_threads,
            "num_keys": num_keys,
        }

    total_ops = iterations * 10  # 10 checks per iteration
    throughput = total_ops / total_time if total_time > 0 else 0

    return BenchmarkResult(
        name=name,
        function="check_rate_limit_concurrent",
        iterations=iterations,
        total_time=total_time,
        avg_time=total_time / iterations if iterations > 0 else 0,
        min_time=min(thread_times) if thread_times else 0,
        max_time=max(thread_times) if thread_times else 0,
        std_dev=statistics.stdev(thread_times) if len(thread_times) > 1 else 0,
        throughput=throughput,
        config=config,
        memory=memory_metrics,
    )


def run_concurrent_connection_pool_benchmark(
    test_data: Dict[str, Any], iterations: int, num_threads: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark connection pool under concurrent access"""
    num_endpoints = test_data.get("connection_endpoints", 5)
    ops_per_thread = iterations // num_threads

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            def rust_worker(thread_id: int) -> float:
                start = time.perf_counter()
                for i in range(ops_per_thread):
                    endpoint = (
                        f"https://api{(thread_id + i) % num_endpoints}.example.com"
                    )
                    try:
                        conn_id = fast_litellm.get_connection(endpoint)
                        if conn_id:
                            # Simulate some work
                            _ = hash(conn_id)
                            fast_litellm.return_connection(conn_id)
                    except Exception:
                        pass
                return time.perf_counter() - start

            # Warmup
            for _ in range(3):
                conn = fast_litellm.get_connection("https://warmup.example.com")
                if conn:
                    fast_litellm.return_connection(conn)

            start_mem = start_memory_tracking()

            start_total = time.perf_counter()
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=num_threads
            ) as executor:
                futures = [executor.submit(rust_worker, i) for i in range(num_threads)]
                thread_times = [
                    f.result() for f in concurrent.futures.as_completed(futures)
                ]
            total_time = time.perf_counter() - start_total

            memory_metrics = stop_memory_tracking(start_mem)

            name = f"Rust Connection Pool ({num_threads} threads)"
            config = {
                "implementation": "rust",
                "threads": num_threads,
                "num_endpoints": num_endpoints,
            }

        except Exception as e:
            return BenchmarkResult(
                name=f"Rust Connection Pool ({num_threads} threads)",
                function="connection_pool_concurrent",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        pool = {}
        conn_counter = [0]
        lock = threading.Lock()

        def python_get_connection(endpoint: str) -> Optional[str]:
            with lock:
                if endpoint not in pool:
                    pool[endpoint] = []
                if pool[endpoint]:
                    return pool[endpoint].pop()
                conn_counter[0] += 1
                return f"conn_{conn_counter[0]}"

        def python_return_connection(endpoint: str, conn_id: str):
            with lock:
                if endpoint not in pool:
                    pool[endpoint] = []
                pool[endpoint].append(conn_id)

        def python_worker(thread_id: int) -> float:
            start = time.perf_counter()
            for i in range(ops_per_thread):
                endpoint = f"https://api{(thread_id + i) % num_endpoints}.example.com"
                conn = python_get_connection(endpoint)
                if conn:
                    _ = hash(conn)
                    python_return_connection(endpoint, conn)
            return time.perf_counter() - start

        # Warmup
        for _ in range(3):
            conn = python_get_connection("https://warmup.example.com")
            if conn:
                python_return_connection("https://warmup.example.com", conn)

        start_mem = start_memory_tracking()

        start_total = time.perf_counter()
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(python_worker, i) for i in range(num_threads)]
            thread_times = [
                f.result() for f in concurrent.futures.as_completed(futures)
            ]
        total_time = time.perf_counter() - start_total

        memory_metrics = stop_memory_tracking(start_mem)

        name = f"Python Connection Pool ({num_threads} threads)"
        config = {
            "implementation": "python",
            "threads": num_threads,
            "num_endpoints": num_endpoints,
        }

    throughput = iterations / total_time if total_time > 0 else 0

    return BenchmarkResult(
        name=name,
        function="connection_pool_concurrent",
        iterations=iterations,
        total_time=total_time,
        avg_time=total_time / iterations if iterations > 0 else 0,
        min_time=min(thread_times) if thread_times else 0,
        max_time=max(thread_times) if thread_times else 0,
        std_dev=statistics.stdev(thread_times) if len(thread_times) > 1 else 0,
        throughput=throughput,
        config=config,
        memory=memory_metrics,
    )


def run_concurrent_routing_benchmark(
    test_data: Dict[str, Any], iterations: int, num_threads: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark routing under concurrent access"""
    model_list = test_data["model_list"]
    models = test_data["models"]
    ops_per_thread = iterations // num_threads

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            def rust_worker(thread_id: int) -> float:
                start = time.perf_counter()
                for i in range(ops_per_thread):
                    model = models[(thread_id + i) % len(models)]
                    try:
                        fast_litellm.get_available_deployment(
                            model_list, model, [], None, {}
                        )
                    except Exception:
                        pass
                return time.perf_counter() - start

            # Warmup
            for _ in range(3):
                fast_litellm.get_available_deployment(
                    model_list, models[0], [], None, {}
                )

            start_mem = start_memory_tracking()

            start_total = time.perf_counter()
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=num_threads
            ) as executor:
                futures = [executor.submit(rust_worker, i) for i in range(num_threads)]
                thread_times = [
                    f.result() for f in concurrent.futures.as_completed(futures)
                ]
            total_time = time.perf_counter() - start_total

            memory_metrics = stop_memory_tracking(start_mem)

            name = f"Rust Routing ({num_threads} threads)"
            config = {
                "implementation": "rust",
                "threads": num_threads,
                "num_deployments": len(model_list),
            }

        except Exception as e:
            return BenchmarkResult(
                name=f"Rust Routing ({num_threads} threads)",
                function="routing_concurrent",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        import random

        lock = threading.Lock()
        # Track metrics like Rust does
        metrics = {}

        def python_route_with_metrics(model_list, target_model):
            with lock:
                available = [m for m in model_list if m["model_name"] == target_model]
                if not available:
                    available = model_list
                selected = random.choice(available) if available else None
                # Update metrics
                if selected:
                    key = selected.get("model_info", {}).get("id", "unknown")
                    if key not in metrics:
                        metrics[key] = {"count": 0, "latency_sum": 0}
                    metrics[key]["count"] += 1
                return selected

        def python_worker(thread_id: int) -> float:
            start = time.perf_counter()
            for i in range(ops_per_thread):
                model = models[(thread_id + i) % len(models)]
                python_route_with_metrics(model_list, model)
            return time.perf_counter() - start

        # Warmup
        for _ in range(3):
            python_route_with_metrics(model_list, models[0])

        start_mem = start_memory_tracking()

        start_total = time.perf_counter()
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(python_worker, i) for i in range(num_threads)]
            thread_times = [
                f.result() for f in concurrent.futures.as_completed(futures)
            ]
        total_time = time.perf_counter() - start_total

        memory_metrics = stop_memory_tracking(start_mem)

        name = f"Python Routing ({num_threads} threads)"
        config = {
            "implementation": "python",
            "threads": num_threads,
            "num_deployments": len(model_list),
        }

    throughput = iterations / total_time if total_time > 0 else 0

    return BenchmarkResult(
        name=name,
        function="routing_concurrent",
        iterations=iterations,
        total_time=total_time,
        avg_time=total_time / iterations if iterations > 0 else 0,
        min_time=min(thread_times) if thread_times else 0,
        max_time=max(thread_times) if thread_times else 0,
        std_dev=statistics.stdev(thread_times) if len(thread_times) > 1 else 0,
        throughput=throughput,
        config=config,
        memory=memory_metrics,
    )


def run_memory_pressure_token_benchmark(
    iterations: int, num_texts: int, text_size: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark token counting under memory pressure (many large texts)."""
    times = []
    memory_metrics = None

    # Generate large texts
    texts = [
        f"This is a test text that will be tokenized multiple times for memory testing. Token {i}. "
        * (text_size // 100)
        for i in range(num_texts)
    ]

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            counter = fast_litellm.SimpleTokenCounter(4096)

            # Warmup
            for text in texts[:3]:
                counter.count_tokens(text, "gpt-4")

            # Start memory tracking
            start_mem = start_memory_tracking()

            for _ in range(iterations):
                start = time.perf_counter()
                for text in texts:
                    counter.count_tokens(text, "gpt-4")
                times.append(time.perf_counter() - start)

            memory_metrics = stop_memory_tracking(start_mem)

            name = f"Rust Memory Pressure ({num_texts} x {text_size} chars)"
            config = {
                "implementation": "rust",
                "num_texts": num_texts,
                "text_size": text_size,
            }

        except Exception as e:
            return BenchmarkResult(
                name=f"Rust Memory Pressure ({num_texts} texts)",
                function="memory_pressure_tokens",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        try:
            import tiktoken

            # Use the same encoding as our Rust impl for fairness
            enc = tiktoken.get_encoding("cl100k_base")

            # Warmup
            for text in texts[:3]:
                enc.encode(text)

            start_mem = start_memory_tracking()

            for _ in range(iterations):
                start = time.perf_counter()
                for text in texts:
                    enc.encode(text)
                times.append(time.perf_counter() - start)

            memory_metrics = stop_memory_tracking(start_mem)

            name = f"Python Memory Pressure ({num_texts} x {text_size} chars)"
            config = {
                "implementation": "python",
                "num_texts": num_texts,
                "text_size": text_size,
            }

        except Exception as e:
            return BenchmarkResult(
                name=f"Python Memory Pressure ({num_texts} texts)",
                function="memory_pressure_tokens",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )

    total_time = sum(times)
    avg_time = statistics.mean(times) if times else 0

    return BenchmarkResult(
        name=name,
        function="memory_pressure_tokens",
        iterations=iterations,
        total_time=total_time,
        avg_time=avg_time,
        min_time=min(times) if times else 0,
        max_time=max(times) if times else 0,
        std_dev=statistics.stdev(times) if len(times) > 1 else 0,
        throughput=iterations / total_time if total_time > 0 else 0,
        config=config,
        memory=memory_metrics,
    )


def run_memory_pressure_rate_limit_benchmark(
    iterations: int, num_keys: int, use_rust: bool
) -> BenchmarkResult:
    """Benchmark rate limiting with many unique keys (memory pressure)."""
    times = []
    memory_metrics = None

    if use_rust:
        try:
            import fast_litellm

            if not fast_litellm.RUST_ACCELERATION_AVAILABLE:
                raise RuntimeError("Rust not available")

            # Warmup
            for i in range(10):
                fast_litellm.check_rate_limit(f"warmup_{i}")

            start_mem = start_memory_tracking()

            for _ in range(iterations):
                start = time.perf_counter()
                for i in range(num_keys):
                    fast_litellm.check_rate_limit(f"unique_key_{i}")
                times.append(time.perf_counter() - start)

            memory_metrics = stop_memory_tracking(start_mem)

            name = f"Rust High-Cardinality Rate Limit ({num_keys} keys)"
            config = {"implementation": "rust", "num_keys": num_keys}

        except Exception as e:
            return BenchmarkResult(
                name=f"Rust High-Cardinality ({num_keys} keys)",
                function="memory_pressure_rate_limit",
                iterations=0,
                total_time=0,
                avg_time=0,
                min_time=0,
                max_time=0,
                std_dev=0,
                throughput=0,
                config={"error": str(e)},
            )
    else:
        import threading
        import time as time_module
        from collections import defaultdict

        # Python rate limiter with sliding window
        class HighCardinalityRateLimiter:
            def __init__(self):
                self.windows = defaultdict(list)
                self.lock = threading.Lock()

            def check(self, key: str) -> bool:
                now = time_module.time()
                with self.lock:
                    window = self.windows[key]
                    window_start = now - 60.0
                    self.windows[key] = [t for t in window if t > window_start]
                    if len(self.windows[key]) >= 60:
                        return False
                    self.windows[key].append(now)
                    return True

        limiter = HighCardinalityRateLimiter()

        # Warmup
        for i in range(10):
            limiter.check(f"warmup_{i}")

        start_mem = start_memory_tracking()

        for _ in range(iterations):
            start = time.perf_counter()
            for i in range(num_keys):
                limiter.check(f"unique_key_{i}")
            times.append(time.perf_counter() - start)

        memory_metrics = stop_memory_tracking(start_mem)

        name = f"Python High-Cardinality Rate Limit ({num_keys} keys)"
        config = {"implementation": "python", "num_keys": num_keys}

    total_time = sum(times)
    avg_time = statistics.mean(times) if times else 0

    return BenchmarkResult(
        name=name,
        function="memory_pressure_rate_limit",
        iterations=iterations,
        total_time=total_time,
        avg_time=avg_time,
        min_time=min(times) if times else 0,
        max_time=max(times) if times else 0,
        std_dev=statistics.stdev(times) if len(times) > 1 else 0,
        throughput=iterations / total_time if total_time > 0 else 0,
        config=config,
        memory=memory_metrics,
    )


def calculate_comparison(
    baseline: BenchmarkResult, accelerated: BenchmarkResult
) -> Optional[ComparisonResult]:
    """Calculate comparison metrics between baseline and accelerated results"""
    if baseline.avg_time == 0 or "error" in baseline.config:
        return None
    if accelerated.avg_time == 0 or "error" in accelerated.config:
        return None

    speedup = baseline.avg_time / accelerated.avg_time
    improvement = ((baseline.avg_time - accelerated.avg_time) / baseline.avg_time) * 100
    throughput_improvement = (
        ((accelerated.throughput - baseline.throughput) / baseline.throughput) * 100
        if baseline.throughput > 0
        else 0
    )

    return ComparisonResult(
        function=baseline.function,
        baseline_avg=baseline.avg_time,
        accelerated_avg=accelerated.avg_time,
        speedup=speedup,
        improvement_percent=improvement,
        throughput_improvement=throughput_improvement,
    )


def generate_markdown_report(
    system_info: Dict[str, Any],
    results: Dict[str, List[BenchmarkResult]],
    comparisons: List[ComparisonResult],
    timestamp: str,
    test_data: Optional[Dict[str, Any]] = None,
) -> str:
    """Generate a markdown report from benchmark results"""
    lines = []

    # Header
    lines.append("# Fast LiteLLM Benchmark Report")
    lines.append("")
    lines.append(f"Generated: {timestamp}")
    lines.append("")

    # System Info
    lines.append("## System Information")
    lines.append("")
    lines.append("| Property | Value |")
    lines.append("|----------|-------|")
    lines.append(f"| Platform | {system_info['platform']} |")
    lines.append(f"| Architecture | {system_info['architecture']} |")
    lines.append(f"| Python Version | {system_info['python_version']} |")
    if "total_memory_gb" in system_info:
        lines.append(f"| Total Memory | {system_info['total_memory_gb']} GB |")
    lines.append(f"| Fast LiteLLM Version | {system_info['fast_litellm_version']} |")
    lines.append(f"| LiteLLM Version | {system_info['litellm_version']} |")
    lines.append(
        f"| Rust Acceleration | {'Available' if system_info['rust_available'] else 'Not Available'} |"
    )
    lines.append(
        f"| Memory Tracking | {'Available' if system_info['memory_tracking'] else 'Not Available'} |"
    )
    lines.append("")

    # Performance Summary
    lines.append("## Performance Summary")
    lines.append("")

    if comparisons:
        lines.append("| Function | Baseline | Accelerated | Speedup | Improvement |")
        lines.append("|----------|----------|-------------|---------|-------------|")

        for comp in comparisons:
            status = "+" if comp.speedup >= 1.0 else ""
            lines.append(
                f"| {comp.function} | {comp.baseline_avg:.6f}s | {comp.accelerated_avg:.6f}s | "
                f"{comp.speedup:.2f}x | {status}{comp.improvement_percent:.1f}% |"
            )
        lines.append("")

        # Overall summary
        avg_speedup = statistics.mean([c.speedup for c in comparisons])
        avg_improvement = statistics.mean([c.improvement_percent for c in comparisons])

        lines.append(f"**Average Speedup:** {avg_speedup:.2f}x")
        lines.append(f"**Average Improvement:** {avg_improvement:.1f}%")
        lines.append("")

        # Performance rating
        if avg_speedup >= 10:
            rating = "Outstanding (10x+ speedup)"
        elif avg_speedup >= 5:
            rating = "Excellent (5x+ speedup)"
        elif avg_speedup >= 2:
            rating = "Very Good (2x+ speedup)"
        elif avg_speedup >= 1.5:
            rating = "Good (1.5x+ speedup)"
        elif avg_speedup >= 1.1:
            rating = "Modest improvement"
        elif avg_speedup >= 1.0:
            rating = "Minimal improvement"
        else:
            rating = "Performance regression"

        lines.append(f"**Performance Rating:** {rating}")
        lines.append("")

    # Detailed Results
    lines.append("## Detailed Results")
    lines.append("")

    for category, benchmark_results in results.items():
        lines.append(f"### {category}")
        lines.append("")
        lines.append(
            "| Benchmark | Iterations | Avg Time | Min Time | Max Time | Std Dev | Throughput |"
        )
        lines.append(
            "|-----------|------------|----------|----------|----------|---------|------------|"
        )

        for result in benchmark_results:
            if "error" in result.config:
                lines.append(
                    f"| {result.name} | - | Error: {result.config['error']} | - | - | - | - |"
                )
            else:
                lines.append(
                    f"| {result.name} | {result.iterations} | {result.avg_time:.6f}s | "
                    f"{result.min_time:.6f}s | {result.max_time:.6f}s | {result.std_dev:.6f}s | "
                    f"{result.throughput:.2f} ops/s |"
                )
        lines.append("")

    # Memory Usage
    lines.append("## Memory Usage")
    lines.append("")
    lines.append("| Benchmark | Peak Memory (MB) | Memory Diff (MB) |")
    lines.append("|-----------|------------------|------------------|")

    for category, benchmark_results in results.items():
        for result in benchmark_results:
            if result.memory and "error" not in result.config:
                lines.append(
                    f"| {result.name} | {result.memory.peak_memory_mb:.2f} | "
                    f"{result.memory.memory_diff_mb:+.2f} |"
                )
    lines.append("")

    # Benchmark Configuration
    lines.append("## Benchmark Configuration")
    lines.append("")
    if test_data:
        config = test_data.get("config", {})
        lines.append(f"- **Workload size:** {test_data.get('size', 'medium')}")
        lines.append(
            f"- **Text samples:** {len(test_data.get('texts', []))} (varying sizes)"
        )
        lines.append(f"- **Models:** {config.get('num_models', 'N/A')}")
        lines.append(f"- **Deployments:** {config.get('num_deployments', 'N/A')}")
        lines.append(f"- **Rate limit keys:** {config.get('rate_limit_keys', 'N/A')}")
        lines.append(
            f"- **Connection endpoints:** {config.get('connection_endpoints', 'N/A')}"
        )
    else:
        lines.append("- **Test data:** Multiple text samples of varying lengths")
        lines.append("- **Models tested:** gpt-3.5-turbo, gpt-4, claude-3-sonnet")
    lines.append("- **Warmup iterations:** 3 per benchmark")
    lines.append("")

    # Component Features
    lines.append("## Component Features")
    lines.append("")
    lines.append("### Token Counting")
    lines.append("- Direct Rust implementation for fast token counting")
    lines.append("- Batch processing support")
    lines.append("- Cost estimation and model limits")
    lines.append("")
    lines.append("### Rate Limiting")
    lines.append("- Thread-safe token bucket algorithm")
    lines.append("- Sliding window counters (per-minute, per-hour)")
    lines.append("- Atomic operations for concurrent access")
    lines.append("")
    lines.append("### Connection Pool")
    lines.append("- Lock-free connection management via DashMap")
    lines.append("- Automatic connection health checks")
    lines.append("- Idle connection cleanup")
    lines.append("")
    lines.append("### Routing")
    lines.append(
        "- Multiple routing strategies (simple_shuffle, least_busy, latency_based, cost_based)"
    )
    lines.append("- Real-time metrics tracking")
    lines.append("- Thread-safe concurrent access")
    lines.append("")

    # Notes
    lines.append("## Notes")
    lines.append("")
    lines.append("### Understanding the Results")
    lines.append("")
    lines.append("- All times are in seconds")
    lines.append("- Speedup > 1.0 means Fast LiteLLM is faster than baseline")
    lines.append("- Throughput is measured in operations per second")
    lines.append("- Results may vary based on system load and hardware")
    lines.append("")
    lines.append("### Implementation Differences")
    lines.append("")
    lines.append("- **Direct Rust** shows raw Rust performance without Python overhead")
    lines.append(
        "- **Shimmed** shows actual user experience (includes monkeypatching overhead)"
    )
    lines.append(
        "- **Python baselines** are production-grade implementations with thread-safety and equivalent features"
    )
    lines.append("")
    lines.append("### Fair Comparison Details")
    lines.append("")
    lines.append("Both Python and Rust implementations include equivalent features:")
    lines.append("")
    lines.append(
        "1. **Token Counting**: Both use tiktoken for accurate BPE tokenization with model-specific encodings."
    )
    lines.append("   Rust uses cached encodings with RwLock optimization.")
    lines.append("")
    lines.append(
        "2. **Rate Limiting**: Both implement token bucket + sliding window counters."
    )
    lines.append("   Python uses `threading.Lock`, Rust uses atomic operations.")
    lines.append("")
    lines.append(
        "3. **Connection Pool**: Both provide thread-safe connection management with health tracking,"
    )
    lines.append("   idle connection cleanup, and max connections per endpoint.")
    lines.append(
        "   Python uses `threading.Lock`, Rust uses DashMap for lock-free concurrent access."
    )
    lines.append("")
    lines.append(
        "4. **Routing**: Both provide multiple strategies (simple_shuffle, least_busy, latency_based, cost_based)"
    )
    lines.append(
        "   with real-time metrics tracking. Python uses `threading.Lock`, Rust uses DashMap."
    )
    lines.append("")
    lines.append("### Performance Factors")
    lines.append("")
    lines.append(
        "1. **PyO3 FFI Overhead**: Each PythonRust call has ~1-5s overhead. For micro-operations,"
    )
    lines.append(
        "   this overhead can be significant. For larger operations (token counting),"
    )
    lines.append("   the Rust speedup outweighs the overhead.")
    lines.append("")
    lines.append(
        "2. **Lock-Free vs Lock-Based**: Rust's DashMap provides lock-free concurrent access,"
    )
    lines.append(
        "   which shows greater benefits under high contention (see concurrent benchmarks)."
    )
    lines.append("")
    lines.append(
        "3. **Memory Efficiency**: Rust implementations typically use less memory due to"
    )
    lines.append("   more efficient data structures and no GC overhead.")
    lines.append("")
    lines.append("### When to Use Rust Acceleration")
    lines.append("")
    lines.append(" **Best for:**")
    lines.append("- Connection pooling (3x+ speedup with DashMap)")
    lines.append("- Rate limiting (1.5x+ speedup with atomic operations)")
    lines.append("- Large text token counting (1.5x speedup for longer texts)")
    lines.append(
        "- High-cardinality workloads (40x+ lower memory for many unique keys)"
    )
    lines.append("- Production deployments requiring thread-safety guarantees")
    lines.append("")
    lines.append(" **Consider carefully for:**")
    lines.append("- Small text token counting (Python tiktoken has lower FFI overhead)")
    lines.append("- Routing with Python objects (FFI conversion overhead dominates)")
    lines.append("- Simple single-threaded use cases")
    lines.append("")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="Run all Fast LiteLLM benchmarks and generate BENCHMARK.md"
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=1000,
        help="Number of benchmark iterations (default: 1000)",
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Output file for JSON results",
    )
    parser.add_argument(
        "--markdown",
        type=str,
        default="BENCHMARK.md",
        help="Output file for markdown report (default: BENCHMARK.md)",
    )
    parser.add_argument(
        "--skip-baseline",
        action="store_true",
        help="Skip baseline Python benchmarks",
    )
    parser.add_argument(
        "--size",
        type=str,
        choices=["small", "medium", "large", "xlarge"],
        default="medium",
        help="Workload size: small, medium, large, xlarge (default: medium)",
    )
    parser.add_argument(
        "--threads",
        type=int,
        default=8,
        help="Number of threads for concurrent benchmarks (default: 8)",
    )
    parser.add_argument(
        "--skip-concurrent",
        action="store_true",
        help="Skip concurrent benchmarks",
    )
    parser.add_argument(
        "--skip-memory-pressure",
        action="store_true",
        help="Skip memory pressure benchmarks",
    )
    parser.add_argument(
        "--memory-texts",
        type=int,
        default=100,
        help="Number of texts for memory pressure benchmark (default: 100)",
    )
    parser.add_argument(
        "--memory-text-size",
        type=int,
        default=1000,
        help="Size of each text in chars for memory pressure benchmark (default: 1000)",
    )
    parser.add_argument(
        "--memory-keys",
        type=int,
        default=1000,
        help="Number of unique keys for rate limit memory pressure benchmark (default: 1000)",
    )

    args = parser.parse_args()

    print("=" * 70)
    print("Fast LiteLLM Unified Benchmark Runner")
    print("=" * 70)
    print(f"Iterations: {args.iterations}")
    print(f"Workload size: {args.size}")
    print(f"Markdown output: {args.markdown}")
    if args.output:
        print(f"JSON output: {args.output}")
    print()

    # Collect system info
    print("Collecting system information...")
    system_info = get_system_info()
    print(f"  Platform: {system_info['platform']} ({system_info['architecture']})")
    print(f"  Python: {system_info['python_version']}")
    print(f"  Fast LiteLLM: {system_info['fast_litellm_version']}")
    print(f"  Rust Available: {system_info['rust_available']}")
    print()

    # Create test data with specified size
    test_data = create_test_data(size=args.size)
    config = test_data["config"]
    print(f"Workload configuration ({args.size}):")
    print(f"  Text samples: {len(test_data['texts'])} (sizes vary by workload)")
    print(f"  Models: {config['num_models']}")
    print(f"  Deployments: {config['num_deployments']}")
    print(f"  Rate limit keys: {config['rate_limit_keys']}")
    print(f"  Connection endpoints: {config['connection_endpoints']}")
    print(f"  Text/model pairs: {len(test_data['text_model_pairs'])}")
    print()

    # Run benchmarks
    results: Dict[str, List[BenchmarkResult]] = {}
    comparisons: List[ComparisonResult] = []

    # Token Counting Benchmarks
    print("Running Token Counting Benchmarks...")
    print("-" * 50)

    token_results = []

    if not args.skip_baseline:
        print("  Python baseline...")
        python_token = run_token_counting_benchmark(
            test_data, args.iterations, use_rust=False
        )
        token_results.append(python_token)
        print(
            f"    Avg: {python_token.avg_time:.6f}s, Throughput: {python_token.throughput:.2f} ops/s"
        )

    print("  Rust direct...")
    rust_token = run_token_counting_benchmark(test_data, args.iterations, use_rust=True)
    token_results.append(rust_token)
    if "error" not in rust_token.config:
        print(
            f"    Avg: {rust_token.avg_time:.6f}s, Throughput: {rust_token.throughput:.2f} ops/s"
        )
    else:
        print(f"    Error: {rust_token.config['error']}")

    print("  Shimmed (user experience)...")
    shimmed_token = run_shimmed_benchmark(test_data, args.iterations)
    token_results.append(shimmed_token)
    if "error" not in shimmed_token.config:
        print(
            f"    Avg: {shimmed_token.avg_time:.6f}s, Throughput: {shimmed_token.throughput:.2f} ops/s"
        )
    else:
        print(f"    Error: {shimmed_token.config['error']}")

    results["Token Counting"] = token_results

    # Calculate comparison for token counting
    if not args.skip_baseline and "error" not in python_token.config:
        if "error" not in shimmed_token.config:
            comp = calculate_comparison(python_token, shimmed_token)
            if comp:
                comparisons.append(comp)
                print(f"  Shimmed vs Python: {comp.speedup:.2f}x speedup")

    print()

    # Rate Limiter Benchmarks
    print("Running Rate Limiter Benchmarks...")
    print("-" * 50)

    rate_limiter_results = []

    if not args.skip_baseline:
        print("  Python baseline...")
        python_rate_limiter = run_rate_limiter_benchmark(
            test_data, args.iterations, use_rust=False
        )
        rate_limiter_results.append(python_rate_limiter)
        print(
            f"    Avg: {python_rate_limiter.avg_time:.6f}s, Throughput: {python_rate_limiter.throughput:.2f} ops/s"
        )

    print("  Rust rate limiter (token bucket + sliding window)...")
    rust_rate_limiter = run_rate_limiter_benchmark(
        test_data, args.iterations, use_rust=True
    )
    rate_limiter_results.append(rust_rate_limiter)
    if "error" not in rust_rate_limiter.config:
        print(
            f"    Avg: {rust_rate_limiter.avg_time:.6f}s, Throughput: {rust_rate_limiter.throughput:.2f} ops/s"
        )
    else:
        print(f"    Error: {rust_rate_limiter.config['error']}")

    results["Rate Limiting"] = rate_limiter_results

    # Calculate comparison for rate limiter (shows gains at scale)
    if not args.skip_baseline and "error" not in python_rate_limiter.config:
        if "error" not in rust_rate_limiter.config:
            comp = calculate_comparison(python_rate_limiter, rust_rate_limiter)
            if comp:
                comparisons.append(comp)
                print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")
    print("    Note: Rust provides thread-safe token bucket + sliding window")

    print()

    # Connection Pool Benchmarks
    print("Running Connection Pool Benchmarks...")
    print("-" * 50)

    conn_pool_results = []

    if not args.skip_baseline:
        print("  Python baseline...")
        python_conn_pool = run_connection_pool_benchmark(
            test_data, args.iterations, use_rust=False
        )
        conn_pool_results.append(python_conn_pool)
        print(
            f"    Avg: {python_conn_pool.avg_time:.6f}s, Throughput: {python_conn_pool.throughput:.2f} ops/s"
        )

    print("  Rust connection pool (DashMap + atomic ops)...")
    rust_conn_pool = run_connection_pool_benchmark(
        test_data, args.iterations, use_rust=True
    )
    conn_pool_results.append(rust_conn_pool)
    if "error" not in rust_conn_pool.config:
        print(
            f"    Avg: {rust_conn_pool.avg_time:.6f}s, Throughput: {rust_conn_pool.throughput:.2f} ops/s"
        )
    else:
        print(f"    Error: {rust_conn_pool.config['error']}")

    results["Connection Pool"] = conn_pool_results

    # Calculate comparison for connection pool
    if not args.skip_baseline and "error" not in python_conn_pool.config:
        if "error" not in rust_conn_pool.config:
            comp = calculate_comparison(python_conn_pool, rust_conn_pool)
            if comp:
                comparisons.append(comp)
                print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")
    print("    Note: Rust provides thread-safe DashMap + atomic operations")

    print()

    # Routing Benchmarks (Rust routing provides thread-safe concurrent access)
    print("Running Routing Benchmarks...")
    print("-" * 50)

    routing_results = []

    if not args.skip_baseline:
        print("  Python baseline...")
        python_routing = run_routing_benchmark(
            test_data, args.iterations, use_rust=False
        )
        routing_results.append(python_routing)
        print(
            f"    Avg: {python_routing.avg_time:.6f}s, Throughput: {python_routing.throughput:.2f} ops/s"
        )

    print("  Rust routing (multi-strategy with DashMap)...")
    rust_routing = run_routing_benchmark(test_data, args.iterations, use_rust=True)
    routing_results.append(rust_routing)
    if "error" not in rust_routing.config:
        print(
            f"    Avg: {rust_routing.avg_time:.6f}s, Throughput: {rust_routing.throughput:.2f} ops/s"
        )
    else:
        print(f"    Error: {rust_routing.config['error']}")

    results["Routing"] = routing_results

    # Calculate comparison for routing
    if not args.skip_baseline and "error" not in python_routing.config:
        if "error" not in rust_routing.config:
            comp = calculate_comparison(python_routing, rust_routing)
            if comp:
                comparisons.append(comp)
                print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")
    print("    Note: Rust provides multi-strategy routing with metrics tracking")

    print()

    # Concurrent Benchmarks - This is where Rust shines!
    if not args.skip_concurrent:
        print("=" * 70)
        print(f"CONCURRENT BENCHMARKS ({args.threads} threads)")
        print("=" * 70)
        print("Testing lock-free Rust vs lock-based Python under contention...")
        print()

        # Concurrent Rate Limiter
        print("Running Concurrent Rate Limiter Benchmarks...")
        print("-" * 50)

        concurrent_rate_results = []

        if not args.skip_baseline:
            print(f"  Python baseline ({args.threads} threads, with locks)...")
            python_concurrent_rate = run_concurrent_rate_limiter_benchmark(
                test_data, args.iterations, args.threads, use_rust=False
            )
            concurrent_rate_results.append(python_concurrent_rate)
            print(
                f"    Total time: {python_concurrent_rate.total_time:.4f}s, Throughput: {python_concurrent_rate.throughput:.2f} ops/s"
            )

        print(f"  Rust ({args.threads} threads, lock-free DashMap)...")
        rust_concurrent_rate = run_concurrent_rate_limiter_benchmark(
            test_data, args.iterations, args.threads, use_rust=True
        )
        concurrent_rate_results.append(rust_concurrent_rate)
        if "error" not in rust_concurrent_rate.config:
            print(
                f"    Total time: {rust_concurrent_rate.total_time:.4f}s, Throughput: {rust_concurrent_rate.throughput:.2f} ops/s"
            )
        else:
            print(f"    Error: {rust_concurrent_rate.config['error']}")

        results["Concurrent Rate Limiting"] = concurrent_rate_results

        if not args.skip_baseline and "error" not in python_concurrent_rate.config:
            if "error" not in rust_concurrent_rate.config:
                comp = calculate_comparison(
                    python_concurrent_rate, rust_concurrent_rate
                )
                if comp:
                    comparisons.append(comp)
                    print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")

        print()

        # Concurrent Connection Pool
        print("Running Concurrent Connection Pool Benchmarks...")
        print("-" * 50)

        concurrent_pool_results = []

        if not args.skip_baseline:
            print(f"  Python baseline ({args.threads} threads, with locks)...")
            python_concurrent_pool = run_concurrent_connection_pool_benchmark(
                test_data, args.iterations, args.threads, use_rust=False
            )
            concurrent_pool_results.append(python_concurrent_pool)
            print(
                f"    Total time: {python_concurrent_pool.total_time:.4f}s, Throughput: {python_concurrent_pool.throughput:.2f} ops/s"
            )

        print(f"  Rust ({args.threads} threads, lock-free DashMap)...")
        rust_concurrent_pool = run_concurrent_connection_pool_benchmark(
            test_data, args.iterations, args.threads, use_rust=True
        )
        concurrent_pool_results.append(rust_concurrent_pool)
        if "error" not in rust_concurrent_pool.config:
            print(
                f"    Total time: {rust_concurrent_pool.total_time:.4f}s, Throughput: {rust_concurrent_pool.throughput:.2f} ops/s"
            )
        else:
            print(f"    Error: {rust_concurrent_pool.config['error']}")

        results["Concurrent Connection Pool"] = concurrent_pool_results

        if not args.skip_baseline and "error" not in python_concurrent_pool.config:
            if "error" not in rust_concurrent_pool.config:
                comp = calculate_comparison(
                    python_concurrent_pool, rust_concurrent_pool
                )
                if comp:
                    comparisons.append(comp)
                    print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")

        print()

        # Concurrent Routing
        print("Running Concurrent Routing Benchmarks...")
        print("-" * 50)

        concurrent_routing_results = []

        if not args.skip_baseline:
            print(f"  Python baseline ({args.threads} threads, with locks)...")
            python_concurrent_routing = run_concurrent_routing_benchmark(
                test_data, args.iterations, args.threads, use_rust=False
            )
            concurrent_routing_results.append(python_concurrent_routing)
            print(
                f"    Total time: {python_concurrent_routing.total_time:.4f}s, Throughput: {python_concurrent_routing.throughput:.2f} ops/s"
            )

        print(f"  Rust ({args.threads} threads, lock-free DashMap)...")
        rust_concurrent_routing = run_concurrent_routing_benchmark(
            test_data, args.iterations, args.threads, use_rust=True
        )
        concurrent_routing_results.append(rust_concurrent_routing)
        if "error" not in rust_concurrent_routing.config:
            print(
                f"    Total time: {rust_concurrent_routing.total_time:.4f}s, Throughput: {rust_concurrent_routing.throughput:.2f} ops/s"
            )
        else:
            print(f"    Error: {rust_concurrent_routing.config['error']}")

        results["Concurrent Routing"] = concurrent_routing_results

        if not args.skip_baseline and "error" not in python_concurrent_routing.config:
            if "error" not in rust_concurrent_routing.config:
                comp = calculate_comparison(
                    python_concurrent_routing, rust_concurrent_routing
                )
                if comp:
                    comparisons.append(comp)
                    print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")

        print()

    # Memory Pressure Benchmarks
    if not args.skip_memory_pressure:
        print("=" * 70)
        print("MEMORY PRESSURE BENCHMARKS")
        print("=" * 70)
        print("Testing performance and memory usage under high memory pressure...")
        print()

        # Memory Pressure Token Counting
        print(
            f"Running Memory Pressure Token Counting ({args.memory_texts} x {args.memory_text_size} chars)..."
        )
        print("-" * 50)

        memory_token_results = []
        memory_iterations = min(
            args.iterations, 50
        )  # Fewer iterations for memory tests

        if not args.skip_baseline:
            print(f"  Python tiktoken...")
            python_memory_tokens = run_memory_pressure_token_benchmark(
                memory_iterations,
                args.memory_texts,
                args.memory_text_size,
                use_rust=False,
            )
            memory_token_results.append(python_memory_tokens)
            if "error" not in python_memory_tokens.config:
                print(
                    f"    Avg: {python_memory_tokens.avg_time:.4f}s, Memory: {python_memory_tokens.memory.peak_memory_mb:.2f}MB peak"
                )
            else:
                print(f"    Error: {python_memory_tokens.config['error']}")

        print(f"  Rust tiktoken-rs...")
        rust_memory_tokens = run_memory_pressure_token_benchmark(
            memory_iterations, args.memory_texts, args.memory_text_size, use_rust=True
        )
        memory_token_results.append(rust_memory_tokens)
        if "error" not in rust_memory_tokens.config:
            print(
                f"    Avg: {rust_memory_tokens.avg_time:.4f}s, Memory: {rust_memory_tokens.memory.peak_memory_mb:.2f}MB peak"
            )
        else:
            print(f"    Error: {rust_memory_tokens.config['error']}")

        results["Memory Pressure Token Counting"] = memory_token_results

        if not args.skip_baseline and "error" not in python_memory_tokens.config:
            if "error" not in rust_memory_tokens.config:
                comp = calculate_comparison(python_memory_tokens, rust_memory_tokens)
                if comp:
                    comparisons.append(comp)
                    print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")
                # Memory comparison
                if python_memory_tokens.memory and rust_memory_tokens.memory:
                    mem_ratio = python_memory_tokens.memory.peak_memory_mb / max(
                        rust_memory_tokens.memory.peak_memory_mb, 0.01
                    )
                    print(
                        f"  Memory efficiency: {mem_ratio:.2f}x (Rust uses {'less' if mem_ratio > 1 else 'more'} memory)"
                    )

        print()

        # Memory Pressure Rate Limiting (High Cardinality)
        print(
            f"Running High-Cardinality Rate Limiting ({args.memory_keys} unique keys)..."
        )
        print("-" * 50)

        memory_rate_results = []

        if not args.skip_baseline:
            print(f"  Python baseline...")
            python_memory_rate = run_memory_pressure_rate_limit_benchmark(
                memory_iterations, args.memory_keys, use_rust=False
            )
            memory_rate_results.append(python_memory_rate)
            if "error" not in python_memory_rate.config:
                print(
                    f"    Avg: {python_memory_rate.avg_time:.4f}s, Memory: {python_memory_rate.memory.peak_memory_mb:.2f}MB peak"
                )
            else:
                print(f"    Error: {python_memory_rate.config['error']}")

        print(f"  Rust DashMap...")
        rust_memory_rate = run_memory_pressure_rate_limit_benchmark(
            memory_iterations, args.memory_keys, use_rust=True
        )
        memory_rate_results.append(rust_memory_rate)
        if "error" not in rust_memory_rate.config:
            print(
                f"    Avg: {rust_memory_rate.avg_time:.4f}s, Memory: {rust_memory_rate.memory.peak_memory_mb:.2f}MB peak"
            )
        else:
            print(f"    Error: {rust_memory_rate.config['error']}")

        results["Memory Pressure Rate Limiting"] = memory_rate_results

        if not args.skip_baseline and "error" not in python_memory_rate.config:
            if "error" not in rust_memory_rate.config:
                comp = calculate_comparison(python_memory_rate, rust_memory_rate)
                if comp:
                    comparisons.append(comp)
                    print(f"  Rust vs Python: {comp.speedup:.2f}x speedup")
                # Memory comparison
                if python_memory_rate.memory and rust_memory_rate.memory:
                    mem_ratio = python_memory_rate.memory.peak_memory_mb / max(
                        rust_memory_rate.memory.peak_memory_mb, 0.01
                    )
                    print(
                        f"  Memory efficiency: {mem_ratio:.2f}x (Rust uses {'less' if mem_ratio > 1 else 'more'} memory)"
                    )

        print()

    # Generate timestamp
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

    # Generate markdown report
    print("Generating markdown report...")
    markdown_content = generate_markdown_report(
        system_info, results, comparisons, timestamp, test_data
    )

    # Write markdown file
    markdown_path = Path(args.markdown)
    markdown_path.write_text(markdown_content)
    print(f"  Written to: {markdown_path}")

    # Write JSON results if requested
    if args.output:
        json_results = {
            "timestamp": timestamp,
            "system_info": system_info,
            "results": {
                category: [asdict(r) for r in benchmark_results]
                for category, benchmark_results in results.items()
            },
            "comparisons": [asdict(c) for c in comparisons],
        }

        output_path = Path(args.output)
        output_path.write_text(json.dumps(json_results, indent=2))
        print(f"  JSON written to: {output_path}")

    print()
    print("=" * 70)
    print("Benchmark Complete!")
    print("=" * 70)

    # Print summary
    if comparisons:
        avg_speedup = statistics.mean([c.speedup for c in comparisons])
        print(f"Average Speedup: {avg_speedup:.2f}x")

        for comp in comparisons:
            status = "faster" if comp.speedup >= 1.0 else "slower"
            print(f"  {comp.function}: {comp.speedup:.2f}x {status}")


if __name__ == "__main__":
    main()
