{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e4f949",
   "metadata": {},
   "source": [
    "# Tutorial 2: Verbalized Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d133499",
   "metadata": {},
   "source": [
    "There has been growing evidence that verbalized distribution can achieve high performance when asking LLMs Multiple Choice Questions. QSTN supports this option out of the box. We will show this on a simple example tp see how models predict the 2024 US election."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4f3fe",
   "metadata": {},
   "source": [
    "## Setting up the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc0c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/anaconda3/envs/qstn2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qstn.prompt_builder import LLMPrompt\n",
    "from qstn.utilities import placeholder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt = \"You are an expert political analyst.\"\n",
    "\n",
    "# We can add any state election we want to predict here.\n",
    "elections_to_predict = [\n",
    "    \"2024 US Presidential Election\",\n",
    "    \"2024 United States presidential election in Illinois\",\n",
    "]\n",
    "\n",
    "# The placeholders automatically define at which point of the prompt the questions are asked.\n",
    "formatted_tasks = [\n",
    "    f\"Please predict the outcome of the {election}. {placeholder.PROMPT_OPTIONS} {placeholder.PROMPT_AUTOMATIC_OUTPUT_INSTRUCTIONS} {placeholder.PROMPT_QUESTIONS}\"\n",
    "    for election in elections_to_predict\n",
    "]\n",
    "\n",
    "# If we want to ask multiple questions we can define them here or save them in a csv\n",
    "questionnaire = pd.DataFrame(\n",
    "    [{\"questionnaire_item_id\": 1, \"question_content\": \"Percentage of each Candidate\"}]\n",
    ")\n",
    "\n",
    "interviews: list[LLMPrompt] = []\n",
    "\n",
    "# This creates a system prompt and an instruction for the model, which is not in the system prompt. We also set a seed for reproducibility.\n",
    "for task, election in zip(formatted_tasks, elections_to_predict):\n",
    "    interviews.append(\n",
    "        LLMPrompt(\n",
    "            questionnaire_source=questionnaire,\n",
    "            questionnaire_name=election,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=task,\n",
    "            seed=42,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be13e4",
   "metadata": {},
   "source": [
    "## Using Verbalized Distribution\n",
    "\n",
    "To now get valid verbalized distribution output for our model we need to do two things:\n",
    "\n",
    "1. Define the Response Generation Method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a85bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.inference.response_generation import JSONVerbalizedDistribution\n",
    "\n",
    "# We can also adjut the automatic template to our liking. \n",
    "# If we don't want create an automatic template, we can just not put it into the prompt.\n",
    "response_generation_method = JSONVerbalizedDistribution(\n",
    "    output_template=\"Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves.\",\n",
    "    output_index_only=False, # If we want to save tokens we can output only the index of our answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ece79",
   "metadata": {},
   "source": [
    "2. Define the options the LLM should have when responding. For now we choose 5 candidates that had some chances at the end of LLamas pretraining cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn.prompt_builder import generate_likert_options\n",
    "\n",
    "# Our five most likely candidates and how they are presented to the model\n",
    "options = generate_likert_options(\n",
    "    n=5,\n",
    "    answer_texts=[\"Biden\", \"Trump\", \"Harris\", \"DeSantis\", \"Kennedy\"],\n",
    "    response_generation_method=response_generation_method,\n",
    "    list_prompt_template=\"The candidates are {options}.\", # Our automatic Option Prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abd8b8",
   "metadata": {},
   "source": [
    "Finally we have to prepare the prompt with all the options that we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796e114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for interview in interviews:\n",
    "    interview.prepare_prompt(\n",
    "        question_stem=f\"Please predict the {placeholder.QUESTION_CONTENT} now. The percentage of each candidate should add up to 100%.\",\n",
    "        answer_options=options,\n",
    "        randomized_item_order=True, # We can easily randomize the options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e8e59",
   "metadata": {},
   "source": [
    "And look at the whole prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0522bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt: You are an expert political analyst.\n",
      "Prompt: Please predict the outcome of the 2024 US Presidential Election. The candidates are 1: Biden, 2: Trump, 3: Harris, 4: DeSantis, 5: Kennedy. Respond only in JSON format, where the keys are the names of the candidates and the values are the percentage of votes the candidate achieves. Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%.\n"
     ]
    }
   ],
   "source": [
    "system_prompt, prompt = interviews[0].get_prompt_for_questionnaire_type()\n",
    "\n",
    "print(f\"System Prompt: {system_prompt}\")\n",
    "print(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d1807",
   "metadata": {},
   "source": [
    "And we can run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93a1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 10:26:24 [utils.py:253] non-default args: {'max_model_len': 1000, 'disable_log_stats': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}\n",
      "INFO 12-01 10:26:25 [model.py:631] Resolved architecture: LlamaForCausalLM\n",
      "INFO 12-01 10:26:25 [model.py:1745] Using max model len 1000\n",
      "INFO 12-01 10:26:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 12-01 10:26:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "INFO 12-01 10:26:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://134.155.63.55:56709 backend=nccl\n",
      "INFO 12-01 10:26:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 12-01 10:26:27 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 12-01 10:26:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "INFO 12-01 10:26:27 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "INFO 12-01 10:26:29 [default_loader.py:314] Loading weights took 0.89 seconds\n",
      "INFO 12-01 10:26:29 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.640539 seconds\n",
      "INFO 12-01 10:26:32 [backends.py:631] Using cache directory: /home/maxi/.cache/vllm/torch_compile_cache/4bdcebe47f/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-01 10:26:32 [backends.py:647] Dynamo bytecode transform time: 2.89 s\n",
      "INFO 12-01 10:26:34 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.117 s\n",
      "INFO 12-01 10:26:35 [monitor.py:34] torch.compile takes 4.01 s in total\n",
      "INFO 12-01 10:26:36 [gpu_worker.py:359] Available KV cache memory: 6.87 GiB\n",
      "INFO 12-01 10:26:36 [kv_cache_utils.py:1229] GPU KV cache size: 64,336 tokens\n",
      "INFO 12-01 10:26:36 [kv_cache_utils.py:1234] Maximum concurrency for 1,000 tokens per request: 63.83x\n",
      "INFO 12-01 10:26:40 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 0.50 GiB\n",
      "INFO 12-01 10:26:40 [core.py:250] init engine (profile, create kv cache, warmup model) took 10.80 seconds\n",
      "INFO 12-01 10:26:42 [llm.py:352] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from vllm import LLM\n",
    "\n",
    "# First we create the model\n",
    "model = LLM(\"meta-llama/Llama-3.2-3B-Instruct\", max_model_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d84e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questionnaires:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 10:26:43 [chat_utils.py:557] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 2/2 [00:00<00:00, 956.95it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s, est. speed input: 175.52 toks/s, output: 71.77 toks/s]\n",
      "Processing questionnaires: 100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from qstn.survey_manager import conduct_survey_single_item\n",
    "# Second we run inference\n",
    "results = conduct_survey_single_item(\n",
    "    model,\n",
    "    llm_prompts=interviews,\n",
    "    max_tokens=500,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d3dc6",
   "metadata": {},
   "source": [
    "## Parsing Output\n",
    "\n",
    "We can easily parse the output now, as it is in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455bc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qstn import parser\n",
    "\n",
    "parsed_response = parser.parse_json(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3c30b",
   "metadata": {},
   "source": [
    "We get one DataFrame for each of our Interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b755112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   questionnaire_item_id | question                                                                                                     |   1: Biden |   2: Trump |   3: Harris |   4: DeSantis |   5: Kennedy |\n",
      "|---:|------------------------:|:-------------------------------------------------------------------------------------------------------------|-----------:|-----------:|------------:|--------------:|-------------:|\n",
      "|  0 |                       1 | Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%. |         25 |         40 |           0 |            30 |            5 |\n"
     ]
    }
   ],
   "source": [
    "df = parsed_response[interviews[0]]\n",
    "df2 = parsed_response[interviews[1]]\n",
    "\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378735f",
   "metadata": {},
   "source": [
    "We can also get both answers in a combined df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6148c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | questionnaire_name                                   |   questionnaire_item_id | question                                                                                                     |   1: Biden |   2: Trump |   3: Harris |   4: DeSantis |   5: Kennedy |\n",
      "|---:|:-----------------------------------------------------|------------------------:|:-------------------------------------------------------------------------------------------------------------|-----------:|-----------:|------------:|--------------:|-------------:|\n",
      "|  0 | 2024 US Presidential Election                        |                       1 | Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%. |       25   |       40   |         0   |          30   |          5   |\n",
      "|  1 | 2024 United States presidential election in Illinois |                       1 | Please predict the Percentage of each Candidate now. The percentage of each candidate should add up to 100%. |       42.5 |       35.8 |        12.5 |           8.9 |          0.3 |\n"
     ]
    }
   ],
   "source": [
    "from qstn.utilities import create_one_dataframe\n",
    "\n",
    "df_complete = create_one_dataframe(parsed_response)\n",
    "print(df_complete.to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qstn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
