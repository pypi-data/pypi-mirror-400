# Justfile
# Generated by container-magic: https://github.com/markhedleyjones/container-magic
# Source config: cm.yaml or container-magic.yaml
# Install: pip install container-magic
# Update: cm update
# Config hash: {{ config_hash }}

# Container runtime configuration
RUNTIME := "{{ runtime }}"
IMAGE_NAME := "{{ project_name }}"
WORKSPACE_NAME := "{{ workspace_name }}"
USER_CWD := ""

# Check if configuration has changed and warn user
_check-config force="":
    #!/usr/bin/env bash
    expected="{{ config_hash }}"

    # Find config file (prefer cm.yaml over container-magic.yaml)
    if [ -f "cm.yaml" ]; then
        config_file="cm.yaml"
    elif [ -f "container-magic.yaml" ]; then
        config_file="container-magic.yaml"
    else
        echo "âš ï¸  No config file found (cm.yaml or container-magic.yaml)"
        exit 1
    fi

    current=$(sha256sum "$config_file" | cut -d' ' -f1)
    if [ "$current" != "$expected" ]; then
        # Check if auto_update is enabled in the config file (read at runtime)
        auto_update=$(grep -E '^\s*auto_update:\s*true' "$config_file" || echo "")

        if [ -n "$auto_update" ]; then
            # Auto-update enabled - regenerate files
            echo "ðŸ“ Config changed, regenerating files..."
            cm update
        else
            if [ "{{ force }}" != "--force" ]; then
                echo "âš ï¸  $config_file has changed since last generation"
                echo "   Run 'cm update' to regenerate Justfile and Dockerfile"
                echo "   Or use --force to build anyway"
                echo "   Or set project.auto_update: true in your config"
                echo ""
                exit 1
            fi
        fi
    fi

# Build development image
build force="": (_check-config force)
    {% if use_host_user %}
    {{runtime}} build \
        --target {{ dev_stage }} \
        --build-arg USER_GID=$(id --group) \
        --build-arg USER_UID=$(id --user) \
        --build-arg USER_NAME=$(id --user --name) \
        --build-arg USER_HOME=$(echo ~) \
        --build-arg WORKDIR=$(echo ~) \
        --tag {{ project_name }}:development \
        .
    {% else %}
    {{runtime}} build \
        --target {{ dev_stage }} \
        --build-arg USER_GID={{ dev_user_gid }} \
        --build-arg USER_UID={{ dev_user_uid }} \
        --build-arg USER_NAME={{ dev_user_name }} \
        --build-arg USER_HOME={{ dev_user_home }} \
        --build-arg WORKDIR={{ dev_user_home }} \
        --tag {{ project_name }}:development \
        .
    {% endif %}

# Build production image (if production stage exists)
build-production: _check-config
    {{runtime}} build \
        --target production \
        --tag {{ project_name }}:latest \
        .

# Run command in container
run *args:
    #!/usr/bin/env bash
    set -euo pipefail

    # Container configuration
    CONTAINER_NAME="{{ project_name }}-development"
    IMAGE="{{ project_name }}:development"

    # Build run arguments
    RUN_ARGS=()
    RUN_ARGS+=("{{ runtime }}" "run")
    RUN_ARGS+=("--name" "${CONTAINER_NAME}")
    RUN_ARGS+=("--rm")
    {% if network %}
    RUN_ARGS+=("--net" "{{ network }}")
    {% endif %}

    {% if runtime == "podman" %}
    # Podman-specific configuration
    RUN_ARGS+=("--replace")
    RUN_ARGS+=("--userns=keep-id")
    {% endif %}
    {% if privileged %}

    # Privileged mode
    RUN_ARGS+=("--privileged")
    {% endif %}

    {% if mount_workspace %}
    # Mount workspace
    RUN_ARGS+=("-v" "$(pwd)/{{ workspace_name }}:{{ container_home }}/{{ workspace_name }}:z")
    {% endif %}
    {% if features.display %}

    # Display support (Wayland and X11)
    if [[ -n "${WAYLAND_DISPLAY:-}" ]]; then
        wayland_socket="${XDG_RUNTIME_DIR}/${WAYLAND_DISPLAY}"
        RUN_ARGS+=("-e" "WAYLAND_DISPLAY")
        RUN_ARGS+=("-e" "XDG_RUNTIME_DIR=${XDG_RUNTIME_DIR}")
        RUN_ARGS+=("-v" "${wayland_socket}:${wayland_socket}:rw")
    fi
    if [[ -n "${DISPLAY:-}" ]]; then
        xsock=/tmp/.X11-unix
        xauth=/tmp/.docker.xauth
        touch $xauth
        if [[ -f "$HOME/.Xauthority" ]]; then
            xauth nlist "${DISPLAY}" | sed -e 's/^..../ffff/' | xauth -f "${xauth}" nmerge -
        fi
        RUN_ARGS+=("-e" "DISPLAY")
        RUN_ARGS+=("-e" "XAUTHORITY=${xauth}")
        RUN_ARGS+=("-v" "${xauth}:${xauth}:rw")
        RUN_ARGS+=("-v" "${xsock}:${xsock}:rw")
        RUN_ARGS+=("--env" "QT_X11_NO_MITSHM=1")
    fi
    {% endif %}
    {% if features.gpu %}

    # GPU support
    if [[ -d /dev/dri ]]; then
        RUN_ARGS+=("--device" "/dev/dri:/dev/dri")
    fi
    if command -v nvidia-smi >/dev/null 2>&1; then
        RUN_ARGS+=("-e" "NVIDIA_DRIVER_CAPABILITIES=all")
        {% if runtime == "docker" %}
        RUN_ARGS+=("--gpus=all")
        {% elif runtime == "podman" %}
        RUN_ARGS+=("--device" "nvidia.com/gpu=all")
        RUN_ARGS+=("--annotation=run.oci.keep_original_groups=1")
        RUN_ARGS+=("--security-opt=label=disable")
        {% endif %}
    fi
    {% endif %}
    {% if features.audio %}

    # Audio support (PulseAudio/PipeWire)
    if [[ -S "/run/user/$(id -u)/pulse/native" ]]; then
        RUN_ARGS+=("-v" "/run/user/$(id -u)/pulse/native:/run/user/$(id -u)/pulse/native")
        RUN_ARGS+=("-e" "PULSE_SERVER=unix:/run/user/$(id -u)/pulse/native")
    fi
    {% endif %}
    {% if features.aws_credentials %}

    # AWS credentials
    if [[ -d "$HOME/.aws" ]]; then
        RUN_ARGS+=("-v" "$HOME/.aws:{{ container_home }}/.aws:z")
    fi
    {% endif %}

    # Working directory - translate user's cwd to container path
    if [[ -n "{{ '{{USER_CWD}}' }}" ]]; then
        # Calculate relative path from project root to user's cwd
        PROJECT_ROOT="$(pwd)"
        USER_PATH="{{ '{{USER_CWD}}' }}"

        # Get relative path
        REL_PATH=$(realpath --relative-to="$PROJECT_ROOT" "$USER_PATH" 2>/dev/null || echo "")

        if [[ -n "$REL_PATH" && "$REL_PATH" != "." ]]; then
            # User is in a subdirectory - set container workdir accordingly
            RUN_ARGS+=("--workdir={{ container_home }}/${REL_PATH}")
        else
            # User is at project root
            RUN_ARGS+=("--workdir={{ container_home }}")
        fi
    else
        # No USER_CWD set (called via 'just' directly) - use project root
        RUN_ARGS+=("--workdir={{ container_home }}")
    fi

    # Add TTY flags if available (for real-time output)
    if [[ -t 1 ]]; then
        RUN_ARGS+=("--interactive" "--tty")
    fi

    # Image and command
    RUN_ARGS+=("${IMAGE}")

    # Execute command or shell
    # Note: Just passes args via {{args}} - we need set +u to allow undefined variables in user commands
    set +u
    COMMAND="{{ '{{args}}' }}"
    set -u

    if [[ -n "${COMMAND}" ]]; then
        RUN_ARGS+=("{{ shell }}" "-c" "${COMMAND}")
    else
        RUN_ARGS+=("{{ shell }}")
    fi

    # Check if container already running
    if ! {{ runtime }} ps --quiet --filter name="${CONTAINER_NAME}" | grep -q .; then
        "${RUN_ARGS[@]}"
    else
        # Use exec instead for running container
        EXEC_ARGS=("{{ runtime }}" "exec")
        if [[ -t 1 ]]; then
            EXEC_ARGS+=("--interactive" "--tty")
        fi
        EXEC_ARGS+=("${CONTAINER_NAME}")
        if [[ -n "${COMMAND}" ]]; then
            EXEC_ARGS+=("{{ shell }}" "-c" "${COMMAND}")
        else
            EXEC_ARGS+=("{{ shell }}")
        fi
        "${EXEC_ARGS[@]}"
    fi

# Open interactive shell
shell: _check-config
    @just run

# Stop and remove containers
clean:
    -{{ runtime }} stop {{ project_name }}-development 2>/dev/null
    -{{ runtime }} rm {{ project_name }}-development 2>/dev/null

# Remove images
clean-images:
    -{{ runtime }} rmi {{ project_name }}:development 2>/dev/null
    -{{ runtime }} rmi {{ project_name }}:latest 2>/dev/null
