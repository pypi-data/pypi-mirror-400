# OrKa System Self-Assessment Workflow
# =====================================
# 
# This workflow exercises all major orchestration patterns (fork/join, routing, loops)
# and performs deterministic health checks on the execution.
#
# CRITICAL DESIGN PRINCIPLES:
# 
# 1. SEPARATE WORKER AND JUDGE MODELS
#    - Worker agents (data_preparation, processors, validators): Use standard model
#    - Judge agents (invariant_collector, final_assessment, health_check_gate): Use different model or greedy decoding
#    - This reduces correlated failure where degraded runtime also degrades the evaluator
#
# 2. DETERMINISTIC INVARIANTS BEFORE LLM INTERPRETATION
#    - Hard facts (fork/join integrity, routing validity, cycles, tool errors, schema compliance)
#      are computed deterministically BEFORE the LLM sees them
#    - LLM's job is to EXPLAIN violations, not DISCOVER them
#
# 3. FAIL HARD ON CRITICAL VIOLATIONS
#    - If invariants show critical failures, health check must return FAIL regardless of output quality
#    - If evaluator output is invalid, health check must return FAIL with reason "evaluator_invalid_output"
#
# 4. GREEDY EVALUATION FOR STABILITY
#    - All judge agents use temperature=0.0 for deterministic, reproducible assessments
#
# RECOMMENDED CONFIGURATION:
#   Worker model: openai/gpt-oss-20b at temperature 0.3-0.5
#   Judge model: openai/gpt-oss-20b at temperature 0.0 OR different model (e.g., qwen2.5:14b)

orchestrator:
  id: system-self-assessment
  strategy: sequential
  agents:
    - data_preparation
    - fork_parallel_analysis
    - join_results
    - routing_decision
    - router_reprocess
    - loop_processor
    - validation_check
    - invariant_collector  # NEW: Collect deterministic facts before LLM
    - final_assessment

agents:
  # Phase 1: Prepare test data
  - id: data_preparation
    type: local_llm
    prompt: |
      Generate a structured dataset for system evaluation containing:
      1. Three different computational scenarios (simple, moderate, complex)
      2. Expected outcomes for each scenario
      3. Performance metrics to track (accuracy, consistency, completeness)
      
      Return as JSON with format:
      {
        "scenarios": [
          {"id": 1, "complexity": "simple", "input": "...", "expected": "..."},
          {"id": 2, "complexity": "moderate", "input": "...", "expected": "..."},
          {"id": 3, "complexity": "complex", "input": "...", "expected": "..."}
        ],
        "metrics": ["accuracy", "consistency", "completeness"]
      }
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.3
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
          types:
            response: object

  # Phase 2: Fork - Parallel processing paths
  - id: fork_parallel_analysis
    type: fork
    targets:
      - - path_a_processor
        - path_a_validator
      - - path_b_processor
        - path_b_validator
      - - path_c_processor
        - path_c_validator
    depends_on: data_preparation

  # Path A: Simple processing
  - id: path_a_processor
    type: local_llm
    prompt: |
      Validate and process the dataset structure from data preparation:
      {{ get_agent_response('data_preparation') }}
      
      Analyze all scenarios and return the complete dataset with:
      1. Verification that all scenarios are present
      2. Validation of expected outputs format
      3. Completeness check of metrics array
      4. Return the FULL JSON dataset as-is (do not solve the math problems)
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.5
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
          types:
            response: object

  - id: path_a_validator
    type: local_llm
    prompt: |
      Validate the processing result from path A:
      Input: {{ get_agent_response('path_a_processor') }}
      Expected from dataset: {{ get_agent_response('data_preparation') }}
      
      Compare actual vs expected and rate:
      - Accuracy score (0-100%)
      - Deviation analysis
      - Pass/Fail determination
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.2
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
          types:
            response: object

  # Path B: Moderate processing
  - id: path_b_processor
    type: local_llm
    prompt: |
      Validate dataset structure and consistency from data preparation:
      {{ get_agent_response('data_preparation') }}
      
      Perform structural validation:
      1. Parse and verify JSON syntax
      2. Check scenarios array completeness
      3. Verify metrics array presence
      4. Return the FULL JSON dataset with all scenarios intact (do not compute results)
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.5
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
          types:
            response: object

  - id: path_b_validator
    type: local_llm
    prompt: |
      Validate the processing result from path B:
      Input: {{ get_agent_response('path_b_processor') }}
      Expected from dataset: {{ get_agent_response('data_preparation') }}
      
      Perform deep validation:
      - Step-by-step correctness
      - Logical consistency
      - Accuracy score (0-100%)
      - Error analysis if any
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.2
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
          types:
            response: object

  # Path C: Complex processing
  - id: path_c_processor
    type: local_llm
    prompt: |
      Analyze dataset completeness and quality from data preparation:
      {{ get_agent_response('data_preparation') }}
      
      Perform comprehensive structural analysis:
      1. Multi-level JSON structure validation
      2. Cross-reference scenario complexity levels
      3. Verify expected output field consistency
      4. Validate metrics array integrity
      5. Return the COMPLETE JSON dataset unchanged (preserve all scenarios)
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.5
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
          types:
            response: object

  - id: path_c_validator
    type: local_llm
    prompt: |
      Validate the processing result from path C:
      Input: {{ get_agent_response('path_c_processor') }}
      Expected from dataset: {{ get_agent_response('data_preparation') }}
      
      Perform deep validation:
      - Step-by-step correctness
      - Logical consistency
      - Accuracy score (0-100%)
      - Error analysis if any
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.2
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number
          types:
            response: object

  # Phase 3: Join all parallel paths
  - id: join_results
    type: join
    group: fork_parallel_analysis

  # Phase 4: Router - Decide next action based on results
  - id: routing_decision
    type: local_llm
    prompt: |
      Analyze all validation results from the three paths:
      {{ get_agent_response('join_results') }}
      
      Determine if reprocessing is needed.
      Answer ONLY with "true" or "false":
      - "true" if any path has accuracy < 70% or failed validation
      - "false" if all paths passed with accuracy >= 70%
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.1
    depends_on: join_results
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  - id: router_reprocess
    type: router
    params:
      decision_key: routing_decision
      routing_map:
        true: 
          - reprocess_failed
          - reprocess_validator
        false: aggregation_analyzer
    depends_on: routing_decision

  # Router path - Reprocessing if needed
  - id: reprocess_failed
    type: local_llm
    prompt: |
      Identify and reprocess failed scenarios:
      Original results: {{ get_agent_response('join_results') }}
      Validation decision: {{ get_agent_response('routing_decision') }}
      
      Reanalyze failed scenarios with corrective measures:
      1. Identify failure points
      2. Apply corrections
      3. Recompute results
      4. Provide new confidence scores
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.4
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  - id: reprocess_validator
    type: local_llm
    prompt: |
      Validate reprocessed results:
      Reprocessed: {{ get_agent_response('reprocess_failed') }}
      
      Check if improvements were made:
      - New accuracy scores
      - Improvement delta
      - Final pass/fail status
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.2
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  # Router path - Direct aggregation if all passed
  - id: aggregation_analyzer
    type: local_llm
    prompt: |
      Aggregate successful results from all paths:
      {{ get_agent_response('join_results') }}
      
      Synthesize findings:
      1. Overall accuracy across all paths
      2. Consistency patterns
      3. Performance highlights
      4. Readiness for next phase
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.3
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  # Phase 5: Loop processor - Iterative refinement
  - id: loop_processor
    type: loop
    max_loops: 3
    score_threshold: 0.30
    fallback_score: 0.25
    timeout_score: 0.20
    
    # Boolean scoring for iterative refinement convergence
    scoring:
      preset: lenient
      context: loop_convergence
      mode: loop
    
    # Ensure required upstream signals are present inside LoopNode.previous_outputs.
    # (LoopNode passes only depends_on outputs through as previous_outputs to the internal workflow.)
    #
    # Also depends on BOTH possible router paths (at least one will execute)
    depends_on:
      - join_results
      - routing_decision
      - reprocess_validator
      - aggregation_analyzer
    
    # Pass external data to internal workflow as input
    prompt: |
      Routing Decision: {{ get_agent_response('routing_decision') }}
      Join Results: {{ get_agent_response('join_results') }}
      {% if get_agent_response('reprocess_validator') %}
      Reprocessed Results: {{ get_agent_response('reprocess_validator') }}
      {% endif %}
      {% if get_agent_response('aggregation_analyzer') %}
      Aggregated Results: {{ get_agent_response('aggregation_analyzer') }}
      {% endif %}
    
    internal_workflow:
      orchestrator:
        id: loop-refinement-workflow
        strategy: sequential
        agents:
          - iteration_analyzer
          - synthesis
          - loop_convergence_validator
      
      agents:
        - id: iteration_analyzer
          type: local_llm
          prompt: |
            ITERATION {{ get_loop_number() }} - Refinement Analysis
            
            == DATA TO ANALYZE ==
            {{ get_input() }}
            
            {% if get_loop_number() > 1 %}
            == PREVIOUS ITERATION ANALYSIS ==
            {{ get_past_loops() }}
            {% endif %}
            
            == REFINEMENT TASK ==
            Analyze the execution flow and identify:
            1. Accuracy gaps between expected vs actual results
            2. Error patterns across parallel paths
            3. Validation effectiveness (what passed/failed and why)
            4. Convergence metrics (are results improving?)
            
            For iteration 1:
            - Base accuracy: Extract accuracy scores from validation results
            - Baseline error count: Number of failed validations
            - Initial convergence: 85 (starting point since all validations passed)
            
            For iteration > 1:
            - Compare with previous iteration metrics
            - Calculate improvement delta
            
            Return JSON: {"iteration": {{ get_loop_number() }}, "accuracy_delta": X, "error_count": Y, "improvements": [...], "convergence_score": 0-100}
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          temperature: 0.4
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [iteration, accuracy_delta, error_count, convergence_score]
                optional:
                  improvements: array
                types:
                  iteration: integer
                  accuracy_delta: number
                  error_count: integer
                  convergence_score: number

        - id: synthesis
          type: local_llm
          prompt: |
            SYNTHESIS - Iteration {{ get_loop_number() }}
            
            Analysis: {{ get_agent_response('iteration_analyzer') }}
            
            {% if get_loop_number() > 1 %}
            Previous Synthesis: {{ get_past_loops() }}
            
            Compare current vs previous iteration:
            - Did accuracy improve?
            - Are errors decreasing?
            - Is convergence score increasing?
            {% endif %}
            
            Provide comprehensive assessment:
            1. **Convergence Score (0-100%)**: How close to optimal solution?
            2. **Iteration Delta**: Improvement from last iteration (if iteration > 1)
            3. **Key Improvements**: What got better this iteration?
            4. **Remaining Issues**: What still needs work?
            5. **Recommendation**: Continue iterating or stop?
            
            Return JSON: {"convergence_score": X, "delta_from_previous": Y, "improvements": [...], "issues": [...], "should_continue": boolean}
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          temperature: 0.3
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [convergence_score, delta_from_previous, improvements, issues, should_continue]
                types:
                  convergence_score: number
                  delta_from_previous: number
                  improvements: array
                  issues: array
                  should_continue: boolean

        - id: loop_convergence_validator
          type: loop_validator
          llm_model:  openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          scoring_preset: lenient
          scoring_context: loop_convergence  # Evaluate refinement convergence
          evaluation_target: synthesis
          temperature: 0.1
          prompt: |
            Evaluate convergence of iteration {{ get_loop_number() }}:
            
            Current Synthesis: {{ get_agent_response('synthesis') }}
            
            {% if get_loop_number() > 1 %}
            TAKE DEEP CONSIDERATION OF ALREADY PASSED CHECKS.
            Previous Iterations: {{ get_past_loops() }}
            {% endif %}
            
            Assess these convergence criteria:
            1. **Improvement**: Is this iteration better than the previous?
            2. **Stability**: Are results consistent and not degrading?
            3. **Delta Trend**: Is the rate of improvement decreasing (approaching limit)?
            4. **Target Proximity**: Are we close to the 85% threshold?
            
            Determine if loop should continue or converge.  # Phase 6: Final validation check
  - id: validation_check
    type: local_llm
    prompt: |
      Perform comprehensive validation of all processing stages:
      
      1. Initial data preparation: {{ get_agent_response('data_preparation') }}
      2. Parallel processing (joined): {{ get_agent_response('join_results') }}
      3. Routing decision: {{ get_agent_response('routing_decision') }}
      4. Loop refinement: {{ get_loop_output('loop_processor').past_loops | tojson }}
      
      Validate:
      - Data integrity through all stages
      - Logic consistency
      - Result reliability
      - Error handling effectiveness
      
      Provide validation report with pass/fail for each stage.
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.2
    depends_on: loop_processor
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [reprocessing_needed, validation_report]
          types:
            reprocessing_needed: boolean
            validation_report: object

  # Phase 6.5: Collect deterministic execution invariants
  # This uses the invariant_validator agent with auto-collected execution data
  - id: invariant_collector
    type: invariant_validator
    depends_on: validation_check
    prompt: |
      EXECUTION_DATA_JSON:
      {{ get_execution_artifacts(previous_outputs) | to_json_string }}
    params:
      max_depth: 50  # Allow deep nesting for complex workflows
      allow_reentrant_nodes: [loop_processor]  # Loop nodes can execute multiple times
      strict_tool_errors: false  # Don't fail on tool warnings in this demo

  # Phase 7: Comprehensive final assessment
  - id: final_assessment
    type: local_llm
    prompt: |
      COMPREHENSIVE SYSTEM SELF-ASSESSMENT REPORT
      
      You are a diagnostic agent analyzing orchestrator execution health.
      Your role is INTERPRETATION of hard facts, not discovery of facts.
      
      == HARD EXECUTION INVARIANTS (DETERMINISTIC FACTS) ==
      The following invariants were validated deterministically:
      
      {{ get_agent_response('invariant_collector') }}
      
      These are FACTS, not opinions. If any critical_failures_detected = true,
      the system is objectively broken regardless of output quality.
      
      == EXECUTION DATA (for context only) ==
      Phase 1 - Data Preparation:
      {{ get_agent_response('data_preparation') }}
      
      Phase 2 - Parallel Processing (Fork/Join):
      Path A: {{ get_agent_response('path_a_validator') }}
      Path B: {{ get_agent_response('path_b_validator') }}
      Path C: {{ get_agent_response('path_c_validator') }}
      Joined: {{ get_agent_response('join_results') }}
      
      Phase 3 - Conditional Routing:
      Decision: {{ get_agent_response('routing_decision') }}
      {% if get_agent_response('reprocess_validator') %}
      Reprocessing: {{ get_agent_response('reprocess_validator') }}
      {% else %}
      Aggregation: {{ get_agent_response('aggregation_analyzer') }}
      {% endif %}
      
      Phase 4 - Loop Refinement:
      {{ get_loop_output('loop_processor').past_loops | tojson }}

      Phase 5 - Final Validation:
      {{ get_agent_response('validation_check') }}
      
      == YOUR ASSESSMENT TASK ==
      You must interpret the hard invariants above and provide diagnostic analysis:
      
      CRITICAL RULE: If critical_failures_detected = true in execution_invariants,
      you MUST set overall_system_health_rating to "Poor" regardless of output quality.
      Broken invariants mean the runtime is objectively degraded.
      
      Evaluate these aspects (use invariants as ground truth):
      
      1. **PARALLEL PROCESSING PERFORMANCE**
         - Use fork_join_integrity facts from invariants
         - Did all paths execute? Check fork_join_integrity.status
         - Were results aggregated? Check for violations
      
      2. **CONDITIONAL ROUTING EFFECTIVENESS**
         - Use routing_integrity facts from invariants
         - Did router choose valid target? Check routing_integrity.status
         - Was target reachable? Check for violations
      
      3. **LOOP CONVERGENCE QUALITY**
         - Use cycle_detection facts from invariants
         - Were there unexpected cycles? Check cycle_detection.cycles_found
         - Was convergence genuine or fabricated?
      
      4. **SCHEMA VALIDATION ROBUSTNESS**
         - Use schema_compliance facts from invariants
         - Did outputs match schemas? Check schema_compliance.status
         - Were there format contract violations?
      
      5. **TOOL CALL INTEGRITY**
         - Use tool_integrity facts from invariants
         - Were tool errors propagated? Check tool_integrity.status
         - Was execution allowed to continue after failures?
      
      6. **DEPTH CONSTRAINT COMPLIANCE**
         - Use depth_compliance facts from invariants
         - Did paths respect max_depth? Check depth_compliance.status
      
      == OUTPUT FORMAT (STRICT JSON SCHEMA) ==
      Return ONLY a JSON object with this exact structure:
      {
        "summary": "string: Executive summary explaining invariant violations and their impact",
        "invariant_based_scores": {
          "fork_join_integrity": "number: 0-100 based on fork_join_integrity.status",
          "routing_integrity": "number: 0-100 based on routing_integrity.status",
          "cycle_prevention": "number: 0-100 based on cycle_detection.status",
          "schema_compliance": "number: 0-100 based on schema_compliance.status",
          "tool_integrity": "number: 0-100 based on tool_integrity.status",
          "depth_compliance": "number: 0-100 based on depth_compliance.status"
        },
        "critical_issues": [
          "string: List each FAIL status from invariants with explanation"
        ],
        "warning_issues": [
          "string: List each WARNING from invariants"
        ],
        "recommendations": [
          "string: Specific fixes for each violated invariant"
        ],
        "overall_system_health_rating": "string: Poor if critical_failures_detected, else Good/Fair based on warnings",
        "diagnostic_confidence": "number: 0-100, lower if evaluator itself had errors"
      }
      
      IMPORTANT: Your output will be validated against this schema. If you return
      any other format, the health check FAILS with reason "evaluator_invalid_output".
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.0  # Deterministic evaluation, no creativity
    depends_on: invariant_collector
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number