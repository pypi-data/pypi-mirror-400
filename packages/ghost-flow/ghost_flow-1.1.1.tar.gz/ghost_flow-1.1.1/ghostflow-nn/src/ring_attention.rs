//! Ring Attention
//!
//! Implements memory-efficient attention for extremely long sequences:
//! - Ring-based communication pattern
//! - Blockwise computation
//! - Support for sequences up to millions of tokens
//! - Distributed attention computation
//! - Causal and bidirectional variants

use ghostflow_core::Tensor;
use std::cmp;

/// Ring Attention configuration
#[derive(Debug, Clone)]
pub struct RingAttentionConfig {
    /// Block size for ring communication
    pub block_size: usize,
    /// Number of devices in ring
    pub num_devices: usize,
    /// Current device rank
    pub device_rank: usize,
    /// Whether to use causal masking
    pub causal: bool,
    /// Scale factor (usually 1/sqrt(d_k))
    pub scale: f32,
    /// Enable overlapped communication
    pub overlap_comm: bool,
}

impl Default for RingAttentionConfig {
    fn default() -> Self {
        RingAttentionConfig {
            block_size: 1024,
            num_devices: 1,
            device_rank: 0,
            causal: false,
            scale: 1.0,
            overlap_comm: true,
        }
    }
}

impl RingAttentionConfig {
    /// Configuration for causal ring attention
    pub fn causal(block_size: usize, num_devices: usize, device_rank: usize, scale: f32) -> Self {
        RingAttentionConfig {
            block_size,
            num_devices,
            device_rank,
            causal: true,
            scale,
            overlap_comm: true,
        }
    }
    
    /// Configuration for bidirectional ring attention
    pub fn bidirectional(block_size: usize, num_devices: usize, device_rank: usize, scale: f32) -> Self {
        RingAttentionConfig {
            block_size,
            num_devices,
            device_rank,
            causal: false,
            scale,
            overlap_comm: true,
        }
    }
}

/// Ring Attention implementation
pub struct RingAttention {
    config: RingAttentionConfig,
}

impl RingAttention {
    /// Create new Ring Attention
    pub fn new(config: RingAttentionConfig) -> Self {
        RingAttention { config }
    }
    
    /// Forward pass with Ring Attention
    pub fn forward(
        &self,
        query: &Tensor,
        key: &Tensor,
        value: &Tensor,
    ) -> Result<Tensor, String> {
        let q_dims = query.dims();
        
        if q_dims.len() != 3 {
            return Err("Expected 3D tensor [batch, seq_len, d_model]".to_string());
        }
        
        let batch_size = q_dims[0];
        let seq_len = q_dims[1];
        let d_model = q_dims[2];
        
        // Process each batch independently
        let mut batch_outputs = Vec::new();
        
        for b in 0..batch_size {
            let q_batch = self.extract_batch(query, b)?;
            let k_batch = self.extract_batch(key, b)?;
            let v_batch = self.extract_batch(value, b)?;
            
            let output = self.ring_attention_single_batch(&q_batch, &k_batch, &v_batch)?;
            batch_outputs.push(output);
        }
        
        // Concatenate batch outputs
        self.concatenate_batches(&batch_outputs, batch_size, seq_len, d_model)
    }
    
    /// Ring Attention for single batch
    fn ring_attention_single_batch(
        &self,
        query: &Tensor,
        key: &Tensor,
        value: &Tensor,
    ) -> Result<Tensor, String> {
        let q_data = query.data_f32();
        let k_data = key.data_f32();
        let v_data = value.data_f32();
        
        let seq_len = query.dims()[0];
        let d_model = query.dims()[1];
        
        let mut output = vec![0.0f32; seq_len * d_model];
        let mut row_max = vec![f32::NEG_INFINITY; seq_len];
        let mut row_sum = vec![0.0f32; seq_len];
        
        // Simulate ring communication pattern
        // In real implementation, KV blocks would be passed around the ring
        for ring_step in 0..self.config.num_devices {
            let kv_device = (self.config.device_rank + ring_step) % self.config.num_devices;
            
            // Compute which KV block this device owns
            let kv_start = kv_device * self.config.block_size;
            let kv_end = cmp::min(kv_start + self.config.block_size, seq_len);
            
            if kv_start >= seq_len {
                continue;
            }
            
            // Process this KV block against all queries
            self.process_kv_block(
                &q_data,
                &k_data,
                &v_data,
                &mut output,
                &mut row_max,
                &mut row_sum,
                kv_start,
                kv_end,
                seq_len,
                d_model,
            )?;
        }
        
        // Final normalization
        for i in 0..seq_len {
            if row_sum[i] > 0.0 {
                for d in 0..d_model {
                    output[i * d_model + d] /= row_sum[i];
                }
            }
        }
        
        Tensor::from_slice(&output, &[seq_len, d_model])
            .map_err(|e| format!("Failed to create output tensor: {:?}", e))
    }
    
    /// Process a KV block in the ring
    fn process_kv_block(
        &self,
        q_data: &[f32],
        k_data: &[f32],
        v_data: &[f32],
        output: &mut [f32],
        row_max: &mut [f32],
        row_sum: &mut [f32],
        kv_start: usize,
        kv_end: usize,
        seq_len: usize,
        d_model: usize,
    ) -> Result<(), String> {
        // Compute attention for this KV block
        for i in 0..seq_len {
            let mut block_max = f32::NEG_INFINITY;
            let mut scores = Vec::new();
            
            // Compute scores Q_i @ K_j^T for this KV block
            for j in kv_start..kv_end {
                // Apply causal mask
                if self.config.causal && j > i {
                    scores.push(f32::NEG_INFINITY);
                    continue;
                }
                
                let mut score = 0.0;
                for d in 0..d_model {
                    score += q_data[i * d_model + d] * k_data[j * d_model + d];
                }
                score *= self.config.scale;
                
                scores.push(score);
                block_max = block_max.max(score);
            }
            
            // Update global max
            let old_max = row_max[i];
            let new_max = old_max.max(block_max);
            row_max[i] = new_max;
            
            // Compute exponentials and sum
            let mut block_sum = 0.0;
            for score in &mut scores {
                if *score != f32::NEG_INFINITY {
                    *score = (*score - new_max).exp();
                    block_sum += *score;
                } else {
                    *score = 0.0;
                }
            }
            
            // Update running sum with correction
            let correction = (old_max - new_max).exp();
            row_sum[i] = row_sum[i] * correction + block_sum;
            
            // Update output with weighted values
            for (idx, &score) in scores.iter().enumerate() {
                let j = kv_start + idx;
                if j < seq_len {
                    for d in 0..d_model {
                        output[i * d_model + d] = output[i * d_model + d] * correction
                            + score * v_data[j * d_model + d];
                    }
                }
            }
        }
        
        Ok(())
    }
    
    /// Extract single batch from tensor
    fn extract_batch(&self, tensor: &Tensor, batch_idx: usize) -> Result<Tensor, String> {
        let data = tensor.data_f32();
        let dims = tensor.dims();
        let seq_len = dims[1];
        let d_model = dims[2];
        
        let start = batch_idx * seq_len * d_model;
        let end = start + seq_len * d_model;
        
        Tensor::from_slice(&data[start..end], &[seq_len, d_model])
            .map_err(|e| format!("Failed to extract batch: {:?}", e))
    }
    
    /// Concatenate batch outputs
    fn concatenate_batches(
        &self,
        batches: &[Tensor],
        batch_size: usize,
        seq_len: usize,
        d_model: usize,
    ) -> Result<Tensor, String> {
        let mut result = Vec::with_capacity(batch_size * seq_len * d_model);
        
        for batch in batches {
            result.extend_from_slice(&batch.data_f32());
        }
        
        Tensor::from_slice(&result, &[batch_size, seq_len, d_model])
            .map_err(|e| format!("Failed to concatenate batches: {:?}", e))
    }
    
    /// Estimate maximum sequence length
    pub fn max_sequence_length(&self, memory_budget_gb: f32, d_model: usize) -> usize {
        // Ring attention memory: O(block_size * d_model)
        let bytes_per_element = 4; // f32
        let memory_bytes = memory_budget_gb * 1024.0 * 1024.0 * 1024.0;
        
        let elements_per_block = self.config.block_size * d_model;
        let memory_per_block = elements_per_block * bytes_per_element;
        
        // Can handle sequences much longer than memory allows
        let blocks_in_memory = (memory_bytes / memory_per_block as f32) as usize;
        blocks_in_memory * self.config.block_size * self.config.num_devices
    }
    
    /// Get memory usage compared to standard attention
    pub fn memory_usage_ratio(&self, seq_len: usize) -> f32 {
        // Standard attention: O(seq_len^2)
        let standard_memory = seq_len * seq_len;
        
        // Ring attention: O(block_size * seq_len)
        let ring_memory = self.config.block_size * seq_len;
        
        ring_memory as f32 / standard_memory as f32
    }
}

/// Striped Ring Attention for better load balancing
pub struct StripedRingAttention {
    ring_attention: RingAttention,
    stripe_size: usize,
}

impl StripedRingAttention {
    /// Create new Striped Ring Attention
    pub fn new(config: RingAttentionConfig, stripe_size: usize) -> Self {
        StripedRingAttention {
            ring_attention: RingAttention::new(config),
            stripe_size,
        }
    }
    
    /// Forward pass with striped pattern
    pub fn forward(
        &self,
        query: &Tensor,
        key: &Tensor,
        value: &Tensor,
    ) -> Result<Tensor, String> {
        // Striped pattern distributes load more evenly
        // Each device gets stripes of size stripe_size
        
        // For simplicity, delegate to standard ring attention
        // In practice, would implement striped distribution
        self.ring_attention.forward(query, key, value)
    }
    
    /// Get load balance factor (1.0 = perfect balance)
    pub fn load_balance_factor(&self, seq_len: usize) -> f32 {
        let num_stripes = (seq_len + self.stripe_size - 1) / self.stripe_size;
        let stripes_per_device = (num_stripes + self.ring_attention.config.num_devices - 1) 
            / self.ring_attention.config.num_devices;
        
        let ideal_work = seq_len as f32 / self.ring_attention.config.num_devices as f32;
        let actual_work = stripes_per_device * self.stripe_size;
        
        ideal_work / actual_work as f32
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_ring_attention_config() {
        let config = RingAttentionConfig::default();
        assert_eq!(config.block_size, 1024);
        assert!(!config.causal);
        
        let causal = RingAttentionConfig::causal(512, 4, 0, 0.125);
        assert!(causal.causal);
        assert_eq!(causal.num_devices, 4);
    }
    
    #[test]
    fn test_ring_attention_forward() {
        let config = RingAttentionConfig::default();
        let ring_attn = RingAttention::new(config);
        
        let batch_size = 2;
        let seq_len = 128;
        let d_model = 64;
        
        let query = Tensor::randn(&[batch_size, seq_len, d_model]);
        let key = Tensor::randn(&[batch_size, seq_len, d_model]);
        let value = Tensor::randn(&[batch_size, seq_len, d_model]);
        
        let output = ring_attn.forward(&query, &key, &value).unwrap();
        assert_eq!(output.dims(), &[batch_size, seq_len, d_model]);
    }
    
    #[test]
    fn test_causal_ring_attention() {
        let config = RingAttentionConfig::causal(64, 2, 0, 1.0);
        let ring_attn = RingAttention::new(config);
        
        let query = Tensor::randn(&[1, 128, 32]);
        let key = Tensor::randn(&[1, 128, 32]);
        let value = Tensor::randn(&[1, 128, 32]);
        
        let output = ring_attn.forward(&query, &key, &value).unwrap();
        assert_eq!(output.dims(), &[1, 128, 32]);
    }
    
    #[test]
    fn test_memory_usage_ratio() {
        let config = RingAttentionConfig {
            block_size: 1024,
            num_devices: 4,
            ..Default::default()
        };
        let ring_attn = RingAttention::new(config);
        
        let ratio = ring_attn.memory_usage_ratio(4096);
        assert!(ratio < 1.0); // Ring attention should use less memory
        assert!(ratio > 0.0);
    }
    
    #[test]
    fn test_max_sequence_length() {
        let config = RingAttentionConfig {
            block_size: 1024,
            num_devices: 8,
            ..Default::default()
        };
        let ring_attn = RingAttention::new(config);
        
        let max_len = ring_attn.max_sequence_length(16.0, 512);
        assert!(max_len > 100_000); // Should support very long sequences
    }
    
    #[test]
    fn test_striped_ring_attention() {
        let config = RingAttentionConfig::default();
        let striped = StripedRingAttention::new(config, 256);
        
        let query = Tensor::randn(&[1, 512, 64]);
        let key = Tensor::randn(&[1, 512, 64]);
        let value = Tensor::randn(&[1, 512, 64]);
        
        let output = striped.forward(&query, &key, &value).unwrap();
        assert_eq!(output.dims(), &[1, 512, 64]);
    }
    
    #[test]
    fn test_load_balance_factor() {
        let config = RingAttentionConfig {
            block_size: 1024,
            num_devices: 4,
            ..Default::default()
        };
        let striped = StripedRingAttention::new(config, 256);
        
        let balance = striped.load_balance_factor(4096);
        assert!(balance > 0.0);
        assert!(balance <= 1.0);
    }
    
    #[test]
    fn test_extract_batch() {
        let config = RingAttentionConfig::default();
        let ring_attn = RingAttention::new(config);
        
        let tensor = Tensor::randn(&[3, 64, 32]);
        let batch = ring_attn.extract_batch(&tensor, 1).unwrap();
        
        assert_eq!(batch.dims(), &[64, 32]);
    }
}
