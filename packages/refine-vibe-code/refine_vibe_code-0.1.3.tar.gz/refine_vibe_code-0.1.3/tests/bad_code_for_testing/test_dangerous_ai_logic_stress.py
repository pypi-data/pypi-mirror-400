"""
Comprehensive stress test file for dangerous AI logic checker.
Tests various AI/ML security scenarios, unsafe practices, and edge cases.
"""

import torch
import tensorflow as tf
import openai
import pickle
import joblib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import requests
import os
import subprocess
import tempfile
from pathlib import Path

# ==========================================
# MODEL LOADING AND SERIALIZATION RISKS
# ==========================================

def dangerous_pickle_loading():
    """CRITICAL: Loading models with pickle."""
    # This should be flagged as CRITICAL
    with open("malicious_model.pkl", "rb") as f:
        model = pickle.load(f)  # DANGEROUS - pickle deserialization

    return model

def dangerous_joblib_loading():
    """CRITICAL: Loading models with joblib."""
    # This should be flagged as CRITICAL
    model = joblib.load("malicious_model.joblib")  # DANGEROUS - joblib deserialization

    return model

def unsafe_torch_load():
    """HIGH: Loading PyTorch models without weights_only."""
    # This should be flagged as HIGH
    model = torch.load("model.pth")  # Missing weights_only=True

    # Also dangerous: loading from HTTP
    model2 = torch.load("https://untrusted-site.com/model.pth")  # CRITICAL

    return model, model2

def unsafe_tensorflow_load():
    """HIGH: Loading TensorFlow models unsafely."""
    # Loading from HTTP source
    model = tf.saved_model.load("https://malicious-site.com/model")  # CRITICAL

    # Loading with unsafe options
    model2 = tf.keras.models.load_model("http://unsafe.com/model.h5")  # CRITICAL

    return model, model2

def dangerous_custom_unpickling():
    """CRITICAL: Custom unpickling functions."""
    import pickle

    class UnsafeUnpickler(pickle.Unpickler):
        def find_class(self, module, name):
            # This allows loading ANY class - EXTREMELY DANGEROUS
            return super().find_class(module, name)

    with open("model.pkl", "rb") as f:
        unpickler = UnsafeUnpickler(f)
        model = unpickler.load()  # CRITICAL - custom unpickling

    return model

# ==========================================
# CODE EXECUTION FROM AI OUTPUT
# ==========================================

def dangerous_exec_ai_output():
    """CRITICAL: Executing code generated by AI."""
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt="Generate Python code to calculate fibonacci",
        max_tokens=100
    )

    code = response.choices[0].text.strip()
    # CRITICAL: Direct execution of AI-generated code
    result = eval(code)  # EXTREMELY DANGEROUS

    return result

def dangerous_exec_file_ai_output():
    """CRITICAL: Executing AI-generated code from files."""
    # Read AI-generated code from file
    with open("ai_generated_code.py", "r") as f:
        code = f.read()

    # Execute it directly
    exec(code)  # CRITICAL - executing AI-generated file content

def dangerous_subprocess_ai_output():
    """CRITICAL: Running AI-generated shell commands."""
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt="Generate a shell command to list files",
        max_tokens=50
    )

    command = response.choices[0].text.strip()
    # CRITICAL: Executing AI-generated shell commands
    result = subprocess.run(command, shell=True, capture_output=True)

    return result

# ==========================================
# DATA POISONING VULNERABILITIES
# ==========================================

def data_poisoning_from_user_input():
    """CRITICAL: Training on user-provided data without validation."""
    def train_on_user_data():
        # CRITICAL: Direct user input into training data
        user_data = input("Enter training data (JSON format): ")
        data = eval(user_data)  # DANGEROUS - eval of user input

        # Convert to DataFrame and train
        df = pd.DataFrame(data)
        model = LinearRegression()
        model.fit(df.drop('target', axis=1), df['target'])

        return model

    return train_on_user_data

def unsafe_data_loading():
    """HIGH: Loading data from untrusted sources."""
    # Loading CSV from HTTP without validation
    url = "https://untrusted-site.com/training_data.csv"
    df = pd.read_csv(url)  # DANGEROUS - unvalidated data loading

    # Training on untrusted data
    model = LinearRegression()
    model.fit(df.drop('target', axis=1), df['target'])

    return model

def dangerous_featurization():
    """HIGH: Unsafe feature engineering with user input."""
    def create_features(user_expression):
        # Allow users to define feature engineering expressions
        feature_expr = user_expression  # e.g., "np.log(data['col1']) + data['col2']"

        # Execute user-defined feature engineering
        df = pd.DataFrame({"col1": [1, 2, 3], "col2": [4, 5, 6]})
        new_feature = eval(feature_expr)  # DANGEROUS - eval of user expression

        return new_feature

    return create_features

# ==========================================
# PREDICTION AND INFERENCE RISKS
# ==========================================

def unsafe_prediction_input():
    """HIGH: No input validation before prediction."""
    model = torch.load("model.pth")

    def predict_unvalidated(input_data):
        # DANGEROUS: Direct prediction on unvalidated input
        return model.predict(input_data)  # No validation!

    return predict_unvalidated

def dangerous_model_chaining():
    """HIGH: Chaining models with unsafe data flow."""
    model1 = torch.load("preprocessor.pth")
    model2 = torch.load("classifier.pth")

    def chained_prediction(raw_input):
        # Process through first model
        processed = model1(raw_input)  # No validation

        # Feed to second model
        result = model2(processed)  # No validation of intermediate results

        return result

    return chained_prediction

def unsafe_batch_prediction():
    """HIGH: Batch predictions without size limits."""
    model = torch.load("model.pth")

    def predict_batch(batch_data):
        # No batch size limits - could cause OOM or DoS
        # No input validation
        return model.predict(batch_data)

    return predict_batch

# ==========================================
# MODEL INVERSION AND DATA EXTRACTTION
# ==========================================

def model_inversion_attack():
    """CRITICAL: Code that attempts to extract training data."""
    model = torch.load("trained_model.pth")

    def extract_training_data():
        """Attempt to recover training data from model parameters."""
        params = list(model.parameters())

        # This could be used for model inversion attacks
        reconstructed_data = []
        for param in params:
            # Attempt reconstruction
            reconstructed = param.data.numpy()
            reconstructed_data.append(reconstructed)

        return reconstructed_data

    return extract_training_data

def dangerous_gradient_access():
    """HIGH: Accessing gradients for inversion attacks."""
    model = torch.load("model.pth")
    input_data = torch.randn(1, 784)

    # Enable gradient computation
    input_data.requires_grad_(True)

    # Forward pass
    output = model(input_data)

    # Backward pass to get gradients
    output.backward()

    # Access gradients - could be used for attacks
    gradients = input_data.grad

    return gradients

# ==========================================
# HARDWARE AND RESOURCE ATTACKS
# ==========================================

def infinite_training_loop():
    """CRITICAL: Infinite training loops."""
    model = torch.nn.Linear(10, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    # CRITICAL: This will run forever and exhaust resources
    while True:
        # Generate fake data endlessly
        x = torch.randn(32, 10)
        y = torch.randn(32, 1)

        # Train forever
        optimizer.zero_grad()
        output = model(x)
        loss = torch.nn.functional.mse_loss(output, y)
        loss.backward()
        optimizer.step()

def resource_exhaustion_attack():
    """HIGH: Resource exhaustion through large inputs."""
    model = torch.load("model.pth")

    def predict_large_input():
        # Create extremely large input to exhaust memory
        large_input = torch.randn(1000000, 784)  # 1M samples
        result = model(large_input)  # Will likely OOM

        return result

    return predict_large_input

def cpu_intensive_operations():
    """HIGH: CPU-intensive operations without limits."""
    def compute_heavy_calculation(user_param):
        # Allow arbitrary computational complexity
        iterations = user_param  # User controls iterations

        result = 0
        for i in range(iterations):  # Could be billions
            result += i ** 2  # CPU intensive

        return result

    return compute_heavy_calculation

# ==========================================
# API AND NETWORKING RISKS
# ==========================================

def unsafe_api_key_usage():
    """CRITICAL: Hardcoded API keys in AI applications."""
    # Hardcoded API key - should be detected by secrets checker too
    OPENAI_API_KEY = "sk-1234567890abcdef1234567890abcdef12345678"

    def call_ai_api(prompt):
        import openai
        openai.api_key = OPENAI_API_KEY  # DANGEROUS - hardcoded key

        response = openai.Completion.create(
            engine="text-davinci-003",
            prompt=prompt,
            max_tokens=100
        )

        return response.choices[0].text

    return call_ai_api

def dangerous_model_download():
    """CRITICAL: Downloading models from untrusted sources."""
    def download_and_load_model(url):
        # Download model from arbitrary URL
        response = requests.get(url)
        model_data = response.content

        # Save to temporary file
        with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as f:
            f.write(model_data)
            temp_path = f.name

        # Load the downloaded model
        model = torch.load(temp_path)  # DANGEROUS

        # Clean up
        os.unlink(temp_path)

        return model

    return download_and_load_model

def unsafe_webhook_endpoints():
    """HIGH: AI models exposed via webhooks without validation."""
    from flask import Flask, request
    import pickle

    app = Flask(__name__)

    # Load model once
    model = pickle.load(open("model.pkl", "rb"))  # DANGEROUS loading

    @app.route('/predict', methods=['POST'])
    def predict():
        # No input validation
        data = request.get_json()
        input_data = data['input']  # Direct user input

        # Predict without validation
        result = model.predict([input_data])

        return {'prediction': result.tolist()}

    return app

# ==========================================
# TRAINING DATA RISKS
# ==========================================

def biased_training_data():
    """HIGH: Training on potentially biased data without checks."""
    # Load training data
    df = pd.read_csv("training_data.csv")

    # No bias checking, no fairness validation
    X = df.drop('target', axis=1)
    y = df['target']

    # Split data (but no stratification for fairness)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Train model without fairness constraints
    model = LinearRegression()
    model.fit(X_train, y_train)

    return model

def data_leakage_vulnerable():
    """HIGH: Data leakage in train/test splits."""
    # Load all data
    df = pd.read_csv("all_data.csv")

    # VULNERABLE: Split after feature engineering
    df['new_feature'] = df['feature1'] * df['feature2']  # Feature engineering

    # Split AFTER feature engineering - DATA LEAKAGE!
    X_train, X_test, y_train, y_test = train_test_split(
        df.drop('target', axis=1), df['target'], test_size=0.2
    )

    model = LinearRegression()
    model.fit(X_train, y_train)

    # Test on leaked data
    score = model.score(X_test, y_test)  # Overly optimistic score

    return model, score

def unsafe_data_augmentation():
    """HIGH: Unsafe data augmentation with user input."""
    def augment_data(user_augmentation_code):
        """
        Allow users to define custom data augmentation.
        This is EXTREMELY dangerous.
        """
        df = pd.DataFrame({"feature1": [1, 2, 3], "feature2": [4, 5, 6]})

        # Execute user-defined augmentation code
        augmented_df = eval(user_augmentation_code)  # DANGEROUS

        return augmented_df

    return augment_data

# ==========================================
# MODEL EXPORT AND DEPLOYMENT RISKS
# ==========================================

def unsafe_model_export():
    """HIGH: Exporting models with sensitive information."""
    model = torch.nn.Linear(10, 1)

    # Train model (simplified)
    # ... training code ...

    # Export model with pickle (unsafe)
    with open("exported_model.pkl", "wb") as f:
        pickle.dump(model, f)  # DANGEROUS - pickle serialization

    return model

def dangerous_model_sharing():
    """CRITICAL: Sharing models that contain sensitive data."""
    class ModelWithSecrets:
        def __init__(self):
            self.api_key = "sk-secret-key-here"  # Hardcoded secret
            self.database_password = "db_password_123"  # Another secret
            self.model = torch.nn.Linear(10, 1)

        def forward(self, x):
            return self.model(x)

    # Create model with secrets
    model = ModelWithSecrets()

    # Save entire object (including secrets)
    torch.save(model, "model_with_secrets.pth")  # CRITICAL

    return model

# ==========================================
# AUTOENCODER AND GENERATIVE MODEL RISKS
# ==========================================

def dangerous_autoencoder():
    """HIGH: Autoencoders that could be used for data reconstruction."""
    class DangerousAutoencoder(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.encoder = torch.nn.Sequential(
                torch.nn.Linear(784, 128),
                torch.nn.ReLU(),
                torch.nn.Linear(128, 64)
            )
            self.decoder = torch.nn.Sequential(
                torch.nn.Linear(64, 128),
                torch.nn.ReLU(),
                torch.nn.Linear(128, 784)
            )

        def forward(self, x):
            encoded = self.encoder(x)
            decoded = self.decoder(encoded)
            return decoded

    model = DangerousAutoencoder()

    # This could be used to reconstruct sensitive data
    # if trained on private information

    return model

def unsafe_generative_model():
    """HIGH: Generative models without output filtering."""
    class UnsafeGenerator(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.model = torch.nn.Sequential(
                torch.nn.Linear(100, 256),
                torch.nn.ReLU(),
                torch.nn.Linear(256, 784),
                torch.nn.Tanh()
            )

        def forward(self, noise):
            return self.model(noise)

    generator = UnsafeGenerator()

    def generate_unfiltered():
        # Generate without any content filtering
        noise = torch.randn(1, 100)
        generated = generator(noise)

        # Could generate harmful or private content
        return generated

    return generate_unfiltered

# ==========================================
# FEDERATED LEARNING RISKS
# ==========================================

def unsafe_federated_aggregation():
    """HIGH: Unsafe model aggregation in federated learning."""
    def aggregate_models(client_models):
        """
        Aggregate model updates from clients.
        Without proper validation, this could be poisoned.
        """
        # No validation of client models
        aggregated_params = {}

        for model in client_models:  # No validation!
            for name, param in model.named_parameters():
                if name not in aggregated_params:
                    aggregated_params[name] = param.data.clone()
                else:
                    aggregated_params[name] += param.data

        # Average the parameters
        num_clients = len(client_models)
        for name in aggregated_params:
            aggregated_params[name] /= num_clients

        return aggregated_params

    return aggregate_models

# ==========================================
# ADVERSARIAL ATTACK VULNERABILITIES
# ==========================================

def no_adversarial_defenses():
    """HIGH: Models without adversarial defenses."""
    model = torch.load("trained_model.pth")

    def predict_without_defense(input_data):
        """
        Predict without any adversarial input detection.
        Vulnerable to adversarial attacks.
        """
        # No input preprocessing or adversarial detection
        return model(input_data)

    return predict_without_defense

def unsafe_adversarial_training():
    """HIGH: Improper adversarial training."""
    model = torch.nn.Linear(10, 2)

    def adversarial_training_step(data, labels):
        # Generate adversarial examples (simplified)
        adversarial_data = data + 0.01 * torch.randn_like(data)

        # Train on both clean and adversarial data
        # But without proper validation, this could be gamed
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

        # Loss on clean data
        clean_output = model(data)
        clean_loss = torch.nn.functional.cross_entropy(clean_output, labels)

        # Loss on adversarial data
        adv_output = model(adversarial_data)
        adv_loss = torch.nn.functional.cross_entropy(adv_output, labels)

        # Combine losses
        total_loss = clean_loss + adv_loss

        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        return total_loss.item()

    return adversarial_training_step

# ==========================================
# MONITORING AND LOGGING RISKS
# ==========================================

def dangerous_model_monitoring():
    """HIGH: Logging sensitive information during model operations."""
    model = torch.load("model.pth")

    def predict_with_dangerous_logging(input_data):
        # Log input data - privacy violation!
        print(f"Input data: {input_data}")

        result = model(input_data)

        # Log predictions - could leak sensitive information
        print(f"Prediction: {result}")

        # Log model parameters - extreme privacy violation
        for name, param in model.named_parameters():
            print(f"Parameter {name}: {param.data}")

        return result

    return predict_with_dangerous_logging

# ==========================================
# DEPENDENCY AND SUPPLY CHAIN RISKS
# ==========================================

def unsafe_dependency_loading():
    """CRITICAL: Loading dependencies from untrusted sources."""
    def load_external_library(lib_name):
        # Dynamically import from user-specified library
        # This could load malicious code
        exec(f"import {lib_name}")  # DANGEROUS

        # Use the loaded library
        lib = eval(lib_name)  # DANGEROUS

        return lib

    return load_external_library

def dangerous_custom_layers():
    """HIGH: Custom model layers with unsafe operations."""
    class UnsafeCustomLayer(torch.nn.Module):
        def __init__(self, user_code):
            super().__init__()
            self.user_code = user_code  # User-provided code

        def forward(self, x):
            # Execute user-provided code during forward pass
            result = eval(self.user_code)  # EXTREMELY DANGEROUS
            return result

    return UnsafeCustomLayer

# ==========================================
# CONFIGURATION AND HYPERPARAMETER RISKS
# ==========================================

def dangerous_hyperparameter_tuning():
    """HIGH: Hyperparameter tuning with user input."""
    def tune_hyperparameters(user_config):
        """
        Allow users to specify hyperparameters.
        Without validation, this could cause resource exhaustion.
        """
        config = eval(user_config)  # DANGEROUS - eval of user config

        # Extract potentially dangerous parameters
        learning_rate = config.get('lr', 0.001)
        batch_size = config.get('batch_size', 32)
        epochs = config.get('epochs', 100)

        # These could be set to extreme values
        model = torch.nn.Linear(10, 1)
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

        # Training loop with user-controlled epochs
        for epoch in range(epochs):  # Could be millions
            # Training code that could run forever
            pass

        return model

    return tune_hyperparameters

# ==========================================
# TESTING AND VALIDATION RISKS
# ==========================================

def unsafe_model_testing():
    """HIGH: Testing models with unsafe practices."""
    model = torch.load("model.pth")

    def test_with_arbitrary_data(test_code):
        """
        Allow arbitrary test code execution.
        Extremely dangerous for testing.
        """
        test_data = torch.randn(10, 784)

        # Execute user-provided test code
        exec(test_code)  # DANGEROUS

        return "Test completed"

    return test_with_arbitrary_data

# ==========================================
# LEGACY AND DEPRECATED API USAGE
# ==========================================

def dangerous_legacy_apis():
    """HIGH: Using deprecated or unsafe APIs."""
    import tensorflow as tf

    # Using deprecated TensorFlow v1 style
    tf.compat.v1.disable_eager_execution()  # Deprecated

    # Using unsafe operations
    @tf.function
    def unsafe_operation(x):
        # Using deprecated tf.print
        tf.print("Input:", x)  # Deprecated

        # Using potentially unsafe operations
        result = tf.py_function(lambda x: eval(str(x)), [x], tf.float32)  # DANGEROUS
        return result

    return unsafe_operation

# ==========================================
# CONCURRENCY AND MULTITHREADING RISKS
# ==========================================

def unsafe_concurrent_inference():
    """HIGH: Concurrent model inference without proper synchronization."""
    model = torch.load("model.pth")

    # Shared model across threads without synchronization
    def predict_thread_unsafe(input_data):
        # No thread synchronization
        # Model state could be corrupted
        return model(input_data)

    return predict_thread_unsafe

def dangerous_model_sharing_threads():
    """CRITICAL: Sharing model instances across threads unsafely."""
    import threading

    model = torch.load("model.pth")
    results = []
    lock = threading.Lock()  # Add thread synchronization

    def inference_worker(input_data):
        # All threads share the same model instance
        # Now with proper synchronization - race conditions prevented
        with lock:  # Acquire lock before using model
            result = model(input_data)
        results.append(result)

    threads = []
    for i in range(10):
        input_data = torch.randn(1, 784)
        thread = threading.Thread(target=inference_worker, args=(input_data,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    return results

# ==========================================
# CLOUD AND DISTRIBUTED COMPUTING RISKS
# ==========================================

def unsafe_cloud_model_loading():
    """CRITICAL: Loading models from cloud storage without validation."""
    def load_from_s3(bucket, key):
        """
        Load model from S3 without proper validation.
        Could load malicious models.
        """
        import boto3

        s3 = boto3.client('s3')
        response = s3.get_object(Bucket=bucket, Key=key)
        model_data = response['Body'].read()

        # Deserialize without validation
        model = pickle.loads(model_data)  # DANGEROUS

        return model

    return load_from_s3

def dangerous_distributed_training():
    """HIGH: Distributed training without proper validation."""
    import torch.distributed as dist

    def distributed_training_step():
        # Initialize distributed training
        dist.init_process_group(backend='nccl')

        model = torch.nn.Linear(10, 1)

        # No validation of participating nodes
        # No secure aggregation
        # Vulnerable to poisoning attacks

        return model

    return distributed_training_step

