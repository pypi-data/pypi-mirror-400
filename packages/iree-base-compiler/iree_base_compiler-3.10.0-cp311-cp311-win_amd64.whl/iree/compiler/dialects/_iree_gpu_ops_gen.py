
# Autogenerated by mlir-tblgen; don't manually edit.

from ._ods_common import _cext as _ods_cext
from ._ods_common import (
    equally_sized_accessor as _ods_equally_sized_accessor,
    get_default_loc_context as _ods_get_default_loc_context,
    get_op_results_or_values as _get_op_results_or_values,
    segmented_accessor as _ods_segmented_accessor,
)
_ods_ir = _ods_cext.ir
_ods_cext.globals.register_traceback_file_exclusion(__file__)

import builtins
from typing import Sequence as _Sequence, Union as _Union, Optional as _Optional


@_ods_cext.register_dialect
class _Dialect(_ods_ir.Dialect):
  DIALECT_NAMESPACE = "iree_gpu"

@_ods_cext.register_operation(_Dialect)
class BarrierRegionOp(_ods_ir.OpView):
  r"""
  This op is designed to represent synchronization of workers on the operands
  and results of the given region. This operation naturally arises when combining
  the regions of producer-consumer `scf.forall` operations that share a
  mapping type.
  
  For example, consider the following pair of parallel loops.
  ```mlir
    %0 = scf.forall (%idy, %idx) in (2, 32) shared_outs(%init = %empty) -> (tensor<4x128xf32>) {
      %in = ...
      %2 = affine.apply #affine_map<(d0) -> (d0 * 2)> (%idy)
      %3 = affine.apply #affine_map<(d0) -> (d0 * 4)> (%idx)
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %in into %init[%2, %3] [2, 4] [1, 1]
          : tensor<2x4xf32> into tensor<4x128xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %1 = scf.forall (%idy, %idx) in (8, 8) -> (tensor<128x128xf32>) {
      %4 = affine.apply #affine_map<(d0) -> (d0 * 16)> (%idx)
      %extracted_slice = tensor.extract_slice %0[0, %4] [4, 16] [1, 1]
        : tensor<4x128xf32> to tensor<4x16xf32>
      ...
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  ```
  
  Because these loops share the same worker type and total count, the bodies
  of these two loops can be merged with a barrier an insert_slice and a
  shuffle where the boundary of the loops currently is.
  
  ```mlir
    %0 = scf.forall (%idy, %idx) in (8, 8) -> (tensor<4x128xf32>) {
      %alloc = bufferization.alloc_tensor {memory_space = #gpu.address_space<workgroup>}
        : tensor<4x128xf32>
      %barrier = iree_gpu.barrier_region %alloc {
      ^bb0(%shared: tensor<4x128xf32>):
        %ids = affine.delinearize_index %idy * 8 + %idx to (2, 32) : index
        %in = ...
        %2 = affine.apply #affine_map<(d0) -> (d0 * 2)> (%ids#0)
        %3 = affine.apply #affine_map<(d0) -> (d0 * 4)> (%ids#1)
        %inserted_slice = tensor.insert_slice %in into %shared[%2, %3] [2, 4] [1, 1]
          : tensor<2x4xf32> to tensor<4x128xf32>
        iree_gpu.yield %slice : tensor<4x16xf32>
      } : tensor<4x128xf32> -> tensor<4x16xf32>
      %4 = affine.apply #affine_map<(d0) -> (d0 * 16)> (%idx)
      %slice = tensor.extract_slice %barrier[0, %4] [4, 16] [1, 1] : tensor<4x128xf32> to tensor<4x16xf32>
      ...
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  ```
  
  A barrier_region can be lowered to two barriers, one on the input operands
  and a second one on the results.
  
  Movtivation and Intended Use Cases:
  
  The primary way this op is generated is when fusing parallel loops with
  tensor results. This operation helps to make lowerings more progressive
  and flexible.
    - Lowering directly to an alloc + reads and writes breaks the dependency
      chain making transformations like barrier placement and pipelining
      potentially more difficult.
    - Allows the option of non-vector based lowering paths.
  """

  OPERATION_NAME = "iree_gpu.barrier_region"

  _ODS_REGIONS = (1, True)

  def __init__(self, results_, inputs, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(inputs))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.extend(results_)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def inputs(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

  @builtins.property
  def results_(self) -> _ods_ir.OpResultList:
    _ods_variadic_group_length = len(self.operation.results) - 1 + 1
    return self.operation.results[0:0 + _ods_variadic_group_length]

  @builtins.property
  def region(self) -> _ods_ir.Region:
    return self.regions[0]

def barrier_region(results_, inputs, *, loc=None, ip=None) -> _Union[_ods_ir.OpResult, _ods_ir.OpResultList, BarrierRegionOp]:
  op = BarrierRegionOp(results_=results_, inputs=inputs, loc=loc, ip=ip); results = op.results
  return results if len(results) > 1 else (results[0] if len(results) == 1 else op)

@_ods_cext.register_operation(_Dialect)
class BufferResourceCastOp(_ods_ir.OpView):
  r"""
  Nominal cast of a tensor to AMDGPU buffer resource memory space before
  bufferization. This op takes the parameters with which to perform the cast
  if |input| bufferizes to `storage_buffer` memory space. If |input| resolves
  to any other memory space this op is silently dropped and has no effect.
  
  If |cache_swizzle_stride| is present, there is verification before
  bufferization that all producers of |input| are view-like and single source
  and user (i.e. trivially no alias). In all other cases this op is best
  effort and has no verification or failure modes.
  
  // TODO: Add other parameters for casting as needed.
  """

  OPERATION_NAME = "iree_gpu.buffer_resource_cast"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, input, *, cache_swizzle_stride=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(input)
    if cache_swizzle_stride is not None: operands.append(cache_swizzle_stride)
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def input(self) -> _ods_ir.Value[_ods_ir.RankedTensorType]:
    return self.operation.operands[0]

  @builtins.property
  def cache_swizzle_stride(self) -> _Optional[_ods_ir.Value[_ods_ir.IndexType]]:
    return None if len(self.operation.operands) < 2 else self.operation.operands[1]

  @builtins.property
  def result(self) -> _ods_ir.OpResult[_ods_ir.RankedTensorType]:
    return self.operation.results[0]

def buffer_resource_cast(result, input, *, cache_swizzle_stride=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return BufferResourceCastOp(result=result, input=input, cache_swizzle_stride=cache_swizzle_stride, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class CoalescedGatherDMAOp(_ods_ir.OpView):
  r"""
  Performs a coalesced gather operation.
  This operation can exist in two forms: a tensor-based
  (value-semantic) form and a buffer-based (memref-semantic) form.
  
  In both forms, it reads elements from a source operand based on the
  optional `indices` operand and writes the gathered data into the
  destination `init` operand using destination-passing style.
  
  The `indices` operand is optional. The `source` represents the data
  loaded by this thread, while `init` is the collective output for all
  threads in the subgroup. Therefore, `source` and `init` may have
  different shapes (typically source is smaller, representing one
  thread's portion).
  
  The operation is specifically designed for subgroup-level
  parallelism, where threads within a subgroup cooperatively gather
  data with coalesced memory accesses. It implements
  ParallelCombiningOpInterface and must live inside an op implementing
  `InParallelOpInterface`, such as `scf.forall.in_parallel`.
  
  ## Lowering Paths
  
  Two lowering strategies are supported:
  1. Lowers to `amdgpu.gather_to_lds` operations when lowering
     requirements are met.
  2. Default lowering using `vector.gather` operations.
  
  ## Operands and Results
  
  * `$indices`: The variadic `indices` operand is an optional tensor
    or vector of indices to gather from `source`. If the indices are
    present, their shape must be a prefix of the `init`/`result` type.
    Each element of `indices` must be a 1D tensor or vector whose
    length matches the length of the corresponding dimension of `source`.
  
    The values in `indices` form indices into the memref starting at
    `source` from which a given thread will gather data, and each
    tensor/vector component in `indices` corresponds to one index
    dimension in `source`.
    Any component that is not specified is implicitly assumed to be
    `[0, 1, ..., len - 1]` where `len` is the length of the
    corresponding dimension of source. That is, gather all the
    elements along that dimension.
  
    This operation will gather data into its result as by setting:
    ```
    forall (i0, i1, ... iN) in (dim(source, 0), dim(source, 1), ...
        dim(source, N)):
      result[i0, i1, ... iN-1, iN + lane_id * dim(init, N)] =
          source[indices[0][i0], indices[1][i1], ..., indices[N][iN]]
    ```
    where `lane_id` is the ID of the thread within its subgroup.
  
    Note that, in order to enable efficient gathers, the trailing
    dimension of `source` must have unspecified indices and the dim's
    size must be a supported DMA width for your target.
  
    `$indices` supports both index and i32 element types. The reason
    is that one lowering path (from linalg_ext.gather) already have
    indices in i32 type.
  
  * `$source`: Source tensor/memref containing the data to be gathered.
  * `$init`: Destination tensor/memref receiving the gathered data
    (destination-passing style).
  * `lane`: The lane that specifies the coalescing store's offset within the
    workgroup/shared memory.
  
  
  ## Example of a single subgroup using coalesced_gather_dma in copy mode
     for transferring tensor<4x128xf32>, with an intended DMA width of 128 bits
     (4 x f32), with subgroup size 32:
  ```mlir
  scf.forall (%arg6) in (32) ... {
      %2 = %arg6 * 4
      %thread_slice = ... : tensor<4x128xf32>
      %dest_slice = ... : tensor<4x128xf32>
      scf.forall.in_parallel {
        iree_gpu.coalesced_gather_dma %thread_slice into %dest_slice lane(%arg6) :  ...
      }
    } {mapping = [#gpu.lane_id<linear_dim_0>]}
  ```
  """

  OPERATION_NAME = "iree_gpu.coalesced_gather_dma"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, source, indices, init, lane, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.extend(_get_op_results_or_values(indices))
    operands.append(init)
    operands.append(lane)
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    if result is not None: results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 4 + 1
    return self.operation.operands[1:1 + _ods_variadic_group_length]

  @builtins.property
  def init(self) -> _ods_ir.Value:
    _ods_variadic_group_length = len(self.operation.operands) - 4 + 1
    return self.operation.operands[2 + _ods_variadic_group_length - 1]

  @builtins.property
  def lane(self) -> _ods_ir.Value[_ods_ir.IndexType]:
    _ods_variadic_group_length = len(self.operation.operands) - 4 + 1
    return self.operation.operands[3 + _ods_variadic_group_length - 1]

  @builtins.property
  def result(self) -> _Optional[_ods_ir.OpResult]:
    return None if len(self.operation.results) < 1 else self.operation.results[0]

def coalesced_gather_dma(result, source, indices, init, lane, *, loc=None, ip=None) -> _Union[_ods_ir.OpResult, _ods_ir.OpResultList, CoalescedGatherDMAOp]:
  op = CoalescedGatherDMAOp(result=result, source=source, indices=indices, init=init, lane=lane, loc=loc, ip=ip); results = op.results
  return results if len(results) > 1 else (results[0] if len(results) == 1 else op)

@_ods_cext.register_operation(_Dialect)
class ValueBarrierOp(_ods_ir.OpView):
  r"""
  This operation acts as a barrier on a value semantic SSA values (tensor or
  vector). It takes multiple operands and produces a value equivalent to each
  input. This does not have copy and/or data movement semantics and simply
  represents a barrier on all writes in the tensor case, and a barrier until
  all threads acquire the input vector in the vector case.
  
  The inputs must be either all tensors, or all vectors.
  
  This operation is a no-op when not present in a parallel context. This
  operation is pure as it only requires synchronization for the value it
  produces.
  """

  OPERATION_NAME = "iree_gpu.value_barrier"

  _ODS_REGIONS = (0, True)

  def __init__(self, results_, inputs, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(inputs))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.extend(results_)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def inputs(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

  @builtins.property
  def results_(self) -> _ods_ir.OpResultList:
    _ods_variadic_group_length = len(self.operation.results) - 1 + 1
    return self.operation.results[0:0 + _ods_variadic_group_length]

def value_barrier(results_, inputs, *, loc=None, ip=None) -> _Union[_ods_ir.OpResult, _ods_ir.OpResultList, ValueBarrierOp]:
  op = ValueBarrierOp(results_=results_, inputs=inputs, loc=loc, ip=ip); results = op.results
  return results if len(results) > 1 else (results[0] if len(results) == 1 else op)

@_ods_cext.register_operation(_Dialect)
class YieldOp(_ods_ir.OpView):
  r"""
  This operation is used to yield values from a within a region.
  """

  OPERATION_NAME = "iree_gpu.yield"

  _ODS_REGIONS = (0, True)

  def __init__(self, values, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(values))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def values(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

def yield_(values, *, loc=None, ip=None) -> YieldOp:
  return YieldOp(values=values, loc=loc, ip=ip)
