"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .streamoptions import StreamOptions, StreamOptionsTypedDict
from .tokensequence import TokenSequence, TokenSequenceTypedDict
from friendli.core.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict

CompletionsStreamDedicatedBodyWithTokensSeedTypedDict = TypeAliasType(
    "CompletionsStreamDedicatedBodyWithTokensSeedTypedDict", Union[List[int], int]
)
"Seed to control random procedure. If nothing is given, the API generate the seed randomly, use it for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations."
CompletionsStreamDedicatedBodyWithTokensSeed = TypeAliasType(
    "CompletionsStreamDedicatedBodyWithTokensSeed", Union[List[int], int]
)
"Seed to control random procedure. If nothing is given, the API generate the seed randomly, use it for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations."


class CompletionsStreamDedicatedBodyWithTokensTypedDict(TypedDict):
    model: str
    'ID of target endpoint. If you want to send request to specific adapter, use the format \\"YOUR_ENDPOINT_ID:YOUR_ADAPTER_ROUTE\\". Otherwise, you can just use \\"YOUR_ENDPOINT_ID\\" alone.'
    tokens: List[int]
    "The tokenized prompt (i.e., input tokens). Either `prompt` or `tokens` field is required."
    bad_word_tokens: NotRequired[Nullable[List[TokenSequenceTypedDict]]]
    "Same as the above `bad_words` field, but receives token sequences instead of text phrases. This is similar to Hugging Face's [`bad_word_ids`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.bad_words_ids) argument."
    bad_words: NotRequired[Nullable[List[str]]]
    'Text phrases that should not be generated.\n    For a bad word phrase that contains N tokens, if the first N-1 tokens appears at the last of the generated result, the logit for the last token of the phrase is set to -inf.\n    Before checking whether a bard word is included in the result, the word is converted into tokens.\n    We recommend using `bad_word_tokens` because it is clearer.\n    For example, after tokenization, phrases \\"clear\\" and \\" clear\\" can result in different token sequences due to the prepended space character.\n    Defaults to empty list.\n\n    '
    embedding_to_replace: NotRequired[Nullable[List[float]]]
    "A list of flattened embedding vectors used for replacing the tokens at the specified indices provided via `token_index_to_replace`."
    encoder_no_repeat_ngram: NotRequired[Nullable[int]]
    "If this exceeds 1, every ngram of that size occurring in the input token sequence cannot appear in the generated result. 1 means that this mechanism is disabled (i.e., you cannot prevent 1-gram from being generated repeatedly). Only allowed for encoder-decoder models. Defaults to 1. This is similar to Hugging Face's [`encoder_no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.encoder_no_repeat_ngram_size) argument."
    encoder_repetition_penalty: NotRequired[Nullable[float]]
    "Penalizes tokens that have already appeared in the input tokens. Should be positive value. 1.0 means no penalty. Only allowed for encoder-decoder models. See [Keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`encoder_repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.encoder_repetition_penalty) argument."
    eos_token: NotRequired[Nullable[List[int]]]
    "A list of endpoint sentence tokens."
    forced_output_tokens: NotRequired[Nullable[List[int]]]
    "A token sequence that is enforced as a generation output. This option can be used when evaluating the model for the datasets with multi-choice problems (e.g., [HellaSwag](https://huggingface.co/datasets/hellaswag), [MMLU](https://huggingface.co/datasets/cais/mmlu)). Use this option with `logprobs` to get logprobs for the evaluation."
    frequency_penalty: NotRequired[Nullable[float]]
    "Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled, taking into account their frequency in the preceding text. This penalization diminishes the model's tendency to reproduce identical lines verbatim."
    logprobs: NotRequired[Nullable[int]]
    "Include the log probabilities on the logprobs most likely output tokens, as well the chosen tokens."
    max_tokens: NotRequired[Nullable[int]]
    "The maximum number of tokens to generate. For decoder-only models like GPT, the length of your input tokens plus `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI GPT-3). For encoder-decoder models like T5 or BlenderBot, `max_tokens` should not exceed the model's maximum output length. This is similar to Hugging Face's [`max_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens) argument."
    max_total_tokens: NotRequired[Nullable[int]]
    "The maximum number of tokens including both the generated result and the input tokens. Only allowed for decoder-only models. Only one argument between `max_tokens` and `max_total_tokens` is allowed. Default value is the model's maximum length. This is similar to Hugging Face's [`max_length`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_length) argument."
    min_p: NotRequired[Nullable[float]]
    "A scaling factor used to determine the minimum token probability threshold. This threshold is calculated as `min_p` multiplied by the probability of the most likely token. Tokens with probabilities below this scaled threshold are excluded from sampling. Values range from 0.0 (inclusive) to 1.0 (inclusive). Higher values result in stricter filtering, while lower values allow for greater diversity. The default value of 0.0 disables filtering, allowing all tokens to be considered for sampling."
    min_tokens: NotRequired[Nullable[int]]
    "The minimum number of tokens to generate. Default value is 0. This is similar to Hugging Face's [`min_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.min_new_tokens) argument.\n\n    **This field is unsupported when `response_format` is specified.**\n    "
    min_total_tokens: NotRequired[Nullable[int]]
    "The minimum number of tokens including both the generated result and the input tokens. Only allowed for decoder-only models. Only one argument between `min_tokens` and `min_total_tokens` is allowed. This is similar to Hugging Face's [`min_length`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.min_length) argument."
    n: NotRequired[Nullable[int]]
    "The number of independently generated results for the prompt. Defaults to 1. This is similar to Hugging Face's [`num_return_sequences`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences) argument."
    no_repeat_ngram: NotRequired[Nullable[int]]
    "If this exceeds 1, every ngram of that size can only occur once among the generated result (plus the input tokens for decoder-only models). 1 means that this mechanism is disabled (i.e., you cannot prevent 1-gram from being generated repeatedly). Defaults to 1. This is similar to Hugging Face's [`no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size) argument."
    presence_penalty: NotRequired[Nullable[float]]
    "Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled at least once in the existing text."
    repetition_penalty: NotRequired[Nullable[float]]
    "Penalizes tokens that have already appeared in the generated result (plus the input tokens for decoder-only models). Should be positive value (1.0 means no penalty). See [keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.repetition_penalty) argument."
    response_format: NotRequired[Nullable[ResponseFormatTypedDict]]
    seed: NotRequired[Nullable[CompletionsStreamDedicatedBodyWithTokensSeedTypedDict]]
    "Seed to control random procedure. If nothing is given, the API generate the seed randomly, use it for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations."
    stop: NotRequired[Nullable[List[str]]]
    "When one of the stop phrases appears in the generation result, the API will stop generation.\n    The stop phrases are excluded from the result.\n    Defaults to empty list.\n    "
    stop_tokens: NotRequired[Nullable[List[TokenSequenceTypedDict]]]
    "Stop generating further tokens when generated token corresponds to any of the tokens in the sequence."
    stream: NotRequired[Nullable[bool]]
    "Whether to stream generation result. When set true, each token will be sent as [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) once generated."
    stream_options: NotRequired[Nullable[StreamOptionsTypedDict]]
    "Options related to stream.\n    It can only be used when `stream: true`.\n    "
    temperature: NotRequired[Nullable[float]]
    "Sampling temperature. Smaller temperature makes the generation result closer to greedy, argmax (i.e., `top_k = 1`) sampling. Defaults to 1.0. This is similar to Hugging Face's [`temperature`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.temperature) argument."
    token_index_to_replace: NotRequired[Nullable[List[int]]]
    "A list of token indices where to replace the embeddings of input tokens provided via either `tokens` or `prompt`."
    top_k: NotRequired[Nullable[int]]
    "Limits sampling to the top k tokens with the highest probabilities. Values range from 0 (no filtering) to the model's vocabulary size (inclusive). The default value of 0 applies no filtering, allowing all tokens."
    top_p: NotRequired[Nullable[float]]
    "Keeps only the smallest set of tokens whose cumulative probabilities reach `top_p` or higher. Values range from 0.0 (exclusive) to 1.0 (inclusive). The default value of 1.0 includes all tokens, allowing maximum diversity."
    xtc_threshold: NotRequired[Nullable[float]]
    "A probability threshold used to identify “top choice” tokens for exclusion in XTC (Exclude Top Choices) sampling. Tokens with probabilities at or above this threshold are considered viable candidates, and all but the least likely viable token are excluded from sampling. This option reduces the dominance of highly probable tokens while preserving some diversity by keeping the least confident “top choice.” Values range from 0.0 (inclusive) to 1.0 (inclusive). Higher values make the filtering more selective by requiring higher probabilities to trigger exclusion, while lower values apply filtering more broadly. The default value of 0.0 disables XTC filtering entirely."
    xtc_probability: NotRequired[Nullable[float]]
    "The probability that XTC (Exclude Top Choices) filtering will be applied for each sampling decision. When XTC is triggered, high-probability tokens above the `xtc_threshold` are excluded except for the least likely viable token. This stochastic activation allows for a balance between standard sampling and creativity-boosting exclusion filtering. Values range from 0.0 (inclusive) to 1.0 (inclusive), where 0.0 means XTC is never applied, 1.0 means XTC is always applied when viable tokens exist, and intermediate values provide probabilistic activation. The default value of 0.0 disables XTC filtering."


class CompletionsStreamDedicatedBodyWithTokens(BaseModel):
    model: str
    'ID of target endpoint. If you want to send request to specific adapter, use the format \\"YOUR_ENDPOINT_ID:YOUR_ADAPTER_ROUTE\\". Otherwise, you can just use \\"YOUR_ENDPOINT_ID\\" alone.'
    tokens: List[int]
    "The tokenized prompt (i.e., input tokens). Either `prompt` or `tokens` field is required."
    bad_word_tokens: OptionalNullable[List[TokenSequence]] = UNSET
    "Same as the above `bad_words` field, but receives token sequences instead of text phrases. This is similar to Hugging Face's [`bad_word_ids`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.bad_words_ids) argument."
    bad_words: OptionalNullable[List[str]] = UNSET
    'Text phrases that should not be generated.\n    For a bad word phrase that contains N tokens, if the first N-1 tokens appears at the last of the generated result, the logit for the last token of the phrase is set to -inf.\n    Before checking whether a bard word is included in the result, the word is converted into tokens.\n    We recommend using `bad_word_tokens` because it is clearer.\n    For example, after tokenization, phrases \\"clear\\" and \\" clear\\" can result in different token sequences due to the prepended space character.\n    Defaults to empty list.\n\n    '
    embedding_to_replace: OptionalNullable[List[float]] = UNSET
    "A list of flattened embedding vectors used for replacing the tokens at the specified indices provided via `token_index_to_replace`."
    encoder_no_repeat_ngram: OptionalNullable[int] = UNSET
    "If this exceeds 1, every ngram of that size occurring in the input token sequence cannot appear in the generated result. 1 means that this mechanism is disabled (i.e., you cannot prevent 1-gram from being generated repeatedly). Only allowed for encoder-decoder models. Defaults to 1. This is similar to Hugging Face's [`encoder_no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.encoder_no_repeat_ngram_size) argument."
    encoder_repetition_penalty: OptionalNullable[float] = UNSET
    "Penalizes tokens that have already appeared in the input tokens. Should be positive value. 1.0 means no penalty. Only allowed for encoder-decoder models. See [Keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`encoder_repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.encoder_repetition_penalty) argument."
    eos_token: OptionalNullable[List[int]] = UNSET
    "A list of endpoint sentence tokens."
    forced_output_tokens: OptionalNullable[List[int]] = UNSET
    "A token sequence that is enforced as a generation output. This option can be used when evaluating the model for the datasets with multi-choice problems (e.g., [HellaSwag](https://huggingface.co/datasets/hellaswag), [MMLU](https://huggingface.co/datasets/cais/mmlu)). Use this option with `logprobs` to get logprobs for the evaluation."
    frequency_penalty: OptionalNullable[float] = UNSET
    "Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled, taking into account their frequency in the preceding text. This penalization diminishes the model's tendency to reproduce identical lines verbatim."
    logprobs: OptionalNullable[int] = UNSET
    "Include the log probabilities on the logprobs most likely output tokens, as well the chosen tokens."
    max_tokens: OptionalNullable[int] = UNSET
    "The maximum number of tokens to generate. For decoder-only models like GPT, the length of your input tokens plus `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI GPT-3). For encoder-decoder models like T5 or BlenderBot, `max_tokens` should not exceed the model's maximum output length. This is similar to Hugging Face's [`max_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens) argument."
    max_total_tokens: OptionalNullable[int] = UNSET
    "The maximum number of tokens including both the generated result and the input tokens. Only allowed for decoder-only models. Only one argument between `max_tokens` and `max_total_tokens` is allowed. Default value is the model's maximum length. This is similar to Hugging Face's [`max_length`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_length) argument."
    min_p: OptionalNullable[float] = UNSET
    "A scaling factor used to determine the minimum token probability threshold. This threshold is calculated as `min_p` multiplied by the probability of the most likely token. Tokens with probabilities below this scaled threshold are excluded from sampling. Values range from 0.0 (inclusive) to 1.0 (inclusive). Higher values result in stricter filtering, while lower values allow for greater diversity. The default value of 0.0 disables filtering, allowing all tokens to be considered for sampling."
    min_tokens: OptionalNullable[int] = UNSET
    "The minimum number of tokens to generate. Default value is 0. This is similar to Hugging Face's [`min_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.min_new_tokens) argument.\n\n    **This field is unsupported when `response_format` is specified.**\n    "
    min_total_tokens: OptionalNullable[int] = UNSET
    "The minimum number of tokens including both the generated result and the input tokens. Only allowed for decoder-only models. Only one argument between `min_tokens` and `min_total_tokens` is allowed. This is similar to Hugging Face's [`min_length`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.min_length) argument."
    n: OptionalNullable[int] = UNSET
    "The number of independently generated results for the prompt. Defaults to 1. This is similar to Hugging Face's [`num_return_sequences`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences) argument."
    no_repeat_ngram: OptionalNullable[int] = UNSET
    "If this exceeds 1, every ngram of that size can only occur once among the generated result (plus the input tokens for decoder-only models). 1 means that this mechanism is disabled (i.e., you cannot prevent 1-gram from being generated repeatedly). Defaults to 1. This is similar to Hugging Face's [`no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size) argument."
    presence_penalty: OptionalNullable[float] = UNSET
    "Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled at least once in the existing text."
    repetition_penalty: OptionalNullable[float] = UNSET
    "Penalizes tokens that have already appeared in the generated result (plus the input tokens for decoder-only models). Should be positive value (1.0 means no penalty). See [keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.repetition_penalty) argument."
    response_format: OptionalNullable[ResponseFormat] = UNSET
    seed: OptionalNullable[CompletionsStreamDedicatedBodyWithTokensSeed] = UNSET
    "Seed to control random procedure. If nothing is given, the API generate the seed randomly, use it for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations."
    stop: OptionalNullable[List[str]] = UNSET
    "When one of the stop phrases appears in the generation result, the API will stop generation.\n    The stop phrases are excluded from the result.\n    Defaults to empty list.\n    "
    stop_tokens: OptionalNullable[List[TokenSequence]] = UNSET
    "Stop generating further tokens when generated token corresponds to any of the tokens in the sequence."
    stream: OptionalNullable[bool] = UNSET
    "Whether to stream generation result. When set true, each token will be sent as [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) once generated."
    stream_options: OptionalNullable[StreamOptions] = UNSET
    "Options related to stream.\n    It can only be used when `stream: true`.\n    "
    temperature: OptionalNullable[float] = UNSET
    "Sampling temperature. Smaller temperature makes the generation result closer to greedy, argmax (i.e., `top_k = 1`) sampling. Defaults to 1.0. This is similar to Hugging Face's [`temperature`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.temperature) argument."
    token_index_to_replace: OptionalNullable[List[int]] = UNSET
    "A list of token indices where to replace the embeddings of input tokens provided via either `tokens` or `prompt`."
    top_k: OptionalNullable[int] = UNSET
    "Limits sampling to the top k tokens with the highest probabilities. Values range from 0 (no filtering) to the model's vocabulary size (inclusive). The default value of 0 applies no filtering, allowing all tokens."
    top_p: OptionalNullable[float] = UNSET
    "Keeps only the smallest set of tokens whose cumulative probabilities reach `top_p` or higher. Values range from 0.0 (exclusive) to 1.0 (inclusive). The default value of 1.0 includes all tokens, allowing maximum diversity."
    xtc_threshold: OptionalNullable[float] = UNSET
    "A probability threshold used to identify “top choice” tokens for exclusion in XTC (Exclude Top Choices) sampling. Tokens with probabilities at or above this threshold are considered viable candidates, and all but the least likely viable token are excluded from sampling. This option reduces the dominance of highly probable tokens while preserving some diversity by keeping the least confident “top choice.” Values range from 0.0 (inclusive) to 1.0 (inclusive). Higher values make the filtering more selective by requiring higher probabilities to trigger exclusion, while lower values apply filtering more broadly. The default value of 0.0 disables XTC filtering entirely."
    xtc_probability: OptionalNullable[float] = UNSET
    "The probability that XTC (Exclude Top Choices) filtering will be applied for each sampling decision. When XTC is triggered, high-probability tokens above the `xtc_threshold` are excluded except for the least likely viable token. This stochastic activation allows for a balance between standard sampling and creativity-boosting exclusion filtering. Values range from 0.0 (inclusive) to 1.0 (inclusive), where 0.0 means XTC is never applied, 1.0 means XTC is always applied when viable tokens exist, and intermediate values provide probabilistic activation. The default value of 0.0 disables XTC filtering."

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "bad_word_tokens",
            "bad_words",
            "embedding_to_replace",
            "encoder_no_repeat_ngram",
            "encoder_repetition_penalty",
            "eos_token",
            "forced_output_tokens",
            "frequency_penalty",
            "logprobs",
            "max_tokens",
            "max_total_tokens",
            "min_p",
            "min_tokens",
            "min_total_tokens",
            "n",
            "no_repeat_ngram",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "stop_tokens",
            "stream",
            "stream_options",
            "temperature",
            "token_index_to_replace",
            "top_k",
            "top_p",
            "xtc_threshold",
            "xtc_probability",
        ]
        nullable_fields = [
            "bad_word_tokens",
            "bad_words",
            "embedding_to_replace",
            "encoder_no_repeat_ngram",
            "encoder_repetition_penalty",
            "eos_token",
            "forced_output_tokens",
            "frequency_penalty",
            "logprobs",
            "max_tokens",
            "max_total_tokens",
            "min_p",
            "min_tokens",
            "min_total_tokens",
            "n",
            "no_repeat_ngram",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "stop_tokens",
            "stream",
            "stream_options",
            "temperature",
            "token_index_to_replace",
            "top_k",
            "top_p",
            "xtc_threshold",
            "xtc_probability",
        ]
        null_default_fields = []
        serialized = handler(self)
        m = {}
        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)
            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )
            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                k not in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val
        return m
