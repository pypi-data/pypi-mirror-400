<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NIDRA manual</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="sidebar">
        <br><br><br>
        <ul>
            <li><a href="#installation">Installation</a></li>
            <li><a href="#preparing-data">Preparing your data</a></li>
            <li><a href="#gui-walkthrough">GUI guide</a></li>
            <li><a href="#model-performance">Model validation</a></li>
            <li><a href="#understanding-results">Understanding results</a></li>
            <li><a href="#python-package">Python endpoints</a></li>
            <li><a href="#faq">FAQ & troubleshooting</a></li>            
            <li><a href="#how-to-cite">How to cite</a></li>
            <li><a href="#attribution">Attribution</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </nav>
    <main class="content">
        <section id="introduction">
            <div class="intro-container">
                <div class="intro-logo">
                    <img src="logo.png" alt="NIDRA Logo" class="logo">
                </div>
                <div>
                    <h1>NIDRA v0.2.3 - super simple sleep scoring</h1>
                    <a href="https://github.com/paulzerr/nidra" class="intro-link">https://github.com/paulzerr/nidra</a>
                    <p>An easy way to use powerful machine learning models to autoscore sleep recordings with excellent accuracy. No programming required, but Python endpoints are available. NIDRA can accurately score recordings from 2-channel EEG wearables such as ZMax (using ez6 and ez6moe models), as well as full PSG recordings (using U-Sleep 2.0 via sleepyland).</p>

                    <p><h2><strong>Download <a href="https://github.com/paulzerr/nidra/releases/latest/download/NIDRA_installer.exe"> >> NIDRA for Windows 10/11 <<</a> </strong></h1></p>
                <p></p>
                <br>
                </div>
            </div>
        </section>

        <section id="installation">
            <h2>Installation</h2>
            <h3>Option 1: Standalone installer</h3>
            <p>The easiest way to use NIDRA on Windows is to <a href="https://github.com/paulzerr/nidra/releases/latest/download/NIDRA.exe">download</a> the portable self-extracting archive which requires no installation.</p>
<p><strong>Note:</strong> due to the restrictive environment of Windows, you may see a "Search on app store" popup window. Click "No". You may also see the blue Smartscreen warning. In that case click "More info" and then "run anyway". Should this fail, try installing via pip (see below).</p>

            <h3>Option 2: Install with pip</h3>
            <p>It is highly recommended to create a clean virtual environment to install NIDRA. This prevents conflicts with other packages. Python 3.10 recommended.</p>
            <p><strong>Windows:</strong></p>
            <pre><code>python -m venv nidra-env
nidra-env\Scripts\activate
pip install nidra</code></pre>
            <p><strong>Mac/Linux:</strong></p>
            <pre><code>python -m venv nidra-env
source nidra-env/bin/activate
pip install nidra</code></pre>
            <p><strong>Or install using Conda (Windows/Mac/Linux):</strong></p>
            First install e.g., <a href="https://www.anaconda.com/download/success">Miniconda</a>.</p>
            <pre><code>conda create -n nidra-env
conda activate nidra-env
pip install nidra</code></pre>
            <p><strong>Launch the graphical interface:</strong></p>
            <pre><code>nidra</code></pre>
            <p><strong>Note:</strong> If you installed via pip, the first time you run NIDRA, the necessary model files will be automatically downloaded from <a href="https://huggingface.co/pzerr/NIDRA_models/">https://huggingface.co/pzerr/NIDRA_models/</a> (~152MB). </p>
            <h3>Option 3: Install from source</h3>
            <pre><code>git clone https://github.com/paulzerr/nidra.git
cd NIDRA
pip install .</code></pre>
        </section>

        <section id="gui-guide">
            <h2>Graphical user interface (GUI)</h2>
            <p>The GUI provides an intuitive, point-and-click option to score sleep recordings. The easiest way to launch the GUI is by downloading and starting the self-extracting archive (see above). Or, if you installed NIDRA as a Python package, you can launch it by opening your terminal or cmd and running the command: <code>nidra</code></p>
            <img src="gui.png" alt="Screenshot of the NIDRA GUI" style="width: 98%; display: block; margin: 20px 0;">
            <p><b>Fig.1</b> - Screenshot of the GUI. </p>
            <img src="dashboard.png" alt="Screenshot of the NIDRA dashboard" style="width: 98%; display: block; margin: 20px 0;">
            <p><b>Fig.2</b> - Example output figure. </p>

            <h2>GUI Walkthrough</h2>
            <h3>Step 1: Start the NIDRA GUI</h3>
            <p>For installation see top of this page. If installed via the standalone version, go to the directory where you extracted the files and double-click <code>NIDRA.exe</code>. </p>
            <p><strong>Note:</strong> due to the restrictive environment of Windows, you may see a "Search on app store" popup window. Click "No". You may also see the blue Smartscreen warning. In that case click "More info" and then "run anyway". Should this fail, try installing via pip (see installation section).</p>
            <p>Alternatively, if you installed NIDRA as a Python package, open your terminal (On Windows: press CTRL+R, type in cmd, press Enter) and type <code>nidra</code>, then press Enter. The main application window will appear.</p>

            <h3>Step 2: Set input and output paths</h3>
            <p>Tell NIDRA where your data is located and where the results should go.</p>
            <ul>
                <li><strong>Input:</strong> Click "Browse...". You can select a single recording file (EDF), a folder containing multiple recordings, or a text file (.txt) listing file paths. For processing multiple recordings, select the main project folder containing your recordings. NIDRA will automatically detect .edf files in subfolders. Please see "Preparing your data" section above for details.</li>
                <li><strong>Output Directory:</strong> Specify the folder where NIDRA will save the scoring results. If you leave this blank, NIDRA will automatically create a new folder named <code>autoscorer_output</code> inside your input directory. </li>
            </ul>
            
            <h3>Step 3: Select recording type and model</h3>
            <ul>
                <li><strong>Data Source:</strong> Select the type of recording you have. Choose "EEG wearable" for 2-channel wearable data (e.g., ZMax), or "PSG" for standard polysomnography data.</li>
                <li><strong>Model:</strong> The available models will change based on your recording type. For wearabnle EEG, <code>ez6moe</code> is recommended for its high accuracy. This model will take approximately 30 seconds to score one night. Alternatively, use <code>ez6</code>, which runs in about 5 seconds, but has slightly lower accuracy. For PSG, <code>u-sleep-nsrr-2024</code> (USleep 2.0) is the standard choice. This model uses all available EEG and EOG channels and will run one scoring process for each possible combination of single EEG and EOG channels, and then take a majority vote from each scoring process. If there are no EOG channels available, NIDRA will automatically use the EEG-only <code>u-sleep-nsrr-2024_eeg</code> model.</li>
            </ul>
            <h3>Step 4: Run the analysis</h3>
            <p>Sleep stage output is always generated. You can choose whether to additionally generate classifier probability output, graphs, sleep statistics using the checkboxes. Click the <strong>"Run Autoscoring"</strong> button to begin. You can monitor the real-time progress in the console panel on the right. This log will show which files are being processed and report any warnings or errors.</p>
        </section>


        <section id="preparing-data">
            <h2>Preparing your data</h2>
            <ul>
                <li>NIDRA automatically detects your input format. When specifying a single .edf file, NIDRA will score that file. When specifying a folder, NIDRA will score all recordings in that folder and all of its subfolders. You can also input a text file that contains a list of file paths.</li>
                <li>When scoring PSG data, and if you use standard channel labels (e.g., 'EEG Fpz-Cz', 'EOG left', 'Fp1', 'O2-M1'), NIDRA uses these to identify channel types. If no clear channel names are provided, all channels are used and assumed to be EEG.</li>
                <li>When scoring ZMax or other EEG wearable data, your recordings may contain both channels in the same file, or you can have one channel per file. In the latter case, the file pairs need to be in the same folder and named *L.edf and *R.edf.</li>
                
</ul>
<br>
<p><strong>Structure for Forehead EEG (e.g., ZMax):</strong></p>
<ul>
  

            <li><p>For ZMax EEG data in the original format, the left and right channel files must be in the same directory. These are typically named <code>EEG_L.edf</code> and <code>EEG_R.edf</code>. However, you can also supply ZMax recordings (or data from any forehead EEG device) with both EEG channels in the same .edf file. NIDRA will automatically detect this.</p></li>
            <pre>
forehead_study/
├── subject_01/
|   ├── EEG_L.edf
|   └── EEG_R.edf
├── subject_02/
|   ├── night01_L.edf
|   └── night01_R.edf
</pre>
</ul>
<br>

<p><strong>Selecting channels</strong></p>
<ul><p><li>You can optionally specify which channels to use for scoring. NIDRA will automatically detect EEG and EOG channels based on their names, and ignore other channels as they are not used by any of the models. </p></li>
</ul>


        </section>


        <section id="model-performance">
            <h2>Model validation and performance</h2>
            <p>The models included in NIDRA have been rigorously validated against manually scored data from human experts. Below is a summary of their performance.</p>
            <h3>ezscore-f (ez6 and ez6moe) for wearable forehead EEG</h3>
            <p><code>ez6moe</code> is recommended for its high accuracy. This model will take approximately 30 seconds to score one night. Alternatively, use <code>ez6</code>, which runs in about 5 seconds, but has slightly lower accuracy. This is mostly useful for testing purposes or real-time applications.
                The confusion matrix below shows the <code>ez6moe</code> model's predictions (y-axis) versus the expert labels (x-axis). The diagonal represents correct classifications. The model shows high agreement with the expert scorer, particularly for Wake, N2, and REM sleep. For further details, please see the <a href="https://www.biorxiv.org/content/10.1101/2025.06.02.657451v1">ezscore-f paper</a>.</p>
            <img src="matrix.png" alt="Confusion matrix of the ez6moe model" style="width: 60%; max-width: 600px; display: block; margin: 20px 0;">
            <p><b>Fig.2</b> - Confusion matrix (vs. manually scored PSG) of the artefact-aware ez6moe model.</p>
            <p>Key performance metrics, such as accuracy and Cohen's Kappa (a measure of inter-rater agreement), are comparable to the agreement levels seen between different human experts. For full details, please refer to the original publication.</p>
            <h3>U-Sleep for PSG</h3>
            <p>The U-Sleep model is a well-established, state-of-the-art algorithm for PSG sleep scoring. The version used in NIDRA (<code>u-sleep-nsrr-2024</code>) is a robust implementation trained on a large dataset. It demonstrates high performance across diverse populations and recording conditions. For detailed performance metrics, please see the original <a href="https://www.nature.com/articles/s41746-021-00440-5">U-Sleep</a> and <a href="https://arxiv.org/abs/2506.08574v1">SLEEPYLAND</a> publications.</p>
        </section>

        <section id="understanding-results">
            <h2>Understanding your results</h2>
            <table class="minimal">
                <thead>
                    <tr><th style="text-align:left">Output file</th><th style="text-align:left">Description</th></tr>
                </thead>
                <tbody>
                    <tr><td><code>..._hypnogram.csv</code></td><td>A CSV with one integer per 30 s epoch (sleep stage code).</td></tr>
                    <tr><td><code>..._hypnodensity.csv</code></td><td>Classifier probabilities per epoch; one column per stage (Wake, N1, N2, N3, REM, Artifact).</td></tr>
                    <tr><td><code>..._figure.png</code></td><td>Plot with hypnogram, time-frequency spectrogram, and hypnodensity.</td></tr>
                    <tr><td><code>..._sleep_statistics.csv</code></td><td>Summary metrics (TST, efficiency, time in each stage, etc.).</td></tr>
                </tbody>
            </table>
<br><br>
            <table class="minimal">
                <tbody>
                    <tr>
                        <th style="text-align:left">Sleep stage</th>
                        <td>Wake</td>
                        <td>N1</td>
                        <td>N2</td>
                        <td>N3</td>
                        <td>REM</td>
                        <td>Artifact</td>
                    </tr>
                    <tr>
                        <th style="text-align:left">Code</th>
                        <td>0</td>
                        <td>1</td>
                        <td>2</td>
                        <td>3</td>
                        <td>5</td>
                        <td>6</td>
                    </tr>
                </tbody>
            </table>
        </section>


        <section id="python-package">





            <h2>Python endpoints</h2>

<h3>Example 1: Scoring a single PSG recording (minimal example)</h3>
            <pre>
import NIDRA

scorer = NIDRA.scorer(
    type  = 'psg',
    input = '/path/to/recording.edf'
)

scorer.score()
</pre>

            <h3>Example 2: Scoring a single wearable-EEG recording File (specifying optional parameters)</h3>
            <pre>
import NIDRA

scorer = NIDRA.scorer(
    type = 'forehead',
    input        = '/path/to/recording/',
    output       = '/path/to/output/folder/',
    model        = 'ez6moe',
    channels     = ['eegl','eegr'],
    hypnogram    = True,
    hypnodensity = True,
    plot         = True,
)

hypnogram, probabilities = scorer.score()
</pre>


            <h3>Example 3: Scoring multiple recordings</h3>
            <p>You can score multiple recordings in one go by simply specifying a folder path, which contains recordings or subfolders with recordings. NIDRA will scan subfolders recursively and score all edf files present. </p>
            <p>You can also specify the path of a .txt file that contains one path to a recording (or folder containing one or more recordings) per line.</p>

            <pre>
import NIDRA

scorer = NIDRA.scorer(
    type = 'psg',
    input = '/path/to/folder/'
)

scorer.score()
</pre>
            <h3>Example 4: Scoring in-memory data (e.g. real-time application)</h3>
            <p>You can also score data that you already have in memory as an array. This is useful for real-time applications or custom data loading pipelines. When scoring PSG data from an array, you must provide the sampling frequency (<code>sfreq</code>). Providing channel names is recommended but optional; if not provided, they will be auto-generated. All channels are then assumed to contain EEG data. By default, no output files are generated, but this can be enabled if desired.</p>
            <pre>
# create some dummy PSG data
import numpy as np
sfreq      = 256
channels   = ['F3-A2', 'C4-A1', 'O2-A1', 'EOG-L']
n_samples  = sfreq * 60 * 60  # 1 hour of data
dummy_data = np.random.randn(len(channels), n_samples)

import NIDRA

scorer = NIDRA.scorer(
    type='psg',
    input=dummy_data,
    sfreq=sfreq,
    channels=channels
)
hypnogram, probabilities = scorer.score()

</pre>



<br>
<h3>Reference</h3>

<p>Single user-facing entry point <code>NIDRA.scorer(...)</code>. The data source (PSG or wearable EEG) is selected by <code>scorer_type</code>. Then call <code>.score()</code> to run inference. This unified reference clarifies which parameters are required, optional, and their defaults across PSG/Forehead and file/array scenarios.</p>

<h4>Entrypoint</h4>
<pre><code>NIDRA.scorer(scorer_type: str, **kwargs) -> Scorer
Scorer.score(plot: bool = False) -> tuple[numpy.ndarray, numpy.ndarray]</code></pre>

<h4>Returns</h4>
<ul>
  <li><code>hypnogram</code> — 1D numpy array of integer stage codes for each 30 s epoch (see <a href="#understanding-results">Sleep Stage Key</a>)</li>
  <li><code>probabilities</code> — 2D array [n_epochs, n_classes] with per-epoch, per-class probabilities</li>
</ul>

<h4>Output files (when enabled)</h4>
<ul>
  <li><code>..._hypnogram.csv</code> — sleep stage code per epoch</li>
  <li><code>..._hypnodensity.csv</code> — per-epoch class probabilities</li>
  <li><code>..._figure.png</code> — plot (if <code>plot=True</code> and file writing enabled)</li>
</ul>
<br><br>

<table class="minimal">
  <thead>
    <tr>
      <th style="text-align:left">Parameter</th>
      <th style="text-align:left">Type</th>
      <th style="text-align:left">Default</th>
      <th style="text-align:left">Required?</th>
      <th style="text-align:left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>type</code></td>
      <td><code>str</code></td>
      <td>—</td>
      <td><strong>Yes</strong></td>
      <td>Type of data source: <code>'psg'</code> (full PSG with EEG and optionally EOG data) or <code>'forehead'</code> (wearable 2-channel EEG).</td>
    </tr>

    <tr>
      <td><code>input</code></td>
      <td><code>str | Path | numpy.ndarray</code></td>
      <td>—</td>
      <td><strong>Yes</strong></td>
      <td>
        Location of recording(s) to score.<br/>
        <strong>File path (str/Path):</strong> Path to recording file (EDF/BDF), folder, or .txt file with one path per line.<br/>
        <strong>Array (numpy.ndarray):</strong> In-memory signal data.<br/>
        PSG array: shape (C, N) — any number of channels C.<br/>
        Forehead array: shape (2, N) — exactly 2 channels required.
      </td>
    </tr>
    <tr>
      <td><code>output</code></td>
      <td><code>str</code></td>
      <td><code>input/autoscorer_output</code></td>
      <td>—</td>
      <td>Directory for outputs. Defaults to <code>autoscorer_output</code> in input folder. Required if using in-memory data and saving files.</td>
    </tr>    
    <tr>
      <td><code>sfreq</code></td>
      <td><code>float</code></td>
      <td>—</td>
      <td><strong>Required when <code>input</code> is an array</strong></td>
      <td>Sampling frequency (Hz) for array input.</td>
    </tr>
    <tr>
      <td><code>channels</code></td>
      <td><code>list[str]</code></td>
      <td>—</td>
      <td>—</td>
      <td>
        Define list of channels to use for scoring. In case of "forehead" data source, provide exactly two channels, unless scoring original ZMax file pairs, in which case do not provide any channels.
      </td>
    </tr>
    <tr>
      <td><code>model</code></td>
      <td><code>str</code></td>
      <td>
        PSG: <code>'u-sleep-nsrr-2024'</code><br/>
        Forehead: <code>'ez6'</code>
      </td>
      <td>—</td>
      <td>
        Selects the model to use.<br/>
        PSG: Default is <code>'u-sleep-nsrr-2024'</code> (USleep 2.0). Automatically falls back to EEG-only version (<code>..._eeg</code>) if no EOG channels are detected.<br/>
        Forehead: Use <code>'ez6'</code> (fast) or <code>'ez6moe'</code> (higher accuracy).
      </td>
    </tr>
    <tr>
      <td><code>hypnogram</code></td>
      <td><code>bool | None</code></td>
      <td>
        <code>True</code> when <code>input</code> is a file;<br/>
        <code>False</code> when <code>input</code> is an array
      </td>
      <td>—</td>
      <td>Controls whether hypnogram CSV is written to <code>output</code>.</td>
    </tr>
    <tr>
      <td><code>hypnodensity</code></td>
      <td><code>bool</code></td>
      <td><code>False</code></td>
      <td>—</td>
      <td>Controls whether probability CSV is written to <code>output</code>.</td>
    </tr>
    <tr>
      <td><code>plot</code></td>
      <td><code>bool</code></td>
      <td><code>False</code></td>
      <td>—</td>
      <td>Controls whether summary plot is written to <code>output</code>.</td>
    </tr>
  </tbody>
</table>

<h4>Additional semantics</h4>
<ul>
  <li><code>input</code> can handle file paths, folders or in-memory arrays.</li>
  <li>Forehead array input must be exactly two channels (2×N).</li>
  <li>Forehead file input: pass the LEFT or RIGHT channel EDF (other inferred) OR a single EDF containing both channels.</li>
  <li>Forehead single-file: optionally specify <code>channels</code>; otherwise the first two channels are used.</li>
  <li>All models operate on 30-second epochs; outputs align to full epochs.</li>
</ul>


<h4>Machine-readable schema</h4>
<pre><code>{
  "entrypoint": "NIDRA.scorer",
  "parameters": {
    "type": { "type": "string", "enum": ["psg", "forehead"], "required": true },
    "output":  { "type": "string", "required": false, "default_logic": "autoscorer_output folder in input directory" },

    "input":  {
      "type": "string | array[number]",
      "required": true,
      "description": "Path to file/folder/.txt list OR numpy array (channels x samples)"
    },
    "sfreq":       { "type": "number", "required_if": "input is array" },

    "channels":    { "type": "array[string]", "required": false },

    "model":  {
      "type": "string",
      "required": false,
      "defaults": { "psg": "u-sleep-nsrr-2024", "forehead": "ez6" }
    },

    "hypnogram": {
      "type": "boolean|null",
      "required": false,
      "default_logic": "True if input is file, False if array"
    },
    "hypnodensity": { "type": "boolean", "required": false, "default": false },
    "plot": { "type": "boolean", "required": false, "default": false }
  },

  "returns": {
    "hypnogram": { "type": "array[int]", "epoch_sec": 30 },
    "probabilities": { "type": "array[array[number]]", "shape": "[n_epochs, n_classes]" }
  },

  "artifacts": [
    "hypnogram.csv", "probabilities.csv", "figure.png (when plot=True)"
  ]
}</code></pre>


        <section id="faq">
            <h2>FAQ & troubleshooting</h2>
            <h3>Installation issues</h3>
            <p><strong>Q: I see a warning when trying to run the Windows .exe (Smartscreen or "Search on app store").</strong><br>
            A: Due to the restrictive environment of Windows, you may see a "Search on app store" popup window. Click "No". You may also see the blue Smartscreen warning. In that case click "More info" and then "run anyway". Should this fail, try installing via pip (see Installation section).</p>
            <p><strong>Q: I'm having trouble installing with pip. What can I do?</strong><br>
            A: Here are a few common troubleshooting steps:
            <ul>
                <li><strong>1. Update packages:</strong> Ensure you have the latest versions of pip, setuptools, and wheel, as outdated versions can cause issues. You can update them by running:
                    <pre><code>pip install --upgrade pip setuptools wheel</code></pre>
                </li>
                <li><strong>2. Use a clean virtual environment:</strong> Conflicts with other installed packages are a common source of errors. Creating a fresh virtual environment ensures that NIDRA's dependencies are installed in an isolated space. </li>
                <li><strong>3. Try installing with conda:</strong> Conda's package management is often better at resolving complex dependencies. Follow the Conda installation instructions in the <a href="#installation">Installation</a> section.</li>
            </ul></p>
            <h3>GUI issues</h3>
            <p><strong>Q: The GUI window is not appearing when I run <code>nidra</code>.</strong><br>
            A: Check the terminal for any error messages. This could be due to a missing dependency. Try reinstalling NIDRA in a clean virtual environment.</p>
            <p><strong>Q: Why is the "Run Autoscoring" button disabled?</strong><br>
            A: The button is disabled until you have selected a valid recording to sco.</p>
            <h3>Scoring errors</h3>
            <p><strong>Q: I got an error about "missing channels" or "could not find required channels".</strong><br>
            A: This is a common error that occurs when the channel labels in your EDF file do not match what the model expects, or when you've selected the wrong "Data Source". For example, selecting "PSG" for a forehead EEG file will cause this error. Double-check your data source selection and ensure your EDF channel labels are standard.</p>
            <h3>General questions</h3>
            <p><strong>Q: Can I trust the results?</strong><br>
            A: The models in NIDRA are validated and perform at a level comparable to human experts. However, like any automated algorithm, they are not perfect. It is always good practice to visually inspect the generated hypnogram plot for any obvious anomalies, especially for noisy or unusual recordings.</p>
        </section>

</section>

        <section id="how-to-cite">
            <h2>How to cite NIDRA</h2>
            <p>If you use NIDRA in your research, please cite both the NIDRA software itself and the paper for the specific model you used.</p>
            <h3>1. Citing the NIDRA Software</h3>
            <p>Please cite this repository to ensure reproducibility:</p>
            <pre>
Zerr, P. (2025). NIDRA: super simple sleep scoring. GitHub. https://github.com/paulzerr/nidra
</pre>
            <h3>2. Citing the Scoring Model</h3>
            <p><strong>If you used the ez6 or ez6moe models:</strong></p>
            <pre>
Coon WG, Zerr P, Milsap G, et al. (2025). ezscore-f: A Set of Freely Available, Validated Sleep Stage Classifiers for Forehead EEG. bioRxiv. doi: 10.1101/2025.06.02.657451.
</pre>
            <p><strong>If you used the u-sleep-nsrr-2024 model:</strong></p>
            <p>Please cite the original U-Sleep paper and the SLEEPYLAND paper for the re-trained model weights:</p>
            <pre>
Perslev, M., et al. (2021). U-Sleep: resilient high-frequency sleep staging. NPJ digital medicine.
Rossi, A. D., et al. (2025). SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models. arXiv preprint.
</pre>
        </section>


        <section id="attribution">
            <h2>Attribution</h2>
            <p>ez6 and ez6moe models were developed by Coon et al., see:
            <br>Coon WG, Zerr P, Milsap G, Sikder N, Smith M, Dresler M, Reid M.
            <br><strong>ezscore-f: A Set of Freely Available, Validated Sleep Stage Classifiers for Forehead EEG.</strong>
            <br><a href="https://www.biorxiv.org/content/10.1101/2025.06.02.657451v1">https://www.biorxiv.org/content/10.1101/2025.06.02.657451v1</a>
            <br><a href="https://github.com/coonwg1/ezscore">github.com/coonwg1/ezscore</a></p>

            <p>U-Sleep models were developed by  Perslev et al., see:
            <br>Perslev, M., Darkner, S., Kempfner, L., Nikolic, M., Jennum, P. J., & Igel, C. (2021).
            <br><strong>U-Sleep: resilient high-frequency sleep staging.</strong> NPJ digital medicine
            <br><a href="https://www.nature.com/articles/s41746-021-00440-5">https://www.nature.com/articles/s41746-021-00440-5</a>
            <br><a href="https://github.com/perslev/U-Time">https://github.com/perslev/U-Time</a></p>

            <p>The U-Sleep model weights used in this repo were re-trained by Rossi et al., see:
            <br>Rossi, A. D., Metaldi, M., Bechny, M., Filchenko, I., van der Meer, J., Schmidt, M. H., ... & Fiorillo, L. (2025).
            <br><strong>SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models.</strong> arXiv preprint arXiv:2506.08574.
            <br><a href="https://arxiv.org/abs/2506.08574v1">https://arxiv.org/abs/2506.08574v1</a>
            <br><a href="https://github.com/biomedical-signal-processing/sleepyland">https://github.com/biomedical-signal-processing/sleepyland</a></p>

          </section>

        <section id="license">
            <h2>License</h2>
            <p>This project is licensed under the MIT License. See the LICENSE file for details.</p>
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>For questions, bug reports, or feedback, please contact Paul Zerr at <a href="mailto:zerr.paul@gmail.com">zerr.paul@gmail.com</a> or open a github issue.</p>
        </section>

    </main>
</body>
</html>

