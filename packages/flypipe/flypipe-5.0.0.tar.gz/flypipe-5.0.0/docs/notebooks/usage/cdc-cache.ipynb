{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CDC Cache - Change Data Capture\n",
        "\n",
        "CDC (Change Data Capture) is a pattern used to track and process only the changes (inserts, updates, deletes) in your data, rather than reprocessing the entire dataset every time.\n",
        "\n",
        "The `Cache` class in Flypipe supports optional CDC functionality through unified `read()` and `write()` methods with optional CDC parameters. This enables incremental processing where only new or changed data flows through your pipeline.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- **Unified Cache Interface**: CDC is built into the base `Cache` class through optional parameters\n",
        "- **`create_cdc_table()`**: Creates the CDC metadata table (optional, for caches with CDC support)\n",
        "- **`read()` with CDC**: Accepts optional `from_node` and `to_node` parameters for filtering\n",
        "- **`write()` with CDC**: Accepts optional `upstream_nodes`, `to_node`, and `datetime_started_transformation` parameters\n",
        "- **`cdc_datetime_updated`**: Timestamp column tracking when each row was last processed\n",
        "- **Incremental Processing**: Process only new/changed rows and append to existing results\n",
        "- **Static Nodes**: Mark nodes with `.static()` to skip CDC filtering and always load complete cached data\n",
        "- **Parallel Execution**: Runner executes independent nodes in parallel using ThreadPoolExecutor\n",
        "\n",
        "## When to Use CDC Cache\n",
        "\n",
        "- **Large datasets**: When reprocessing all data is expensive\n",
        "- **Incremental loads**: When source data arrives in batches over time\n",
        "- **Data warehousing**: Building fact tables with regular updates\n",
        "- **Real-time pipelines**: Processing streaming data in micro-batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing CDC Cache\n",
        "\n",
        "To implement CDC cache, you need to:\n",
        "\n",
        "1. Create a `CDCManager` class to track CDC metadata (timestamps, source/destination nodes)\n",
        "2. Create a cache class that extends `Cache` and implements:\n",
        "   - Standard cache methods: `read()`, `write()`, `exists()`\n",
        "   - Optionally override `create_cdc_table()` for CDC support\n",
        "   - The `read()` method should handle optional `from_node` and `to_node` parameters for CDC filtering\n",
        "   - The `write()` method should handle optional `upstream_nodes`, `to_node`, and `datetime_started_transformation` parameters for CDC metadata\n",
        "   - The `CDCManager` is created internally within the cache class\n",
        "   - Specify `merge_keys` for Delta Lake MERGE INTO operations\n",
        "3. Use `CacheMode.MERGE` to trigger upsert operations on cached data\n",
        "4. The Runner handles parallel execution and calls `create_cdc_table()` before executing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: CDC Manager with Spark Table\n",
        "\n",
        "The `CDCManager` tracks when each edge (source â†’ destination) in the pipeline was last processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
        "\n",
        "\n",
        "class CDCManager:\n",
        "    \"\"\"Manager that tracks CDC metadata using a Spark table\"\"\"\n",
        "    \n",
        "    def __init__(self, table=\"cdc_metadata\", schema=\"default\", catalog=None):\n",
        "        self.table = table\n",
        "        self.schema = schema\n",
        "        self.catalog = catalog\n",
        "    \n",
        "    @property\n",
        "    def full_table_name(self):\n",
        "        \"\"\"Returns the fully qualified table name\"\"\"\n",
        "        if self.catalog:\n",
        "            return f\"{self.catalog}.{self.schema}.{self.table}\"\n",
        "        return f\"{self.schema}.{self.table}\"\n",
        "    \n",
        "    def create_table(self, spark):\n",
        "        \"\"\"\n",
        "        Create CDC metadata table if it doesn't exist.\n",
        "        This is called by the Runner before parallel execution to avoid concurrent creation conflicts.\n",
        "        \"\"\"\n",
        "        if spark.catalog.tableExists(self.full_table_name):\n",
        "            return\n",
        "        \n",
        "        cdc_schema = StructType([\n",
        "            StructField(\"source\", StringType(), False),\n",
        "            StructField(\"destination\", StringType(), False),\n",
        "            StructField(\"cdc_datetime_updated\", TimestampType(), False)\n",
        "        ])\n",
        "        \n",
        "        # Create database if it doesn't exist\n",
        "        if not spark.catalog.databaseExists(self.schema):\n",
        "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.schema}\")\n",
        "        \n",
        "        try:\n",
        "            # Create empty table\n",
        "            empty_df = spark.createDataFrame([], cdc_schema)\n",
        "            empty_df.write.format(\"delta\").mode(\"append\").partitionBy(\n",
        "                    \"source\", \"destination\"\n",
        "                ).saveAsTable(self.full_table_name)\n",
        "            print(f\"Created CDC metadata table: {self.full_table_name}\")\n",
        "        except Exception as e:\n",
        "            # If another process created it concurrently, ignore\n",
        "            if not spark.catalog.tableExists(self.full_table_name):\n",
        "                raise e\n",
        "    \n",
        "    def write(self, spark, upstream_node, to_node, timestamp):\n",
        "        \"\"\"\n",
        "        Write CDC timestamp entry for upstream_node -> to_node.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        spark : SparkSession\n",
        "            Spark session\n",
        "        upstream_node : Node\n",
        "            The upstream node that provided data\n",
        "        to_node : Node\n",
        "            Destination/target node being processed\n",
        "        timestamp : datetime\n",
        "            Timestamp when processing occurred\n",
        "        \"\"\"\n",
        "        from pyspark.sql.functions import lit\n",
        "        \n",
        "        # Create entry for this edge\n",
        "        entry = (upstream_node.__name__, to_node.__name__, timestamp)\n",
        "        \n",
        "        cdc_schema = StructType([\n",
        "            StructField(\"source\", StringType(), False),\n",
        "            StructField(\"destination\", StringType(), False),\n",
        "            StructField(\"cdc_datetime_updated\", TimestampType(), False)\n",
        "        ])\n",
        "        new_entry_df = spark.createDataFrame([entry], cdc_schema)\n",
        "        \n",
        "        # Append to CDC metadata table\n",
        "        new_entry_df.write.format(\"delta\").mode(\"append\").partitionBy(\n",
        "            \"source\", \"destination\"\n",
        "        ).saveAsTable(self.full_table_name, mergeSchema=True)\n",
        "    \n",
        "    def filter(self, spark, from_node, to_node, df):\n",
        "        \"\"\"\n",
        "        Filter dataframe to return only rows with cdc_datetime_updated after last processed timestamp.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        spark : SparkSession\n",
        "            Spark session\n",
        "        from_node : Node\n",
        "            Source node\n",
        "        to_node : Node\n",
        "            Destination node\n",
        "        df : DataFrame\n",
        "            DataFrame to filter\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        DataFrame\n",
        "            Filtered dataframe with only new rows\n",
        "        \"\"\"\n",
        "        if not from_node or not to_node:\n",
        "            return df\n",
        "\n",
        "        # Get the last processed timestamp for this edge\n",
        "        edge_query = f\"\"\"\n",
        "            SELECT MAX(cdc_datetime_updated) as last_timestamp\n",
        "            FROM {self.full_table_name}\n",
        "            WHERE source = '{from_node.__name__}' \n",
        "              AND destination = '{to_node.__name__}'\n",
        "        \"\"\"\n",
        "        \n",
        "        last_timestamp_df = spark.sql(edge_query)\n",
        "        last_timestamp = last_timestamp_df.collect()[0][\"last_timestamp\"]\n",
        "        \n",
        "        if last_timestamp is None:\n",
        "            # No previous run, return all data\n",
        "            print(f\"  â†’ No CDC history found for {from_node.__name__} â†’ {to_node.__name__}, processing all data\")\n",
        "            return df\n",
        "        \n",
        "        # Filter dataframe to return only rows updated after last timestamp\n",
        "        if \"cdc_datetime_updated\" in df.columns:\n",
        "            filtered_df = df.filter(df[\"cdc_datetime_updated\"] > last_timestamp)\n",
        "            row_count = filtered_df.count()\n",
        "            print(f\"  â†’ CDC filter: {row_count} new rows since {last_timestamp}\")\n",
        "            return filtered_df\n",
        "        \n",
        "        # If no cdc_datetime_updated column, return all data\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: CDC Cache Implementation\n",
        "\n",
        "This cache implementation uses Delta Lake tables for storage and integrates with the CDC manager for timestamp tracking. It uses Delta Lake's MERGE INTO for upsert operations based on merge keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flypipe.cache import Cache\n",
        "\n",
        "\n",
        "class IncrementalCDCCache(Cache):\n",
        "    \"\"\"Cache that supports incremental CDC processing with Spark tables\"\"\"\n",
        "    \n",
        "    def __init__(self, table, merge_keys, schema=\"default\", catalog=None, cdc_table=\"cdc_metadata\"):\n",
        "        super().__init__()\n",
        "        self.table = table\n",
        "        self.merge_keys = merge_keys\n",
        "        self.schema = schema\n",
        "        self.catalog = catalog\n",
        "        self.cdc_table = cdc_table\n",
        "        self.cdc_manager = CDCManager(table=cdc_table, schema=schema, catalog=catalog)\n",
        "    \n",
        "    @property\n",
        "    def full_table_name(self):\n",
        "        \"\"\"Returns the fully qualified table name\"\"\"\n",
        "        if self.catalog:\n",
        "            return f\"{self.catalog}.{self.schema}.{self.table}\"\n",
        "        return f\"{self.schema}.{self.table}\"\n",
        "    \n",
        "    def read(self, spark, from_node=None, to_node=None, is_static=False):\n",
        "        \"\"\"\n",
        "        Read cached data from Delta table with optional CDC filtering.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        from_node : Node, optional\n",
        "            Source node for CDC filtering\n",
        "        to_node : Node, optional\n",
        "            Destination node for CDC filtering\n",
        "        is_static : bool, optional\n",
        "            If True, skip CDC filtering and load complete cached data (default: False)\n",
        "        \"\"\"\n",
        "        df = spark.table(self.full_table_name)\n",
        "        \n",
        "        # Apply CDC filtering if nodes are provided and is_static is False\n",
        "        if from_node is not None and to_node is not None and not is_static:\n",
        "            df = self.cdc_manager.filter(spark, from_node, to_node, df)\n",
        "        elif is_static:\n",
        "            row_count = df.count()\n",
        "            print(f\"  â†’ Static node {from_node.__name__}, loaded {row_count} rows (no CDC filtering)\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def write(self, spark, df, upstream_nodes=None, to_node=None, datetime_started_transformation=None):\n",
        "        \"\"\"\n",
        "        Write cache - merge into existing Delta table using MERGE INTO with optional CDC metadata.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        df : DataFrame\n",
        "            DataFrame to cache\n",
        "        upstream_nodes : List[Node], optional\n",
        "            List of upstream cached nodes for CDC tracking\n",
        "        to_node : Node, optional\n",
        "            Destination node for CDC tracking\n",
        "        datetime_started_transformation : datetime, optional\n",
        "            Timestamp when transformation started for CDC tracking\n",
        "        \"\"\"\n",
        "        \n",
        "        # Handle regular cache write\n",
        "        if df.isEmpty():\n",
        "            return\n",
        "            \n",
        "        \n",
        "        if not self.exists(spark):\n",
        "            # First write - create table\n",
        "            print(f\"  â†’ Creating table {self.full_table_name}\")\n",
        "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(self.full_table_name)\n",
        "        else:\n",
        "            # Subsequent writes - merge into existing table\n",
        "            print(f\"  â†’ Merging into table {self.full_table_name}\")\n",
        "            df.createOrReplaceTempView(\"updates\")\n",
        "            \n",
        "            # Build merge condition based on merge keys\n",
        "            merge_condition = \" AND \".join([f\"target.{key} = source.{key}\" for key in self.merge_keys])\n",
        "            \n",
        "            merge_query = f\"\"\"\n",
        "                MERGE INTO {self.full_table_name} AS target\n",
        "                USING updates AS source\n",
        "                ON {merge_condition}\n",
        "                WHEN MATCHED THEN UPDATE SET *\n",
        "                WHEN NOT MATCHED THEN INSERT *\n",
        "            \"\"\"\n",
        "            \n",
        "            spark.sql(merge_query)\n",
        "        \n",
        "        # Handle CDC metadata write\n",
        "        if upstream_nodes and to_node and datetime_started_transformation:\n",
        "            for upstream_node in upstream_nodes:\n",
        "                self.cdc_manager.write(spark, upstream_node, to_node, datetime_started_transformation)\n",
        "    \n",
        "    def exists(self, spark):\n",
        "        return spark.catalog.tableExists(self.table, self.schema)\n",
        "    \n",
        "    def create_cdc_table(self, spark):\n",
        "        \"\"\"\n",
        "        Ensure CDC metadata table exists (for thread-safe parallel execution).\n",
        "        Called by the Runner before parallel execution to avoid concurrent creation conflicts.\n",
        "        \"\"\"\n",
        "        self.cdc_manager.create_table(spark)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Define the Pipeline with CDC Cache\n",
        "\n",
        "We'll create a simple pipeline:\n",
        "\n",
        "```\n",
        "    t1 (input passthrough, no cache)\n",
        "     |\n",
        "     v\n",
        "    t2 (transformation with CDC cache)\n",
        "     |\n",
        "     v\n",
        "    t3 (final node with CDC cache)\n",
        "```\n",
        "\n",
        "Each transformation adds a `cdc_datetime_updated` timestamp to track when rows were processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flypipe import node\n",
        "from pyspark.sql.functions import current_timestamp\n",
        "\n",
        "\n",
        "@node(type=\"pyspark\")\n",
        "def t1():\n",
        "    \"\"\"Source node - passthrough for input data\"\"\"\n",
        "    return spark.createDataFrame(\n",
        "        data=[(1, \"Alice\"), (2, \"Bob\")],\n",
        "        schema=[\"id\", \"name\"]\n",
        "    )\n",
        "\n",
        "\n",
        "@node(\n",
        "    type=\"pyspark\",\n",
        "    dependencies=[t1],\n",
        "    cache=IncrementalCDCCache(\n",
        "        table=\"customer_data_processed\",\n",
        "        merge_keys=[\"id\"],\n",
        "        schema=\"default\",\n",
        "        cdc_table=\"cdc_metadata\"\n",
        "    )\n",
        ")\n",
        "def t2(t1):\n",
        "    \"\"\"\n",
        "    Transformation node with CDC cache.\n",
        "    Adds processing logic and timestamp.\n",
        "    \"\"\"\n",
        "    return t1.selectExpr(\n",
        "        \"id\",\n",
        "        \"name\",\n",
        "        \"CONCAT(name, '_processed') as processed_name\"\n",
        "    ).withColumn(\"cdc_datetime_updated\", current_timestamp())\n",
        "\n",
        "\n",
        "@node(\n",
        "    type=\"pyspark\",\n",
        "    dependencies=[t2],\n",
        "    cache=IncrementalCDCCache(\n",
        "        table=\"customer_data_final\",\n",
        "        merge_keys=[\"id\"],\n",
        "        schema=\"default\",\n",
        "        cdc_table=\"cdc_metadata\"\n",
        "    )\n",
        ")\n",
        "def t3(t2):\n",
        "    \"\"\"\n",
        "    Final node with CDC cache.\n",
        "    Adds business logic and maintains timestamp.\n",
        "    \"\"\"\n",
        "    return t2.selectExpr(\n",
        "        \"id\",\n",
        "        \"name\",\n",
        "        \"processed_name\",\n",
        "        \"id * 100 as customer_score\",\n",
        "        \"cdc_datetime_updated\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup - Clean Environment\n",
        "\n",
        "Before running our example, let's clean up any existing tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up any existing tables\n",
        "spark.sql(\"DROP TABLE IF EXISTS default.customer_data_processed\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS default.customer_data_final\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS default.cdc_metadata\")\n",
        "\n",
        "print(\"âœ“ Environment cleaned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Run - Initial Data Load\n",
        "\n",
        "Process the initial batch of customer data (customers 1 and 2).\n",
        "\n",
        "We use `CacheMode.MERGE` to enable incremental appending to the cache tables. The `max_workers=2` parameter enables parallel execution, allowing independent nodes to run concurrently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flypipe.cache import CacheMode\n",
        "\n",
        "# First batch of customer data\n",
        "initial_input = spark.createDataFrame(\n",
        "    data=[(1, \"Alice\"), (2, \"Bob\")],\n",
        "    schema=[\"id\", \"name\"]\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FIRST RUN - Processing initial data (customers 1, 2)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result1 = t3.run(\n",
        "    spark,\n",
        "    inputs={t1: initial_input},\n",
        "    cache={t2: CacheMode.MERGE, t3: CacheMode.MERGE},\n",
        "    max_workers=2  # Enable parallel execution with 2 workers\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ“Š Result of first run:\")\n",
        "result1.show()\n",
        "\n",
        "print(\"\\nðŸ“¦ Cached data in final table:\")\n",
        "spark.table(\"default.customer_data_final\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second Run - Incremental Load (CDC in Action!)\n",
        "\n",
        "Now we'll process a new batch with only new customers (3 and 4).\n",
        "\n",
        "The CDC cache will:\n",
        "1. Filter the data based on timestamps\n",
        "2. Process only the new rows\n",
        "3. Append results to the existing cache tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Second batch - only NEW customers\n",
        "new_input = spark.createDataFrame(\n",
        "    data=[(3, \"Charlie\"), (4, \"Diana\")],  # Only new rows\n",
        "    schema=[\"id\", \"name\"]\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SECOND RUN - Processing new data (customers 3, 4)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result2 = t3.run(\n",
        "    spark,\n",
        "    inputs={t1: new_input},\n",
        "    cache={t2: CacheMode.MERGE, t3: CacheMode.MERGE},\n",
        "    max_workers=2  # Enable parallel execution with 2 workers\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ“Š Result of second run (only new rows processed):\")\n",
        "result2.show()\n",
        "\n",
        "print(\"\\nðŸ“¦ Full cached data in final table (all historical data):\")\n",
        "spark.table(\"default.customer_data_final\").orderBy(\"id\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing CDC Metadata\n",
        "\n",
        "The CDC manager tracks when each edge (source â†’ destination) in the pipeline was last processed, in the context of a specific root/target node.\n",
        "\n",
        "The CDC metadata table has the following columns:\n",
        "- **source**: The upstream node name\n",
        "- **destination**: The downstream/target node name  \n",
        "- **cdc_datetime_updated**: Timestamp when this edge was processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“‹ CDC Metadata Table:\")\n",
        "print(\"Tracks when each pipeline edge was last processed (source â†’ destination)\\n\")\n",
        "spark.table(\"default.cdc_metadata\").orderBy(\"source\", \"destination\", \"cdc_datetime_updated\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How It Works\n",
        "\n",
        "### The Runner\n",
        "\n",
        "Flypipe uses a `Runner` class to execute the transformation graph:\n",
        "\n",
        "1. **Creates Execution Plan**: Organizes nodes into levels based on dependencies\n",
        "2. **Parallel Execution**: Nodes at the same level can run in parallel (controlled by `max_workers`)\n",
        "3. **CDC Table Creation**: Calls `create_cdc_table()` before executing each level to avoid concurrent creation conflicts\n",
        "4. **Memoization**: Stores intermediate results in `run_context.node_results` to avoid re-computation\n",
        "\n",
        "### First Run (Initial Load)\n",
        "- **Input**: Customers 1 and 2\n",
        "- **Processing**: \n",
        "  - Runner creates execution plan: Level 0: [t1], Level 1: [t2], Level 2: [t3]\n",
        "  - All nodes execute transformations\n",
        "  - With `max_workers=2`, independent nodes could run in parallel\n",
        "- **Caching**: Results written to Delta tables with timestamps (CREATE TABLE)\n",
        "- **CDC Metadata**: Records when edges were processed (source â†’ destination)\n",
        "  - Example: `t1 â†’ t2`, `t2 â†’ t3` with timestamps\n",
        "        - Each upstream node writes its own CDC metadata entry\n",
        "\n",
        "### Second Run (Incremental Load)\n",
        "- **Input**: Customers 3 and 4 (NEW data only)\n",
        "- **CDC Filtering**: \n",
        "  - `read_cdc()` checks timestamps from CDC metadata for this specific source â†’ destination edge\n",
        "  - Returns only rows with `cdc_datetime_updated` > last processed time for that edge\n",
        "  - In this case, all rows are new (no filtering needed)\n",
        "- **Processing**: \n",
        "  - Runner re-creates execution plan for new data\n",
        "  - Only new customers are transformed\n",
        "  - Parallel execution when possible (based on `max_workers`)\n",
        "- **Caching**: Results merged into existing tables using Delta Lake's MERGE INTO\n",
        "  - Uses `merge_keys` (e.g., `id`) to match existing records\n",
        "  - WHEN MATCHED: Updates existing records\n",
        "  - WHEN NOT MATCHED: Inserts new records\n",
        "- **Final State**: Cache contains all 4 customers (1, 2, 3, 4)\n",
        "\n",
        "### Benefits\n",
        "\n",
        "âœ… **Performance**: Only process new/changed data  \n",
        "âœ… **Storage**: Maintain complete historical data in cache  \n",
        "âœ… **Scalability**: Handle large datasets incrementally  \n",
        "âœ… **Auditability**: Track when each edge was processed  \n",
        "âœ… **Upsert Logic**: Automatically handles inserts and updates with MERGE INTO  \n",
        "âœ… **Parallel Execution**: Independent nodes execute concurrently for better performance  \n",
        "âœ… **Thread-Safe**: CDC table creation is handled before parallel execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Querying CDC Metadata\n",
        "\n",
        "You can query the CDC metadata to understand your pipeline's processing history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the last processing time for each edge (source -> destination)\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        source,\n",
        "        destination,\n",
        "        MAX(cdc_datetime_updated) as last_processed,\n",
        "        COUNT(*) as run_count\n",
        "    FROM default.cdc_metadata\n",
        "    GROUP BY source, destination\n",
        "    ORDER BY source, destination\n",
        "\"\"\"\n",
        "\n",
        "print(\"ðŸ“Š CDC Processing Summary (grouped by root, source, destination):\")\n",
        "spark.sql(query).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Static Nodes - Skipping CDC Filtering\n",
        "\n",
        "Sometimes you have nodes that contain reference data or lookup tables that don't change frequently, or you want to always load the complete cached dataset regardless of CDC timestamps. For these cases, you can mark a node as **static** using the `.static()` method.\n",
        "\n",
        "### What is a Static Node?\n",
        "\n",
        "A static node is a cached node that **skips CDC filtering** when reading from cache. When a node is marked as static:\n",
        "\n",
        "- âœ… The entire cached dataset is loaded (no timestamp filtering)\n",
        "- âœ… CDC metadata is still tracked for the node\n",
        "- âœ… The node can still use `CacheMode.MERGE` to update its cache\n",
        "- âœ… Downstream nodes receive the complete dataset, not just incremental changes\n",
        "\n",
        "### When to Use Static Nodes\n",
        "\n",
        "Use `.static()` for:\n",
        "\n",
        "- **Reference/lookup tables**: Product catalogs, country codes, currency exchange rates\n",
        "- **Dimension tables**: Small dimension tables that are always needed in full\n",
        "- **Configuration data**: Application settings, business rules\n",
        "- **Small datasets**: When the cost of loading all data is negligible\n",
        "- **Expensive computations**: Complex transformations you want to compute once and reuse in full\n",
        "\n",
        "### Example: Product Catalog with Static Lookup\n",
        "\n",
        "```python\n",
        "@node(type=\"pyspark\", cache=ProductCache(...))\n",
        "def product_catalog():\n",
        "    \"\"\"Load product reference data (updated weekly)\"\"\"\n",
        "    return spark.read.parquet(\"s3://data/products/\")\n",
        "\n",
        "@node(type=\"pyspark\", cache=OrderCache(...))\n",
        "def orders():\n",
        "    \"\"\"Load orders (updated daily)\"\"\"\n",
        "    return spark.read.parquet(\"s3://data/orders/\")\n",
        "\n",
        "@node(type=\"pyspark\", dependencies=[orders, product_catalog.static()])\n",
        "def enriched_orders(orders, product_catalog):\n",
        "    \"\"\"Join orders with COMPLETE product catalog (no CDC filtering)\"\"\"\n",
        "    return orders.join(product_catalog, \"product_id\", \"left\")\n",
        "\n",
        "# First run: Process 100 orders, cache product catalog\n",
        "enriched_orders.run(\n",
        "    spark,\n",
        "    cache={\n",
        "        orders: CacheMode.MERGE,\n",
        "        product_catalog: CacheMode.MERGE\n",
        "    },\n",
        "    max_workers=2\n",
        ")\n",
        "\n",
        "# Second run: Process 50 new orders\n",
        "# - orders: Only new 50 orders (CDC filtered)\n",
        "# - product_catalog: ALL products loaded (static, no CDC filtering)\n",
        "enriched_orders.run(\n",
        "    spark,\n",
        "    cache={\n",
        "        orders: CacheMode.MERGE,\n",
        "        product_catalog: CacheMode.MERGE\n",
        "    },\n",
        "    max_workers=2\n",
        ")\n",
        "```\n",
        "\n",
        "### How It Works\n",
        "\n",
        "1. **Without `.static()`** (default CDC behavior):\n",
        "   ```python\n",
        "   @node(dependencies=[product_catalog])  # CDC filtering applied\n",
        "   def enriched_orders(product_catalog):\n",
        "       # product_catalog contains only NEW/CHANGED products since last run\n",
        "       pass\n",
        "   ```\n",
        "\n",
        "2. **With `.static()`** (skip CDC filtering):\n",
        "   ```python\n",
        "   @node(dependencies=[product_catalog.static()])  # NO CDC filtering\n",
        "   def enriched_orders(product_catalog):\n",
        "       # product_catalog contains ALL products (complete cache)\n",
        "       pass\n",
        "   ```\n",
        "\n",
        "### Technical Details\n",
        "\n",
        "When implementing a cache with CDC support, you must accept and check the `is_static` parameter before applying CDC filtering.\n",
        "\n",
        "**Important**: Only call `cdc_manager.filter()` if `is_static` is `False`:\n",
        "\n",
        "```python\n",
        "def read(self, spark, from_node=None, to_node=None, is_static=False):\n",
        "    \"\"\"Read cached data with optional CDC filtering\"\"\"\n",
        "    df = spark.table(self.full_table_name)\n",
        "    \n",
        "    # Apply CDC filtering ONLY if nodes are provided AND is_static is False\n",
        "    if from_node is not None and to_node is not None and not is_static:\n",
        "        df = self.cdc_manager.filter(spark, from_node, to_node, df)\n",
        "    elif is_static:\n",
        "        row_count = df.count()\n",
        "        print(f\"  â†’ Static node {from_node.__name__}, loaded {row_count} rows (no CDC filtering)\")\n",
        "    \n",
        "    return df\n",
        "```\n",
        "\n",
        "**Key points:**\n",
        "- Accept `is_static` parameter in your `read()` method signature\n",
        "- Check `is_static` parameter before filtering\n",
        "- If `is_static` is `True`, skip CDC filtering entirely\n",
        "- The complete cached dataset is returned for static nodes\n",
        "- Optional: Log when loading static nodes for debugging\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "âœ… **DO** use `.static()` for tables needed in full  \n",
        "âœ… **DO** combine static nodes with CDC-filtered fact tables\n",
        "âŒ **DON'T** use `.static()` when you need incremental processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Use Cases\n",
        "\n",
        "### Example 1: Daily Batch Processing\n",
        "\n",
        "```python\n",
        "# Day 1: Process initial load\n",
        "initial_customers = spark.read.parquet(\"s3://data/customers/2024-01-01/\")\n",
        "pipeline.run(spark, inputs={source_node: initial_customers}, cache={...CacheMode.MERGE})\n",
        "\n",
        "# Day 2: Process only new/changed customers\n",
        "new_customers = spark.read.parquet(\"s3://data/customers/2024-01-02/\")\n",
        "pipeline.run(spark, inputs={source_node: new_customers}, cache={...CacheMode.MERGE})\n",
        "# CDC automatically filters and processes only new rows\n",
        "```\n",
        "\n",
        "### Example 2: Streaming Data Processing\n",
        "\n",
        "```python\n",
        "# Micro-batch 1 (10:00 AM)\n",
        "batch1 = spark.read.format(\"kafka\")...\n",
        "pipeline.run(spark, inputs={source: batch1}, cache={...CacheMode.MERGE})\n",
        "\n",
        "# Micro-batch 2 (10:05 AM) - only new events\n",
        "batch2 = spark.read.format(\"kafka\")...\n",
        "pipeline.run(spark, inputs={source: batch2}, cache={...CacheMode.MERGE})\n",
        "```\n",
        "\n",
        "### Example 3: Data Warehouse Updates\n",
        "\n",
        "```python\n",
        "# Weekly dimension table update\n",
        "# CDC ensures only new/changed dimension records are processed\n",
        "# and merged into the existing warehouse table\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "CDC Cache in Flypipe enables efficient incremental data processing by:\n",
        "\n",
        "1. **Tracking timestamps** - CDC metadata table records when each pipeline edge was processed (source â†’ destination)\n",
        "2. **Filtering data** - `read()` returns only rows updated after the last processing timestamp (unless node is static)\n",
        "3. **Static nodes** - Use `.static()` to skip CDC filtering and load complete cached datasets for reference data\n",
        "4. **Appending results** - `CacheMode.MERGE` uses Delta Lake's MERGE INTO for upsert operations\n",
        "5. **Maintaining history** - Full dataset is preserved in cache while processing only deltas\n",
        "6. **Parallel execution** - Runner executes independent nodes concurrently (controlled by `max_workers`)\n",
        "\n",
        "### Key Components\n",
        "\n",
        "- **Runner**: Executes the transformation graph with parallel execution support\n",
        "  - Creates execution plan with levels based on dependencies\n",
        "  - Calls `create_cdc_table()` before each level to ensure thread-safe CDC table creation\n",
        "  - Executes nodes in parallel within each level (when `max_workers` > 1)\n",
        "  - Memoizes results in `run_context.node_results`\n",
        "\n",
        "- **CDCManager**: Manages CDC metadata in a Spark Delta table (created internally by the cache)\n",
        "  - `create_table()`: Creates CDC metadata table (called by Runner before parallel execution)\n",
        "  - `write(spark, upstream_node, to_node, timestamp)`: Records when edges were processed\n",
        "  - `filter(spark, from_node, to_node, df)`: Filters DataFrame based on CDC timestamps (skipped for static nodes)\n",
        "\n",
        "- **IncrementalCDCCache**: Extends `Cache` with CDC-specific methods\n",
        "  - Accepts `spark`, `table`, `merge_keys`, `schema`, `catalog`, and `cdc_table` parameters\n",
        "  - `create_cdc_table(spark)`: Ensures CDC metadata table exists (for thread-safe parallel execution)\n",
        "  - `read(spark, from_node, to_node)`: Reads cached data with optional CDC filtering (skipped if `from_node.static`)\n",
        "  - `write(spark, df, upstream_nodes, to_node, datetime_started_transformation)`: Writes data and CDC metadata\n",
        "  - Uses Delta Lake's MERGE INTO for upsert operations based on `merge_keys`\n",
        "  - Creates `CDCManager` internally in the constructor\n",
        "\n",
        "- **Static Nodes**: Use `.static()` method on node dependencies to skip CDC filtering\n",
        "  - Loads complete cached dataset instead of filtering by timestamp\n",
        "  - Ideal for reference tables, dimensions, and configuration data\n",
        "- **cdc_datetime_updated**: Timestamp column added to track row processing time\n",
        "- **CacheMode.MERGE**: Cache mode that triggers MERGE INTO for incremental updates\n",
        "- **max_workers**: Parameter to control parallel execution (defaults to `os.cpu_count() - 1`)\n",
        "\n",
        "This pattern is ideal for large-scale data pipelines where reprocessing all data would be inefficient or expensive.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
