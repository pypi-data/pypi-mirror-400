{"id":"s-5o87","uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","title":"ATLAS System Architecture","file_path":"specs/s-5o87_atlas_system_architecture.md","content":"# ATLAS System Architecture\n\n## Overview\n\nATLAS (Adaptive Trajectory Learning and Abstraction System) is a modular meta-learning framework that learns from agent trajectories to improve task-solving performance over time.\n\n**Core Insight**: The trajectory is the curriculum. Rather than designing tasks, we let agents attempt tasks and learn from the resulting trajectories.\n\n## Design Principles\n\n1. **Modular & Composable**: Every component works standalone and can be composed with any subset of other components\n2. **Emergent Organization**: Domains self-organize via embeddings rather than explicit hierarchy\n3. **Ablation-Friendly**: Easy to enable/disable pieces for experimentation\n4. **Protocol-Based**: Components depend on protocols, not concrete implementations\n5. **Agent-Centric**: Use ACP (Agent Client Protocol) as the abstraction layer - ATLAS orchestrates, agents execute\n\n## Agent Abstraction Layer\n\n**Key Design Decision**: ATLAS uses the Agent Client Protocol (ACP) via `acp-factory` as the primary agent abstraction.\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                       ATLAS                             │\n│  ┌─────────────┐  ┌──────────┐  ┌───────────────────┐  │\n│  │   Memory    │  │  Search  │  │     Learning      │  │\n│  │   System    │  │  Engine  │  │     Pipeline      │  │\n│  └──────┬──────┘  └────┬─────┘  └─────────┬─────────┘  │\n│         └──────────────┼──────────────────┘             │\n│                        ▼                                │\n│              ┌─────────────────┐                        │\n│              │  TaskExecutor   │                        │\n│              │  (ACP wrapper)  │                        │\n│              └────────┬────────┘                        │\n└────────────────────────┬────────────────────────────────┘\n                         ▼\n┌─────────────────────────────────────────────────────────┐\n│               ACP (acp-factory)                         │\n│     AgentFactory → Session → SessionUpdates             │\n│     (Claude Code, Codex, Gemini, OpenCode)              │\n└─────────────────────────────────────────────────────────┘\n```\n\n**Implications**:\n- No raw LLM wrapper needed - agents handle LLM interactions\n- All cognitive tasks (including internal ATLAS operations like analysis) go through agents\n- ATLAS focuses on orchestration, memory, and learning - not agent internals\n- Session forking enables search strategy branching (Mind Evolution, MCTS)\n\n## Core Primitives\n\n### Trajectory\nThe atomic unit of learning. Immutable record of an agent's attempt at a task.\n\n```\nTrajectory\n├── Task (id, domain, description, context, verification)\n├── Steps[] (thought, action, observation, metadata)\n├── Outcome (success, partial_score, error_info, verification_details)\n└── Metadata (agent_id, timestamp, embeddings)\n```\n\n### Environment\nExecution context for verification, with optional sandboxing.\n\n```\nEnvironment\n├── verify(task, candidate) → Outcome\n├── get_sandbox_handlers() → ClientHandlers | None  # Optional sandboxing\n└── Properties: deterministic, max_steps\n```\n\n**Execution Modes**:\n- **Free**: Agent runs freely, Environment only verifies results\n- **Sandboxed**: Environment provides handlers to control file/terminal access\n\n### TaskExecutor\nBridges ACP sessions and ATLAS's task/trajectory model.\n\n```\nTaskExecutor\n├── execute(task, env) → Trajectory\n├── execute_with_session(task, env, session) → Trajectory  # For forking\n└── Memory integration (upfront context + MCP tools)\n```\n\n## Three Pillars\n\n### Pillar 1: Memory Systems\n\nThree complementary memory types at different abstraction levels:\n\n| Memory | Purpose | Abstraction Level | Key Insight |\n|--------|---------|-------------------|-------------|\n| **Strategy Bank** | Abstract reasoning patterns | High (concept-level) | ArcMemo: concept > instance at all scales |\n| **Experience Memory** | Task-level retrieval | Medium (task-level) | ReMem: search-synthesize-evolve loop |\n| **Concept Library** | Reusable code patterns | Low (code-level) | Stitch+LILO: fast, interpretable extraction |\n\n**Memory Access** (Hybrid Approach):\n1. **Upfront Context**: Structured prompt with relevant experiences, strategies, concepts\n2. **On-Demand**: Memory MCP server exposes search tools for mid-execution queries\n\n**Interfaces**:\n- `ConceptLibrary`: add, search, compose, compress\n- `ExperienceMemory`: store, search, refine, prune  \n- `StrategyBank`: write, read, update_stats\n\n### Pillar 2: Search Methods\n\nTask Router decides which search strategy to use:\n\n| Strategy | Best For | Cost | Algorithm |\n|----------|----------|------|-----------|\n| **Direct** | High-confidence memory match | ~1 call | Retrieve → Adapt → Verify |\n| **Mind Evolution** | ARC-AGI, clear fitness | ~100 calls | Population-based evolutionary |\n| **SWE-Search** | SWE, sequential edits | ~200 calls | MCTS with UCB selection |\n\n**Session Forking**: ACP's `session.fork()` enables branching for:\n- Mind Evolution population exploration\n- MCTS tree expansion\n- Parallel candidate evaluation\n\n**Interfaces**:\n- `TaskRouter`: route(task, memory) → RoutingDecision\n- `SearchEngine`: search(task, routing, env) → List[Candidate]\n- `Verifier`: verify(task, candidate) → Outcome\n\n### Pillar 3: Learning Engine\n\nExtracts knowledge from trajectories to update memory:\n\n| Component | Purpose | Method |\n|-----------|---------|--------|\n| **Trajectory Analyzer** | Credit assignment, error patterns | Step attribution |\n| **Abstraction Extractor** | Code patterns, strategies | Stitch compression, ArcMemo abstraction |\n| **Hindsight Learner** | Model improvement | SOAR fine-tuning or SAGE memory-only |\n\n**Interfaces**:\n- `TrajectoryAnalyzer`: analyze(trajectory) → AnalysisResult\n- `AbstractionExtractor`: extract_code_patterns, extract_strategies, auto_document\n- `HindsightLearner`: prepare_training_data, should_finetune, finetune\n\n## Data Flow\n\n```\nTask → Memory Query → Router → Search → Verifier → Outcome\n              │                   │                   │\n              │            [ACP Sessions]             │\n              │          [Session Forking]            │\n              │                                       ▼\n              │                               Learning Engine\n              │                                       │\n              └───────────────── Memory Update ◄──────┘\n```\n\n## Modularity Requirements\n\nEach component MUST:\n1. Work standalone with sensible defaults\n2. Accept optional dependencies via constructor\n3. Handle missing dependencies gracefully (return empty, not crash)\n4. Be testable in isolation\n\nExample:\n```python\n# Full system\nsolver = ATLASSolver(\n    memory=MemorySystem(experience=exp, concepts=lib, strategies=bank),\n    executor=TaskExecutor(agent_type=\"claude-code\", memory=memory),\n)\n\n# Experience-only ablation\nsolver = ATLASSolver(\n    memory=MemorySystem(experience=exp),  # concepts/strategies omitted\n    executor=TaskExecutor(agent_type=\"claude-code\", memory=memory),\n)\n\n# No memory baseline\nsolver = ATLASSolver(\n    memory=MemorySystem(),  # empty\n    executor=TaskExecutor(agent_type=\"claude-code\"),\n)\n```\n\n## Module Structure\n\n```\natlas/\n├── core/           # Primitives: Trajectory, Task, Step, Outcome\n├── execution/      # TaskExecutor, TrajectoryBuilder, PromptFormatter\n├── mcp/            # Memory MCP server for agent access\n├── memory/         # Pillar 1: ConceptLibrary, ExperienceMemory, StrategyBank\n├── search/         # Pillar 2: Router, MindEvolution, SWESearch, Verifier\n├── learning/       # Pillar 3: Analyzer, Extractor, HindsightLearner\n├── environments/   # ARC, SWE environments (verification + optional sandbox)\n├── embeddings/     # BGE embeddings\n└── vector/         # ChromaDB, Pinecone adapters\n```\n\n## Key Technical Decisions\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| Agent Abstraction | ACP via acp-factory | Supports multiple agents (Claude Code, Codex, Gemini, OpenCode), session forking for search |\n| Embedding Model | BAAI/bge-base-en-v1.5 | Validated in ReMem paper |\n| Vector Store | ChromaDB (start) | Local, simple, migrate if needed |\n| Domain Organization | Emergent via embeddings | Self-organizing, no manual taxonomy |\n| Memory Access | Hybrid (upfront + MCP) | Best of both: immediate context + on-demand queries |\n\n## Implementation Phases\n\n| Phase | Focus | Key Deliverables |\n|-------|-------|------------------|\n| 1 ✓ | Protocols & Types | Core types, all protocol definitions |\n| 2 | Infrastructure | TaskExecutor, Memory MCP, Embeddings, Vector |\n| 3 | Memory | ExperienceMemory, ConceptLibrary, StrategyBank |\n| 4 | Minimal Solver | DirectSolver, TaskRouter, basic Verifier |\n| 5 | Advanced Search | MindEvolution, SWESearch |\n| 6 | Learning | Analyzer, Extractor, HindsightLearner |\n| 7 | Environments | ARC, SWE environments with sandbox support |\n\n## References\n\n- ACP Factory: `references/acp-factory/` (git submodule)\n- Research synthesis: [[s-research]] (to be created)\n- Modular design: See `docs/modular-design.md`","priority":0,"archived":0,"archived_at":null,"created_at":"2025-12-07 08:19:15","updated_at":"2026-01-07 06:53:23","parent_id":null,"parent_uuid":null,"relationships":[],"tags":["acp","architecture","core","system-design"]}
{"id":"s-3aok","uuid":"a0408b8b-cef6-49ab-9e4e-a7edbbe9515f","title":"Core Primitives","file_path":"specs/s-3aok_core_primitives.md","content":"# Core Primitives\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nThe foundational data structures that all ATLAS components operate on. These are the \"atoms\" of the system.\n\n## Trajectory\n\nThe **atomic unit of learning**. Everything in ATLAS flows through trajectories.\n\n### Structure\n\n```python\n@dataclass\nclass Trajectory:\n    task: Task\n    steps: List[Step]\n    outcome: Outcome\n    agent_id: str\n    timestamp: datetime\n    \n    # Optional metadata\n    llm_calls: int = 0\n    total_tokens: int = 0\n    wall_time_seconds: float = 0.0\n```\n\n### Design Decisions\n\n- **Immutable**: Trajectories are never modified after creation\n- **Complete**: Every trajectory has an outcome (even partial/failed ones)\n- **Attributable**: Agent ID enables analysis of which agents produce what\n- **Embeddable**: Can precompute embeddings for efficient retrieval\n\n## Task\n\nDomain-agnostic representation of work to be done.\n\n### Structure\n\n```python\n@dataclass\nclass Task:\n    id: str\n    domain: TaskDomain  # emergent, not hard-coded enum\n    description: str\n    context: Dict[str, Any]  # domain-specific data\n    verification: VerificationSpec\n    embedding: Optional[np.ndarray] = None\n```\n\n### Domain Handling\n\nDomains are **emergent via embeddings**, not a fixed enum:\n- Frontend tasks cluster with frontend tasks\n- Backend tasks cluster with backend tasks\n- No manual taxonomy required\n\nFor routing purposes, can use string tags:\n```python\ntask.context[\"domain_hints\"] = [\"swe\", \"frontend\", \"react\"]\n```\n\n## Step\n\nSingle step in a trajectory following ReAct pattern.\n\n### Structure\n\n```python\n@dataclass\nclass Step:\n    thought: Optional[str]   # Agent's reasoning (if available)\n    action: str              # Action taken\n    observation: str         # Result/feedback\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    # Computed during analysis\n    attribution_score: Optional[float] = None\n```\n\n### Metadata Examples\n\n```python\nstep.metadata = {\n    \"tool\": \"bash\",\n    \"duration_ms\": 1234,\n    \"tokens_used\": 500,\n    \"files_modified\": [\"src/main.py\"],\n}\n```\n\n## Outcome\n\nResult of a trajectory attempt.\n\n### Structure\n\n```python\n@dataclass\nclass Outcome:\n    success: bool\n    partial_score: Optional[float] = None  # 0.0 - 1.0\n    error_info: Optional[str] = None\n    verification_details: Dict[str, Any] = field(default_factory=dict)\n```\n\n### Partial Scoring\n\nSupport for partial credit enables:\n- Ranking candidates during search\n- Learning from partially successful attempts\n- More granular feedback\n\n## Environment\n\nExecution context where tasks are solved.\n\n### Interface\n\n```python\nclass Environment(ABC):\n    @abstractmethod\n    def reset(self, task: Task) -> str:\n        \"\"\"Reset with new task, return initial observation\"\"\"\n        pass\n    \n    @abstractmethod\n    def step(self, action: str) -> Tuple[str, float, bool, Dict]:\n        \"\"\"Execute action → (observation, reward, done, info)\"\"\"\n        pass\n    \n    @abstractmethod\n    def verify(self, solution: Any) -> Outcome:\n        \"\"\"Verify a solution\"\"\"\n        pass\n    \n    @property\n    @abstractmethod\n    def max_steps(self) -> int:\n        \"\"\"Maximum steps before timeout\"\"\"\n        pass\n```\n\n### Design Decisions\n\n- **Gymnasium-like**: Familiar interface for RL practitioners\n- **Verification built-in**: Not external, enables self-contained testing\n- **Sandboxed**: Reproducible, isolated execution\n- **Partial scoring**: Supports ranking and learning from failures\n\n## Agent\n\nActor that produces trajectories.\n\n### Interface\n\n```python\nclass Agent(ABC):\n    @abstractmethod\n    def solve(self, task: Task, env: Environment) -> Trajectory:\n        \"\"\"Attempt to solve task, return trajectory\"\"\"\n        pass\n    \n    @abstractmethod\n    def step(self, observation: str) -> str:\n        \"\"\"Given observation, return action\"\"\"\n        pass\n    \n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset agent state\"\"\"\n        pass\n```\n\n### Agent Types\n\n1. **External Agents**: Wrapped versions of Claude Code, OpenHands, SWE-agent\n2. **ATLAS Agent**: Memory-augmented agent with library awareness\n\n## Serialization\n\nAll primitives must support:\n- JSON serialization for storage/transfer\n- Pickle for fast local caching\n- Embedding storage (numpy arrays)\n\n```python\ntrajectory.to_json() → str\nTrajectory.from_json(data: str) → Trajectory\n```\n\n## File Location\n\n```\natlas/core/\n├── __init__.py\n├── trajectory.py    # Trajectory, Step, Outcome\n├── task.py          # Task, VerificationSpec\n├── environment.py   # Environment ABC\n├── agent.py         # Agent ABC\n└── types.py         # Shared types, enums\n```","priority":1,"archived":0,"archived_at":null,"created_at":"2025-12-07 08:21:07","updated_at":"2025-12-07 08:21:07","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-3aok","from_type":"spec","to":"s-5o87","to_type":"spec","type":"implements"}],"tags":["core","data-structures","primitives"]}
{"id":"s-3c37","uuid":"0f82673e-83cc-4b0a-8e6c-de2e1cc03759","title":"Memory Systems (Pillar 1)","file_path":"specs/s-3c37_memory_systems_pillar_1.md","content":"# Memory Systems (Pillar 1)\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nMemory is **what to remember** - the accumulated knowledge from past trajectories. Three complementary memory types at different abstraction levels.\n\n## Memory Hierarchy\n\n```\nStrategy Bank    →  \"For symmetry tasks, reflect then duplicate\"  (most abstract)\n        ↓\nExperience Memory →  \"Task #123 was similar, here's what worked\"   (task-level)\n        ↓\nConcept Library  →  \"Use rotate_90() followed by flood_fill()\"    (most concrete)\n```\n\n## Experience Memory\n\n**Purpose**: Task-level retrieval of similar past experiences (ReMem-style)\n\n### Data Structure\n\n```python\n@dataclass\nclass Experience:\n    id: str\n    task_input: str        # Task description\n    solution_output: str   # Solution attempt\n    feedback: str          # Outcome/error info\n    success: bool\n    embedding: np.ndarray  # BAAI/bge-base-en-v1.5\n    trajectory_id: str     # Source tracking\n    timestamp: datetime\n    metadata: Dict[str, Any] = field(default_factory=dict)\n```\n\n### Interface\n\n```python\nclass ExperienceMemory(ABC):\n    @abstractmethod\n    def store(self, trajectory: Trajectory) -> str:\n        \"\"\"Store trajectory as experience, return ID\"\"\"\n        pass\n    \n    @abstractmethod\n    def search(self, task: Task, k: int = 4) -> List[Experience]:\n        \"\"\"Find similar experiences via embedding similarity\"\"\"\n        pass\n    \n    @abstractmethod\n    def refine(self, experiences: List[Experience]) -> List[Experience]:\n        \"\"\"ReMem-style: exploit useful, prune noise, reorganize\"\"\"\n        pass\n    \n    @abstractmethod\n    def prune(self, criteria: Dict[str, Any]) -> int:\n        \"\"\"Remove low-value experiences, return count\"\"\"\n        pass\n```\n\n### Key Insight: ReMem Loop\n\n```\n(task, memory) →[search] retrieved →[synthesis] solution →[evolve] updated_memory\n```\n\n- Search: Top-k retrieval via cosine similarity\n- Synthesis: Use retrieved context in LLM prompt\n- Evolve: Store new experience, refine existing\n\n### Configuration\n\n- Embedding model: BAAI/bge-base-en-v1.5 (validated in ReMem)\n- Default k: 4 (from ReMem paper)\n- Vector index: ChromaDB (local) or Pinecone (cloud)\n\n## Concept Library\n\n**Purpose**: Reusable code patterns and compositions (Stitch/LILO-style)\n\n### Data Structure\n\n```python\n@dataclass\nclass CodeConcept:\n    id: str\n    name: str              # Human-readable name (AutoDoc)\n    description: str       # What it does\n    code: str              # The actual code\n    signature: str         # Type signature\n    examples: List[Tuple[str, str]]  # (input, output) pairs\n    \n    # Stats\n    usage_count: int = 0\n    success_rate: float = 0.0\n    \n    # Retrieval\n    embedding: Optional[np.ndarray] = None\n    \n    # Metadata\n    source: str = \"primitive\"  # \"primitive\", \"learned\", \"composed\"\n```\n\n### Interface\n\n```python\nclass ConceptLibrary(ABC):\n    @abstractmethod\n    def add(self, concept: CodeConcept) -> str:\n        \"\"\"Add concept, return ID\"\"\"\n        pass\n    \n    @abstractmethod\n    def search(self, query: str, k: int = 5) -> List[CodeConcept]:\n        \"\"\"Find relevant concepts by semantic similarity\"\"\"\n        pass\n    \n    @abstractmethod\n    def get(self, concept_id: str) -> Optional[CodeConcept]:\n        \"\"\"Get by ID\"\"\"\n        pass\n    \n    @abstractmethod\n    def compose(self, concept_ids: List[str]) -> Optional[CodeConcept]:\n        \"\"\"Compose multiple concepts into one\"\"\"\n        pass\n    \n    @abstractmethod\n    def compress(self, trajectories: List[Trajectory]) -> List[CodeConcept]:\n        \"\"\"Extract new concepts via Stitch compression\"\"\"\n        pass\n```\n\n### Key Insight: Stitch + LILO\n\n1. **Stitch**: Anti-unification for fast pattern extraction (1000x faster than DreamCoder)\n2. **LILO AutoDoc**: LLM generates names/descriptions for interpretability\n\n### Primitives vs Learned\n\n- **Primitives**: Domain-specific base operations (loaded at init)\n- **Learned**: Extracted from trajectories via compression\n- **Composed**: Combinations of existing concepts\n\n## Strategy Bank\n\n**Purpose**: Abstract reasoning patterns (ArcMemo-style)\n\n### Data Structure\n\n```python\n@dataclass\nclass Strategy:\n    id: str\n    situation: str         # When to apply\n    suggestion: str        # What to do\n    parameters: List[Dict[str, str]]  # Typed parameters\n    \n    # Stats\n    usage_count: int = 0\n    success_rate: float = 0.5\n    \n    # Retrieval\n    embedding: Optional[np.ndarray] = None\n```\n\n### Interface\n\n```python\nclass StrategyBank(ABC):\n    @abstractmethod\n    def write(self, trajectory: Trajectory) -> Optional[Strategy]:\n        \"\"\"Abstract trajectory into strategy\"\"\"\n        pass\n    \n    @abstractmethod\n    def read(self, task: Task, k: int = 5) -> List[Strategy]:\n        \"\"\"Find applicable strategies\"\"\"\n        pass\n    \n    @abstractmethod\n    def update_stats(self, strategy_id: str, success: bool) -> None:\n        \"\"\"Update usage statistics\"\"\"\n        pass\n```\n\n### Key Insight: ArcMemo\n\n**Concept-level beats instance-level at ALL compute scales.**\n\nInstead of: \"Task #123 had this exact solution\"\nUse: \"For tasks with symmetry patterns, check horizontal/vertical reflection\"\n\n## Memory System Aggregator\n\nCombines all three into unified interface:\n\n```python\nclass MemorySystem:\n    def __init__(\n        self,\n        experience_memory: Optional[ExperienceMemory] = None,\n        concept_library: Optional[ConceptLibrary] = None,\n        strategy_bank: Optional[StrategyBank] = None,\n    ):\n        # All optional for ablation studies\n        pass\n    \n    def query(self, task: Task, k: int = 5) -> MemoryQueryResult:\n        \"\"\"Query all available components\"\"\"\n        pass\n    \n    def store(self, trajectory: Trajectory) -> Dict[str, Any]:\n        \"\"\"Store in all available components\"\"\"\n        pass\n```\n\n## Emergent Domain Organization\n\nDomains self-organize via embedding space:\n- Frontend tasks cluster with frontend\n- Backend tasks cluster with backend\n- No manual taxonomy required\n\nSearch naturally returns domain-appropriate results.\n\n## File Location\n\n```\natlas/memory/\n├── __init__.py\n├── experience_memory.py   # ExperienceMemory, Experience\n├── concept_library.py     # ConceptLibrary, CodeConcept\n├── strategy_bank.py       # StrategyBank, Strategy\n├── system.py              # MemorySystem aggregator\n├── embeddings.py          # EmbeddingProvider protocol\n└── vector_index.py        # VectorIndex protocol + implementations\n```","priority":1,"archived":0,"archived_at":null,"created_at":"2025-12-07 08:21:07","updated_at":"2025-12-07 08:21:07","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-3c37","from_type":"spec","to":"s-5o87","to_type":"spec","type":"implements"}],"tags":["concepts","experience","memory","pillar-1","strategies"]}
{"id":"s-3nub","uuid":"8408a08a-551b-479f-a29d-d1d4e9daaebb","title":"Search Methods (Pillar 2)","file_path":"specs/s-3nub_search_methods_pillar_2.md","content":"# Search Methods (Pillar 2)\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nSearch is **how to solve** - the algorithms for finding solutions. Different methods suit different task types.\n\n## Task Router\n\nDecides which search strategy to use based on task + memory context.\n\n### Interface\n\n```python\nclass TaskRouter(ABC):\n    @abstractmethod\n    def route(\n        self,\n        task: Task,\n        memory: MemorySystem,\n    ) -> RoutingDecision:\n        \"\"\"Decide how to approach a task\"\"\"\n        pass\n```\n\n### Routing Decision\n\n```python\n@dataclass\nclass RoutingDecision:\n    strategy: SearchStrategy  # direct | evolutionary | mcts | adapt\n    relevant_concepts: List[CodeConcept]\n    similar_experiences: List[Experience]\n    suggested_strategies: List[Strategy]\n    estimated_difficulty: float  # 0.0 - 1.0\n    search_budget: int  # max LLM calls\n```\n\n### Routing Logic\n\n| Condition | Strategy | Rationale |\n|-----------|----------|-----------|\n| High similarity + success | `adapt` | Modify existing solution |\n| Clear strategy available | `direct` | Apply strategy directly |\n| ARC domain | `evolutionary` | Mind Evolution for fitness-based |\n| SWE domain | `mcts` | SWE-Search for sequential edits |\n| Unknown/low confidence | `evolutionary` | Default to population search |\n\n## Search Engine Interface\n\n```python\nclass SearchEngine(ABC):\n    def __init__(\n        self,\n        llm: LLM,\n        memory: Optional[MemorySystem] = None,  # optional!\n        config: Optional[SearchConfig] = None,\n    ):\n        pass\n    \n    @abstractmethod\n    def search(\n        self,\n        task: Task,\n        routing: RoutingDecision,\n        env: Environment,\n    ) -> List[Candidate]:\n        \"\"\"Search for candidate solutions\"\"\"\n        pass\n    \n    @abstractmethod\n    def refine(\n        self,\n        candidate: Candidate,\n        feedback: str,\n        task: Task,\n    ) -> Candidate:\n        \"\"\"Refine candidate based on feedback\"\"\"\n        pass\n```\n\n### Candidate\n\n```python\n@dataclass\nclass Candidate:\n    solution: Any           # Domain-specific solution\n    confidence: float       # 0.0 - 1.0\n    reasoning: str          # Explanation\n    source: str             # \"generated\", \"adapted\", \"retrieved\"\n    fitness: Optional[float] = None\n    parent_ids: List[str] = field(default_factory=list)\n```\n\n## Mind Evolution Search\n\n**Best for**: ARC-AGI, tasks with clear fitness functions\n\n### Algorithm\n\n```\n1. Initialize population (50% from memory, 50% novel)\n2. Evaluate fitness (via verifier)\n3. Select elites (top 50%)\n4. Mutate/crossover to fill population\n5. Repeat for 5-10 generations\n```\n\n### Configuration\n\n```python\n@dataclass\nclass MindEvolutionConfig:\n    population_size: int = 20\n    generations: int = 10\n    elite_fraction: float = 0.5\n    memory_init_fraction: float = 0.5  # % from memory\n    mutation_temperature: float = 0.7\n```\n\n### Cost\n\n~100 LLM calls per task (population_size × generations / 2)\n\n### Memory Integration\n\nPopulation initialization:\n1. Search memory for similar experiences\n2. Adapt top-k experiences to current task\n3. Search concepts and generate candidates using them\n4. Fill remaining with novel generation\n\n## SWE-Search (MCTS)\n\n**Best for**: Software engineering, sequential edit tasks\n\n### Algorithm\n\n```\n1. Root = task state\n2. Select node via UCB (exploitation + exploration)\n3. Expand with LLM-generated actions\n4. Simulate rollout\n5. Backpropagate value estimate\n6. Repeat 50-200 times\n```\n\n### Configuration\n\n```python\n@dataclass\nclass SWESearchConfig:\n    max_expansions: int = 100\n    ucb_constant: float = 1.414\n    max_depth: int = 20\n    rollout_depth: int = 5\n    use_discriminator: bool = True\n```\n\n### Cost\n\n~200 LLM calls per task\n\n### Discriminator\n\nOptional model to estimate solution quality:\n- Ranks candidates without full verification\n- Enables early pruning of bad branches\n- Can be trained on trajectory outcomes\n\n## Direct Solver\n\n**Best for**: High-confidence memory matches\n\n### Algorithm\n\n```\n1. Retrieve most similar experience\n2. Adapt solution to current task\n3. Verify\n4. If fail, try next most similar\n```\n\n### Cost\n\n~1-5 LLM calls per task\n\n### When to Use\n\n- Experience similarity > 0.9\n- Previous success on similar task\n- Strategy bank has clear match\n\n## Verifier\n\nVerifies candidate solutions and enables inference-time scaling.\n\n### Interface\n\n```python\nclass Verifier(ABC):\n    @abstractmethod\n    def verify(self, task: Task, candidate: Candidate) -> Outcome:\n        \"\"\"Verify a solution\"\"\"\n        pass\n    \n    @abstractmethod\n    def rank(\n        self,\n        task: Task,\n        candidates: List[Candidate],\n    ) -> List[Tuple[Candidate, float]]:\n        \"\"\"Rank candidates by estimated quality\"\"\"\n        pass\n```\n\n### Domain Implementations\n\n**ARC Verifier**:\n- Exact grid match on training examples\n- Partial score: cell-by-cell similarity\n\n**SWE Verifier**:\n- Execute tests in Docker sandbox\n- Partial score: fraction of tests passing\n\n### Key Insight\n\nVerification enables **best-of-k scaling**:\n- Generate k candidates\n- Verify all\n- Return best\n\nSWE-Gym showed: 10% → 13.3% with best-of-8\n\n## File Location\n\n```\natlas/search/\n├── __init__.py\n├── router.py              # TaskRouter, RoutingDecision\n├── base.py                # SearchEngine ABC, SearchConfig\n├── mind_evolution.py      # MindEvolutionSearch\n├── swe_search.py          # SWESearch (MCTS)\n├── direct.py              # DirectSolver\n└── verifier.py            # Verifier ABC + implementations\n```","priority":1,"archived":0,"archived_at":null,"created_at":"2025-12-07 08:21:07","updated_at":"2025-12-07 08:21:07","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-3nub","from_type":"spec","to":"s-5o87","to_type":"spec","type":"implements"}],"tags":["mcts","mind-evolution","pillar-2","search","verifier"]}
{"id":"s-w56w","uuid":"ce205fa1-4233-4fbc-b56f-d1455e27962c","title":"Learning Engine (Pillar 3)","file_path":"specs/s-w56w_learning_engine_pillar_3.md","content":"# Learning Engine (Pillar 3)\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nLearning is **how to improve** - extracting knowledge from trajectories to update memory and optionally fine-tune models.\n\n## Data Flow\n\n```\nTrajectories (success + failure)\n        ↓\nTrajectory Analyzer → AnalysisResult\n        ↓\nAbstraction Extractor → CodeConcepts, Strategies\n        ↓\nMemory Update\n        ↓\nHindsight Learner → Training Data → (optional) Fine-tune\n```\n\n## Trajectory Analyzer\n\n**Purpose**: Extract learning signals from trajectories\n\n### Interface\n\n```python\nclass TrajectoryAnalyzer(ABC):\n    @abstractmethod\n    def analyze(self, trajectory: Trajectory) -> AnalysisResult:\n        \"\"\"Full analysis of a trajectory\"\"\"\n        pass\n    \n    @abstractmethod\n    def attribute_outcome(\n        self,\n        trajectory: Trajectory,\n    ) -> List[Tuple[int, float]]:\n        \"\"\"Credit assignment: (step_index, contribution_score)\"\"\"\n        pass\n```\n\n### Analysis Result\n\n```python\n@dataclass\nclass AnalysisResult:\n    success: bool\n    key_steps: List[int]              # Indices of critical steps\n    step_attribution: List[float]     # Credit per step\n    error_patterns: List[Dict]        # Detected failure modes\n    abstractable: bool                # Worth extracting patterns?\n    training_examples: List[Dict]     # For hindsight learning\n```\n\n### Credit Assignment\n\nIdentify which steps contributed to outcome:\n- Success: which steps led to solution?\n- Failure: which step caused the error?\n\nMethods:\n1. **Simple**: Last successful action gets credit\n2. **LLM-based**: Ask LLM to identify key steps\n3. **Counterfactual**: Would removing this step change outcome?\n\n## Abstraction Extractor\n\n**Purpose**: Extract reusable patterns from trajectories\n\n### Interface\n\n```python\nclass AbstractionExtractor(ABC):\n    @abstractmethod\n    def extract_code_patterns(\n        self,\n        trajectories: List[Trajectory],\n    ) -> List[CodeConcept]:\n        \"\"\"Stitch-style compression\"\"\"\n        pass\n    \n    @abstractmethod\n    def extract_strategies(\n        self,\n        trajectories: List[Trajectory],\n    ) -> List[Strategy]:\n        \"\"\"ArcMemo-style abstraction\"\"\"\n        pass\n    \n    @abstractmethod\n    def auto_document(self, concept: CodeConcept) -> CodeConcept:\n        \"\"\"LILO-style documentation generation\"\"\"\n        pass\n```\n\n### Code Pattern Extraction (Stitch)\n\n1. Parse successful code to AST\n2. Find common subtrees via anti-unification\n3. Score by MDL (compression benefit)\n4. Extract top-k as CodeConcepts\n\n```python\ndef extract_code_patterns(self, trajectories):\n    # Extract code from successful trajectories\n    code_corpus = [extract_code(t) for t in trajectories if t.outcome.success]\n    \n    # Run Stitch compression\n    abstractions = self.stitch.compress(code_corpus)\n    \n    # Convert to CodeConcepts\n    return [self._to_concept(a) for a in abstractions]\n```\n\n### Strategy Extraction (ArcMemo)\n\n1. Identify successful trajectory\n2. Abstract task description → situation\n3. Abstract solution approach → suggestion\n4. Extract typed parameters\n\n```python\ndef extract_strategies(self, trajectories):\n    strategies = []\n    for traj in trajectories:\n        if traj.outcome.success and self._is_abstractable(traj):\n            strategy = Strategy(\n                situation=self._abstract_situation(traj.task),\n                suggestion=self._abstract_approach(traj),\n                parameters=self._extract_params(traj),\n            )\n            strategies.append(strategy)\n    return strategies\n```\n\n### AutoDoc (LILO)\n\nGenerate human-readable documentation for concepts:\n\n```python\ndef auto_document(self, concept: CodeConcept) -> CodeConcept:\n    prompt = f\"\"\"\n    Given this code pattern:\n    {concept.code}\n    \n    And these examples:\n    {concept.examples}\n    \n    Generate:\n    1. Concise name (snake_case)\n    2. One-sentence description\n    3. Type signature\n    \"\"\"\n    response = self.llm.generate(prompt)\n    concept.name, concept.description, concept.signature = parse(response)\n    return concept\n```\n\n## Hindsight Learner\n\n**Purpose**: Learn from trajectories to improve model\n\n### Interface\n\n```python\nclass HindsightLearner(ABC):\n    @abstractmethod\n    def prepare_training_data(\n        self,\n        trajectories: List[Trajectory],\n    ) -> Dict[str, Any]:\n        \"\"\"Convert trajectories to training format\"\"\"\n        pass\n    \n    @abstractmethod\n    def should_finetune(self) -> bool:\n        \"\"\"Check if enough data for fine-tuning\"\"\"\n        pass\n    \n    @abstractmethod\n    def finetune(self, training_data: Dict) -> FinetuneResult:\n        \"\"\"Execute fine-tuning\"\"\"\n        pass\n```\n\n### SOAR-Style Training Data\n\nSeparate sampling and refinement capabilities:\n\n```python\ndef prepare_training_data(self, trajectories):\n    sampling_data = []      # Learn to generate good initial solutions\n    refinement_data = []    # Learn to refine based on feedback\n    \n    for traj in trajectories:\n        if traj.outcome.success:\n            # Learn successful solutions (2x weight)\n            sampling_data.append({\n                \"input\": format_task(traj.task),\n                \"output\": extract_solution(traj),\n                \"weight\": 2.0,\n            })\n            \n            # Learn key intermediate steps (1.5x weight)\n            for i, step in enumerate(traj.steps):\n                if is_key_step(traj, i):\n                    refinement_data.append({\n                        \"input\": format_with_history(traj, i),\n                        \"output\": step.action,\n                        \"weight\": 1.5,\n                    })\n        else:\n            # Learn from failures (1x weight)\n            if traj.outcome.error_info:\n                refinement_data.append({\n                    \"input\": format_with_history(traj, len(traj.steps)),\n                    \"output\": f\"[ERROR] {traj.outcome.error_info}\",\n                    \"weight\": 1.0,\n                })\n    \n    return {\"sampling\": sampling_data, \"refinement\": refinement_data}\n```\n\n### Fine-tuning Triggers\n\n```python\ndef should_finetune(self) -> bool:\n    return (\n        len(self.accumulated_trajectories) >= 100 and\n        time_since_last_finetune() >= timedelta(hours=24) and\n        quality_threshold_met()\n    )\n```\n\n### SAGE Alternative (Training-Free)\n\nIf fine-tuning not desired, use memory-only improvement:\n- Store plans as retrievable documents\n- In-context learning with retrieved plans\n- No weight updates, only memory growth\n\n## Learning Pipeline\n\nOrchestrates the full learning process:\n\n```python\nclass LearningPipeline:\n    def __init__(\n        self,\n        memory: MemorySystem,\n        analyzer: TrajectoryAnalyzer,\n        extractor: AbstractionExtractor,\n        hindsight: Optional[HindsightLearner] = None,\n    ):\n        pass\n    \n    def process_trajectory(self, trajectory: Trajectory):\n        \"\"\"Process single trajectory\"\"\"\n        # 1. Store in experience memory\n        self.memory.experience_memory.store(trajectory)\n        \n        # 2. Analyze\n        analysis = self.analyzer.analyze(trajectory)\n        \n        # 3. Extract strategy if abstractable\n        if analysis.abstractable:\n            strategy = self.extractor.extract_strategy(trajectory)\n            if strategy:\n                self.memory.strategy_bank.write(strategy)\n        \n        # 4. Accumulate for batch processing\n        self.trajectory_buffer.append(trajectory)\n    \n    def run_batch_learning(self, min_trajectories: int = 50):\n        \"\"\"Periodic batch learning\"\"\"\n        if len(self.trajectory_buffer) < min_trajectories:\n            return\n        \n        # Extract code patterns\n        concepts = self.extractor.extract_code_patterns(self.trajectory_buffer)\n        for c in concepts:\n            self.memory.concept_library.add(c)\n        \n        # Maybe fine-tune\n        if self.hindsight and self.hindsight.should_finetune():\n            data = self.hindsight.prepare_training_data(self.trajectory_buffer)\n            self.hindsight.finetune(data)\n        \n        self.trajectory_buffer = []\n```\n\n## File Location\n\n```\natlas/learning/\n├── __init__.py\n├── analyzer.py      # TrajectoryAnalyzer, AnalysisResult\n├── extractor.py     # AbstractionExtractor\n├── hindsight.py     # HindsightLearner, SOAR/SAGE implementations\n└── pipeline.py      # LearningPipeline\n```","priority":1,"archived":0,"archived_at":null,"created_at":"2025-12-07 08:21:08","updated_at":"2025-12-07 08:21:08","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-w56w","from_type":"spec","to":"s-5o87","to_type":"spec","type":"implements"}],"tags":["abstraction","fine-tuning","learning","pillar-3","trajectory-analysis"]}
{"id":"s-6qr1","uuid":"51de9b9f-5e1b-478d-a3f3-e831505605ca","title":"Modular Design & Ablation Support","file_path":"specs/s-6qr1_modular_design_ablation_support.md","content":"# Modular Design & Ablation Support\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nEvery ATLAS component must be independently usable and composable. This enables:\n- Ablation studies (test each component's contribution)\n- Incremental development (build and test pieces independently)\n- Flexible deployment (use only what you need)\n\n## Design Principles\n\n### 1. Standalone Operation\n\nEach component works without other ATLAS components:\n\n```python\n# Just experience memory\nfrom atlas.memory import ExperienceMemory\nmemory = ExperienceMemory()\nmemory.store(trajectory)\nsimilar = memory.search(task)\n\n# Just concept library\nfrom atlas.memory import ConceptLibrary\nlibrary = ConceptLibrary()\nlibrary.add(concept)\nrelevant = library.search(\"rotate grid\")\n\n# Just search (no memory)\nfrom atlas.search import MindEvolutionSearch\nsearch = MindEvolutionSearch(llm=llm)  # memory=None is valid\ncandidates = search.search(task, env)\n```\n\n### 2. Optional Dependencies\n\nComponents accept optional dependencies via constructor:\n\n```python\nclass ExperienceMemory:\n    def __init__(\n        self,\n        embedding_provider: Optional[EmbeddingProvider] = None,  # optional\n        vector_index: Optional[VectorIndex] = None,              # optional\n    ):\n        # Use defaults if not provided\n        self.embedder = embedding_provider or DefaultEmbeddingProvider()\n        self.index = vector_index or InMemoryVectorIndex()\n```\n\n### 3. Graceful Degradation\n\nMissing dependencies return empty results, not errors:\n\n```python\nclass MemorySystem:\n    def query(self, task: Task, k: int = 5) -> MemoryQueryResult:\n        experiences = []\n        concepts = []\n        strategies = []\n        \n        # Each is optional - missing components just return empty\n        if self.experience_memory:\n            experiences = self.experience_memory.search(task, k)\n        if self.concept_library:\n            concepts = self.concept_library.search(task.description, k)\n        if self.strategy_bank:\n            strategies = self.strategy_bank.read(task, k)\n        \n        return MemoryQueryResult(experiences, concepts, strategies)\n```\n\n### 4. Protocol-Based Interfaces\n\nDepend on protocols, not concrete implementations:\n\n```python\nfrom typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass EmbeddingProvider(Protocol):\n    def encode(self, text: str) -> np.ndarray: ...\n    def encode_batch(self, texts: List[str]) -> List[np.ndarray]: ...\n\n@runtime_checkable\nclass VectorIndex(Protocol):\n    def add(self, id: str, vector: np.ndarray, metadata: dict) -> None: ...\n    def search(self, vector: np.ndarray, k: int) -> List[tuple]: ...\n    def delete(self, id: str) -> bool: ...\n```\n\n## Ablation Configurations\n\n### Configuration Enum\n\n```python\nclass MemoryConfig(Enum):\n    NONE = auto()               # No memory (baseline)\n    EXPERIENCE_ONLY = auto()    # Only experience memory\n    CONCEPT_ONLY = auto()       # Only concept library\n    STRATEGY_ONLY = auto()      # Only strategy bank\n    EXPERIENCE_CONCEPT = auto() # Experience + concept\n    EXPERIENCE_STRATEGY = auto()# Experience + strategy\n    CONCEPT_STRATEGY = auto()   # Concept + strategy\n    FULL = auto()               # All three\n\nclass SearchConfig(Enum):\n    DIRECT = auto()             # Single-shot\n    EVOLUTION = auto()          # Mind Evolution\n    MCTS = auto()               # SWE-Search\n    HYBRID = auto()             # Combined\n\nclass LearningConfig(Enum):\n    NONE = auto()               # No learning\n    MEMORY_ONLY = auto()        # SAGE-style (no fine-tuning)\n    FINETUNE = auto()           # SOAR-style\n    FULL = auto()               # Both\n```\n\n### Predefined Ablations\n\n```python\nABLATION_CONFIGS = {\n    # Baselines\n    \"baseline\": AblationConfig(\n        memory=MemoryConfig.NONE,\n        search=SearchConfig.DIRECT,\n        learning=LearningConfig.NONE,\n    ),\n    \n    \"baseline_search\": AblationConfig(\n        memory=MemoryConfig.NONE,\n        search=SearchConfig.EVOLUTION,\n        learning=LearningConfig.NONE,\n    ),\n    \n    # Memory ablations\n    \"experience_only\": AblationConfig(\n        memory=MemoryConfig.EXPERIENCE_ONLY,\n        search=SearchConfig.EVOLUTION,\n        learning=LearningConfig.MEMORY_ONLY,\n    ),\n    \n    \"concept_only\": AblationConfig(\n        memory=MemoryConfig.CONCEPT_ONLY,\n        search=SearchConfig.EVOLUTION,\n        learning=LearningConfig.MEMORY_ONLY,\n    ),\n    \n    \"strategy_only\": AblationConfig(\n        memory=MemoryConfig.STRATEGY_ONLY,\n        search=SearchConfig.DIRECT,\n        learning=LearningConfig.MEMORY_ONLY,\n    ),\n    \n    # Full system\n    \"full\": AblationConfig(\n        memory=MemoryConfig.FULL,\n        search=SearchConfig.EVOLUTION,\n        learning=LearningConfig.FULL,\n    ),\n}\n```\n\n### Factory Functions\n\n```python\ndef create_from_config(config: AblationConfig) -> ATLASSolver:\n    \"\"\"Create solver from ablation config\"\"\"\n    # Build memory system\n    exp_mem = ExperienceMemory() if config.needs_experience else None\n    concept_lib = ConceptLibrary() if config.needs_concepts else None\n    strat_bank = StrategyBank() if config.needs_strategies else None\n    \n    memory = MemorySystem(\n        experience_memory=exp_mem,\n        concept_library=concept_lib,\n        strategy_bank=strat_bank,\n    )\n    \n    # Build search\n    search = create_search(config.search, memory)\n    \n    # Build learning\n    learning = create_learning(config.learning, memory)\n    \n    return ATLASSolver(memory=memory, search=search, learning=learning)\n```\n\n## Running Ablation Studies\n\n```python\ndef run_ablation_study(\n    tasks: List[Task],\n    configs: List[str] = None,\n) -> Dict[str, Dict]:\n    \"\"\"Run ablation study\"\"\"\n    configs = configs or list(ABLATION_CONFIGS.keys())\n    results = {}\n    \n    for config_name in configs:\n        config = ABLATION_CONFIGS[config_name]\n        solver = create_from_config(config)\n        \n        successes = 0\n        total_cost = 0\n        \n        for task in tasks:\n            trajectory = solver.solve(task)\n            if trajectory.outcome.success:\n                successes += 1\n            total_cost += trajectory.total_tokens\n        \n        results[config_name] = {\n            \"accuracy\": successes / len(tasks),\n            \"cost\": total_cost,\n            \"config\": config,\n        }\n    \n    return results\n\n# Usage\nresults = run_ablation_study(\n    tasks=test_tasks,\n    configs=[\"baseline\", \"experience_only\", \"concept_only\", \"full\"]\n)\n\n# Expected output:\n# baseline: 32.5%\n# experience_only: 41.2%\n# concept_only: 38.7%\n# full: 52.1%\n```\n\n## Default Implementations\n\nEach component provides sensible defaults:\n\n| Component | Default Implementation |\n|-----------|----------------------|\n| EmbeddingProvider | `DefaultEmbeddingProvider` (BAAI/bge-base-en-v1.5) |\n| VectorIndex | `InMemoryVectorIndex` (simple numpy) |\n| LLM | Must be provided (no default) |\n| Compressor | `None` (compression disabled) |\n| AutoDocumenter | `None` (no documentation) |\n\n## Lazy Loading\n\nHeavy dependencies load only when needed:\n\n```python\nclass DefaultEmbeddingProvider:\n    def __init__(self, model_name: str = \"BAAI/bge-base-en-v1.5\"):\n        self._model = None  # Lazy\n        self._model_name = model_name\n    \n    @property\n    def model(self):\n        if self._model is None:\n            from sentence_transformers import SentenceTransformer\n            self._model = SentenceTransformer(self._model_name)\n        return self._model\n    \n    def encode(self, text: str) -> np.ndarray:\n        return self.model.encode(text)\n```\n\n## Testing in Isolation\n\nEach component can be tested independently:\n\n```python\n# Test experience memory alone\ndef test_experience_memory():\n    memory = ExperienceMemory()\n    \n    # Store\n    exp_id = memory.store(mock_trajectory)\n    assert exp_id is not None\n    \n    # Search\n    results = memory.search(mock_task, k=3)\n    assert len(results) <= 3\n    \n    # Prune\n    count = memory.prune({\"max_age_days\": 30})\n    assert count >= 0\n\n# Test search without memory\ndef test_search_no_memory():\n    search = MindEvolutionSearch(llm=mock_llm)  # No memory\n    candidates = search.search(mock_task, mock_env)\n    assert len(candidates) > 0\n```\n\n## File Location\n\n```\natlas/\n├── experiments/\n│   ├── __init__.py\n│   ├── ablation.py      # AblationConfig, ABLATION_CONFIGS\n│   ├── runner.py        # run_ablation_study\n│   └── analysis.py      # Result analysis utilities\n```","priority":1,"archived":0,"archived_at":null,"created_at":"2025-12-07 08:21:43","updated_at":"2025-12-07 08:21:43","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-6qr1","from_type":"spec","to":"s-5o87","to_type":"spec","type":"implements"}],"tags":["ablation","design-patterns","modular","testing"]}
{"id":"s-8fnx","uuid":"7b5a1dd5-414a-4a57-902a-dee0988267f2","title":"Python Package Structure","file_path":"specs/s-8fnx_python_package_structure.md","content":"# Python Package Structure\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Design Philosophy\n\n1. **Interfaces first**: Define ABCs and Protocols before implementations\n1. **Leverage existing code**: Wrap proven libraries rather than reimplement\n1. **Thin wrappers**: Our code is glue, not the engine\n1. **Dependency injection**: All heavy deps are optional and injectable\n\n## Existing Codebases to Leverage\n\n| Component | Existing Library | What We Build |\n| --- | --- | --- |\n| **Embeddings** | `sentence-transformers` | Thin wrapper protocol |\n| **Vector Store** | `chromadb`, `faiss` | Protocol + adapter |\n| **Stitch Compression** | `stitch` (Rust via PyO3) | Python bindings wrapper |\n| **LILO AutoDoc** | `lilo` (Python) | Import directly or fork |\n| **Mind Evolution** | `lucidrains/mind-evolution` | Adapter to our interfaces |\n| **LLM Calls** | `litellm`, `anthropic`, `openai` | Protocol wrapper |\n| **ARC Environment** | `arckit`, `arc-dsl` | Adapter |\n| **SWE Environment** | `swe-gym`, `docker` | Adapter |\n\n## What We Actually Build\n\n### Must Build (Core Interfaces)\n\n```\natlas/\n├── core/              # ~200 lines - dataclasses only\n├── protocols/         # ~150 lines - ABCs and Protocols\n└── memory/system.py   # ~100 lines - aggregator\n```\n\n### Thin Wrappers (~50-100 lines each)\n\n```\natlas/\n├── memory/\n│   ├── adapters/\n│   │   ├── chroma.py      # ChromaDB adapter\n│   │   └── faiss.py       # FAISS adapter\n├── integration/\n│   ├── stitch.py          # Stitch wrapper\n│   ├── lilo.py            # LILO wrapper\n│   ├── mind_evolution.py  # Mind Evolution adapter\n│   └── llm.py             # LiteLLM wrapper\n```\n\n### Light Implementation (~200-300 lines each)\n\n```\natlas/\n├── memory/\n│   ├── experience.py      # ExperienceMemory (ReMem-style)\n│   ├── concepts.py        # ConceptLibrary\n│   └── strategies.py      # StrategyBank\n├── search/\n│   ├── router.py          # Task routing logic\n│   └── verifier.py        # Verification logic\n├── learning/\n│   └── pipeline.py        # Orchestration\n```\n\n## Package Structure\n\n```\natlas/\n├── __init__.py\n├── py.typed                    # PEP 561 marker\n│\n├── core/                       # Core data structures\n│   ├── __init__.py\n│   ├── types.py               # Trajectory, Task, Step, Outcome (~150 lines)\n│   └── serialization.py       # JSON/pickle helpers (~50 lines)\n│\n├── protocols/                  # Abstract interfaces\n│   ├── __init__.py\n│   ├── memory.py              # Memory protocols (~100 lines)\n│   ├── search.py              # Search protocols (~80 lines)\n│   ├── learning.py            # Learning protocols (~60 lines)\n│   ├── environment.py         # Environment protocol (~40 lines)\n│   └── llm.py                 # LLM protocol (~30 lines)\n│\n├── memory/                     # Pillar 1 implementations\n│   ├── __init__.py\n│   ├── system.py              # MemorySystem aggregator (~100 lines)\n│   ├── experience.py          # ExperienceMemory (~250 lines)\n│   ├── concepts.py            # ConceptLibrary (~200 lines)\n│   ├── strategies.py          # StrategyBank (~150 lines)\n│   ├── embeddings.py          # Embedding providers (~80 lines)\n│   └── adapters/\n│       ├── __init__.py\n│       ├── chroma.py          # ChromaDB VectorIndex (~60 lines)\n│       ├── faiss.py           # FAISS VectorIndex (~60 lines)\n│       └── memory_index.py    # In-memory fallback (~40 lines)\n│\n├── search/                     # Pillar 2 implementations\n│   ├── __init__.py\n│   ├── router.py              # TaskRouter (~150 lines)\n│   ├── direct.py              # DirectSolver (~80 lines)\n│   ├── verifier.py            # Verifier base + impls (~120 lines)\n│   └── adapters/\n│       ├── __init__.py\n│       └── mind_evolution.py  # Wrap lucidrains lib (~100 lines)\n│\n├── learning/                   # Pillar 3 implementations\n│   ├── __init__.py\n│   ├── analyzer.py            # TrajectoryAnalyzer (~150 lines)\n│   ├── pipeline.py            # LearningPipeline (~200 lines)\n│   └── adapters/\n│       ├── __init__.py\n│       ├── stitch.py          # Stitch compression (~80 lines)\n│       └── lilo.py            # LILO AutoDoc (~60 lines)\n│\n├── environments/               # Environment adapters\n│   ├── __init__.py\n│   ├── base.py                # Base classes (~50 lines)\n│   └── adapters/\n│       ├── __init__.py\n│       ├── arc.py             # ARC environment (~100 lines)\n│       └── swe.py             # SWE environment (~150 lines)\n│\n├── llm/                        # LLM integration\n│   ├── __init__.py\n│   ├── base.py                # LLM protocol (~40 lines)\n│   └── adapters/\n│       ├── __init__.py\n│       ├── litellm.py         # LiteLLM adapter (~50 lines)\n│       ├── anthropic.py       # Direct Anthropic (~50 lines)\n│       └── openai.py          # Direct OpenAI (~50 lines)\n│\n├── solver.py                   # ATLASSolver orchestrator (~150 lines)\n│\n└── experiments/                # Ablation support\n    ├── __init__.py\n    ├── configs.py             # Ablation configs (~100 lines)\n    └── runner.py              # Experiment runner (~150 lines)\n```\n\n## Dependencies\n\n### Required (Core)\n\n```toml\n[project]\ndependencies = [\n    \"numpy>=1.24\",\n    \"pydantic>=2.0\",        # Serialization\n]\n```\n\n### Optional (Feature Groups)\n\n```toml\n[project.optional-dependencies]\nembeddings = [\n    \"sentence-transformers>=2.2\",\n]\nvector-stores = [\n    \"chromadb>=0.4\",\n    # or: \"faiss-cpu>=1.7\",\n]\nllm = [\n    \"litellm>=1.0\",\n    # or specific: \"anthropic>=0.18\", \"openai>=1.0\"\n]\ncompression = [\n    \"stitch-core>=0.1\",     # If available\n]\narc = [\n    \"arckit>=0.1\",          # ARC utilities\n]\nswe = [\n    \"docker>=6.0\",\n]\nall = [\n    \"atlas[embeddings,vector-stores,llm,arc,swe]\",\n]\ndev = [\n    \"pytest>=7.0\",\n    \"pytest-asyncio>=0.21\",\n    \"ruff>=0.1\",\n    \"mypy>=1.0\",\n]\n```\n\n## Implementation Priority\n\n### Phase 1: Interfaces (Day 1-2)\n\n```\natlas/\n├── core/types.py          # Data structures\n├── protocols/             # All protocols\n└── py.typed\n```\n\n### Phase 2: Memory (Day 3-5)\n\n```\natlas/memory/\n├── system.py              # Aggregator\n├── experience.py          # With in-memory index\n├── embeddings.py          # Default provider\n└── adapters/memory_index.py\n```\n\n### Phase 3: Minimal Solver (Day 6-7)\n\n```\natlas/\n├── search/router.py       # Basic routing\n├── search/direct.py       # Direct solver\n├── solver.py              # Orchestrator\n└── llm/adapters/litellm.py\n```\n\n### Phase 4: Learning (Day 8-10)\n\n```\natlas/learning/\n├── analyzer.py\n└── pipeline.py\n```\n\n### Phase 5: Advanced (Week 2+)\n\n- Mind Evolution adapter\n- Stitch/LILO integration\n- SWE environment\n- Full search methods\n\n## Key Interfaces to Define First\n\n### 1\\. LLM Protocol (everything depends on this)\n\n```python\nclass LLM(Protocol):\n    def generate(self, prompt: str, **kwargs) -> str: ...\n    async def agenerate(self, prompt: str, **kwargs) -> str: ...\n```\n\n### 2\\. EmbeddingProvider Protocol\n\n```python\nclass EmbeddingProvider(Protocol):\n    def encode(self, text: str) -> np.ndarray: ...\n    def encode_batch(self, texts: List[str]) -> np.ndarray: ...\n```\n\n### 3\\. VectorIndex Protocol\n\n```python\nclass VectorIndex(Protocol):\n    def add(self, id: str, vector: np.ndarray, metadata: Dict) -> None: ...\n    def search(self, vector: np.ndarray, k: int) -> List[Tuple[str, float]]: ...\n    def delete(self, id: str) -> bool: ...\n```\n\n### 4\\. Memory Protocols\n\n```python\nclass ExperienceMemory(Protocol):\n    def store(self, trajectory: Trajectory) -> str: ...\n    def search(self, task: Task, k: int) -> List[Experience]: ...\n\nclass ConceptLibrary(Protocol):\n    def add(self, concept: CodeConcept) -> str: ...\n    def search(self, query: str, k: int) -> List[CodeConcept]: ...\n\nclass StrategyBank(Protocol):\n    def write(self, trajectory: Trajectory) -> Optional[Strategy]: ...\n    def read(self, task: Task, k: int) -> List[Strategy]: ...\n```\n\n## Testing Strategy\n\n```\ntests/\n├── unit/\n│   ├── test_types.py          # Core data structures\n│   ├── test_memory.py         # Memory components\n│   └── test_search.py         # Search components\n├── integration/\n│   ├── test_solver.py         # End-to-end\n│   └── test_ablation.py       # Ablation configs\n└── fixtures/\n    ├── trajectories.py        # Sample trajectories\n    └── tasks.py               # Sample tasks\n```\n\n## File to Create First\n\n`atlas/protocols/__init__.py` - Export all protocols so implementations can import from one place:\n\n```python\nfrom atlas.protocols.llm import LLM\nfrom atlas.protocols.memory import (\n    EmbeddingProvider,\n    VectorIndex,\n    ExperienceMemory,\n    ConceptLibrary,\n    StrategyBank,\n)\nfrom atlas.protocols.search import SearchEngine, Verifier, TaskRouter\nfrom atlas.protocols.learning import TrajectoryAnalyzer, AbstractionExtractor\nfrom atlas.protocols.environment import Environment\n\n__all__ = [\n    \"LLM\",\n    \"EmbeddingProvider\",\n    \"VectorIndex\",\n    \"ExperienceMemory\",\n    \"ConceptLibrary\",\n    \"StrategyBank\",\n    \"SearchEngine\",\n    \"Verifier\",\n    \"TaskRouter\",\n    \"TrajectoryAnalyzer\",\n    \"AbstractionExtractor\",\n    \"Environment\",\n]\n```","priority":0,"archived":0,"archived_at":null,"created_at":"2025-12-07 08:36:25","updated_at":"2025-12-07 08:41:51","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-8fnx","from_type":"spec","to":"s-5o87","to_type":"spec","type":"implements"}],"tags":["dependencies","implementation","package-structure","python"]}
{"id":"s-7h2g","uuid":"d4747f7d-ae45-4696-ac9e-f5c75d4c05e8","title":"Phase 3: Memory Implementations","file_path":"specs/s-7h2g_phase_3_memory_implementations.md","content":"# Phase 3: Memory Implementations\n\nParent: [[s-5o87|ATLAS System Architecture]]\nImplements: [[s-3c37|Memory Systems (Pillar 1)]]\nDepends on: [[s-7xs8|Phase 2: Infrastructure Layer]]\n\n## Overview\n\nConcrete implementations of the three memory protocols defined in `src/atlas/protocols/memory.py`. Each component works standalone and can be composed via MemorySystem. The implementations are designed with **flexible strategy interfaces** to enable experimentation with different approaches.\n\n## Design Principles\n\n1. **Strategy Pattern for Experimentation**: Complex operations (refine, compose, compress, success updates) use pluggable strategy protocols\n2. **Separation of Storage and Processing**: Vector DB handles storage/retrieval; strategies handle intelligent processing\n3. **Success/Failure Differentiation**: Both stored, but processed differently based on outcome\n4. **Async-First**: All query operations support asyncio for parallel execution\n\n---\n\n## Storage Architecture\n\n### Database Strategy\n\n**Hybrid Approach:**\n- **Vector Storage**: ChromaDB collections (separate per component)\n- **Metadata Storage**: TinyDB for lightweight JSON-based metadata\n- **Abstraction Layer**: `VectorStore` protocol for swapping implementations\n\n```python\n# Storage abstraction for future flexibility\nclass VectorStore(Protocol):\n    \"\"\"Abstract vector storage - allows swapping ChromaDB for other backends.\"\"\"\n    async def add(self, ids: list[str], embeddings: list[list[float]],\n                  metadatas: list[dict], documents: list[str]) -> None: ...\n    async def query(self, embedding: list[float], k: int,\n                    where: dict | None = None) -> QueryResult: ...\n    async def delete(self, ids: list[str]) -> None: ...\n    async def get(self, ids: list[str]) -> list[dict]: ...\n```\n\n### Collections\n\n| Component | Collection Name | Embedding Source | Metadata DB |\n|-----------|----------------|------------------|-------------|\n| ExperienceMemory | `atlas_experiences` | Task description | `experiences_meta.json` |\n| ConceptLibrary | `atlas_concepts` | Name + description | `concepts_meta.json` |\n| StrategyBank | `atlas_strategies` | Situation field | `strategies_meta.json` |\n\n---\n\n## Component 1: ExperienceMemory Implementation\n\nChromaDB-backed experience storage with ReMem-style retrieval.\n\n### Core Design Decisions\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| Experience IDs | UUIDs (`uuid4`) | Globally unique, no collision concerns |\n| Embedding source | Task description (configurable) | Start simple, option for LLM summarization later |\n| Similarity key | Task description only (configurable) | Can expand to include context |\n| Refine operation | **Agentic** (LLM meta-reasoning) | ReMem literature: requires intelligent pruning/reorganizing |\n| Store behavior | Both success/failure stored | Different processing based on outcome |\n\n### Strategy Protocols\n\n```python\n@runtime_checkable\nclass ExperienceExtractor(Protocol):\n    \"\"\"Extracts searchable content from trajectory for embedding.\"\"\"\n    def extract(self, trajectory: Trajectory) -> str:\n        \"\"\"Return text to embed for similarity search.\"\"\"\n        ...\n\n@runtime_checkable\nclass RefineStrategy(Protocol):\n    \"\"\"ReMem-style refinement: exploit useful, prune noise, reorganize.\"\"\"\n    async def refine(self, experiences: list[Experience],\n                     context: Task | None = None) -> list[Experience]:\n        \"\"\"\n        Meta-reasoning over retrieved experiences.\n\n        Implementations may:\n        - Merge similar experiences\n        - Remove low-quality/noisy ones\n        - Reorganize for better relevance\n        - Filter based on task context\n        \"\"\"\n        ...\n```\n\n### Default Implementations\n\n```python\nclass SimpleExperienceExtractor(ExperienceExtractor):\n    \"\"\"Extract task description only (default).\"\"\"\n    def extract(self, trajectory: Trajectory) -> str:\n        return trajectory.task.description\n\nclass LLMSummarizingExtractor(ExperienceExtractor):\n    \"\"\"LLM summarizes trajectory into searchable text (future).\"\"\"\n    def __init__(self, llm_client: LLMClient):\n        self.llm = llm_client\n\n    def extract(self, trajectory: Trajectory) -> str:\n        # Summarize task + key steps + outcome\n        ...\n\nclass PassthroughRefineStrategy(RefineStrategy):\n    \"\"\"No-op refinement (baseline for ablation).\"\"\"\n    async def refine(self, experiences: list[Experience],\n                     context: Task | None = None) -> list[Experience]:\n        return experiences\n\nclass LLMRefineStrategy(RefineStrategy):\n    \"\"\"LLM-based meta-reasoning refinement (ReMem-style).\"\"\"\n    def __init__(self, llm_client: LLMClient):\n        self.llm = llm_client\n\n    async def refine(self, experiences: list[Experience],\n                     context: Task | None = None) -> list[Experience]:\n        # LLM analyzes experiences, decides which to keep/merge/reorganize\n        ...\n```\n\n### Implementation Signature\n\n```python\nclass ChromaExperienceMemory:\n    \"\"\"ExperienceMemory implementation backed by ChromaDB.\"\"\"\n\n    def __init__(\n        self,\n        embedder: Embedder,\n        vector_store: VectorStore,\n        extractor: ExperienceExtractor = SimpleExperienceExtractor(),\n        refine_strategy: RefineStrategy = PassthroughRefineStrategy(),\n        collection_name: str = \"atlas_experiences\",\n    ): ...\n\n    def store(self, trajectory: Trajectory) -> str:\n        \"\"\"\n        Store trajectory as experience.\n\n        - Generates UUID for experience_id\n        - Extracts embedding text via extractor\n        - Stores success/failure status in metadata\n        - Successful trajectories prioritized in search ranking\n        \"\"\"\n        ...\n\n    def search(self, task: Task, k: int = 4) -> list[Experience]:\n        \"\"\"\n        Find similar experiences via embedding similarity.\n\n        - Default k=4 from ReMem paper\n        - Prioritizes successful experiences in ranking\n        - Returns both success/failure for learning (metadata indicates which)\n        \"\"\"\n        ...\n\n    def get(self, experience_id: str) -> Experience | None: ...\n\n    def refine(self, experiences: list[Experience]) -> list[Experience]:\n        \"\"\"Delegates to refine_strategy.\"\"\"\n        ...\n\n    def prune(self, criteria: dict[str, Any]) -> int:\n        \"\"\"\n        Remove low-value experiences.\n\n        Criteria:\n        - min_success_rate: float - minimum success rate to keep\n        - max_age_days: int - maximum age in days\n        - keep_diverse: bool - preserve diversity even if low-performing\n        \"\"\"\n        ...\n```\n\n---\n\n## Component 2: ConceptLibrary Implementation\n\nCode pattern storage with Stitch/LILO-style compression and semantic search.\n\n### Core Design Decisions\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| Concept IDs | UUIDs (`uuid4`) | Consistent with other components |\n| Embedding source | Name + description | Captures semantic meaning |\n| Composition | **Agentic** (LLM-guided) | LILO: LLM determines meaningful combinations |\n| Compression | **Hybrid** (Stitch + LLM AutoDoc) | Fast symbolic extraction + interpretable naming |\n| Primitives | Domain-specific, loaded at init | Hardcoded base operations per domain |\n\n### Strategy Protocols\n\n```python\n@runtime_checkable\nclass CompositionStrategy(Protocol):\n    \"\"\"Combines multiple concepts into a new composed concept.\"\"\"\n    async def compose(self, concepts: list[CodeConcept]) -> CodeConcept | None:\n        \"\"\"\n        Compose concepts into a new one.\n\n        Returns None if composition is not meaningful/possible.\n        \"\"\"\n        ...\n\n@runtime_checkable\nclass CompressionStrategy(Protocol):\n    \"\"\"Extracts new concepts from trajectories (Stitch/LILO-style).\"\"\"\n    async def compress(self, trajectories: list[Trajectory]) -> list[CodeConcept]:\n        \"\"\"\n        Extract reusable patterns from successful trajectories.\n\n        Implementations may use:\n        - Anti-unification (Stitch)\n        - LLM-based pattern recognition\n        - Hybrid approaches\n        \"\"\"\n        ...\n\n@runtime_checkable\nclass ConceptDocumenter(Protocol):\n    \"\"\"Generates documentation for concepts (LILO AutoDoc).\"\"\"\n    async def document(self, concept: CodeConcept,\n                       usage_examples: list[tuple[str, str]]) -> CodeConcept:\n        \"\"\"Add name, description, signature, usage guidance.\"\"\"\n        ...\n```\n\n### Default Implementations\n\n```python\nclass LLMCompositionStrategy(CompositionStrategy):\n    \"\"\"LLM determines how to combine concepts meaningfully.\"\"\"\n    def __init__(self, llm_client: LLMClient):\n        self.llm = llm_client\n\n    async def compose(self, concepts: list[CodeConcept]) -> CodeConcept | None:\n        # LLM analyzes concepts and generates composed code\n        ...\n\nclass StitchCompressionStrategy(CompressionStrategy):\n    \"\"\"Symbolic anti-unification for pattern extraction.\"\"\"\n    def __init__(self, documenter: ConceptDocumenter | None = None):\n        self.documenter = documenter\n\n    async def compress(self, trajectories: list[Trajectory]) -> list[CodeConcept]:\n        # 1. Extract successful code from trajectories\n        # 2. Parse to AST, find common patterns via anti-unification\n        # 3. Score by MDL compression benefit\n        # 4. Optionally document with AutoDoc\n        ...\n\nclass LLMAutoDocumenter(ConceptDocumenter):\n    \"\"\"LILO-style AutoDoc using LLM.\"\"\"\n    def __init__(self, llm_client: LLMClient):\n        self.llm = llm_client\n\n    async def document(self, concept: CodeConcept,\n                       usage_examples: list[tuple[str, str]]) -> CodeConcept:\n        # Generate: name, description, signature, usage_guidance\n        ...\n```\n\n### Primitive Loading\n\n```python\nclass PrimitiveLoader(Protocol):\n    \"\"\"Loads domain-specific primitive concepts.\"\"\"\n    def load(self) -> dict[str, CodeConcept]: ...\n\nclass ARCPrimitiveLoader(PrimitiveLoader):\n    \"\"\"Load ARC-AGI grid manipulation primitives.\"\"\"\n    def load(self) -> dict[str, CodeConcept]:\n        return {\n            \"get_objects\": CodeConcept(...),\n            \"flood_fill\": CodeConcept(...),\n            \"rotate_90\": CodeConcept(...),\n            \"mirror_horizontal\": CodeConcept(...),\n            \"get_background_color\": CodeConcept(...),\n            # ... more ARC primitives\n        }\n\nclass SWEPrimitiveLoader(PrimitiveLoader):\n    \"\"\"Load software engineering primitives.\"\"\"\n    def load(self) -> dict[str, CodeConcept]:\n        return {\n            \"read_file\": CodeConcept(...),\n            \"write_file\": CodeConcept(...),\n            \"search_codebase\": CodeConcept(...),\n            # ... more SWE primitives\n        }\n```\n\n### Implementation Signature\n\n```python\nclass ChromaConceptLibrary:\n    \"\"\"ConceptLibrary implementation backed by ChromaDB.\"\"\"\n\n    def __init__(\n        self,\n        embedder: Embedder,\n        vector_store: VectorStore,\n        primitive_loader: PrimitiveLoader | None = None,\n        composition_strategy: CompositionStrategy | None = None,\n        compression_strategy: CompressionStrategy | None = None,\n        collection_name: str = \"atlas_concepts\",\n    ): ...\n\n    def add(self, concept: CodeConcept) -> str:\n        \"\"\"Add concept, embed name + description.\"\"\"\n        ...\n\n    def search(self, query: str, k: int = 5) -> list[CodeConcept]:\n        \"\"\"Semantic search by natural language query.\"\"\"\n        ...\n\n    def get(self, concept_id: str) -> CodeConcept | None: ...\n\n    def compose(self, concept_ids: list[str]) -> CodeConcept | None:\n        \"\"\"Delegates to composition_strategy.\"\"\"\n        ...\n\n    def compress(self, trajectories: list[Trajectory]) -> list[CodeConcept]:\n        \"\"\"Delegates to compression_strategy.\"\"\"\n        ...\n\n    def update_stats(self, concept_id: str, success: bool) -> None:\n        \"\"\"Update usage count and success rate.\"\"\"\n        ...\n```\n\n---\n\n## Component 3: StrategyBank Implementation\n\nAbstract strategy storage (ArcMemo-style) for high-level reasoning patterns.\n\n### Core Design Decisions\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| Strategy IDs | UUIDs (`uuid4`) | Consistent with other components |\n| Embedding source | Situation field (configurable) | Start with \"when to apply\" text |\n| Write operation | **Agentic** (LLM abstraction) | Requires understanding trajectory to extract abstract pattern |\n| Success rate update | Configurable (EMA default) | Exponential moving average, easy to swap |\n| Source trajectories | **Success only** | Only abstract winning strategies |\n\n### Strategy Protocols\n\n```python\n@runtime_checkable\nclass StrategyAbstractor(Protocol):\n    \"\"\"Abstracts a trajectory into a reusable strategy.\"\"\"\n    async def abstract(self, trajectory: Trajectory) -> Strategy | None:\n        \"\"\"\n        Extract high-level strategy from successful trajectory.\n\n        Returns None if trajectory is not abstractable (e.g., too specific).\n        \"\"\"\n        ...\n\n@runtime_checkable\nclass SuccessRateUpdater(Protocol):\n    \"\"\"Updates success rate statistics for strategies.\"\"\"\n    def update(self, current_rate: float, current_count: int,\n               success: bool) -> tuple[float, int]:\n        \"\"\"\n        Returns (new_rate, new_count).\n\n        Implementations may use:\n        - Simple average\n        - Exponential moving average\n        - Bayesian update\n        - Recency-weighted\n        \"\"\"\n        ...\n```\n\n### Default Implementations\n\n```python\nclass LLMStrategyAbstractor(StrategyAbstractor):\n    \"\"\"LLM extracts abstract strategy from trajectory.\"\"\"\n    def __init__(self, llm_client: LLMClient):\n        self.llm = llm_client\n\n    async def abstract(self, trajectory: Trajectory) -> Strategy | None:\n        # LLM analyzes trajectory and generates:\n        # - situation: when to apply this strategy\n        # - approach: high-level steps\n        # - rationale: why this works\n        ...\n\nclass EMASuccessUpdater(SuccessRateUpdater):\n    \"\"\"Exponential moving average for success rate.\"\"\"\n    def __init__(self, alpha: float = 0.1):\n        self.alpha = alpha\n\n    def update(self, current_rate: float, current_count: int,\n               success: bool) -> tuple[float, int]:\n        new_value = 1.0 if success else 0.0\n        new_rate = self.alpha * new_value + (1 - self.alpha) * current_rate\n        return (new_rate, current_count + 1)\n\nclass SimpleAverageUpdater(SuccessRateUpdater):\n    \"\"\"Simple running average (baseline).\"\"\"\n    def update(self, current_rate: float, current_count: int,\n               success: bool) -> tuple[float, int]:\n        new_count = current_count + 1\n        new_rate = (current_rate * current_count + (1.0 if success else 0.0)) / new_count\n        return (new_rate, new_count)\n\nclass BayesianUpdater(SuccessRateUpdater):\n    \"\"\"Beta-Bernoulli Bayesian update (future experiment).\"\"\"\n    def __init__(self, prior_alpha: float = 1.0, prior_beta: float = 1.0):\n        self.prior_alpha = prior_alpha\n        self.prior_beta = prior_beta\n\n    def update(self, current_rate: float, current_count: int,\n               success: bool) -> tuple[float, int]:\n        # Update beta distribution parameters\n        ...\n```\n\n### Implementation Signature\n\n```python\nclass ChromaStrategyBank:\n    \"\"\"StrategyBank implementation backed by ChromaDB.\"\"\"\n\n    def __init__(\n        self,\n        embedder: Embedder,\n        vector_store: VectorStore,\n        abstractor: StrategyAbstractor,\n        success_updater: SuccessRateUpdater = EMASuccessUpdater(),\n        collection_name: str = \"atlas_strategies\",\n    ): ...\n\n    def write(self, trajectory: Trajectory) -> Strategy | None:\n        \"\"\"\n        Abstract trajectory into strategy.\n\n        - Only processes successful trajectories\n        - Delegates to abstractor strategy\n        - Embeds situation field for retrieval\n        \"\"\"\n        ...\n\n    def read(self, task: Task, k: int = 5) -> list[Strategy]:\n        \"\"\"Find applicable strategies for task.\"\"\"\n        ...\n\n    def get(self, strategy_id: str) -> Strategy | None: ...\n\n    def update_stats(self, strategy_id: str, success: bool) -> None:\n        \"\"\"Delegates to success_updater strategy.\"\"\"\n        ...\n```\n\n---\n\n## Component 4: MemorySystem Aggregator\n\nUnified interface combining all three memory components.\n\n### Core Design Decisions\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| Component optionality | All optional | Enable ablation studies |\n| Query parallelism | `asyncio.gather` | Parallel queries across components |\n| Store behavior | Success to all; failure to experience only | Strategies/concepts only from success |\n\n### Implementation Signature\n\n```python\nclass MemorySystemImpl:\n    \"\"\"Aggregator for all memory types.\"\"\"\n\n    def __init__(\n        self,\n        experience: ExperienceMemory | None = None,\n        concepts: ConceptLibrary | None = None,\n        strategies: StrategyBank | None = None,\n    ): ...\n\n    @property\n    def experience_memory(self) -> ExperienceMemory | None: ...\n\n    @property\n    def concept_library(self) -> ConceptLibrary | None: ...\n\n    @property\n    def strategy_bank(self) -> StrategyBank | None: ...\n\n    async def query(self, task: Task, k: int = 5) -> MemoryQueryResult:\n        \"\"\"\n        Query all available components in parallel.\n\n        Uses asyncio.gather for concurrent queries.\n        Missing components return empty results.\n        \"\"\"\n        ...\n\n    def store(self, trajectory: Trajectory) -> dict[str, Any]:\n        \"\"\"\n        Store in available components.\n\n        - ExperienceMemory: Always stores (success and failure)\n        - ConceptLibrary: Only extracts from successful trajectories\n        - StrategyBank: Only abstracts from successful trajectories\n\n        Returns dict with IDs from each component that stored.\n        \"\"\"\n        ...\n```\n\n---\n\n## File Structure\n\n```\nsrc/atlas/memory/\n├── __init__.py\n├── experience.py          # ChromaExperienceMemory + strategies\n├── concepts.py            # ChromaConceptLibrary + strategies\n├── strategies.py          # ChromaStrategyBank + strategies\n├── system.py              # MemorySystemImpl aggregator\n├── storage.py             # VectorStore protocol + ChromaDB adapter\n└── primitives/\n    ├── __init__.py\n    ├── arc.py             # ARCPrimitiveLoader\n    └── swe.py             # SWEPrimitiveLoader\n```\n\n---\n\n## Dependencies\n\n- Phase 2 infrastructure:\n  - `atlas.infra.embeddings.Embedder`\n  - `atlas.infra.vector_index.VectorIndex`\n- Core types from `atlas.core.types`:\n  - `Task`, `Trajectory`, `Experience`, `CodeConcept`, `Strategy`\n- Protocols from `atlas.protocols.memory`\n\n---\n\n## Testing Strategy\n\n### Unit Tests\n- Each component in isolation with mock strategies\n- Strategy implementations independently\n- Storage layer with mock vector store\n\n### Integration Tests\n- Components with real ChromaDB (ephemeral)\n- End-to-end store -> search -> retrieve flows\n- MemorySystem with various component combinations\n\n### Ablation Test Fixtures\n- Memory-only (no concepts, no strategies)\n- Concepts-only (no experience, no strategies)\n- Full system vs individual components\n\n---\n\n## Success Criteria\n\n- [ ] ExperienceMemory stores and retrieves experiences\n  - [ ] UUID generation for experience IDs\n  - [ ] Configurable extraction strategy\n  - [ ] Pluggable refine strategy\n  - [ ] Success/failure differentiation in storage\n- [ ] ConceptLibrary indexes and searches code patterns\n  - [ ] Primitive loading by domain\n  - [ ] Pluggable composition strategy\n  - [ ] Pluggable compression strategy (Stitch + AutoDoc)\n  - [ ] Usage statistics tracking\n- [ ] StrategyBank manages abstract strategies\n  - [ ] Only abstracts successful trajectories\n  - [ ] Configurable success rate update strategy\n  - [ ] Pluggable abstraction strategy\n- [ ] MemorySystem combines all three\n  - [ ] Parallel async queries via asyncio.gather\n  - [ ] Graceful degradation when components missing\n  - [ ] Differential store behavior for success/failure\n- [ ] Each component works in isolation\n- [ ] All strategies are swappable for experimentation\n ","priority":1,"archived":0,"archived_at":null,"created_at":"2026-01-06 02:20:03","updated_at":"2026-01-08T00:01:50.032Z","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-7h2g","from_type":"spec","to":"s-3c37","to_type":"spec","type":"implements"},{"from":"s-7h2g","from_type":"spec","to":"s-3c37","to_type":"spec","type":"references"},{"from":"s-7h2g","from_type":"spec","to":"s-5o87","to_type":"spec","type":"references"},{"from":"s-7h2g","from_type":"spec","to":"s-7xs8","to_type":"spec","type":"depends-on"},{"from":"s-7h2g","from_type":"spec","to":"s-7xs8","to_type":"spec","type":"references"}],"tags":["implementation","memory","phase-3","pillar-1"]}
{"id":"s-7xs8","uuid":"12efd4e8-865b-4b65-91e0-fad14c400a33","title":"Phase 2: Infrastructure Layer","file_path":"specs/s-7xs8_phase_2_infrastructure_layer.md","content":"# Phase 2: Infrastructure Layer\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nFoundation infrastructure needed by all three pillars. This phase establishes the agent execution layer (via ACP), memory access infrastructure, and vector storage.\n\n**Key Design Decision**: ATLAS uses the Agent Client Protocol (ACP) via `acp-factory` as the primary agent abstraction. This means:\n- No raw LLM wrapper needed - agents handle LLM interactions\n- All cognitive tasks (including internal ATLAS operations) go through agents\n- ATLAS focuses on orchestration, memory, and learning - not agent internals\n\n## Configuration\n\nCentralized dataclass-based configuration with sensible defaults:\n\n```python\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\n@dataclass\nclass ExecutorConfig:\n    \"\"\"TaskExecutor configuration.\"\"\"\n    agent_type: str = \"claude-code\"\n    reuse_sessions: bool = False  # Default: new session per task\n    timeout_seconds: int = 300\n    permission_mode: str = \"auto-approve\"\n\n@dataclass\nclass MemoryConfig:\n    \"\"\"Memory context limits for prompts.\"\"\"\n    max_experiences: int = 4      # From ReMem paper\n    max_strategies: int = 3\n    max_concepts: int = 5\n    max_context_tokens: int = 4000\n\n@dataclass\nclass EmbeddingConfig:\n    \"\"\"Embedding provider configuration.\"\"\"\n    model_name: str = \"BAAI/bge-base-en-v1.5\"\n    device: str = \"cpu\"           # \"cpu\", \"cuda\", \"mps\"\n    cache_enabled: bool = True    # In-memory cache\n\n@dataclass\nclass StorageConfig:\n    \"\"\"Storage and persistence configuration.\"\"\"\n    base_path: Path = field(default_factory=lambda: Path(\".atlas\"))\n    chroma_collection_prefix: str = \"\"\n    distance_metric: str = \"cosine\"\n\n@dataclass\nclass ATLASConfig:\n    \"\"\"Root configuration for ATLAS.\"\"\"\n    executor: ExecutorConfig = field(default_factory=ExecutorConfig)\n    memory: MemoryConfig = field(default_factory=MemoryConfig)\n    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)\n    storage: StorageConfig = field(default_factory=StorageConfig)\n```\n\n## Components\n\n### TaskExecutor (ACP Integration)\n\nBridges ACP sessions and ATLAS's task/trajectory model. **Fully async API.**\n\n**Responsibilities:**\n- Spawn and manage ACP agent sessions\n- Convert SessionUpdates → Trajectory Steps\n- Integrate memory context (upfront + MCP tools)\n- Coordinate with Environment for verification\n- Support session forking for search strategies\n\n**Interface:**\n\n```python\nclass TaskExecutor:\n    \"\"\"Executes tasks via ACP agents, produces Trajectories.\"\"\"\n    \n    def __init__(\n        self,\n        config: ExecutorConfig = None,\n        memory: MemorySystem | None = None,\n    ):\n        self._config = config or ExecutorConfig()\n        self._memory = memory\n        self._agent_handle: AgentHandle | None = None\n    \n    async def execute(\n        self,\n        task: Task,\n        env: Environment,\n    ) -> Trajectory:\n        \"\"\"\n        Execute a task and return complete trajectory.\n        \n        1. Query memory for initial context\n        2. Format structured prompt with task + context\n        3. Create ACP session with Memory MCP server\n        4. Run session, collect Steps from updates\n        5. Get solution from environment state\n        6. Verify result via Environment\n        7. Return Trajectory with Outcome\n        \"\"\"\n        ...\n    \n    async def execute_with_session(\n        self,\n        task: Task,\n        env: Environment,\n        session: Session,\n    ) -> Trajectory:\n        \"\"\"Execute using existing session (for forking/branching).\"\"\"\n        ...\n    \n    async def close(self) -> None:\n        \"\"\"Clean up agent handle.\"\"\"\n        ...\n```\n\n**Session Management:**\n\n| Mode | Config | Behavior |\n|------|--------|----------|\n| Fresh (default) | `reuse_sessions=False` | New session per task, clean isolation |\n| Reuse | `reuse_sessions=True` | Persistent agent, new session per task |\n\n### TrajectoryBuilder\n\nConverts ACP SessionUpdates into ATLAS Steps using **tool-centric mapping**.\n\n**Mapping Rules:**\n\n| ACP Update | ATLAS Step Component |\n|------------|---------------------|\n| `agent_message_chunk` (before tool) | `step.thought` |\n| `tool_call` | `step.action` (tool name + args) |\n| `tool_call_update` (completed) | `step.observation` (tool result) |\n| `agent_message_chunk` (after tool) | Next step's `thought` |\n\n**Implementation:**\n\n```python\nclass TrajectoryBuilder:\n    \"\"\"Builds Trajectory from ACP session updates.\"\"\"\n    \n    def __init__(self, task: Task, agent_id: str):\n        self._task = task\n        self._agent_id = agent_id\n        self._steps: list[Step] = []\n        self._current_thought: list[str] = []\n        self._current_tool_call: dict | None = None\n    \n    def process_update(self, update: ExtendedSessionUpdate) -> None:\n        \"\"\"Process a single session update.\"\"\"\n        match update.get(\"session_update\"):\n            case \"agent_message_chunk\":\n                self._handle_message_chunk(update)\n            case \"tool_call\":\n                self._handle_tool_call(update)\n            case \"tool_call_update\":\n                self._handle_tool_update(update)\n    \n    def build(self, outcome: Outcome) -> Trajectory:\n        \"\"\"Finalize and return the trajectory.\"\"\"\n        return Trajectory(\n            task=self._task,\n            steps=self._steps,\n            outcome=outcome,\n            metadata={\"agent_id\": self._agent_id, \"timestamp\": datetime.now()},\n        )\n```\n\n### Memory MCP Server (FastMCP)\n\nExposes ATLAS memory as MCP tools for agent mid-execution access.\n\n**Implementation with FastMCP:**\n\n```python\nfrom fastmcp import FastMCP\n\nmcp = FastMCP(\"atlas-memory\")\n\n@mcp.tool()\ndef memory_search_experiences(query: str, k: int = 4) -> list[dict]:\n    \"\"\"Search for similar past experiences.\n    \n    Args:\n        query: Natural language description of what you're looking for\n        k: Number of results to return (default: 4)\n    \n    Returns:\n        List of relevant experiences with task, solution, and outcome\n    \"\"\"\n    return memory.experience_memory.search(query, k=k)\n\n@mcp.tool()\ndef memory_search_concepts(query: str, k: int = 5) -> list[dict]:\n    \"\"\"Search for relevant code patterns and concepts.\n    \n    Args:\n        query: Description of the pattern or functionality needed\n        k: Number of results to return (default: 5)\n    \n    Returns:\n        List of code concepts with name, description, and code\n    \"\"\"\n    return memory.concept_library.search(query, k=k)\n\n@mcp.tool()\ndef memory_search_strategies(query: str, k: int = 3) -> list[dict]:\n    \"\"\"Search for applicable high-level strategies.\n    \n    Args:\n        query: Description of the problem or situation\n        k: Number of results to return (default: 3)\n    \n    Returns:\n        List of strategies with situation and suggestion\n    \"\"\"\n    return memory.strategy_bank.read(query, k=k)\n\n@mcp.tool()\ndef memory_get_concept(concept_id: str) -> dict | None:\n    \"\"\"Get a specific code concept by ID.\n    \n    Args:\n        concept_id: The concept identifier\n    \n    Returns:\n        Full concept details or None if not found\n    \"\"\"\n    return memory.concept_library.get(concept_id)\n```\n\n### Prompt Formatter\n\nFormats task and memory context into structured prompts.\n\n**Template:**\n\n```markdown\n## Task\n{task.description}\n\n{if task.context}\n## Context\n{task.context}\n{endif}\n\n## Relevant Memory\n\n### Similar Experiences ({len(experiences)})\n{for exp in experiences}\n- **{exp.task_summary}**\n  - Approach: {exp.solution_summary}\n  - Outcome: {exp.outcome}\n{endfor}\n\n### Applicable Strategies ({len(strategies)})\n{for strategy in strategies}\n- When: {strategy.situation}\n  Try: {strategy.suggestion}\n{endfor}\n\n### Available Concepts ({len(concepts)})\n{for concept in concepts}\n- `{concept.name}`: {concept.description}\n{endfor}\n\n## Notes\n- You can query additional memory using `memory_search_*` tools if needed.\n- Focus on the task at hand and use the provided context as guidance.\n```\n\n**Token Management:**\n\n```python\nclass PromptFormatter:\n    def __init__(self, config: MemoryConfig):\n        self._config = config\n    \n    def format(\n        self,\n        task: Task,\n        memory_result: MemoryQueryResult | None,\n    ) -> str:\n        \"\"\"Format task and memory into prompt, respecting token limits.\"\"\"\n        # Truncate each section to fit within max_context_tokens\n        ...\n```\n\n### Embedding Provider\n\nBGE embeddings with in-memory caching.\n\n```python\nclass BGEEmbeddings:\n    \"\"\"BAAI/bge-base-en-v1.5 embeddings with caching.\"\"\"\n    \n    def __init__(self, config: EmbeddingConfig = None):\n        self._config = config or EmbeddingConfig()\n        self._model = None  # Lazy loading\n        self._cache: dict[str, np.ndarray] = {}  # In-memory cache\n    \n    def encode(self, text: str) -> np.ndarray:\n        \"\"\"Encode text, using cache if available.\"\"\"\n        if self._config.cache_enabled and text in self._cache:\n            return self._cache[text]\n        \n        embedding = self._get_model().encode(text)\n        \n        if self._config.cache_enabled:\n            self._cache[text] = embedding\n        \n        return embedding\n    \n    def encode_batch(self, texts: list[str]) -> np.ndarray:\n        \"\"\"Encode multiple texts efficiently.\"\"\"\n        ...\n    \n    def clear_cache(self) -> None:\n        \"\"\"Clear the embedding cache.\"\"\"\n        self._cache.clear()\n    \n    @property\n    def dimension(self) -> int:\n        return 768  # BGE base dimension\n    \n    @property\n    def model_name(self) -> str:\n        return self._config.model_name\n```\n\n### Vector Index (ChromaDB)\n\nProject-local storage with one collection per memory type.\n\n```python\nclass ChromaIndex:\n    \"\"\"ChromaDB-backed vector index.\"\"\"\n    \n    def __init__(\n        self,\n        collection_name: str,\n        config: StorageConfig = None,\n    ):\n        self._config = config or StorageConfig()\n        self._collection_name = self._prefixed_name(collection_name)\n        self._client = None  # Lazy init\n        self._collection = None\n    \n    def _prefixed_name(self, name: str) -> str:\n        prefix = self._config.chroma_collection_prefix\n        return f\"{prefix}{name}\" if prefix else name\n    \n    def _get_client(self) -> chromadb.Client:\n        if self._client is None:\n            persist_dir = self._config.base_path / \"chroma\"\n            persist_dir.mkdir(parents=True, exist_ok=True)\n            self._client = chromadb.PersistentClient(path=str(persist_dir))\n        return self._client\n    \n    def _get_collection(self) -> chromadb.Collection:\n        if self._collection is None:\n            self._collection = self._get_client().get_or_create_collection(\n                name=self._collection_name,\n                metadata={\"hnsw:space\": self._config.distance_metric},\n            )\n        return self._collection\n    \n    # ... implement VectorIndex protocol methods\n```\n\n**Collections:**\n\n| Collection | Contents |\n|------------|----------|\n| `experiences` | Task-level experiences from trajectories |\n| `concepts` | Code patterns and compositions |\n| `strategies` | Abstract reasoning patterns |\n\n## Error Handling\n\n| Scenario | Handling |\n|----------|----------|\n| Agent crashes mid-execution | Return partial Trajectory with `Outcome(success=False, error_info=\"agent_crashed\")` |\n| Agent times out | Cancel session, return partial Trajectory with `Outcome(success=False, error_info=\"timeout\")` |\n| Verification fails | Return complete Trajectory with `Outcome(success=False)` - normal flow |\n| MCP server unavailable | Continue without memory tools (graceful degradation), log warning |\n| Embedding model fails to load | Raise exception on first use (fail fast) |\n| ChromaDB connection fails | Raise exception (storage is required) |\n\n## Logging\n\nStandard Python logging with module-based loggers:\n\n```python\nimport logging\n\n# Module loggers\nlogger_execution = logging.getLogger(\"atlas.execution\")\nlogger_memory = logging.getLogger(\"atlas.memory\")\nlogger_mcp = logging.getLogger(\"atlas.mcp\")\nlogger_embedding = logging.getLogger(\"atlas.embedding\")\nlogger_storage = logging.getLogger(\"atlas.storage\")\n\n# Usage\nlogger_execution.info(\"Task started\", extra={\"task_id\": task.id})\nlogger_execution.debug(\"Step completed\", extra={\"step\": step.action})\nlogger_memory.info(\"Memory query\", extra={\"k\": k, \"results\": len(results)})\n```\n\n## Solution Extraction\n\nThe solution is the **environment state at the end of the trajectory**.\n\n```python\nasync def execute(self, task: Task, env: Environment) -> Trajectory:\n    # ... run session, collect steps ...\n    \n    # Solution = environment state after agent execution\n    candidate = Candidate(\n        solution=env.get_current_state(),  # Domain-specific\n        confidence=1.0,\n        reasoning=\"Agent execution completed\",\n        source=\"agent\",\n    )\n    \n    # Verify\n    outcome = env.verify(task, candidate)\n    \n    return trajectory_builder.build(outcome)\n```\n\nFor domains needing custom extraction, `Environment` can override:\n\n```python\nclass Environment(Protocol):\n    def get_current_state(self) -> Any:\n        \"\"\"Get current state as solution candidate.\"\"\"\n        ...  # Default: return None, subclasses override\n```\n\n## Dependencies\n\n- `acp-factory` - Agent Client Protocol library (git submodule)\n- `fastmcp` - MCP server framework\n- `sentence-transformers` - Local embeddings\n- `chromadb` - Vector storage\n\n## File Structure\n\n```\natlas/\n├── config.py              # ATLASConfig and sub-configs\n├── execution/\n│   ├── __init__.py\n│   ├── executor.py        # TaskExecutor\n│   ├── trajectory_builder.py  # SessionUpdate → Step conversion\n│   └── prompt_formatter.py    # Task + memory → structured prompt\n├── mcp/\n│   ├── __init__.py\n│   └── memory_server.py   # FastMCP memory server\n├── embeddings/\n│   ├── __init__.py\n│   └── bge.py             # BGE embeddings with cache\n└── vector/\n    ├── __init__.py\n    └── chroma.py          # ChromaDB index\n```\n\n## Success Criteria\n\n- [ ] ATLASConfig provides centralized, typed configuration\n- [ ] TaskExecutor spawns ACP agents and executes tasks\n- [ ] SessionUpdates correctly convert to Steps (tool-centric)\n- [ ] Session reuse is configurable (default: new per task)\n- [ ] Memory MCP server exposes search tools via FastMCP\n- [ ] Upfront memory context is formatted with token limits\n- [ ] BGE embeddings work with in-memory caching\n- [ ] ChromaDB stores vectors in project-local `.atlas/`\n- [ ] Graceful error handling for crashes/timeouts\n- [ ] Standard Python logging throughout\n- [ ] Async API for all I/O operations","priority":0,"archived":0,"archived_at":null,"created_at":"2026-01-06 02:20:03","updated_at":"2026-01-07 07:16:21","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[],"tags":["acp","async","embeddings","fastmcp","infrastructure","mcp","phase-2","vector-store"]}
{"id":"s-2uov","uuid":"41a04d9a-72a2-4d8e-b250-24827e8c3f7e","title":"Phase 4: Minimal Solver","file_path":"specs/s-2uov_phase_4_minimal_solver.md","content":"# Phase 4: Minimal Solver\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nGet a minimal end-to-end system running. Focus on the simplest path: DirectSolver with basic routing and verification.\n\n## Goal\n\nSolve simple tasks using memory retrieval without complex search algorithms. This validates the core loop before adding Mind Evolution or MCTS.\n\n## Components\n\n### DirectSolver\nSimplest search strategy: retrieve → adapt → verify.\n\n**Scope:**\n- Retrieve most similar experience from memory\n- Adapt solution to current task via LLM\n- Verify against task criteria\n- Fall back to next similar if verification fails\n\n**Cost:** ~1-5 LLM calls per task\n\n**Algorithm:**\n```\n1. Query memory for similar experiences\n2. For each experience (by similarity):\n   a. Adapt solution to current task\n   b. Verify adapted solution\n   c. If success, return\n3. If all fail, return best partial result\n```\n\n### TaskRouter (Basic)\nMinimal routing logic to select search strategy.\n\n**Scope:**\n- Query memory for similar experiences\n- Estimate task difficulty\n- Select between direct/evolutionary/mcts\n- Initially: always route to DirectSolver\n\n**Routing Logic (v1):**\n- If similarity > 0.8 and previous success → `direct`\n- Else → `direct` (expand later)\n\n### Verifier (Basic)\nSimple verification for initial testing.\n\n**Scope:**\n- Exact match verification\n- Partial scoring (for ranking)\n- Domain-agnostic interface\n\n**Implementations:**\n- `SimpleVerifier`: String/value equality\n- `FunctionVerifier`: Execute and compare output\n\n## Integration\n\n### ATLASSolver\nTop-level orchestrator combining all components.\n\n```python\nclass ATLASSolver:\n    def __init__(\n        self,\n        memory: MemorySystem,\n        llm: LLM,\n        search: SearchEngine = None,  # defaults to DirectSolver\n    ):\n        pass\n    \n    def solve(self, task: Task, env: Environment) -> Trajectory:\n        routing = self.router.route(task, self.memory)\n        candidates = self.search.search(task, routing, env)\n        best = self.select_best(candidates)\n        return self.build_trajectory(task, best)\n```\n\n## Dependencies\n\n- Phase 2: LLM, Embeddings\n- Phase 3: MemorySystem\n\n## File Structure\n\n```\natlas/search/\n├── __init__.py\n├── router.py          # TaskRouter implementation\n├── direct.py          # DirectSolver implementation\n└── verifier.py        # Verifier implementations\n\natlas/\n└── solver.py          # ATLASSolver orchestrator\n```\n\n## Success Criteria\n\n- [ ] DirectSolver retrieves and adapts solutions\n- [ ] TaskRouter makes basic routing decisions\n- [ ] Verifier validates solutions\n- [ ] End-to-end: task in → trajectory out\n- [ ] Works with empty memory (fallback to generation)\n- [ ] Works with populated memory (retrieval-augmented)","priority":1,"archived":0,"archived_at":null,"created_at":"2026-01-06 02:20:04","updated_at":"2026-01-06 02:20:04","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-2uov","from_type":"spec","to":"s-7h2g","to_type":"spec","type":"depends-on"}],"tags":["direct","end-to-end","minimal","phase-4","solver"]}
{"id":"s-484m","uuid":"c72bef78-a632-4a3f-a410-a7dbab4420be","title":"Phase 7: Environments & Domains","file_path":"specs/s-484m_phase_7_environments_agents.md","content":"# Phase 7: Environments & Domains\n\nParent: [[s-5o87|ATLAS System Architecture]]\n\n## Overview\n\nDomain-specific environments for task verification and optional sandboxed execution. Since agents are managed via ACP (see [[s-7xs8|Phase 2]]), this phase focuses on:\n1. **Verification**: Checking if solutions are correct\n2. **Sandboxing**: Optional controlled execution for untrusted contexts\n3. **Domain primitives**: Task loaders, formatters, and domain-specific utilities\n\n## Environment Protocol\n\nEnvironments provide verification and optional sandbox control:\n\n```python\nclass Environment(Protocol):\n    \"\"\"Environment for task verification with optional sandboxing.\"\"\"\n    \n    def verify(self, task: Task, candidate: Candidate) -> Outcome:\n        \"\"\"Verify a candidate solution against task requirements.\"\"\"\n        ...\n    \n    def get_sandbox_handlers(self) -> ClientHandlers | None:\n        \"\"\"Optional: Return handlers for sandboxed execution.\n        \n        Returns:\n            None: Agent runs freely (default)\n            ClientHandlers: Agent runs in sandbox with controlled access\n        \"\"\"\n        return None\n    \n    @property\n    def supports_partial_scoring(self) -> bool:\n        \"\"\"Whether this environment supports partial scores.\"\"\"\n        ...\n```\n\n## ARC Environment\n\nEnvironment for ARC-AGI grid puzzle tasks.\n\n### Verification\n\n```python\nclass ARCEnvironment:\n    def verify(self, task: Task, candidate: Candidate) -> Outcome:\n        \"\"\"\n        Verify grid solution.\n        \n        - Exact match: success=True, partial_score=1.0\n        - Partial match: success=False, partial_score=matching_cells/total_cells\n        - Wrong dimensions: success=False, partial_score=0.0\n        \"\"\"\n```\n\n### Task Format\n\n```python\n@dataclass\nclass ARCTask:\n    task_id: str\n    train_examples: list[tuple[Grid, Grid]]  # (input, output) pairs\n    test_input: Grid\n    test_output: Grid  # Hidden during solving\n```\n\n### Sandbox (Optional)\n\nARC tasks typically don't need sandboxing since they're pure computation:\n\n```python\ndef get_sandbox_handlers(self) -> ClientHandlers | None:\n    return None  # No sandbox needed for ARC\n```\n\n### Utilities\n\n- `load_arc_dataset(path)` - Load ARC tasks from JSON\n- `format_arc_task(task)` - Format task for agent prompt\n- `parse_grid_response(response)` - Extract grid from agent output\n\n## SWE Environment\n\nEnvironment for software engineering tasks (SWE-bench style).\n\n### Verification\n\n```python\nclass SWEEnvironment:\n    def verify(self, task: Task, candidate: Candidate) -> Outcome:\n        \"\"\"\n        Verify code solution by running tests.\n        \n        - All tests pass: success=True, partial_score=1.0\n        - Some tests pass: success=False, partial_score=passing/total\n        - Build fails: success=False, partial_score=0.0\n        \"\"\"\n```\n\n### Task Format\n\n```python\n@dataclass\nclass SWETask:\n    task_id: str\n    repo_url: str\n    base_commit: str\n    issue_description: str\n    test_command: str\n    setup_commands: list[str]\n```\n\n### Sandbox (Required)\n\nSWE tasks run in Docker sandbox for safety:\n\n```python\nclass SWEEnvironment:\n    def __init__(self, docker_image: str = \"python:3.11\"):\n        self._docker = DockerSandbox(docker_image)\n    \n    def get_sandbox_handlers(self) -> ClientHandlers:\n        \"\"\"Return Docker-backed handlers.\"\"\"\n        return ClientHandlers(\n            on_file_read=self._docker.read_file,\n            on_file_write=self._docker.write_file,\n            on_terminal_create=self._docker.create_terminal,\n            on_terminal_output=self._docker.get_output,\n            on_terminal_kill=self._docker.kill_terminal,\n            on_terminal_release=self._docker.release_terminal,\n            on_terminal_wait_for_exit=self._docker.wait_for_exit,\n        )\n```\n\n### Docker Sandbox\n\n```python\nclass DockerSandbox:\n    \"\"\"Isolated execution environment using Docker.\"\"\"\n    \n    def __init__(self, image: str, timeout: int = 300):\n        ...\n    \n    async def setup(self, task: SWETask) -> None:\n        \"\"\"Clone repo, checkout commit, run setup commands.\"\"\"\n        ...\n    \n    async def run_tests(self, test_command: str) -> TestResult:\n        \"\"\"Execute test command and parse results.\"\"\"\n        ...\n    \n    async def cleanup(self) -> None:\n        \"\"\"Remove container and temporary files.\"\"\"\n        ...\n```\n\n## Generic Environment\n\nBase environment for custom domains.\n\n```python\nclass GenericEnvironment:\n    \"\"\"Flexible environment for custom task types.\"\"\"\n    \n    def __init__(\n        self,\n        verifier: Callable[[Task, Candidate], Outcome],\n        sandbox_handlers: ClientHandlers | None = None,\n    ):\n        self._verifier = verifier\n        self._sandbox_handlers = sandbox_handlers\n    \n    def verify(self, task: Task, candidate: Candidate) -> Outcome:\n        return self._verifier(task, candidate)\n    \n    def get_sandbox_handlers(self) -> ClientHandlers | None:\n        return self._sandbox_handlers\n```\n\n## Domain Registry\n\nRegister environments for different task domains:\n\n```python\nclass DomainRegistry:\n    \"\"\"Registry for domain-specific environments.\"\"\"\n    \n    _environments: dict[str, type[Environment]] = {}\n    \n    @classmethod\n    def register(cls, domain: str, env_class: type[Environment]) -> None:\n        cls._environments[domain] = env_class\n    \n    @classmethod\n    def get(cls, domain: str) -> type[Environment]:\n        return cls._environments[domain]\n    \n    @classmethod\n    def create(cls, domain: str, **kwargs) -> Environment:\n        return cls._environments[domain](**kwargs)\n\n# Register built-in domains\nDomainRegistry.register(\"arc\", ARCEnvironment)\nDomainRegistry.register(\"swe\", SWEEnvironment)\n```\n\n## Agent Note\n\n**Agents are NOT part of this phase.** Agent management is handled via ACP (`acp-factory`) in Phase 2:\n\n- `AgentFactory.spawn(\"claude-code\")` - Spawn Claude Code\n- `AgentFactory.spawn(\"codex\")` - Spawn Codex\n- `AgentFactory.spawn(\"gemini\")` - Spawn Gemini\n- `AgentFactory.spawn(\"opencode\")` - Spawn OpenCode\n\nATLAS's `TaskExecutor` wraps ACP sessions - no additional agent implementations needed.\n\n## Dependencies\n\n- Phase 2: TaskExecutor, ACP integration\n- External: Docker (for SWE sandbox)\n\n## File Structure\n\n```\natlas/environments/\n├── __init__.py\n├── base.py            # Environment protocol, GenericEnvironment\n├── registry.py        # DomainRegistry\n├── arc/\n│   ├── __init__.py\n│   ├── environment.py # ARCEnvironment\n│   ├── loader.py      # Dataset loading\n│   └── types.py       # Grid, ARCTask\n└── swe/\n    ├── __init__.py\n    ├── environment.py # SWEEnvironment\n    ├── sandbox.py     # DockerSandbox\n    └── types.py       # SWETask, TestResult\n```\n\n## Success Criteria\n\n- [ ] ARCEnvironment loads tasks and verifies grid solutions\n- [ ] ARCEnvironment supports partial scoring (cell-by-cell)\n- [ ] SWEEnvironment runs tests in Docker sandbox\n- [ ] SWEEnvironment supports partial scoring (test pass rate)\n- [ ] Sandbox handlers integrate with ACP sessions\n- [ ] DomainRegistry allows custom environment registration\n- [ ] GenericEnvironment enables rapid domain prototyping","priority":3,"archived":0,"archived_at":null,"created_at":"2026-01-06 02:20:04","updated_at":"2026-01-07 06:54:03","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-484m","from_type":"spec","to":"s-2uov","to_type":"spec","type":"depends-on"}],"tags":["arc","environments","phase-7","sandbox","swe","verification"]}
{"id":"s-6d5x","uuid":"883e1201-e690-4a7d-99ce-20a045f37b4c","title":"Phase 5: Advanced Search","file_path":"specs/s-6d5x_phase_5_advanced_search.md","content":"# Phase 5: Advanced Search\n\nParent: [[s-5o87|ATLAS System Architecture]]\nImplements: [[s-3nub|Search Methods (Pillar 2)]]\n\n## Overview\n\nAdvanced search algorithms for harder tasks where DirectSolver isn't enough.\n\n## Implementation Decisions\n\n### Scope\n- Implement **both** MindEvolutionSearch and SWESearch (MCTS)\n- Full environment implementations for ARC (arckit) and SWE (Docker)\n- Full routing logic with configurable thresholds\n- Pydantic config models\n\n### MCTS Value Estimation\nHybrid approach:\n1. LLM discriminator for initial estimates on all nodes\n2. Full/partial agent rollouts on top 10-20% promising nodes\n3. Balances cost (~300-500 calls) with accuracy\n\n### Environment Integration\n- **ARC**: Use `arckit` library for data loading and grid verification\n- **SWE**: Full `swebench` Docker integration with harness\n\n## Components\n\n### 1. Configuration Models\n\n```python\nclass MindEvolutionConfig(BaseModel):\n    \"\"\"Configuration for Mind Evolution search.\"\"\"\n    population_size: int = 20\n    generations: int = 10\n    elite_fraction: float = 0.5\n    memory_init_fraction: float = 0.5  # % from memory\n    mutation_temperature: float = 0.7\n    crossover_rate: float = 0.3\n\nclass SWESearchConfig(BaseModel):\n    \"\"\"Configuration for SWE-Search (MCTS).\"\"\"\n    max_expansions: int = 100\n    ucb_constant: float = 1.414\n    max_depth: int = 20\n    rollout_depth: int = 5\n    use_discriminator: bool = True\n    discriminator_threshold: float = 0.7  # Top % for agent rollout\n\nclass RouterConfig(BaseModel):\n    \"\"\"Configuration for task routing.\"\"\"\n    similarity_threshold: float = 0.9\n    use_domain_routing: bool = True\n    default_strategy: str = \"evolutionary\"\n```\n\n### 2. ARCEnvironment\n\nFull implementation using `arckit`:\n\n```python\nclass ARCEnvironment:\n    \"\"\"ARC-AGI environment with grid verification.\"\"\"\n    \n    def __init__(self, task_id: str | None = None):\n        self._task: Task | None = None\n        self._arc_task: arckit.Task | None = None\n    \n    def reset(self, task: Task) -> str:\n        \"\"\"Load ARC task and return description.\"\"\"\n        # Load from arckit dataset\n        # Parse task.context for grid data\n        ...\n    \n    def verify(self, solution: Any) -> Outcome:\n        \"\"\"Verify grid solution with partial scoring.\"\"\"\n        # Exact match check\n        # Cell-by-cell similarity for partial score\n        ...\n    \n    @property\n    def task(self) -> Task:\n        return self._task\n```\n\n### 3. SWEEnvironment\n\nFull Docker integration with swebench:\n\n```python\nclass SWEEnvironment:\n    \"\"\"SWE-bench environment with Docker evaluation.\"\"\"\n    \n    def __init__(\n        self,\n        docker_client: docker.DockerClient | None = None,\n        timeout: int = 300,\n    ):\n        self._task: Task | None = None\n        self._docker = docker_client or docker.from_env()\n    \n    def reset(self, task: Task) -> str:\n        \"\"\"Set up Docker container for task.\"\"\"\n        # Pull/build image for repo version\n        # Initialize container\n        ...\n    \n    def step(self, action: str) -> tuple[str, float, bool]:\n        \"\"\"Execute action (patch/command) in container.\"\"\"\n        # Apply patch or run command\n        # Return observation, reward, done\n        ...\n    \n    def verify(self, solution: Any) -> Outcome:\n        \"\"\"Run tests and return outcome.\"\"\"\n        # Execute test suite in container\n        # Parse results for pass/fail\n        # Calculate partial score (tests passed / total)\n        ...\n```\n\n### 4. MindEvolutionSearch\n\nPopulation-based evolutionary search:\n\n```python\nclass MindEvolutionSearch:\n    \"\"\"Mind Evolution search for ARC-style tasks.\"\"\"\n    \n    def __init__(\n        self,\n        memory: MemorySystem,\n        llm: SimpleLLM,\n        config: MindEvolutionConfig | None = None,\n    ):\n        ...\n    \n    def search(\n        self,\n        task: Task,\n        routing: RoutingDecision,\n        env: Environment,\n    ) -> list[Candidate]:\n        \"\"\"\n        Algorithm:\n        1. Initialize population (50% memory, 50% novel)\n        2. For each generation:\n           a. Evaluate fitness via env.verify()\n           b. Select elites (top 50%)\n           c. Generate children via mutation/crossover\n        3. Return best candidates\n        \"\"\"\n        ...\n    \n    def _initialize_population(self, task: Task, routing: RoutingDecision) -> list[Candidate]:\n        \"\"\"Initialize from memory + novel generation.\"\"\"\n        ...\n    \n    def _mutate(self, candidate: Candidate, task: Task) -> Candidate:\n        \"\"\"LLM-based mutation.\"\"\"\n        ...\n    \n    def _crossover(self, parent1: Candidate, parent2: Candidate, task: Task) -> Candidate:\n        \"\"\"LLM-based crossover.\"\"\"\n        ...\n```\n\n### 5. SWESearch (MCTS)\n\nMonte Carlo Tree Search:\n\n```python\nclass SWESearch:\n    \"\"\"MCTS-based search for SWE tasks.\"\"\"\n    \n    def __init__(\n        self,\n        memory: MemorySystem,\n        llm: SimpleLLM,\n        executor: TaskExecutor | None = None,  # For agent rollouts\n        config: SWESearchConfig | None = None,\n    ):\n        ...\n    \n    def search(\n        self,\n        task: Task,\n        routing: RoutingDecision,\n        env: Environment,\n    ) -> list[Candidate]:\n        \"\"\"\n        Algorithm:\n        1. Initialize root with task state\n        2. For each expansion:\n           a. Select leaf via UCB\n           b. Expand with LLM-generated actions\n           c. Estimate value (discriminator + selective rollout)\n           d. Backpropagate\n        3. Return best path\n        \"\"\"\n        ...\n    \n    def _select(self, node: MCTSNode) -> MCTSNode:\n        \"\"\"UCB selection.\"\"\"\n        ...\n    \n    def _expand(self, node: MCTSNode, task: Task) -> list[MCTSNode]:\n        \"\"\"LLM-generated action expansion.\"\"\"\n        ...\n    \n    def _estimate_value(self, node: MCTSNode, task: Task, env: Environment) -> float:\n        \"\"\"Hybrid: discriminator + selective agent rollout.\"\"\"\n        ...\n```\n\n### 6. Discriminator\n\nHybrid value estimation:\n\n```python\nclass Discriminator:\n    \"\"\"Estimates solution quality for MCTS and pruning.\"\"\"\n    \n    def __init__(\n        self,\n        llm: SimpleLLM,\n        executor: TaskExecutor | None = None,\n    ):\n        ...\n    \n    def estimate(self, task: Task, candidate: Candidate) -> float:\n        \"\"\"LLM-based quality estimation.\"\"\"\n        ...\n    \n    def estimate_with_rollout(\n        self,\n        task: Task,\n        candidate: Candidate,\n        env: Environment,\n        depth: int = 5,\n    ) -> float:\n        \"\"\"Agent rollout for high-confidence estimation.\"\"\"\n        ...\n    \n    def should_rollout(self, score: float, threshold: float = 0.7) -> bool:\n        \"\"\"Decide if candidate warrants expensive rollout.\"\"\"\n        ...\n```\n\n### 7. Enhanced TaskRouter\n\nSmart routing with configuration:\n\n```python\nclass EnhancedTaskRouter:\n    \"\"\"Smart task routing based on task characteristics.\"\"\"\n    \n    def __init__(\n        self,\n        memory: MemorySystem,\n        config: RouterConfig | None = None,\n    ):\n        ...\n    \n    def route(self, task: Task, memory: MemorySystem) -> RoutingDecision:\n        \"\"\"\n        Routing logic:\n        1. Query memory for similar experiences\n        2. Calculate similarity score\n        3. Check domain\n        4. Apply routing rules:\n           - similarity > 0.9 + success → adapt\n           - clear strategy match → direct\n           - ARC domain → evolutionary\n           - SWE domain → mcts\n           - unknown → evolutionary (default)\n        \"\"\"\n        ...\n    \n    def _calculate_similarity(self, task: Task, experiences: list[Experience]) -> float:\n        \"\"\"Calculate max similarity to past experiences.\"\"\"\n        ...\n```\n\n## File Structure\n\n```\natlas/\n├── config.py                    # Add new config models\n├── environments/\n│   ├── arc.py                   # Full ARCEnvironment\n│   └── swe.py                   # Full SWEEnvironment with Docker\n├── search/\n│   ├── mind_evolution.py        # MindEvolutionSearch\n│   ├── mcts.py                  # SWESearch\n│   ├── discriminator.py         # Discriminator\n│   └── router.py                # EnhancedTaskRouter\n```\n\n## Dependencies\n\n- `arckit>=1.0.1` - ARC data loading and manipulation\n- `swebench` - SWE-bench evaluation harness\n- `docker` - Docker SDK for Python\n\n## Success Criteria\n\n- [ ] MindEvolution improves over DirectSolver on ARC tasks\n- [ ] SWESearch handles multi-step code edits\n- [ ] ARCEnvironment correctly verifies grid solutions\n- [ ] SWEEnvironment runs tests in Docker containers\n- [ ] Router correctly selects strategy by task type\n- [ ] Memory integration boosts population quality\n- [ ] Configurable search budgets work correctly\n- [ ] All components have comprehensive tests","priority":2,"archived":0,"archived_at":null,"created_at":"2026-01-06 02:20:04","updated_at":"2026-01-08 05:58:02","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-6d5x","from_type":"spec","to":"s-2uov","to_type":"spec","type":"depends-on"},{"from":"s-6d5x","from_type":"spec","to":"s-3nub","to_type":"spec","type":"implements"}],"tags":["mcts","mind-evolution","phase-5","pillar-2","search"]}
{"id":"s-7jda","uuid":"49803afb-f589-4d66-94ea-aeb7367a3801","title":"Phase 6: Learning Pipeline","file_path":"specs/s-7jda_phase_6_learning_pipeline.md","content":"# Phase 6: Learning Pipeline\n\nParent: [[s-5o87|ATLAS System Architecture]]\nImplements: [[s-w56w|Learning Engine (Pillar 3)]]\n\n## Overview\n\nExtract knowledge from trajectories to improve memory. Uses SAGE-style memory-only improvement (no fine-tuning for now).\n\n## Implementation Decisions\n\n### Credit Assignment\nImplement all three methods with a strategy pattern:\n1. **Simple**: Last successful action gets credit\n2. **LLM-based**: Ask LLM to identify key steps\n3. **Counterfactual**: Would removing step change outcome?\n\nDefault: LLM-based (most accurate)\n\n### Pattern Extraction\nUse LLM-based pattern extraction + text-based heuristics (no Stitch dependency):\n- LLM extracts conceptual patterns from code\n- Text heuristics find syntactic patterns (AST-based)\n- Both methods configurable\n\n### Fine-tuning Approach\nSAGE-only for now:\n- Store plans as retrievable documents\n- In-context learning with retrieved plans\n- No weight updates, only memory growth\n- `HindsightLearner` prepares data but doesn't execute fine-tuning\n\n### Error Pattern Structure\n```python\nclass ErrorPattern(BaseModel):\n    name: str           # e.g., \"null_pointer_dereference\"\n    signature: str      # Pattern signature/regex\n    frequency: int      # How often seen\n    suggested_fix: str  # Recommended resolution\n    examples: list[str] # Example occurrences\n```\n\n### Abstractability Assessment\nLLM/agent assessment determines if trajectory is worth extracting:\n- Evaluates novelty, generalizability, complexity\n- Returns boolean with reasoning\n\n### Batch Learning Triggers\n- `min_trajectories`: Default 50 (required)\n- `min_hours_since_last`: Optional time-based trigger (default: None)\n- `min_success_rate`: Optional quality threshold (default: None)\n\n## Components\n\n### TrajectoryAnalyzer\nExtract learning signals from trajectories.\n\n**Credit Assignment Strategies:**\n```python\nclass CreditStrategy(Protocol):\n    def attribute(self, trajectory: Trajectory) -> list[tuple[int, float]]:\n        \"\"\"Return (step_index, contribution_score) pairs.\"\"\"\n        ...\n\nclass SimpleCreditStrategy:\n    \"\"\"Last successful action gets credit.\"\"\"\n    \nclass LLMCreditStrategy:\n    \"\"\"Ask LLM to identify key steps.\"\"\"\n    \nclass CounterfactualCreditStrategy:\n    \"\"\"Would removing step change outcome?\"\"\"\n```\n\n**Key Methods:**\n- `analyze(trajectory) → AnalysisResult`\n- `attribute_outcome(trajectory) → list[tuple[int, float]]`\n- `extract_error_patterns(trajectories) → list[ErrorPattern]`\n\n### AbstractionExtractor\nExtract reusable patterns from trajectories.\n\n**Pattern Extraction Methods:**\n```python\nclass PatternExtractor(Protocol):\n    def extract(self, trajectories: list[Trajectory]) -> list[CodeConcept]:\n        ...\n\nclass LLMPatternExtractor:\n    \"\"\"LLM identifies conceptual patterns.\"\"\"\n    \nclass TextPatternExtractor:\n    \"\"\"AST-based syntactic pattern matching.\"\"\"\n```\n\n**Key Methods:**\n- `extract_code_patterns(trajectories) → list[CodeConcept]`\n- `extract_strategies(trajectories) → list[Strategy]`\n- `auto_document(concept) → CodeConcept`\n- `is_abstractable(trajectory) → bool` (LLM assessment)\n\n### HindsightLearner\nSAGE-style memory improvement (no fine-tuning).\n\n**Key Methods:**\n- `prepare_training_data(trajectories) → dict` (for future fine-tuning)\n- `should_finetune() → bool` (always False for SAGE)\n- `accumulate(trajectory)` (add to memory for in-context learning)\n\n**SOAR Training Data Format (prepared but not used):**\n```python\n{\n    \"sampling\": [{\"input\": ..., \"output\": ..., \"weight\": 2.0}],\n    \"refinement\": [{\"input\": ..., \"output\": ..., \"weight\": 1.5}],\n    \"error\": [{\"input\": ..., \"output\": ..., \"weight\": 1.0}],\n}\n```\n\n### LearningPipeline\nOrchestrates the full learning process.\n\n**Key Methods:**\n- `process_trajectory(trajectory) → ProcessResult`\n- `run_batch_learning(min_trajectories=50) → BatchResult`\n\n**Configuration:**\n```python\nclass LearningConfig(BaseModel):\n    credit_strategy: str = \"llm\"  # \"simple\", \"llm\", \"counterfactual\"\n    pattern_extractor: str = \"llm\"  # \"llm\", \"text\", \"both\"\n    min_trajectories: int = 50\n    min_hours_since_last: float | None = None  # Optional time trigger\n    min_success_rate: float | None = None  # Optional quality trigger\n```\n\n**Pipeline Flow:**\n```\nTrajectory → Store in Memory\n          → Analyze (credit assignment)\n          → Check abstractability (LLM assessment)\n          → Extract Strategy (if abstractable)\n          → Accumulate for batch\n\nBatch (when triggers met) → Extract Code Patterns\n                         → Add to ConceptLibrary\n                         → Prune low-value experiences\n```\n\n## Dependencies\n\n- Phase 3: Memory system for storage\n- Phase 4/5: Search for generating trajectories\n- SimpleLLM: For LLM-based analysis and extraction\n\n## File Structure\n\n```\natlas/learning/\n├── __init__.py\n├── analyzer.py        # TrajectoryAnalyzer + CreditStrategies\n├── extractor.py       # AbstractionExtractor + PatternExtractors\n├── hindsight.py       # HindsightLearner (SAGE-style)\n└── pipeline.py        # LearningPipeline + LearningConfig\n```\n\n## Success Criteria\n\n- [ ] Analyzer correctly identifies key steps (all 3 credit strategies)\n- [ ] Extractor finds patterns (LLM + text-based)\n- [ ] Strategies generalize across similar tasks\n- [ ] Pipeline processes trajectories end-to-end\n- [ ] Memory improves with accumulated experience\n- [ ] Batch triggers work correctly (count, time, quality)","priority":2,"archived":0,"archived_at":null,"created_at":"2026-01-06 02:20:04","updated_at":"2026-01-08 07:47:22","parent_id":"s-5o87","parent_uuid":"315749e5-c7a0-41c9-8fd2-8124b1d9c2f7","relationships":[{"from":"s-7jda","from_type":"spec","to":"s-7h2g","to_type":"spec","type":"depends-on"},{"from":"s-7jda","from_type":"spec","to":"s-w56w","to_type":"spec","type":"implements"}],"tags":["abstraction","analysis","learning","phase-6","pillar-3"]}
{"id":"s-9ma3","uuid":"6a5ea1cd-349a-41e5-9be3-c6cad37cc11a","title":"Environment Protocol Design","file_path":"specs/s-9ma3_environment_protocol_design.md","content":"# Environment Protocol Design\n\nParent: [[s-2uov|Phase 4: Minimal Solver]]\n\n## Overview\n\nGeneric interface for task execution environments. Supports different domains (ARC, SWE, custom) through a Gymnasium-like API with built-in verification.\n\n## Design Principles\n\n1. **Minimal Core**: Only `reset()` and `verify()` are required for Phase 4\n2. **Progressive Enhancement**: Full step-by-step interface optional\n3. **Domain Agnostic**: Same interface works for ARC grids, SWE code, etc.\n4. **Verification Built-in**: Enables inference-time scaling via best-of-k\n\n## Protocol Tiers\n\n### Tier 1: Minimal Environment (Phase 4)\n\nRequired for DirectSolver integration:\n\n```python\n@runtime_checkable\nclass MinimalEnvironment(Protocol):\n    \"\"\"Minimum environment for verification-based solving.\"\"\"\n    \n    def reset(self, task: Task) -> str:\n        \"\"\"Initialize environment with task, return initial observation.\"\"\"\n        ...\n    \n    def verify(self, solution: Any) -> Outcome:\n        \"\"\"Verify a candidate solution against task criteria.\"\"\"\n        ...\n    \n    @property\n    def task(self) -> Task:\n        \"\"\"Current task being solved.\"\"\"\n        ...\n```\n\n### Tier 2: Interactive Environment (Future Phases)\n\nFor step-by-step search algorithms (MCTS, Mind Evolution):\n\n```python\n@runtime_checkable  \nclass InteractiveEnvironment(MinimalEnvironment, Protocol):\n    \"\"\"Full interactive environment for search algorithms.\"\"\"\n    \n    def step(self, action: str) -> tuple[str, float, bool, dict[str, Any]]:\n        \"\"\"Execute action, return (observation, reward, done, info).\"\"\"\n        ...\n    \n    @property\n    def max_steps(self) -> int:\n        \"\"\"Maximum steps before timeout.\"\"\"\n        ...\n    \n    @property\n    def is_deterministic(self) -> bool:\n        \"\"\"Whether environment is reproducible.\"\"\"\n        ...\n    \n    def get_state(self) -> dict[str, Any]:\n        \"\"\"Serialize current state for checkpointing.\"\"\"\n        ...\n    \n    def set_state(self, state: dict[str, Any]) -> None:\n        \"\"\"Restore from serialized state.\"\"\"\n        ...\n```\n\n## Domain Implementations\n\n### PassthroughEnvironment (Default)\n\nFor tasks without explicit verification (agent decides success):\n\n```python\nclass PassthroughEnvironment(MinimalEnvironment):\n    def reset(self, task: Task) -> str:\n        return task.description\n    \n    def verify(self, solution: Any) -> Outcome:\n        # Always succeed - verification delegated to agent\n        return Outcome(success=True, partial_score=1.0)\n```\n\n### ARCEnvironment (Stub)\n\n```python\nclass ARCEnvironment(MinimalEnvironment):\n    def verify(self, solution: Any) -> Outcome:\n        # Compare solution grid to expected output\n        # Return Outcome with exact_match success and partial_score\n        raise NotImplementedError(\"ARC environment not yet implemented\")\n```\n\n### SWEEnvironment (Stub)\n\n```python\nclass SWEEnvironment(MinimalEnvironment):\n    def verify(self, solution: Any) -> Outcome:\n        # Execute test suite\n        raise NotImplementedError(\"SWE environment not yet implemented\")\n```\n\n## Factory Pattern\n\n```python\ndef create_environment(task: Task) -> MinimalEnvironment:\n    \"\"\"Create appropriate environment for task domain.\"\"\"\n    if task.domain == \"arc\":\n        return ARCEnvironment()\n    elif task.domain == \"swe\":\n        return SWEEnvironment()\n    else:\n        return PassthroughEnvironment()\n```\n\n## Integration with DirectSolver\n\nDirectSolver uses environment for verification only:\n\n```python\nclass DirectSolver:\n    def solve(self, task: Task, env: MinimalEnvironment) -> Candidate:\n        # 1. Query memory for similar experiences\n        # 2. For each experience:\n        #    a. Adapt solution using TaskExecutor\n        #    b. outcome = env.verify(adapted_solution)\n        #    c. If success, return\n        # 3. Return best partial result\n```\n\n## File Structure\n\n```\natlas/environments/\n├── __init__.py\n├── base.py           # Base classes and PassthroughEnvironment\n├── arc.py            # ARCEnvironment (stub)\n└── swe.py            # SWEEnvironment (stub)\n```\n\n## Phase 4 Scope\n\n- [ ] Update Environment protocol with MinimalEnvironment tier\n- [ ] Implement PassthroughEnvironment (default)\n- [ ] Create stub implementations for ARC/SWE\n- [ ] Add create_environment factory\n- [ ] Export from atlas.environments","priority":1,"archived":0,"archived_at":null,"created_at":"2026-01-08 02:10:45","updated_at":"2026-01-08 02:10:45","parent_id":"s-2uov","parent_uuid":"41a04d9a-72a2-4d8e-b250-24827e8c3f7e","relationships":[],"tags":["design","environment","phase-4","protocol"]}
