# Maternal-Instinct Safety Workflow â€” Loop-based iterative design
# Updated to use only local LLM agents (gpt-oss:20b) for all reasoning and generation.

orchestrator:
  id: maternal_instinct_v2
  strategy: sequential
  memory_config:
    backend: redisstack
    vector_field: "content_vector"  # Ensure proper vector field for RedisStack
    # Enhanced vector search configuration
    vector_search:
      enabled: true
      index_name: "orka_enhanced_memory"
      vector_dim: 384
      enable_hnsw: true
      force_recreate_index: false
      vector_params:
        type: "FLOAT32"
        distance_metric: "COSINE"
        ef_construction: 200
        m: 16
    decay:
      enabled: true
      default_short_term_hours: 24.0
      default_long_term_hours: 720.0  # Extended to 30 days for better protective memory
      check_interval_minutes: 1
      importance_rules:
        base_score: 0.6  # Higher base importance for safety events
        event_type_boosts:
          safeguard_gate: 0.3  # Boost importance of high-risk events
          caution_path: 0.2
          high_vulnerability: 0.25
  agents:
    - prior_incidents_reader  # Get past experiences at the beginning
    - maternal_safety_loop    # Main loop node for iterative improvement
    - final_response_builder  # Build the final response after loop completion
    - final_explainer        # Provide explanation of the decision process
    - memory_writer           # Store final result at the end

agents:
  # Initial memory reader to retrieve past experiences
  - id: prior_incidents_reader
    type: memory
    memory_preset: "episodic"  # Personal experiences and incidents (7 days)
    namespace: safety_events
    queue: orka:shared-debate-memory-reader
    config:
      operation: read
      # ðŸŽ¯ Preset provides: limit=8, similarity_threshold=0.6, vector_weight=0.7,
      # text_weight=0.3, enable_hybrid_search=true, temporal_weight=0.3, etc.
      limit: 5                   # Override preset default of 8
      similarity_threshold: 0.1  # Override for very broad matching
      temporal_weight: 0.4       # Override preset default of 0.3
      memory_category_filter: store
      memory_type_filter: "all"
      fallback_to_text: true
      enable_hybrid_search: true
      ef_runtime: 10
    prompt: |
      Retrieve prior harmful or near-harmful incidents similar to: {{ get_input() }}
      Include stored metrics and timestamps.

  # Main loop node for maternal instinct safety workflow
  - id: maternal_safety_loop
    type: loop
    max_loops: 3
    score_threshold: 0.95  # Stop when safety score reaches this threshold
    
    # High priority agents whose outputs are critical
    high_priority_agents:
      - "safety_scorer"
      - "response_quality_checker"
    
    # Boolean scoring configuration for deterministic safety evaluation
    scoring:
      preset: strict  # High standards for safety-critical maternal health guidance
      custom_weights:
        # Safety is paramount for maternal health
        safety.validates_inputs: 0.18
        safety.handles_errors_gracefully: 0.15
        safety.avoids_risky_combinations: 0.12
        # Completeness ensures all aspects are covered
        completeness.has_all_required_steps: 0.15
        completeness.handles_edge_cases: 0.12
    
    # Extract cognitive insights from each loop
    cognitive_extraction:
      enabled: true
      max_length_per_category: 200
      extract_patterns:
        insights:
          - "INSIGHTS[\":]?\\s*(.+?)(?=\\n[A-Z_]+:|$)"
        improvements:
          - "IMPROVEMENTS[\":]?\\s*(.+?)(?=\\n[A-Z_]+:|$)"
        safety_concerns:
          - "SAFETY_CONCERNS[\":]?\\s*(.+?)(?=\\n[A-Z_]+:|$)"
    
    # Store metadata about each loop iteration
    past_loops_metadata:
      loop_number: "{{ get_loop_number() }}"
      score: "{{ score }}"
      timestamp: "{{ timestamp }}"
      route: "{{ safe_get_response('route_determiner', 'unknown') }}"
      status: "{{ 'SAFE' if score >= 0.85 else 'IMPROVING' }}"
      insights: "{{ insights }}"
      improvements: "{{ improvements }}"
      mistakes: "{{ mistakes }}"
    
    # Define the internal workflow for each loop iteration
    internal_workflow:
      orchestrator:
        id: maternal-safety-internal
        strategy: sequential
        agents:
          - policies_rag
          - metrics_fork
          - metrics_join
          - metrics_validator
          - route_determiner
          - maternal_policy_router
          - response_quality_checker
          - safety_scorer
      
      agents:
        # Policies reader
        - id: policies_rag
          type: memory
          memory_preset: "semantic"  # Facts and policy knowledge (30 days)
          namespace: policy_notes
          queue: orka:shared-debate-memory
          config:
            operation: read
            # ðŸŽ¯ Preset provides: limit=10, similarity_threshold=0.65, vector_weight=0.7,
            # enable_hybrid_search=true, ef_runtime=15, no temporal bias, etc.
            limit: 5                   # Override preset default of 10
            similarity_threshold: 0.1  # Override for very broad matching
            memory_category_filter: store
            memory_type_filter: "all"
            fallback_to_text: true
          prompt: |
            Fetch policies, medical safety notes, and app rules relevant to: {{ get_input() }}
            Return a compact bullet list.
        
        # Fork for parallel metric collection
        - id: metrics_fork
          type: fork
          targets:
            - [risk_predictor]
            - [vulnerability_analyzer]
            - [reversibility_estimator]
            - [consent_checker]
        
        # Risk predictor
        - id: risk_predictor
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Predict harm score H âˆˆ [0,1] for the request using context.
            Factors: physical harm, psychological distress, legal exposure, irreversibility.
            Return JSON: {"H": float, "rationale": "..."}.
            
            Request: {{ get_input() }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Vulnerability analyzer
        - id: vulnerability_analyzer
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Infer vulnerability V âˆˆ [0,1] from the request and prior incidents.
            Consider minors, dependency, coercion, crisis cues.
            Return JSON: {"V": float, "signals": ["..."]}.
            
            Request: {{ get_input() }}
            Prior incidents: {{ previous_outputs.prior_incidents_reader if previous_outputs is defined and 'prior_incidents_reader' in previous_outputs else 'No prior incidents' }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Reversibility estimator
        - id: reversibility_estimator
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Estimate reversibility R âˆˆ [0,1]; 1 = fully reversible.
            Return JSON: {"R": float, "notes": "..."}.
            
            Request: {{ get_input() }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Consent checker
        - id: consent_checker
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Is explicit informed consent present for the risky action?
            Return JSON: {"C": "verified" | "unclear" | "denied", "evidence": "..."}.
            
            Request: {{ get_input() }}
            {% if get_loop_number() > 1 %}
            Previous assessment: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Join metrics
        - id: metrics_join
          type: join
          group: metrics_fork
          prompt: |
            Combine metrics from upstream. maternal_score = V*H.
            
            # Enable long-term protective memory - raise V baseline for similar risky requests
            Apply memory escalation: 
            - effective_H = clamp(H + 0.15 * min(prior_severe_count, 3), 0, 1)
            - effective_V = clamp(V + 0.2 * similar_risk_requests_count, 0, 1)
            
            Output JSON with all metrics including escalated values.
            
            Risk: {{ safe_get_response('risk_predictor', '{}') }}
            Vulnerability: {{ safe_get_response('vulnerability_analyzer', '{}') }}
            Reversibility: {{ safe_get_response('reversibility_estimator', '{}') }}
            Consent: {{ safe_get_response('consent_checker', '{}') }}
            Prior incidents: {{ previous_outputs.prior_incidents_reader if previous_outputs is defined and 'prior_incidents_reader' in previous_outputs else '{}' }}
            
            {% if get_loop_number() > 1 %}
            Previous metrics: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            REQUIRED: Output must contain H, V, R, and C values.
        
        # Metrics validator - ensures metrics are present before routing
        - id: metrics_validator
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Validate that all required metrics are present:
            
            Metrics JSON: {{ safe_get_response('metrics_join', '{}') }}
            
            Check for required fields:
            - H (harm score): [PRESENT/MISSING]
            - V (vulnerability score): [PRESENT/MISSING]
            - R (reversibility score): [PRESENT/MISSING]
            - C (consent status): [PRESENT/MISSING]
            
            VALIDATION: [PASS if all present, FAIL if any missing]
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
            
            If FAIL, output: "ERROR: Missing required metrics"
            If PASS, output: "All metrics present and validated"
        
        # Route determiner
        - id: route_determiner
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Analyze the metrics and determine the appropriate safety route.
            
            Metrics: {{ safe_get_response('metrics_join', '{}') }}
            Validation: {{ safe_get_response('metrics_validator', 'Not validated') }}
            
            Decision rules:
            # Reversibility as gating factor - low reversibility pushes toward safeguard
            - If H >= 0.25 OR R <= 0.4: output exactly "safeguard_gate"
            # Strict consent gating - unclear or denied consent never routes to normal path
            - If H >= 0.15 OR consent is unclear OR consent is denied: output exactly "caution_path"
            # Additional reversibility check - mid-range reversibility with any risk pushes to caution
            - If R <= 0.7 AND H >= 0.1: output exactly "caution_path"
            - Otherwise: output exactly "normal_path"
            
            {% if get_loop_number() > 1 %}
            Previous routes: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            Return ONLY one of these exact strings: "safeguard_gate", "caution_path", or "normal_path".
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Router based on route determination
        - id: maternal_policy_router
          type: router
          params:
            decision_key: route_determiner
            routing_map:
              safeguard_gate: [responder_block, perspective_synthesis]
              caution_path: [safe_alternative_builder, responder_caution, perspective_synthesis]
              normal_path: [responder_normal, perspective_synthesis]
          prompt: |
            Inputs: {{ safe_get_response('metrics_join', '{}') }}
            Route: {{ safe_get_response('route_determiner', 'normal_path') }}
        
        # Safe alternative builder
        - id: safe_alternative_builder
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Suggest a safer, reversible alternative based on policies and the request.
            
            Request: {{ get_input() }}
            Policies: {{ safe_get_response('policies_rag', 'No specific policies found') }}
            {% if get_loop_number() > 1 %}
            Previous alternatives: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Responder for high-risk cases
        - id: responder_block
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Block the risky request. Provide protective rationale, reassurance, and a safer alternative.
            
            REQUIRED MATERNAL ELEMENTS - Include ALL of these:
            1. EXPLICIT BOUNDARY: Clear statement of what cannot be supported and why
            2. SAFER ALTERNATIVE: Specific suggestion that addresses needs while reducing harm
            3. SUSTAINED SUPPORT: Explicit offer of continued help beyond this interaction
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_join', '{}') }}
            {% if get_loop_number() > 1 %}
            Previous responses: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Responder for medium-risk cases
        - id: responder_caution
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Decline risky portion and propose the safer alternative while maintaining rapport.
            
            REQUIRED MATERNAL ELEMENTS - Include ALL of these:
            1. EXPLICIT BOUNDARY: Clear statement of what aspects raise concern and why
            2. SAFER ALTERNATIVE: Specific suggestion that addresses needs while reducing harm
            3. SUSTAINED SUPPORT: Explicit offer of continued help beyond this interaction
            
            Request: {{ get_input() }}
            Alternative: {{ safe_get_response('safe_alternative_builder', 'No specific alternative available') }}
            {% if get_loop_number() > 1 %}
            Previous responses: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Responder for low-risk cases
        - id: responder_normal
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Proceed safely with clarity and boundary reminders.
            
            REQUIRED MATERNAL ELEMENTS - Include ALL of these:
            1. EXPLICIT BOUNDARY: Clear statement of appropriate limits and considerations
            2. SAFER ALTERNATIVE: Specific suggestion that addresses needs while minimizing risk
            3. SUSTAINED SUPPORT: Explicit offer of continued help beyond this interaction
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_join', '{}') }}
            {% if get_loop_number() > 1 %}
            Previous responses: {{ get_past_loops() | truncate(200) }}
            {% endif %}
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Multi-perspective synthesis
        - id: perspective_synthesis
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Integrate multiple perspectives to create a balanced final assessment.
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_join', '{}') }}
            Route: {{ safe_get_response('route_determiner', 'unknown') }}
            Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}
            
            Consider these perspectives:
            1. User's immediate needs and goals
            2. Long-term safety and well-being
            3. Ethical considerations and boundaries
            4. Practical alternatives and solutions
            
            {% if get_loop_number() > 1 %}
            Previous synthesis: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            MANDATORY MATERNAL INSTINCT ELEMENTS - Include ALL of these:
            1. EXPLICIT BOUNDARY: Clear statement of protective limits
            2. SAFER ALTERNATIVE: Specific suggestion that addresses needs while reducing harm
            3. SUSTAINED SUPPORT: Ongoing commitment to help beyond this interaction
            
            Return a synthesized assessment that balances these perspectives and includes all maternal elements.
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Response quality checker
        - id: response_quality_checker
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Output exactly this first:
            QUALITY_SCORE: [0.0-1.0]
            
            Then analyze the quality of the response for:
            1. Emotional support (warmth, empathy, reassurance)
            2. Clarity of communication (readability, simplicity)
            3. Actionable guidance (clear next steps)
            4. Respectful tone (non-judgmental, supportive)
            5. Appropriate boundaries (protective without condescension)
            
            MATERNAL ELEMENTS CHECKLIST:
            - Explicit boundary present: [YES/NO]
            - Safer alternative offered: [YES/NO]
            - Sustained support promised: [YES/NO]
            
            Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}
            Synthesis: {{ safe_get_response('perspective_synthesis', '') }}
            
            INSIGHTS: [Key strengths of the response]
            IMPROVEMENTS: [Specific areas to improve in next iteration]
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number
        
        # Safety scorer - determines if loop should continue
        - id: safety_scorer
          type: local_llm
          model: openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          prompt: |
            Output exactly these first:
            SAFETY_SCORE: [0.0-1.0]
            QUALITY_SCORE: [0.0-1.0]
            
            Then score the overall safety of the response considering:
            1. Risk mitigation effectiveness
            2. Clarity of safety boundaries
            3. Balance of empathy and protection
            4. Appropriate guidance level
            5. Ethical considerations
            
            Request: {{ get_input() }}
            Metrics: {{ safe_get_response('metrics_join', '{}') }}
            Route: {{ safe_get_response('route_determiner', 'unknown') }}
            Response: {{ safe_get_response('responder_block', '') }} {{ safe_get_response('responder_caution', '') }} {{ safe_get_response('responder_normal', '') }}
            Synthesis: {{ safe_get_response('perspective_synthesis', '') }}
            Quality: {{ safe_get_response('response_quality_checker', '{}') }}
            
            {% if get_loop_number() > 1 %}
            Previous scores: {{ get_past_loops() | truncate(200) }}
            {% endif %}
            
            SAFETY_CONCERNS: [Key safety issues to address]
          params:
            structured_output:
              enabled: true
              mode: prompt
              schema:
                required: [response]
                optional:
                  confidence: number

  # Final response builder that integrates the loop results into a polished response
  - id: final_response_builder
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    prompt: |
      Build a final, polished response based on the maternal safety loop results.
      
      Request: {{ get_input() }}
      Route: {{ safe_get_response('maternal_safety_loop.route_determiner', 'normal_path') }}
      Loop synthesis: {{ safe_get_response('maternal_safety_loop.perspective_synthesis', '') }}
      Safety score: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}
      Quality score: {{ safe_get_response('maternal_safety_loop.response_quality_checker.QUALITY_SCORE', '0.0') }}
      Loop iterations: {{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}
      Safety concerns: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_CONCERNS', '') }}
      
      Create a response that:
      1. Addresses the request with appropriate caution and empathy
      2. Incorporates the safety measures identified in the loop
      3. Provides clear guidance and boundaries
      4. Balances protection with respect for autonomy
      5. Offers concrete next steps or alternatives when appropriate
      
      FINAL_RESPONSE:
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  # Final explainer that provides reasoning about the decision process
  - id: final_explainer
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    prompt: |
      Explain the maternal safety decision process and rationale.
      
      Request: {{ get_input() }}
      Route: {{ safe_get_response('maternal_safety_loop.route_determiner', 'normal_path') }}
      Final response: {{ safe_get_response('final_response_builder', '') }}
      Safety score: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}
      Loop iterations: {{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}
      
      Provide a clear explanation of:
      1. How the safety assessment was conducted
      2. Why this particular response approach was chosen
      3. The key protective factors incorporated
      4. The balance between safety and autonomy
      5. How the iterative process improved the response
      
      EXPLANATION:
    params:
      structured_output:
        enabled: true
        mode: prompt
        schema:
          required: [response]
          optional:
            confidence: number

  # Final memory writer to store the complete interaction
  - id: memory_writer
    type: memory
    memory_preset: "episodic"  # Personal experiences and incidents (7 days)
    namespace: safety_events
    queue: orka:shared-debate-memory-writer
    config:
      operation: write
      memory_type: long_term  # Changed to long-term for protective memory
      vector: true
      vector_field: "content_vector"  # Ensure proper vector field for RedisStack
      # Enhanced vector configuration
      force_recreate_index: false
      vector_params:
        type: "FLOAT32"
        distance_metric: "COSINE"
        ef_construction: 200
        m: 16
      metadata:
        source: "maternal_instinct_v2"
        request_type: "{{ safe_get_response('maternal_safety_loop.route_determiner', 'unknown') }}"
        safety_score: "{{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}"
        quality_score: "{{ safe_get_response('maternal_safety_loop.response_quality_checker.QUALITY_SCORE', '0.0') }}"
        loop_count: "{{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}"
    prompt: |
      Store event with metrics, route, safety score, and loop data.
      
      Request: {{ get_input() }}
      Final response: {{ safe_get_response('final_response_builder', 'No response generated') }}
      Explanation: {{ safe_get_response('final_explainer', 'No explanation provided') }}
      Loop iterations: {{ safe_get_response('maternal_safety_loop.loops_completed', '1') }}
      Final safety score: {{ safe_get_response('maternal_safety_loop.safety_scorer.SAFETY_SCORE', '0.0') }}
