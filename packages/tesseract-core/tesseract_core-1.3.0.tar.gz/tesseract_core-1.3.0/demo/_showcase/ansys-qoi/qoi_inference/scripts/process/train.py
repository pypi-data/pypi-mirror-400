from pathlib import Path
from typing import Any

from torch.utils.data import DataLoader, Dataset

from .dataset import cad_collate
from .experiment_tracker import ExperimentTracker
from .models import HybridPointCloudTreeModel
from .scaler import DataScaler
from .utils import set_seed


def train_hybrid_models(
    train_dataset: Dataset,
    val_dataset: Dataset,
    test_dataset: Dataset,
    model_configs: dict[str, dict[str, Any]],
    training_config: dict[str, Any],
    save_dir: Path,
    config_path: Path | None = None,
    split_info: dict[str, Any] | None = None,
    scaler: DataScaler | None = None,
) -> dict[str, Any] | None:
    """Train hybrid PointNeXt + Tree models on CAD data.

    Args:
        train_dataset: Training dataset
        val_dataset: Validation dataset
        test_dataset: Test dataset
        model_configs: Dictionary mapping model names to their configurations
        training_config: Training hyperparameters (epochs, lr, batch_size, etc.)
        save_dir: Directory to save models and experiment results
        config_path: Path to configuration file (optional)
        split_info: Information about data split (optional)
        scaler: Data scaler for preprocessing (optional)

    Returns:
        Dictionary containing experiment results and metrics
    """
    # Set random seed for reproducibility
    if split_info and "seed" in split_info:
        seed = split_info["seed"]
        set_seed(seed)
        print(f"ðŸŽ² Random seed set to: {seed}")

    if model_configs is None:
        print("No hybrid model configurations found.")
        return

    if save_dir:
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)

    # Initialize experiment tracker
    tracker = ExperimentTracker(
        base_dir=save_dir, experiment_type="hybrid", config_path=config_path
    )

    # Log dataset split information
    if split_info:
        tracker.log_dataset_info(split_info)

    # Save scaler to experiment directory for reproducibility
    scaler_path = None
    if scaler is not None:
        scaler_path = tracker.run_dir / "scaler.pkl"
        scaler.save(scaler_path)
        print(f"  âœ… Saved scaler to: {scaler_path.relative_to(save_dir)}")

    # Create PyTorch datasets and loaders (need full data for point clouds)

    batch_size = training_config.get("batch_size", 32)
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=cad_collate
    )
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=cad_collate
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, collate_fn=cad_collate
    )

    results = {}

    for name, config in model_configs.items():
        # Reset seed for each model to ensure reproducibility
        if split_info and "seed" in split_info:
            set_seed(split_info["seed"])
            print(f"ðŸŽ² Reset random seed to: {split_info['seed']} for {name}")

        print(f"\n{'=' * 60}")
        print(f"Training Hybrid Model: {name}")
        print(f"{'=' * 60}")

        # Create model
        model_config = config.copy()
        model_type = model_config.pop("type")

        if model_type == "hybrid_pc_tree":
            model = HybridPointCloudTreeModel(name=name, **model_config)
            # Attach scaler to model for reproducibility
            model.scaler = scaler
        else:
            print(f"Unknown hybrid model type: {model_type}")
            continue

        # Train model
        model.fit(
            train_data=train_loader, val_data=val_loader, training_args=training_config
        )

        # Evaluate on all splits
        print("\nEvaluating model...")
        train_metrics = model.evaluate(train_loader)
        val_metrics = model.evaluate(val_loader)
        test_metrics = model.evaluate(test_loader)

        # Log metrics to tracker
        tracker.log_model_metrics(name, train_metrics.__dict__, split="train")
        tracker.log_model_metrics(name, val_metrics.__dict__, split="val")
        tracker.log_model_metrics(name, test_metrics.__dict__, split="test")

        # Print results
        print(f"\n{name} Results:")
        print(f"  Train - MAE: {train_metrics.mae:.6f}, RÂ²: {train_metrics.r2:.6f}")
        print(f"  Val   - MAE: {val_metrics.mae:.6f}, RÂ²: {val_metrics.r2:.6f}")
        print(f"  Test  - MAE: {test_metrics.mae:.6f}, RÂ²: {test_metrics.r2:.6f}")

        # Save model to experiment directory
        model_path = tracker.get_model_path(name, extension=".pkl")
        model.save(model_path)
        print(f"  Model saved to: {model_path}")

        # Store results with metrics and model path
        results[name] = {
            "model_path": model_path,
            "scaler_path": scaler_path,
            "train_metrics": train_metrics.__dict__,
            "val_metrics": val_metrics.__dict__,
            "test_metrics": test_metrics.__dict__,
        }

    # Finalize experiment (saves all metadata and creates README)
    tracker.finalize()

    return results
