"""
Main entry point for CodenRetriever.
"""
import argparse
import json
import logging
import subprocess
import sys
import time
import traceback
from pathlib import Path
import io

from .cache import CacheManager
from .config import get_central_cache_root, get_project_cache_dir
from .config import OutputFormat
from .config_loader import (
    get_config,
    load_config,
    save_config,
    get_config_file,
    reset_config,
    _config_to_dict,
)
from .daemon.client import (
    DaemonClient,
    get_daemon_status,
    stop_daemon,
    try_daemon_search,
    try_daemon_hotspots,
)
from .daemon.protocol import (
    WINDOWS_CREATE_NEW_PROCESS_GROUP,
    WINDOWS_DETACHED_PROCESS,
    GraphAnalysisParams,
    SearchParams,
)
from .daemon.server import get_log_file, is_daemon_running, run_daemon
from .formatters.terminal_style import get_terminal_style
from .pipeline import SearchConfig, SearchPipeline

logger = logging.getLogger(__name__)


def parse_duration(duration_str: str) -> int:
    """Parse a duration string (e.g., '30m', '1h', '90s') to seconds."""
    if not duration_str:
        return 0

    duration_str = duration_str.strip().lower()

    if duration_str.endswith('s'):
        return int(duration_str[:-1])
    elif duration_str.endswith('m'):
        return int(duration_str[:-1]) * 60
    elif duration_str.endswith('h'):
        return int(duration_str[:-1]) * 3600
    else:
        return int(duration_str)


def print_search_output(
    formatted_output: str,
    tree_output: str | None,
    stats_output: str | None,
    reverse: bool,
) -> None:
    """Print search output in correct order based on reverse flag.

    Args:
        formatted_output: The main formatted search results
        tree_output: Optional directory tree output
        stats_output: Optional ranking statistics (always printed to stderr)
        reverse: If True, show results first then tree then stats.
                 If False, show stats first then tree then results.
    """
    if reverse:
        # Reversed mode: results first, then tree, then stats (highest score last)
        print(formatted_output)

        if tree_output:
            print("\n" + "=" * 60 + "\n")
            print(tree_output)

        if stats_output:
            print(stats_output, file=sys.stderr)
    else:
        # Normal mode: stats first, then tree, then results
        if stats_output:
            print(stats_output, file=sys.stderr)

        if tree_output:
            print(tree_output)
            print("\n" + "=" * 60 + "\n")

        print(formatted_output)


def format_hotspots_output(
    hotspots: list[dict],
    output_format: str = "tree",
    reverse: bool = False,
) -> str:
    """Format hotspots result for CLI output.

    Args:
        hotspots: List of hotspot dicts from daemon
        output_format: Output format (tree, json, etc.)
        reverse: If True, show highest risk first (opposite of other modes)

    Returns:
        Formatted string for display
    """
    if output_format == "json":
        import json
        return json.dumps(hotspots, indent=2)

    if not hotspots:
        return "No refactoring hotspots found."

    # Get terminal style for coloring
    style = get_terminal_style()

    # Find max risk for color scaling
    max_risk = max(h.get("risk_score", 0) for h in hotspots) if hotspots else 1.0
    display_hotspots = hotspots if reverse else list(reversed(hotspots))

    lines = []

    # Table header
    header = f"{'Rank':<4} │ {'Risk':<7} │ {'Coupling':<13} │ {'CC':<4} │ {'Category':<12} │ {'Lines':<5} │ {'Entity'}"
    lines.append(header)
    lines.append("─" * 110)

    for i, h in enumerate(display_hotspots, 1):
        # Calculate display rank (accounts for reverse)
        rank = i if reverse else len(display_hotspots) - i + 1

        category = h.get("category", "Unknown")
        risk_score = h.get("risk_score", 0)
        name = h.get("name", "unknown")
        file_path = h.get("file", "")
        line = h.get("line", 0)
        fan_in = h.get("fan_in", 0)
        fan_out = h.get("fan_out", 0)
        complexity = h.get("complexity", 1)
        line_count = h.get("lines", 0)

        # Truncate long entity names
        if len(name) > 35:
            name = "..." + name[-32:]

        # Color the risk score based on its value relative to max
        tier = style.get_score_tier(risk_score, max_risk)
        tier_num = int(tier.split('_')[1])
        inverted_tier = f"tier_{11 - tier_num}"
        risk_str = f"{risk_score:>6.1f}"
        colored_risk = style.render_to_string(style.colorize(risk_str, inverted_tier))

        # Color the entity name
        colored_entity = style.format_stats_entity(
            name, file_path, line, max_risk - risk_score + 1, max_risk
        )

        # Coupling display
        coupling_str = f"{fan_in}in/{fan_out}out"

        lines.append(
            f"{rank:<4} │ {colored_risk} │ {coupling_str:<13} │ {complexity:<4} │ {category:<12} │ {line_count:<5} │ {colored_entity}"
        )

    lines.append("─" * 110)
    return "\n".join(lines)


def format_hotspots_stats(summary: dict) -> str:
    """Format hotspots summary statistics.

    Args:
        summary: Summary dict from daemon

    Returns:
        Formatted stats string for stderr
    """
    total = summary.get('total_functions_analyzed', 0)
    above_threshold = summary.get('functions_above_threshold', 0)

    # Category distribution
    category_dist = summary.get("category_distribution", {})
    danger = category_dist.get("Danger Zone", 0)
    traffic = category_dist.get("Traffic Jam", 0)
    local = category_dist.get("Local Mess", 0)
    low = category_dist.get("Low Risk", 0)

    lines = [
        "",
        "═" * 80,
        f"Hotspots Analysis │ {total:,} functions analyzed │ {above_threshold:,} above threshold",
        "─" * 80,
        f"Risk: avg {summary.get('average_risk_score', 0):.1f} / max {summary.get('max_risk_score', 0):.1f}",
        f"Coupling: avg {summary.get('average_coupling_score', 0):.1f} / max {summary.get('highest_coupling_score', 0)}",
        f"Complexity: avg {summary.get('average_complexity', 1):.1f} / max {summary.get('max_complexity', 1)}",
        "─" * 80,
        f"Categories: Danger Zone: {danger} │ Traffic Jam: {traffic} │ Local Mess: {local} │ Low Risk: {low}",
    ]

    if summary.get("token_budget_exceeded"):
        lines.append("─" * 80)
        lines.append("Note: Results truncated due to token budget")

    lines.append("═" * 80)
    return "\n".join(lines)


def print_hotspots_output(
    formatted_output: str,
    stats_output: str | None,
    reverse: bool,
) -> None:
    """Print hotspots output in correct order based on reverse flag.

    Args:
        formatted_output: The main formatted hotspots results
        stats_output: Optional ranking statistics (always printed to stderr)
        reverse: If True, show results first then stats.
                 If False, show stats first then results.
    """
    if reverse:
        print(formatted_output)
        if stats_output:
            print(stats_output, file=sys.stderr)
    else:
        if stats_output:
            print(stats_output, file=sys.stderr)
        print(formatted_output)


def _daemon_start(host: str, port: int, max_projects: int, idle_timeout: str | None, verbose: bool, no_watch: bool = False) -> int:
    """Start daemon in background."""
    running, pid = is_daemon_running()
    if running:
        print(f"Daemon is already running (PID: {pid})")
        return 0

    # On Windows, use pythonw.exe to avoid console window
    if sys.platform == "win32":
        python_exe = sys.executable.replace("python.exe", "pythonw.exe")
    else:
        python_exe = sys.executable

    cmd = [
        python_exe, "-m", "coden_retriever",
        "daemon", "run",
        "--daemon-host", host,
        "--daemon-port", str(port),
    ]

    if max_projects:
        cmd.extend(["--max-projects", str(max_projects)])
    if idle_timeout:
        cmd.extend(["--idle-timeout", str(idle_timeout)])
    if verbose:
        cmd.append("--verbose")
    if no_watch:
        cmd.append("--no-watch")

    # Start daemon process (platform-specific)
    if sys.platform == "win32":
        subprocess.Popen(
            cmd,
            creationflags=WINDOWS_DETACHED_PROCESS | WINDOWS_CREATE_NEW_PROCESS_GROUP,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            stdin=subprocess.DEVNULL,
        )
    else:
        subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            stdin=subprocess.DEVNULL,
            start_new_session=True,
        )

    # Wait and verify it started
    for _ in range(20):  # Wait up to 2 seconds
        time.sleep(0.1)
        status = get_daemon_status(host, port)
        if status:
            _, pid = is_daemon_running()
            print(f"Daemon started (PID: {pid})")
            print(f"  Address: {host}:{port}")
            print(f"  Log: {get_log_file()}")
            return 0

    print("Daemon failed to start. Check log:", file=sys.stderr)
    print(f"  {get_log_file()}", file=sys.stderr)
    return 1


def _daemon_stop(host: str, port: int) -> int:
    """Stop the daemon."""
    running, pid = is_daemon_running()
    if not running:
        print("Daemon is not running")
        return 0

    if stop_daemon(host, port):
        print(f"Daemon stopped (was PID: {pid})")
        print(f"  Address: {host}:{port}")
        return 0
    else:
        print(f"Failed to stop daemon (PID: {pid})", file=sys.stderr)
        return 1


def _daemon_status(host: str, port: int) -> int:
    """Show daemon status."""
    status = get_daemon_status(host, port)
    if status:
        print("Daemon is running")
        print(json.dumps(status, indent=2))
        return 0

    running, pid = is_daemon_running()
    if running:
        print(f"Daemon process exists (PID: {pid}) but not responding")
        return 1
    else:
        print("Daemon is not running")
        return 1


def _daemon_restart(host: str, port: int, max_projects: int, idle_timeout: str | None, verbose: bool, no_watch: bool = False) -> int:
    """Restart the daemon."""
    stop_daemon(host, port)
    time.sleep(0.5)
    return _daemon_start(host, port, max_projects, idle_timeout, verbose, no_watch)


def _daemon_run(host: str, port: int, max_projects: int, idle_timeout: str | None, verbose: bool, no_watch: bool = False) -> int:
    """Run daemon in foreground."""
    config = get_config()
    max_projects = max_projects or config.daemon.max_projects
    timeout_seconds = parse_duration(idle_timeout) if idle_timeout else None

    return run_daemon(
        host=host,
        port=port,
        max_projects=max_projects,
        idle_timeout=timeout_seconds,
        verbose=verbose,
        foreground=True,
        enable_watch=not no_watch,
    )


def _daemon_clear_cache(host: str, port: int, clear_path: str | None, clear_all: bool) -> int:
    """Clear daemon cache."""
    client = DaemonClient(host=host, port=port, timeout=5.0)
    try:
        result = client.invalidate(source_dir=clear_path, all=clear_all)
        print(f"Cache cleared: {result.get('invalidated', 'none')}")
        return 0
    except Exception as e:
        print(f"Failed to clear cache: {e}", file=sys.stderr)
        return 1


def handle_daemon_command(args: argparse.Namespace) -> int:
    """Handle daemon subcommands by dispatching to specific handlers."""
    config = get_config()
    host = getattr(args, 'daemon_host', config.daemon.host)
    port = getattr(args, 'daemon_port', config.daemon.port)
    verbose = getattr(args, 'verbose', False)
    no_watch = getattr(args, 'no_watch', False)

    action = args.daemon_action

    if action == "start":
        return _daemon_start(
            host, port,
            getattr(args, 'max_projects', config.daemon.max_projects),
            getattr(args, 'idle_timeout', None),
            verbose,
            no_watch
        )
    elif action == "stop":
        return _daemon_stop(host, port)
    elif action == "status":
        return _daemon_status(host, port)
    elif action == "restart":
        return _daemon_restart(
            host, port,
            getattr(args, 'max_projects', config.daemon.max_projects),
            getattr(args, 'idle_timeout', None),
            verbose,
            no_watch
        )
    elif action == "run":
        return _daemon_run(
            host, port,
            getattr(args, 'max_projects', config.daemon.max_projects),
            getattr(args, 'idle_timeout', None),
            verbose,
            no_watch
        )
    elif action == "clear-cache":
        return _daemon_clear_cache(
            host, port,
            getattr(args, 'clear_path', None),
            getattr(args, 'clear_all', False)
        )

    return 0


def handle_config_command(args: list[str]) -> int:
    """Handle config subcommands: show, path, reset, set."""
    if not args or args[0] == "show":
        config = load_config()
        print(json.dumps(_config_to_dict(config), indent=2))
        return 0

    elif args[0] == "path":
        print(get_config_file())
        return 0

    elif args[0] == "reset":
        if reset_config():
            print("Configuration reset to defaults")
            return 0
        else:
            print("Failed to reset configuration", file=sys.stderr)
            return 1

    elif args[0] == "set" and len(args) >= 3:
        # config set <key> <value>
        # Keys: model.default, agent.max_steps, daemon.port, etc.
        key_path = args[1]
        value = args[2]

        config = load_config()
        parts = key_path.split(".")

        if len(parts) != 2:
            print(f"Invalid key format: {key_path}. Use section.key (e.g., model.default)", file=sys.stderr)
            return 1

        section, key = parts

        try:
            if section == "model":
                if key == "default":
                    config.model.default = value
                elif key == "base_url":
                    config.model.base_url = value if value.lower() != "null" else None
                else:
                    print(f"Unknown key: {key_path}", file=sys.stderr)
                    return 1
            elif section == "agent":
                if key == "max_steps":
                    config.agent.max_steps = int(value)
                elif key == "max_retries":
                    config.agent.max_retries = int(value)
                elif key == "debug":
                    config.agent.debug = value.lower() in ("true", "1", "yes")
                else:
                    print(f"Unknown key: {key_path}", file=sys.stderr)
                    return 1
            elif section == "daemon":
                if key == "host":
                    config.daemon.host = value
                elif key == "port":
                    config.daemon.port = int(value)
                elif key == "socket_timeout":
                    config.daemon.socket_timeout = float(value)
                elif key == "max_projects":
                    config.daemon.max_projects = int(value)
                else:
                    print(f"Unknown key: {key_path}", file=sys.stderr)
                    return 1
            elif section == "search":
                if key == "default_tokens":
                    config.search.default_tokens = int(value)
                elif key == "default_limit":
                    config.search.default_limit = int(value)
                elif key == "semantic_model_path":
                    config.search.semantic_model_path = value if value.lower() != "null" else None
                else:
                    print(f"Unknown key: {key_path}", file=sys.stderr)
                    return 1
            else:
                print(f"Unknown section: {section}. Valid sections: model, agent, daemon, search", file=sys.stderr)
                return 1

            save_config(config)
            print(f"Set {key_path} = {value}")
            return 0

        except ValueError as e:
            print(f"Invalid value for {key_path}: {e}", file=sys.stderr)
            return 1

    else:
        print("Usage: coden config [show|path|reset|set <key> <value>]")
        print("\nCommands:")
        print("  show             Show current configuration")
        print("  path             Show config file path")
        print("  reset            Reset configuration to defaults")
        print("  set <key> <val>  Set a configuration value")
        print("\nKeys:")
        print("  model.default, model.base_url")
        print("  agent.max_steps, agent.max_retries, agent.debug")
        print("  daemon.host, daemon.port, daemon.socket_timeout, daemon.max_projects")
        print("  search.default_tokens, search.default_limit, search.semantic_model_path")
        return 1


def handle_cache_command(args: list[str]) -> int:
    """Handle cache subcommands: list, clear, status, path."""
    if not args or args[0] == "list":
        # List all cached projects
        caches = CacheManager.list_all_caches()
        if not caches:
            print("No cached projects found.")
            print(f"Cache directory: {get_central_cache_root()}")
            return 0

        print(f"Cached projects ({len(caches)}):")
        print(f"Cache directory: {get_central_cache_root()}\n")

        total_size = 0
        for cache in caches:
            total_size += cache["size_mb"]
            source = cache["source_dir"]
            # Truncate long paths
            if len(source) > 60:
                source = "..." + source[-57:]
            print(f"  {source}")
            print(f"    Entities: {cache['entity_count']:,} | Files: {cache['file_count']:,} | Size: {cache['size_mb']:.1f} MB")
            if cache.get("updated_at"):
                print(f"    Updated: {cache['updated_at']}")
            print()

        print(f"Total cache size: {total_size:.1f} MB")
        return 0

    elif args[0] == "clear":
        # Check for --all flag
        clear_all = "--all" in args or "-a" in args

        if clear_all:
            # Clear all caches
            count, errors = CacheManager.clear_all_caches()
            if count > 0:
                print(f"Cleared {count} project cache(s)")
            else:
                print("No caches to clear")
            for error in errors:
                print(f"  Warning: {error}", file=sys.stderr)
            return 0 if not errors else 1

        # Clear cache for specific path or current directory
        # Check if a path was provided (argument that's not a flag)
        path_arg = None
        for arg in args[1:]:
            if not arg.startswith("-"):
                path_arg = arg
                break

        target_path = Path(path_arg).resolve() if path_arg else Path.cwd()

        if not target_path.exists():
            print(f"Path does not exist: {target_path}", file=sys.stderr)
            return 1

        if not target_path.is_dir():
            print(f"Path is not a directory: {target_path}", file=sys.stderr)
            return 1

        cache_dir = get_project_cache_dir(target_path)
        if not cache_dir.exists():
            print(f"No cache found for: {target_path}")
            return 0

        if CacheManager.clear_cache_by_source_dir(target_path):
            print(f"Cache cleared for: {target_path}")
            return 0
        else:
            print(f"Failed to clear cache for: {target_path}", file=sys.stderr)
            return 1

    elif args[0] == "status":
        # Show cache status for specific path or current directory
        path_arg = args[1] if len(args) > 1 else None
        target_path = Path(path_arg).resolve() if path_arg else Path.cwd()

        if not target_path.exists():
            print(f"Path does not exist: {target_path}", file=sys.stderr)
            return 1

        if not target_path.is_dir():
            print(f"Path is not a directory: {target_path}", file=sys.stderr)
            return 1

        cache = CacheManager(target_path)
        status = cache.get_cache_status()
        print(json.dumps(status, indent=2))
        return 0

    elif args[0] == "path":
        # Show cache path for specific path or current directory
        path_arg = args[1] if len(args) > 1 else None
        target_path = Path(path_arg).resolve() if path_arg else Path.cwd()

        cache_dir = get_project_cache_dir(target_path)
        print(f"Project: {target_path}")
        print(f"Cache:   {cache_dir}")
        print(f"Exists:  {cache_dir.exists()}")
        return 0

    else:
        print("Usage: coden cache [list|clear|status|path]")
        print("\nCommands:")
        print("  list              List all cached projects")
        print("  clear             Clear cache for current directory")
        print("  clear <path>      Clear cache for specific project")
        print("  clear --all       Clear ALL cached projects")
        print("  status [path]     Show cache status for project")
        print("  path [path]       Show cache directory path for project")
        print(f"\nCache location: {get_central_cache_root()}")
        return 1


class DefaultValueHelpFormatter(argparse.RawDescriptionHelpFormatter):
    """Argparse help formatter that appends default values to each argument help."""

    def _get_help_string(self, action: argparse.Action) -> str:
        help_text = action.help
        if not help_text:
            base_help = super()._get_help_string(action)
            return base_help if base_help is not None else ""

        if (
            "%(default)" not in help_text
            and action.default is not argparse.SUPPRESS
        ):
            default_value = action.default
            default_str = '""' if default_value == "" else str(default_value)
            help_text = f"{help_text} (default: {default_str})"

        return help_text


def _create_common_daemon_parser() -> argparse.ArgumentParser:
    """Create parent parser with common daemon arguments."""
    config = get_config()
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument("--daemon-host", default=config.daemon.host, help="Daemon host address")
    parser.add_argument("--daemon-port", type=int, default=config.daemon.port, help="Daemon port")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose logging")
    return parser


def _create_daemon_settings_parser() -> argparse.ArgumentParser:
    """Create parent parser with daemon settings arguments."""
    config = get_config()
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument("--max-projects", type=int, default=config.daemon.max_projects,
                        help="Max projects to cache")
    parser.add_argument("--idle-timeout", type=str,
                        help="Auto-shutdown after idle (e.g., 30m, 1h)")
    parser.add_argument("--no-watch", action="store_true",
                        help="Disable automatic file watching for index updates")
    return parser


def create_daemon_parser() -> argparse.ArgumentParser:
    """Create parser for daemon commands."""
    common_parser = _create_common_daemon_parser()
    settings_parser = _create_daemon_settings_parser()

    parser = argparse.ArgumentParser(
        prog="coden-retriever daemon",
        description="Manage the daemon for fast responses",
        formatter_class=DefaultValueHelpFormatter,
    )

    subparsers = parser.add_subparsers(dest="daemon_action", help="Daemon action")

    # daemon start (inherits common + settings)
    subparsers.add_parser(
        "start",
        parents=[common_parser, settings_parser],
        help="Start daemon in background"
    )

    # daemon stop (inherits common only)
    subparsers.add_parser(
        "stop",
        parents=[common_parser],
        help="Stop the daemon"
    )

    # daemon status (inherits common only)
    subparsers.add_parser(
        "status",
        parents=[common_parser],
        help="Show daemon status"
    )

    # daemon restart (inherits common + settings)
    subparsers.add_parser(
        "restart",
        parents=[common_parser, settings_parser],
        help="Restart the daemon"
    )

    # daemon run (inherits common + settings)
    subparsers.add_parser(
        "run",
        parents=[common_parser, settings_parser],
        help="Run daemon in foreground (for debugging)"
    )

    # daemon clear-cache (inherits common + custom args)
    clear_cache_parser = subparsers.add_parser(
        "clear-cache",
        parents=[common_parser],
        help="Clear daemon cache"
    )
    clear_cache_parser.add_argument("clear_path", nargs="?", help="Path to clear from cache")
    clear_cache_parser.add_argument("--all", dest="clear_all", action="store_true",
                                    help="Clear all cached projects")

    return parser


def run_direct_search(args: argparse.Namespace, root_path: Path, cache: CacheManager, app_config) -> int:
    """Run search directly (fallback when daemon not available)."""
    try:
        # Create search config from CLI args
        config = SearchConfig(
            root_path=root_path,
            query=args.query or "",
            token_limit=args.tokens,
            output_format=OutputFormat(args.format),
            enable_semantic=args.enable_semantic,
            model_path=app_config.search.semantic_model_path,
            show_deps=args.show_deps,
            dir_tree=args.dir_tree,
            map_mode=args.map,
            find_mode=args.find,
            limit=args.limit,
            verbose=args.verbose,
            show_stats=args.stats,
            reverse=args.reverse,
        )

        # Create and execute pipeline (reuse provided cache)
        pipeline = SearchPipeline(config, cache=cache)
        engine = pipeline.create_engine()
        stats = engine.get_stats()

        if args.verbose:
            print(f"\n{stats}\n", file=sys.stderr)

        if stats.total_entities == 0:
            logger.warning("No code entities found")
            return 0

        # Execute pipeline
        result = pipeline.execute()

        # Print output in correct order based on reverse flag
        print_search_output(
            formatted_output=result.formatted_output,
            tree_output=result.tree_output,
            stats_output=result.stats,
            reverse=args.reverse,
        )

        return 0

    except KeyboardInterrupt:
        logger.info("Interrupted")
        return 130
    except Exception as e:
        logger.error(f"Operation failed: {e}")
        if args.verbose:
            traceback.print_exc()
        return 1


def create_serve_parser(config) -> argparse.ArgumentParser:
    """Create parser for 'serve' subcommand."""
    parser = argparse.ArgumentParser(
        prog="coden serve",
        description="Run as MCP server",
        formatter_class=DefaultValueHelpFormatter,
    )
    parser.add_argument("--transport", choices=["stdio", "http", "sse", "streamable-http"],
                        default="stdio", help="Transport protocol")
    parser.add_argument("--host", type=str, default=config.daemon.host,
                        help="Host address (for http/sse transport)")
    parser.add_argument("--port", "-p", type=int, default=8000,
                        help="Port (for http/sse transport)")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose logging")
    return parser


def create_agent_parser(config) -> argparse.ArgumentParser:
    """Create parser for 'agent' subcommand."""
    parser = argparse.ArgumentParser(
        prog="coden agent",
        description="Interactive coding agent with ReAct reasoning",
        formatter_class=DefaultValueHelpFormatter,
    )
    parser.add_argument("root", nargs="?", default=".",
                        help="Repository root directory")
    parser.add_argument("--model", "-m", type=str, default=config.model.default,
                        help="LLM model (ollama:model, openai:model, or model with --base-url)")
    parser.add_argument("--base-url", type=str, default=config.model.base_url,
                        help="Base URL for OpenAI-compatible endpoints")
    parser.add_argument("--max-steps", type=int, default=config.agent.max_steps,
                        help="Max tool calls per query")
    parser.add_argument("--mcp-timeout", type=float, default=config.agent.mcp_server_timeout,
                        help="MCP server startup timeout (seconds)")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose logging")
    return parser


def create_search_parser(config) -> argparse.ArgumentParser:
    """Create parser for search (default) mode."""
    parser = argparse.ArgumentParser(
        prog="coden",
        description="Coden - code search and context generation",
        formatter_class=DefaultValueHelpFormatter,
        epilog="""
Subcommands:
  serve                 Run as MCP server
  agent (-a)            Interactive coding agent
  daemon                Manage daemon (start, stop, status)
  cache                 Manage caches (list, clear, status)
  config                Manage configuration (show, set, reset)

Examples:
  coden                              # Context map of current directory
  coden /path/to/repo -q "auth"      # Search for "auth"
  coden -q "database" -sr --stats    # Semantic search, reversed, with stats
  coden --find UserAuth --show-deps  # Find identifier with dependencies
  coden -H -r --stats -n 20          # Top 20 refactoring hotspots
  coden serve                        # MCP server (stdio)
  coden serve --transport http -p 8000  # MCP server (HTTP)
  coden -a                           # Interactive agent
  coden agent -m ollama:qwen2.5-coder:14b
        """
    )

    parser.add_argument("root", nargs="?", default=".",
                        help="Repository root directory")
    parser.add_argument("-q", "--query", default="",
                        help="Search query")
    parser.add_argument("--map", action="store_true",
                        help="Generate context map (default when no query)")
    parser.add_argument("--find", metavar="IDENT",
                        help="Find specific identifier")
    parser.add_argument("-H", "--hotspots", action="store_true",
                        help="Find refactoring hotspots (high coupling + complexity)")
    parser.add_argument("--tokens", type=int, default=None,
                        help="Token budget (default: unlimited, only -n/--limit controls result count)")
    parser.add_argument("-n", "--limit", type=int, default=config.search.default_limit,
                        help="Max results")
    parser.add_argument("-f", "--format", choices=["xml", "markdown", "tree", "json"],
                        default="tree", help="Output format")
    parser.add_argument("--show-deps", action="store_true",
                        help="Include dependency context")
    parser.add_argument("--dir-tree", action=argparse.BooleanOptionalAction, default=True,
                        help="Show directory tree")
    parser.add_argument("--stats", action="store_true",
                        help="Print ranking statistics")
    parser.add_argument("-r", "--reverse", action="store_true",
                        help="Reverse result order (highest score last)")
    parser.add_argument("-s", "--semantic", dest="enable_semantic", action="store_true",
                        help="Enable semantic search (Model2Vec)")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="Verbose logging")
    return parser


def handle_serve_command(args: argparse.Namespace) -> int:
    """Handle 'serve' subcommand."""
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    from .mcp.server import create_mcp_server
    mcp = create_mcp_server()
    if mcp:
        logger.info(f"Starting MCP server with {args.transport} transport...")
        # Disable banner for stdio transport to avoid corrupting the MCP protocol
        # (fastmcp 2.14.2+ prints banner to stdout which breaks stdio JSON-RPC)
        show_banner = args.transport != "stdio"
        if args.transport in ["http", "sse", "streamable-http"]:
            logger.info(f"Server will be available at: http://{args.host}:{args.port}")
            mcp.run(transport=args.transport, host=args.host, port=args.port, show_banner=show_banner)
        else:
            mcp.run(transport=args.transport, show_banner=show_banner)
        return 0
    return 1


def handle_agent_command(args: argparse.Namespace, config) -> int:
    """Handle 'agent' subcommand."""
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        from .agent import run_interactive
        import asyncio
    except ImportError:
        logger.error("pydantic-ai not installed. Run: pip install coden-retriever[agent]")
        return 1

    root_path = Path(args.root).resolve()
    if not root_path.exists() or not root_path.is_dir():
        logger.error(f"Invalid root path: {root_path}")
        return 1

    # Save settings if user explicitly provided them
    user_provided_model = args.model != config.model.default
    user_provided_base_url = args.base_url != config.model.base_url
    user_provided_mcp_timeout = args.mcp_timeout != config.agent.mcp_server_timeout

    if user_provided_model or user_provided_base_url or user_provided_mcp_timeout:
        if user_provided_model:
            config.model.default = args.model
        if user_provided_base_url:
            config.model.base_url = args.base_url
        if user_provided_mcp_timeout:
            config.agent.mcp_server_timeout = args.mcp_timeout
        save_config(config)

    try:
        asyncio.run(run_interactive(
            str(root_path),
            args.model,
            args.base_url,
            args.max_steps,
            disabled_tools=config.agent.disabled_tools,
        ))
    except KeyboardInterrupt:
        pass
    return 0


def handle_hotspots_command(args: argparse.Namespace, root_path: Path, config) -> int:
    """Handle hotspots mode (-H/--hotspots flag).

    Args:
        args: Parsed CLI arguments
        root_path: Resolved root path
        config: Application configuration

    Returns:
        Exit code (0 for success, 1 for failure)
    """
    import time as time_module

    start_time = time_module.time()

    # Create GraphAnalysisParams for hotspots
    params = GraphAnalysisParams(
        source_dir=str(root_path),
        limit=args.limit,
        exclude_tests=True,
        token_limit=args.tokens if args.tokens else 4000,
        min_coupling_score=10,
        exclude_private=False,
    )

    # Try daemon mode first
    daemon_result = try_daemon_hotspots(params, host=config.daemon.host, port=config.daemon.port)

    if daemon_result is not None:
        hotspots = daemon_result.get("hotspots", [])
        summary = daemon_result.get("summary", {})

        # Format output
        formatted_output = format_hotspots_output(
            hotspots,
            output_format=args.format,
            reverse=args.reverse,
        )

        # Format stats if requested
        stats_output = format_hotspots_stats(summary) if args.stats else None

        # Print output
        print_hotspots_output(formatted_output, stats_output, args.reverse)

        elapsed_ms = (time_module.time() - start_time) * 1000
        if args.verbose:
            print(f"\n[Daemon mode] Hotspots time: {elapsed_ms:.1f}ms, "
                  f"Results: {len(hotspots)}", file=sys.stderr)
        return 0

    # If daemon not available, try direct mode via MCP tool
    logger.warning("Daemon not available, falling back to direct analysis...")
    try:
        import asyncio
        from .mcp.graph_analysis import coupling_hotspots

        result = asyncio.run(coupling_hotspots(
            root_directory=str(root_path),
            limit=args.limit,
            min_coupling_score=10,
            exclude_tests=True,
            exclude_private=False,
            token_limit=args.tokens if args.tokens else 4000,
        ))

        hotspots = result.get("hotspots", [])
        summary = result.get("summary", {})

        formatted_output = format_hotspots_output(
            hotspots,
            output_format=args.format,
            reverse=args.reverse,
        )
        stats_output = format_hotspots_stats(summary) if args.stats else None
        print_hotspots_output(formatted_output, stats_output, args.reverse)

        elapsed_ms = (time_module.time() - start_time) * 1000
        if args.verbose:
            print(f"\n[Direct mode] Hotspots time: {elapsed_ms:.1f}ms, "
                  f"Results: {len(hotspots)}", file=sys.stderr)
        return 0

    except Exception as e:
        logger.error(f"Hotspots analysis failed: {e}")
        if args.verbose:
            traceback.print_exc()
        return 1


def handle_search_command(args: argparse.Namespace, config) -> int:
    """Handle search (default) mode."""
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    root_path = Path(args.root).resolve()
    if not root_path.exists() or not root_path.is_dir():
        logger.error(f"Invalid root path: {args.root}")
        return 1

    # Handle hotspots mode separately
    if args.hotspots:
        return handle_hotspots_command(args, root_path, config)

    # Create cache manager (use config for model_path)
    cache = CacheManager(
        root_path,
        enable_semantic=args.enable_semantic,
        model_path=config.search.semantic_model_path,
        verbose=args.verbose
    )

    # Try daemon mode first (use config for host/port)
    params = SearchParams(
        source_dir=str(root_path),
        query=args.query,
        enable_semantic=args.enable_semantic,
        model_path=config.search.semantic_model_path,
        limit=args.limit,
        tokens=args.tokens,
        show_deps=args.show_deps,
        output_format=args.format,
        find_identifier=args.find,
        map_mode=args.map or not args.query,
        dir_tree=args.dir_tree,
        stats=args.stats,
        reverse=args.reverse,
    )
    daemon_result = try_daemon_search(params, host=config.daemon.host, port=config.daemon.port)

    if daemon_result is not None:
        print_search_output(
            formatted_output=daemon_result.get("output", ""),
            tree_output=None,
            stats_output=daemon_result.get("stats_output") if args.stats else None,
            reverse=args.reverse,
        )

        if args.verbose:
            print(f"\n[Daemon mode] Search time: {daemon_result.get('search_time_ms', 0):.1f}ms, "
                  f"Results: {daemon_result.get('result_count', 0)}/{daemon_result.get('total_matched', 0)}, "
                  f"Tokens: {daemon_result.get('tokens_used', 0)}", file=sys.stderr)
        return 0

    # Direct mode (fallback)
    return run_direct_search(args, root_path, cache, config)


def main() -> int:
    """Main CLI entry point."""
    # Fix Windows console encoding for Unicode output
    if sys.platform == "win32":
        if isinstance(sys.stdout, io.TextIOWrapper):
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        if isinstance(sys.stderr, io.TextIOWrapper):
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')

    # Load configuration
    config = get_config()

    # Ensure config file exists
    config_file = get_config_file()
    if not config_file.exists():
        print(f"Warning: Config file was missing, recreating at {config_file}", file=sys.stderr)
        save_config(config)

    # Route to subcommands
    if len(sys.argv) > 1:
        cmd = sys.argv[1]

        # Shortcuts: -a/--agent -> agent subcommand
        if cmd in ("-a", "--agent"):
            sys.argv[1] = "agent"
            cmd = "agent"

        if cmd == "serve":
            parser = create_serve_parser(config)
            args = parser.parse_args(sys.argv[2:])
            return handle_serve_command(args)

        if cmd == "agent":
            parser = create_agent_parser(config)
            args = parser.parse_args(sys.argv[2:])
            return handle_agent_command(args, config)

        if cmd == "daemon":
            daemon_parser = create_daemon_parser()
            args = daemon_parser.parse_args(sys.argv[2:])
            if not args.daemon_action:
                daemon_parser.print_help()
                return 1
            return handle_daemon_command(args)

        if cmd == "config":
            return handle_config_command(sys.argv[2:])

        if cmd == "cache":
            return handle_cache_command(sys.argv[2:])

    # Default: search mode
    parser = create_search_parser(config)
    args = parser.parse_args()
    return handle_search_command(args, config)


if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%H:%M:%S",
        stream=sys.stderr,
    )
    sys.exit(main())
